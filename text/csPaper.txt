ABSTRACT_BEGIN
  Recent network traffic studies argue that network arrival processes are much
more faithfully modeled using statistically self-similar processes instead of
traditional Poisson processes [LTWW94,PF95]. One difficulty in dealing with
self-similar models is how to efficiently synthesize traces (sample paths)
corresponding to self-similar traffic. We present a fast Fourier transform
method for synthesizing approximate self-similar sample paths for one type of
self-similar process, Fractional Gaussian Noise, and assess its performance and
validity. We find that the method is as fast or faster than existing methods
and appears to generate close approximations to true self-similar sample paths.
We also discuss issues in using such synthesized sample paths for simulating
network traffic, and how an approximation used by our method can dramatically
speed up evaluation of Whittle's estimator for H, the Hurst parameter giving
the strength of long-range dependence present in a self-similar time series.

ABSTRACT_BEGIN
  Multipoint capabilities are essential for ATM networks to efficiently support
many applications, including IP multicasting and overlay applications. The
current signaling and routing specifications for ATM define point-to-multipoint
capabilities. Multipoint-to-point connection support is also being discussed by
the signaling and PNNI groups, and will be defined in the near future for the
unspecified bit rate (UBR) service. We examine point-to-multipoint and
multipoint-to-point flow control for the available bit rate (ABR) service, as
discussed in the traffic management working group.

ABSTRACT_BEGIN
  An explicit rate switch scheme monitors the load at each link and gives
feedback to the sources. We define the overload factor as the ratio of the
input rate to the available capacity. In this paper, we present four overload
based ABR switch schemes which provide MCR guarantees. The switch schemes
proposed use the overload factor and other quantities to calculate feedback
rates. A dynamic queue control mechanism is used to achieve efficient usage of
the link, control queues and, achieve constant queuing delay at steady state.
The proposed algorithms are studied and compared using several configurations.
The configurations were chosen to test the performance of the algorithms in
presence of link bottlenecks, source bottlenecks and transient sources. A
comparison of the proposed algorithms based on the simulation results is
presented.

ABSTRACT_BEGIN
  The main goals of a switch scheme are high utilization, low queuing delay and
fairness. To achieve high utilization the switch scheme can maintain non-zero
(small) queues in steady state which can be used if the sources do not have
data to send. Queue length (delay) can be controlled if part of the link
capacity is used for draining queues in the event of queue build up. In most
schemes a simple threshold function is used for queue control. Better control
of the queue and hence delay can be achieved by using sophisticated queue
control functions. It is very important to design and analyze such queue
control functions. We study step, linear, hyperbolic and inverse hyperbolic
queue control functions. Analytical explanation and simulation results
consistent with analysis are presented. From the study, we conclude that
inverse hyperbolic is the best control function and to reduce complexity the
linear control function can be used since it performs satisfactorily in most
cases.

ABSTRACT_BEGIN
  In this paper we give a general definition of weighted fairness and show how
this can achieve various fairness definitions, such as those mentioned in the
ATM Forum TM 4.0 Specifications. We discuss how a pricing policy can be mapped
to general weighted (GW) fairness. The GW fairness can be achieved by
calculating the $ExcessFairshare$ (weighted fairshare of the left over
bandwidth) for each VC. We show how a switch algorithm can be modified to
support the GW fairness by using the $ExcessFairshare$. We use ERICA+ as an
example switch algorithm and show how it can be modified to achieve the general
fairness. Simulations results are presented to demonstrate that the modified
switch algorithm achieves GW fairness. An analytical proof for convergence of
the modified ERICA+ algorithm is given in the appendix.

ABSTRACT_BEGIN
  ATM (asynchronous transfer mode) is the technology chosen for the Broadband
Integrated Services Digital Network (B-ISDN). The ATM ABR (available bit rate)
service can be used to transport ``best-effort'' traffic. In this paper, we
extend our earlier work on the buffer requirements problem for TCP over ABR.
Here, a worst case scenario is generated such that TCP sources send a burst of
data at the time when the sources have large congestion windows and the ACRs
(allowed cell rates) for ABR are high. We find that ABR using the ERICA+ switch
algorithm can control the maximum queue lengths (hence the buffer requirements)
even for the worst case. We present analytical arguments for the expected queue
length and simulation results for different number of sources values and
parameter values.

ABSTRACT_BEGIN
  Compressed video is well known to be self-similar in nature. We model VBR
carrying Long-Range Dependent (LRD), multiplexed MPEG-2 video sources. The
actual traffic for the model is generated using fast-fourier transform of
generate the fractional gaussian noise (FGN) sequence. Our model of compressed
video sources bears similarity to an MPEG-2 Transport Stream carrying video,
i.e., it is long-range dependent and generates traffic in a piecewise-CBR
fashion. We study the effect of such VBR traffic on ABR carrying TCP traffic.
The effect of such VBR traffic is that the ABR capacity is highly variant. We
find that a switch algorithm like ERICA+ can tolerate this variance in ABR
capacity while maintaining high throughput and low delay. We present simulation
results for terrestrial and satellite configurations.

ABSTRACT_BEGIN
  In multipoint-to-point connections, the traffic at the root (destination) is
the combination of all traffic originating at the leaves. A crucial concern in
the case of multiple senders is how to define fairness within a multicast group
and among groups and point-to-point connections. Fairness definition can be
complicated since the multipoint connection can have the same identifier
(VPI/VCI) on each link, and senders might not be distinguishable in this case.
Many rate allocation algorithms implicitly assume that there is only one sender
in each VC, which does not hold for multipoint-to-point cases. We give various
possibilities for defining fairness for multipoint connections, and show the
tradeoffs involved. In addition, we show that ATM bandwidth allocation
algorithms need to be adapted to give fair allocations for multipoint-to-point
connections.

ABSTRACT_BEGIN
  Asynchronous transfer mode (ATM) is the new generation of computer and
communication networks that are being deployed throughout the telecommunication
industry as well as in campus backbones. ATM technology distinguishes itself
from the previous networking protocols in that it has the latest traffic
management technology and thus allows guaranteeing delay, throughput, and other
performance measures. This in turn, allows users to integrate voice, video, and
data on the same network. Available bit rate (ABR) service in ATM has been
designed to fairly distribute all unused capacity to data traffic and is
specified in the ATM Forum's Traffic Management (TM4.0) standard. This paper
will describe the OPNET models that have been developed for ATM and ABR design
and analysis.

ABSTRACT_BEGIN
  In this paper we describe a hands-on laboratory oriented instructional
package that we have developed for data communications and networking. The
package consists of a software tool, together with instructional material for a
laboratory based networking curriculum. The software is based on a simulation
environment that enables the student to experiment with various networking
protocols, on an easy to use graphical user interface (GUI). Data message
flows, packet losses, control/routing message flows, virtual circuit setups,
link failures, bit errors etc., are some of the features that can be visualized
in this environment. The student can also modify the networking components
provided, as well as add new components using the C programming language. The
instructional material consists of a set of laboratory exercises for flow and
error control (HDLC), IEEE 802.3 CSMA/CD protocol, the token ring protocol,
interconnecting LANs via bridges, TCP congestion avoidance and control, IP
fragmentation and reassembly, ATM PNNI routing and ATM policing. The laboratory
exercises have facilitated the development of a networking curriculum based on
both the traditional computer networking principles, as well as the new
technologies in telecommunication networking. The laboratory environment has
been used in the networking curriculum at The Ohio State University, and is
being piloted at other universities. The entire package is freely available
over the Internet.

ABSTRACT_BEGIN
  In this paper we present a model to study the end-to-end delay performance of
a satellite-ATM netowrk. We describe a satellite-ATM network architecture. The
architecture presents a trade-off between the on-board switching/processing
features and the complexity of the satellite communication systems. The
end-to-end delay of a connection passing through a satellite constellation
consists of the transmission delay, the uplink and downlink ground
terminal-satellite propagation delay, the inter-satellite link delays, the
on-board switching, processing and buffering delays. In a broadband satellite
network, the propagation and the buffering delays have the most impact on the
overall delay. We present an analysis of the propagation and buffering delay
components for GEO and LEO systems. We model LEO constellations as satellites
evenly spaced in circular orbits around the earth. A simple routing algorithm
for LEO systems calculates locally optimal paths for the end-to-end connection.
This is used to calculate the end-to-end propagation delays for LEO networks.
We present a simulation model to calculate the buffering delay for TCP/IP
traffic over ATM ABR and UBR service categories. We apply this model to
calculate total end-to-end delays for TCP/IP over satellite-ATM networks.

ABSTRACT_BEGIN
  In this paper we study the design issues in improving TCP performance over
the ATM UBR service. ATM-UBR switches respond to congestion by dropping cells
when their buffers become full. TCP connections running over UBR can experience
low throughput and high unfairness. Intelligent switch drop policies and
end-system policies can improve the performance of TCP over UBR with limited
buffers. We describe the various design options available to the network as
well as to the end systems to improve TCP performance over UBR. We study the
effects of Early Packet Discard, and two per-VC accounting based buffer
management policies. We also study the effects of various TCP end system
congestion control policies including slow start and congestion avoidance, fast
retransmit and recovery and selective acknowledgments. We present simulation
results for various small and large latency configurations with varying buffer
sizes and number of sources.

ABSTRACT_BEGIN
  Recent enhancements have been proposed to the ATM Unspecified Bit Rate (UBR)
service that guarantee a minimum rate at the frame level to the UBR VCs. These
enhancements have been called Guaranteed Frame Rate (GFR). In this paper, we
discuss the motivation, design and implementation issues for GFR. We present
the design of buffer management and policing mechanisms to implement GFR. We
study the effects of policing, per-VC buffer allocation, and per-VC queuing on
providing GFR to TCP/IP traffic. We conclude that per-VC scheduling is
necessary to provide minimum rate guarantees to TCP traffic. We examine the
role of frame tagging in the presence of scheduling and buffer management for
providing minumum rate guarantees. The use of GFR to support the Internet
Controlled Load Service is also discussed.

ABSTRACT_BEGIN
  The ATM Guaranteed Frame Rate (GFR) service is intended for best effort
traffic that can benefit from minimum throughput guarantees. Edge devices
connecting LANs to an ATM network can use GFR to transport multiple TCP/IP
connections over a single GFR VC.These devices would typically multiplex VCs
into a single FIFO queue. It has been shown that in general, FIFO queuing is
not sufficient to provide rate guarantees, and per-VC queuing with scheduling
is needed. We show that under conditions of low buffer allocation, it is
possible to control TCP rates with FIFO queuing and buffer management. We
present analysis and simulation results on controlling TCP rates by buffer
management. We present a buffer management policy that provides loose rate
guarantees to SACK TCP sources when the total buffer allocation is low. We
study the performance of this buffer management scheme by simulation.

ABSTRACT_BEGIN
  In performance analysis and design of communication netword modeling data
traffic is important. With introduction of new applications, the
characteristics of the data traffic changes. We present a brief review the
different models of data traffic and how they have evolved. We present results
of data traffic analysis and simulated traffic, which demonstrates that the
packet train model fits the traffic at source destination level and long-memory
(self-similar) model fits the traffic at the aggregate level.

ABSTRACT_BEGIN
  The ABR service is designed to fairly allocate the bandwidth unused by higher
priority services. The network indicates to the ABR sources the rates at which
they should transmit to minimize their cell loss. Switches must constantly
measure the demand and available capacity, and divide the capacity fairly among
the contending connections. In order to compute the fair and efficient
allocation for each connection, a switch needs to determine the effective
number of active connections. In this paper, we propose a method for
determining the number of active connections and the fair bandwidth share for
each. We prove the efficiency and fairness of the proposed method analytically,
and simulate it by incorporating it into the ERICA switch algorithm.

ABSTRACT_BEGIN
  The OSU scheme is a rate-based congestion avoidance scheme for ATM networks
using explicit rate indication. This work was one of the first attempts to
define explicit rate switch mechanisms and the Resource Management (RM) cell
format in Asynchronous Transfer Mode (ATM) networks. The key features of the
scheme include explicit rate feedback, congestion avoidance, fair operation
while maintaining high utilization, use of input rate as a congestion metric,
O(1) complexity. This paper presents an overview of the scheme, presents those
features of the scheme that have now become common features of other switch
algorithms and discusses three extensions of the scheme.

ABSTRACT_BEGIN
  We propose an explicit rate indication scheme for congestion avoidance in ATM
networks. In this scheme, the network switches monitor their load on each link,
determining a load factor, the available capacity, and the number of currently
active virtual channels. This information is used to advise the sources about
the rates at which they should transmit. The algorithm is designed to achieve
efficiency, fairness, controlled queueing delays, and fast transient response.
The algorithm is also robust to measurement errors caused due to variation in
ABR demand and capacity. We present performance analysis of the scheme using
both analytical arguments and simulation results. The scheme is being
implemented by several ATM switch manufacturers.

ABSTRACT_BEGIN
  In this paper, we have provided a summary of the design options in
Satellite-ATM technology. A satellite ATM network consists of a space segment
of satellites connected by inter-satellite crosslinks, and a ground segment of
the various ATM networks. A satellite-ATM interface module connects the
satellite network to the ATM networks and performs various call and control
functions. A network control center performs various network management and
resource allocation functions. Several issues such as the ATM service model,
media access protocols, and traffic management issues must be considered when
designing a satellite ATM network to effectively transport Internet traffic. We
have presented the buffer requirements for TCP/IP traffic over ATM-UBR for
satellite latencies. Our results are based on TCP with selective
acknowledgments and a per-VC buffer management policy at the switches. A buffer
size of about 0.5 * RTT to 1 * RTT is sufficient to provide over 98% throughput
to infinite TCP traffic for long latency networks and a large number of
sources. This buffer requirement is independent of the number of sources. The
fairness is high for a large numbers of sources because of the per-VC buffer
management performed at the switches and the nature of TCP traffic.

ABSTRACT_BEGIN
  We model World Wide Web (WWW) servers and clients running over an ATM network
using the ABR (available bit rate) service. The WWW servers are modeled using a
variant of the SPECweb96 benchmark, while the WWW clients are based on a model
by Mah. The traffic generated by this application is typically bursty, i.e., it
has active and idle periods in transmission. A timeout occurs after given
amount of idle period. During idle period the underlying TCP congestion windows
remain open until a timeout expires. These open windows may be used to send
data in a burst when the application becomes active again. This raises the
possibility of large switch queues if the source rates are not controlled by
ABR. We study this problem and show that ABR scales well with a large number of
bursty TCP sources in the system.

ABSTRACT_BEGIN
  ABR traffic management for point-to-multipoint connections controls the
source rate to the minimum rate supported by all the branches of the multicast
tree. A number of algorithms have been developed for extending ABR congestion
avoidance algorithms to perform feedback consolidation at the branch points.
This paper discusses various design options and implementation alternatives for
the consolidation algorithms, and proposes a number of new algorithms. The
performance of the proposed algorithms and the previous algorithms is compared
under a variety of conditions. Results indicate that the algorithms we propose
eliminate the consolidation noise (caused if the feedback is returned before
all branches respond), while exhibiting a fast transient response.

ABSTRACT_BEGIN
  We study the performance of Selective Acknowledgments with TCP over the
ATM-UBR service category. We examine various UBR drop policies, TCP mechanisms
and network configurations to recommend optimal parameters for TCP over UBR. We
discuss various TCP congestion control mechanisms compare their performance for
LAN and WAN networks. We describe the effect of satellite delays on TCP
performance over UBR and present simulation results for LAN, WAN and satellite
networks. SACK TCP improves the performance of TCP over UBR, especially for
large delay networks. Intelligent drop policies at the switches are an
important factor for good performance in local area networks.

ABSTRACT_BEGIN
  Asynchronous transfer mode (ATM) networks must define multicast capabilities
in order to efficiently support numerous applications, such as video
conferencing and distributed applications, in addition to LAN emulation (LANE)
and Internet protocol (IP) multicasting. Several problems and issues arise in
ATM multicasting, such as signaling, routing, connection admission control, and
traffic management problems. IP integrated services over ATM poses further
challenges to ATM multicasting. Scalability and simplicity are the two main
concerns for ATM multicasting. This paper provides a survey of the current work
on multicasting problems in general, and ATM multicasting in particular. A
number of proposed schemes is examined, such as the schemes MARS, MCS, SEAM,
SMART, RSVP, and various multipoint traffic management and transport-layer
schemes. The paper also indicates a number of key open issues that remain
unresolved.

ABSTRACT_BEGIN
  The testing group at ATM Forum is working on developing a specification for
performance testing of ATM switches and networks. The emphasis is on the user
perceived frame-level performance. This paper explains what is different about
this new effort and gives its status.

ABSTRACT_BEGIN
  The Available Bit Rate (ABR) service in ATM networks has been specified to
allow fair and efficient support of data applications over ATM utilizing
capacity left over after servicing higher priority classes. One of the
architectural features in the ABR specification [tm4] is the Virtual
Source/Virtual Destination (VS/VD) option. This option allows a switch to
divide an end-to-end ABR connection into separately controlled ABR segments by
acting like a destination on one segment, and like a source on the other. The
coupling in the VS/VD switch between the two ABR control segments is
implementation specific. In this paper, we model a VS/VD ATM switch and study
the issues in designing coupling between ABR segments. We identify a number of
implementation options for the coupling. A good choice significantly improves
the stability and transient performance of the system and reduces the buffer
requirements at the switches.

ABSTRACT_BEGIN
  The Available Bit Rate (ABR) service has been developed to support 21st
century data applications over Asynchronous Transfer Mode (ATM). The ABR
service uses a closed-loop rate-based traffic management framework where the
network divides left-over bandwidth among contending sources. The ATM Forum
traffic management group also incorporated open-loop control capabilities to
make the ABR service robust to temporary network failures and source
inactivity. An important problem addressed was whether rate allocations of
sources should be taken away if sources do not use them. The proposed
solutions, popularly known as the Use-It-or-Lose-It (UILI) policies, have had
significant impact on the ABR service capabilities. In this paper we discuss
the design, development, and the final shape of these policies and their impact
on the ABR service. We compare the various alternatives through a performance
evaluation.

ABSTRACT_BEGIN
  ATM-UBR switches respond to congestion by dropping cells when their buffers
become full. TCP connections running over UBR experience low throughput and
high unfairness. For 100% TCP throughput each switch needs buffers equal to the
sum of the window sizes of all the TCP connections. Intelligent drop policies
can improve the performance of TCP over UBR with limited buffers. The UBR+
service proposes enhancements to UBR for intelligent drop. Early Packet Discard
improves throughput but does not attempt to improve fairness. Selective packet
drop based on per-connection buffer occupancy improves fairness. The Fair
Buffer Allocation scheme further improves both throughput and fairness.

ABSTRACT_BEGIN
  We extend our earlier studies of buffer requirements of TCP over ABR in two
directions. First, we study the performance of TCP over ABR in an ATM backbone.
On the backbone, the TCP queues are at the edge router and not inside the ATM
network. The router requires buffer equal to the sum of the receiver window
sizes of the participating TCP connections. Second, we introduce various
patterns of VBR background traffic. The VBR background introduces variance in
the ABR capacity and the TCP traffic introduces variance in the ABR demand.
Some simple switch schemes are unable to keep up with the combined effect of
highly varying demands and highly varying ABR capacity. We present our
experiences with refining the ERICA+ switch scheme to handle these conditions.

ABSTRACT_BEGIN
  The Asynchronous Transfer Mode (ATM) networks are quickly being adopted as
backbones over various parts of the Internet. This paper analyzes the
performance of TCP/IP protocols over ATM network's Available Bit Rate (ABR) and
Unspecified Bit Rate (UBR) services. It is shown that ABR pushes congestion to
the edges of the ATM network while UBR leaves it inside the ATM portion.

ABSTRACT_BEGIN
  We study the buffering requirements for zero cell loss for TCP/IP over
satellite links using the available bit rate (ABR) and unspecified bit rate
(UBR) services of asynchronous transfer mode (ATM) networks. For the ABR
service, we explore the effect of feedback delay (a factor which depends upon
the position of the bottleneck), the switch scheme used, and background
variable bit rate (VBR) traffic. It is shown that the buffer requirement for
TCP over ABR is independent of the number of TCP sources, but depends on the
aforementioned factors. For the UBR service, we show that the buffer
requirement is the sum of the TCP receiver window sizes. We substantiate our
arguments with simulation results.

ABSTRACT_BEGIN
  During the design of ABR traffic management at the ATM Forum, we performed
several analyses to ensure that the ABR service will operate efficiently over
satellite links. In the cases where the performance was unacceptable, we
suggested modifications to the traffic management specifications. This paper
describes one such issue related to the count of missing resource management
cells (Crm) parameter of the ABR source behavior. The analysis presented here
led to the changes which are now part of the ATM traffic management (TM 4.0)
specification. In particular, the size of the transient buffer exposure (TBE)
parameter was set to 24 bits, and no size was enforced for the Crm parameter.
This simple change improved the throughput over OC-3 satellite links from 45
Mbps to 140 Mbps.

ABSTRACT_BEGIN
  Satellite communication systems are the means of realizing a global broadband
integrated services digital network. Due to the statistical nature of the
integrated services traffic, the resulting rate fluctuations and burstiness
render congestion control a complicated, yet indispensable function. The long
propagation delay of the earth-satellite link further imposes severe demands
and constraints on the congestion control schemes, as well as the media access
control techniques and retransmission protocols that can be employed in a
satellite network. The problems in designing satellite network protocols, as
well as some of the solutions proposed to tackle these problems, will be the
primary focus of this survey.

ABSTRACT_BEGIN
  The Available Bit Rate (ABR) service has been developed to support data
applications over Asynchronous Transfer Mode (ATM) networks. The network
continuously monitors its traffic and provides feedback to the source end
systems. This paper explains the rules that the sources have to follow to
achieve a fair and efficient allocation of network resources.

ABSTRACT_BEGIN
  We study the buffering requirements for zero cell loss for TCP over ABR. We
show that the maximum buffers required at the switch is proportional to the
maximum round trip time (RTT) of all VCs through the link. The number of
round-trips depends upon the the switch algorithm used. With our ERICA
[erica-final] switch algorithm, we find that the buffering required is
independent of the number of TCP sources. We substantiate our arguments with
simulation results.

ABSTRACT_BEGIN
  The main goal of this study was to survey current applications of GPS to
distributed systems and networks. Detailed lists of GPS products, current
applications, addresses of manufacturers, and sources for further information
are included in this report.

ABSTRACT_BEGIN
  An explicit rate indication scheme for congestion avoidance in computer and
telecommunication networks is proposed. The sources monitor their load and
provide the information periodically to the switches. The switches, in turn,
compute the load level and ask the sources to adjust their rates up or down.
The scheme achieves high link utilization, fair allocation of rates among
contending sources and provides quick convergence. A backward congestion
notification option is also provided. The conditions under which this option is
useful are indicated.

ABSTRACT_BEGIN
  As the speed and the dynamic range of computer networks evolve, the issue of
efficient traffic management becomes increasingly important. This work
describes an approach to traffic management using explicit rate information
provided to the source by the network. We present an asynchronous distributed
algorithm for optimal rate calculation across the network, where optimality is
understood in the maxmin sense. The algorithm quickly converges to the optimal
rates and is shown to be well-behaved in transience.

ABSTRACT_BEGIN
  Asynchronous Transfer Mode (ATM) has emerged as the most promising technology
in supporting future broadband multimedia communication services. To accelerate
the deployment of ATM technology, the ATM Forum, which is a consortium of
service providers and equipment vendors in the communication industries has
been created to develop implementation and specification agreements. In this
article, we present a brief overview on ATM protocol layers and current
progress on LAN Emulation and Traffic Management in the ATM Forum.

ABSTRACT_BEGIN
  The paper begins with a discussion of current trends in networking and a
historical reviews of past networking technologies some of which failed. This
leads us to the discussion about what it takes for a new technology to succeed
and what challenges we face in making the current dream of a seamless
world-wide high-speed ATM network a reality.
  Issues in using ATM cells for very high speed applications are presented.
Ensuring that the users benefit from ATM networks involves several other
related disciplines. These are reviewed.

ABSTRACT_BEGIN
  Congestion control mechanisms for ATM networks as selected by the ATM Forum
traffic management group are described. Reasons behind these selections are
explained. In particular, selection criteria for selection between rate-based
and credit-based approach and the key points of the debate between the two
approaches are presented. The approach that was finally selected and several
other schemes that were considered are described.

ABSTRACT_BEGIN
  Key issues in upcoming FDDI standards including low-cost fiber, twisted-pair,
SONET mapping, and FDDI follow-on LAN are discussed after a brief introduction
to FDDI and FDDI-II

ABSTRACT_BEGIN
  Using a trace of address references, we compared the efficiency of several
different hashing functions, such as cyclic redundancy checking (CRC)
polynomials, Fletcher checksum, folding of address octets using the
exclusive-or operation and bit extraction from the address. Guidelines are
provided for determining the size of the hashmark required to achieve a
specified level of performance.

ABSTRACT_BEGIN
  Weaknesses in several recently proposed ideas about congestion control and
avoidance in high-speed netwroks are identified. Both sides of the debate
concerning prior reservation of resources versus walk-in service, open-loop
control versus feedback control, rate control versus window control, and
router-based control versus source-based control are presented. The
circumstances under which backpressure is useful or not are discussed, and it
is argued that a single congestion scheme is not sufficient, but that a
combination of several schemes is required for complete congestion management
in a network.

ABSTRACT_BEGIN
  The performance of Fiber-Distributed Data Interface (FDDI) depends upon
several workload parameters; for example; the arrival pattern, frame size, and
configuration parameters, such as the number of stations on the ring, extent of
the ring, and number of stations that are waiting to transmit. In addition, the
performance is affected by a parameter called the Target Token Rotation Time
(TTRT), which can be controlled by the network manager. We considered the
effect of TTRT on various performance metrics for different ring configurations
and concluded that a TTRT value of 8 ms provides a good performance over a wide
range of configurations and workloads.

ABSTRACT_BEGIN
  Fiber Distributed Data Interface (FDDI) is a 100 megabits per second fiber
optic local area network (LAN) standard being developed by the American
National Standard Institute (ANSI).
  We analyze the impact of various design decisions on the error detection
capability of the protocol. In particular, we quantify frame error rate, token
loss rate, and undetected error rate. Several characteristics of the 32-bit
frame check sequence (FCS) polynomial, which is also used in IEEE 802 LAN
protocols, are discussed.

ABSTRACT_BEGIN
  Popular myths that cheaper memory, high-speed links and high-speed processors
will solve the problem of congestion in computer networks are shown to be
false. A simple definition for congestion based on supply and demand of
resources is proposed and is then used to classify various congestion schemes.
The issues that make the congestion problem a difficult one are discussed, and
then the architectural decisions that affect the design of a congestion scheme
are presented. It is argued that long-, medium- and short-term congestion
problems require different solutions. Some of the recent schemes are brifly
surveyed, and areas for further research are discussed.

ABSTRACT_BEGIN
  The size of computer networks, along with their bandwidths, is growing
exponentially. To support these large, high-speed networks, it is neccessary to
be able to forward packets in a few microseconds. One part of the forwarding
operation consists of searching through a large address databse. This problem
is encountered in the design of bridges, routers, gateways and name servers.
  Caching can reduce the lookup time if there is a locality in the address
reference pattern. Using a destination reference trace measured on an extended
local are a network, we attempt to see if the destination refernces do have a
significant locality.
  We compared the performance of MIN, LRU, FIFO, and random cache replacement
algorithms. We found that the interactive (terminal) traffic in our sample had
quite different locality behavior than that of the noninteractive traffic. The
interactive traffic did not follow the LRU stack model while the
noninteractivetraffic did. Examples are shown of the environments in which
caching can help as well as those in which caching can hurt, unless the cache
size is large.

ABSTRACT_BEGIN
  In heterogeneous networks, achieving congestion avoidance is difficult
because the congestion feedback from one subnetwork may have no meaning to
source on other other subnetworks. We propose using changes in round-trip delay
as an implicit feedback. Using a black-box model of the network, we derive an
expression for the optimal window as a function of the gradient of the
delay-window curve.
  The problems of selfish optimum and social optimum are also addressed. It is
shown that without a careful design, it is possible to get into a race
condition during heavy congestion, where each user wants more resources than
others, thereby leading to a diverging congestion
  It is shown that congestion avoidance using round trip delay is a promising
approach. The aproach has the advantage that there is absolutely no overhead
for the network itself. It is exemplified by a simple scheme. The performance
of the scheme is analyzed using a simulation model. The scheme is shown to be
efficient, fair, convergent and adaptive to changes in network configuration.
  The scheme as described works only for networks that can ne modelled with
queueing servers with constant service times. Further research is required to
extend it for implementation in practical networks. Several directions for
future research have beensuggested.

ABSTRACT_BEGIN
  Widespread use of computer networks and the use of varied technology for the
interconnection of computers has made congestion a significant problem.
  In this report, we summarize our research on congestion avoidance. We compare
the concept of congestion avoidance with that of congestion control.
  Briefly, congestion control is a recovery mechanism, while congestion
avoidance is a prevention mechanism. A congestion control scheme helps the
network to recover from the congestion state while a congestion avoidance
scheme allows a network to operate in the region of low delay and high
throughput with minimal queuing, thereby preventing it from entering the
congested state in which packets are lost due to buffer shortage.
  A number of possible alternatives for congestion avoidance were identified.
  From these alternatives we selected one called the binary feedback scheme in
which the network uses a single bit in the network layer header to feed back
the congestion information to its users, which then increase or decrease their
load to make optimal use of the resources. The concept of global optimality in
a distributed system is defined in terms of efficiency and fairness such that
they can be independently quantified and apply to any number of resources and
users.
  The proposed scheme has been simulated and shown to be globally efficient,
fair, responsive, convergent, robust, distributed, and
configuration-independent.

ABSTRACT_BEGIN
  Congestion is said to occur in the network when the resource demands exceed
the capacity and packets are lost due to too much queuing in the network.
During congestion, the network throughput may drop to zero and the path delay
may become very high. A congestion control scheme helps the network to recover
from the congestion state. A congestion avoidance scheme allows a network to
operate in the region of low delay and high throughput. Such schemes prevent a
network from entering the congested state. Congestion avoidance is a prevention
mechanism while congestion control is a recovery mechanism. We compare the
concept of congestion avoidance with that of flow control and congestion
control. A number of possible alternative for congestion avoidance have been
identified. From these a few were selected for study. The criteria for
selection and goals for these schemes have been described. In particular, we
wanted the scheme to be globally efficient, fair, dynamic, convergent, robust,
distributed, configuration independent, etc. These goals and the test cases
used to verify whether a particular scheme has met the goals have been
described. We model the network and the user policies for congestion avoidance
as a feedback control system. The key components of a generic congestion
avoidance scheme are: congestion detection, congestion feedback, feedback
selector, signal filter, decision function, and increase/decrease algorithms.
These components have been explained.

ABSTRACT_BEGIN
  During overload, most networks drop packets due to buffer unavailability. The
resulting timeouts at the source provide an implicit mechanism to convey
congestion signals from the network to the source. On a timeout, a source
should not only retransmit the lost packet, but it should also reduce its load
on the network. Based on this realization, we have developed a simple
congestion control scheme using the acknowledgment timeouts as indications of
packet loss and congestion. This scheme does not require any new message
formats, therefore, it can be used in any network with window flow control,
e.g., ARPAnet or ISO.

ABSTRACT_BEGIN
  The problem of adaptively setting the timeout interval for retransmitting a
packet has been discussed. A layered view of the algorithms has been presented.
It is shown that a timeout algorithm consists of essentially five layers or
procedures which can be independently chosen and modified. A number of timeout
algorithms proposed in the literature have been decomposed into these five
layers.
  One of the key layers not discussed in the literature is that of determining
the sample round trip delay for packets that have been transmitted more than
once. It is shown that this layer has a significant impact on the network
performance.
  Under repeated packet loss, most timeout algorithms either diverge or
converge to a wrong value. A number of alternative schemes have been presented.
It is argued that divergence is preferable to false convergence. It is a
feature that is helpful in reducing network traffic congestion.

ABSTRACT_BEGIN
  In window flow controlled networks, if a packet is lost the destination has
to decide whether to save (cache) subsequent out-of-order packets. Also, the
source has to decide whether to send just one packet or to send all packets
following it. This leads to four different types of caching schemes.
Simulations show, against our immediate intuition, that regardless of whether
the destination is caching or not, the source should retransmit only one
packet. This paper describes the alternatives to, and provides justification
for, schemes used in Digital Network Architecture and ARPAnet TCP.

ABSTRACT_BEGIN
  Fairness is an important performance criterion in all resource allocation
schemes, including those in distributed computer systems. However, it is often
specified only qualitatively. The quantitative measures proposed in the
literature are either too specific to a particular application, or suffer from
some undesirable characteristics. In this paper, we have introduced a
quantitative measure called Indiex of FRairness. The index is applicable to any
resource sharing or allocation problem. It is independent of the amount of the
resource. The fairness index always lies between 0 and 1. This boundedness aids
intuitive understanding of the fairness index. For example, a distribution
algorithm with a fairness of 0.10 means that it is unfair to 90% of the users.
Also, the discrimination index can be defined as 1 - fairness index.

ABSTRACT_BEGIN
  Explanation of ABR service in plain language.

ABSTRACT_BEGIN
  In this paper, a new routing algorithm based on a flooding method is
introduced. Flooding techniques have been used previously, e.g. for
broadcasting the routing table in the ARPAnet [1] and other special purpose
networks [3][4][5]. However, sending data using flooding can often saturate the
network [2] and it is usually regarded as an inefficient broadcast mechanism.
Our approach is to flood a very short packet to explore an optimal route
without relying on a pre-established routing table, and an efficient flood
control algorithm to reduce the signalling traffic overhead. This is an
inherently robust mechanism in the face of a network configuration change,
achieves automatic load sharing across alternative routes and has potential to
solve many contemporary routing problems. An earlier version of this mechanism
was originally developed for virtual circuit establishment in the experimental
Caroline ATM LAN [6][7] at Monash University.

ABSTRACT_BEGIN
  In this paper, three new dynamic multicast routing algorithms based on the
greedy tree technique are proposed; Source Optimised Tree, Topology Based Tree
and Minimum Diameter Tree. A simulation analysis is presented showing various
performance aspects of the algorithms, in which a comparison is made with the
greedy and core based tree techniques. The effects of the tree source location
on dynamic membership change are also examined. The simulations demonstrate
that the Source Optimised Tree algorithm achieves a significant improvement in
terms of delay and link usage when compared to the Core Based Tree, and greedy
algorithm.

ABSTRACT_BEGIN
  The objective is to design and build a small, high-bandwidth switch.

ABSTRACT_BEGIN
  In this paper, we show how Web technologies can be used effectively to (i)
address some of the deficiencies of traditional IP network management
platforms, and (ii) render these expensive platforms redundant. We build on the
concept of embedded management application, proposed by Wellens and Auerbach,
and present two models of network management application designs that rely on
Web technologies. First, the pull model is based on the request/response
paradigm. It is typically used to perform data polling. Several commercial
management platforms already use Web technologies that rely on this model to
provide for ad hoc management; we demonstrate how to extend this to regular
management. Second, the push model is a novel approach which relies on the
publish/subscribe/distribute paradigm. It is better suited to regular
management than the pull model, and allows administrators to conserve network
bandwidth as well as CPU time on the management station. It can be seen as a
generalization of the paradigm commonly used for notification delivery.
Finally, we introduce the concept of the collapsed network management platform,
where these two models coexist.

ABSTRACT_BEGIN
  A TCP trunk is an IP tunnel under TCP control, capable of carrying packets
from any number of user flows. By exploiting properties of TCP, a TCP trunk
provides elastic and reliable transmission over a network, and automatically
shares the network fairly with other competing trunks. Moreover, by aggregating
user flows into a single trunk flow, TCP trunking can significantly reduce the
number of flows that the network needs to manage, thereby allowing use of
simplified management to achieve improved perfor mance. For example, when
dealing with only a small number of TCP trunk flows, a router with a simple
FIFO buffer can experience low packet loss rates.
  A TCP trunk is a "soft" circuit in the sense that it requires no flow states
to be maintained inside the network. Setting up a TCP trunk involves only
configuring the two end nodes. This is in contrast with traditional methods of
configuring circuits via signaling of network nodes.
  A simple packet-dropping mechanism based on packet accounting at the
transmitter of a TCP trunk assures that, when the trunk reduces its bandwidth
in response to network congestion, user TCP flows carried by the trunk will
reduce their bandwidths by the same proportion. Simu lation results have
demonstrated that TCP trunks can provide improved network performance to users,
while achieving high network utilization.

ABSTRACT_BEGIN
  The Internet has revolutionized the computer and communications world like
nothing before. The invention of the telegraph, telephone, radio, and computer
set the stage for this unprecedented integration of capabilities. The Internet
is at once a world-wide broadcasting capability, a mechanism for information
dissemination, and a medium for collaboration and interaction between
individuals and their computers without regard for geographic location.
  In this paper, several of us involved in the development and evolution of the
Internet share our views of its origins and history. This is intended to be a
brief, necessarily cursory and incomplete history. This history revolves around
four distinct aspects. There is the technological evolution that began with
early research on packet switching and the ARPANET (and related technologies),
and where current research continues to expand the horizons of the
infrastructure along several dimensions, such as scale, performance, and higher
level functionality. There is the operations and management aspect of a global
and complex operational infrastructure. There is the social aspect, which
resulted in a broad community of Internauts working together to create and
evolve the technology. And there is the commercialization aspect, resulting in
an extremely effective transition of research results into a broadly deployed
and available information infrastructure.

ABSTRACT_BEGIN
  The Cross-Industry Working Team (XIWT), with the support of the Stanford
University Consortium for Research on Information Security and Policy (CRISP),
sponsored a symposium on cross-industry activities aimed at improving the
reliability, dependability, and robustness of the information infrastructure.
Held 3-4 November 1998 in Crystal City, Virginia, the symposium engaged
representatives from industry, academia, and government in discussion of
current and potential cross-industry, cross-sector activities including
information exchange, collaborative operations, and cooperative research and
development. This proceedings summarizes the discussions and results of the
meeting.

ABSTRACT_BEGIN
  This paper introduces a novel algorithm, the Active Virtual Network
Management Protocol (AVNMP), for predictive network management. It explains how
the AVNMP facilitates the management of an active network by allowing future
predicted state information within an active network to be available to network
management algorithms. This is accomplished by coupling ideas from optimistic
discrete event simulation with active networking. The optimistic discrete event
simulation method used is a form of self-adjusting Time Warp. It is
self-adjusting because the system adjusts for predictions which are inaccurate
beyond a given tolerance. The concept of a streptichron and autoanaplasis are
introduced as mechanisms which take advantage of the enhanced flexibility and
intelligence of active packets. Finally, it is demonstrated that the AVNMP is a
feasible concept.

ABSTRACT_BEGIN
  There is a trend toward the use of predictive systems in communications
networks. At the systems and network management level predictive capabilities
are focused on anticipating network faults and performance degradation.
Simultaneously, mobile communication networks are being developed with
predictive location and tracking mechanisms. The interactions and synergies
between these systems present a new set of problems. A new predictive network
management framework is developed and examined. The interaction between a
predictive mobile network and the proposed network management system is
discussed. The Rapidly Deployable Radio Network is used as a specific example
to illustrate these interactions.

ABSTRACT_BEGIN
  This paper describes the design of a control and management network
(orderwire) for a mobile wireless Asynchronous Transfer Mode (ATM) network.
This mobile wireless ATM network is part of the Rapidly Deployable Radio
Network (RDRN). The orderwire system consists of a packet radio network which
overlays the mobile wireless ATM network, each network element in this network
uses Global Positioning System (GPS) information to control a beamforming
antenna subsystem which provides for spatial reuse. This paper also proposes a
novel Virtual Network Configuration (VNC) algorithm for predictive network
configuration. A mobile ATM Private Network-Network Interface (PNNI) based on
VNC is also discussed. Finally, as a prelude to the system implementation,
results of a Maisie simulation of the orderwire system are discussed.

ABSTRACT_BEGIN
  This paper extends a stochastic theory for buffer fill distribution for
multiple ``on'' and ``off'' sources to a mobile environment. Queue fill
distribution is described by a set of differential equations assuming sources
alternate asynchronously between exponentially distributed periods in ``on''
and ``off'' states. This paper includes the probabilities that mobile sources
have links to a given queue. The sources represent mobile user nodes, and the
queue represents the capacity of a switch. This paper presents a method of
analysis which uses mobile parameters such as speed, call rates per unit area,
cell area, and call duration and determines queue fill distribution at the ATM
cell level. The analytic results are compared with simulation results.

ABSTRACT_BEGIN
  This research concentrates on the design and analysis of an algorithm
referred to as Virtual Network Configuration (VNC) which uses predicted future
states of a system for faster network configuration and management. VNC is
applied to the configuration of a wireless mobile ATM network. VNC is built on
techniques from parallel discrete event simulation merged with constraints from
real-time systems and applied to mobile ATM configuration and handoff.
  Configuration in a mobile network is a dynamic and continuous process.
Factors such as load, distance, capacity and topology are all constantly
changing in a mobile environment. The VNC algorithm anticipates configuration
changes and speeds the reconfiguration process by pre-computing and caching
results. VNC propagates local prediction results throughout the VNC enhanced
system. The Global Positioning System is an enabling technology for the use of
VNC in mobile networks because it provides location information and accurate
time for each node.
  This research has resulted in well defined structures for the encapsulation
of physical processes within Logical Processes and a generic library for
enhancing a system with VNC. Enhancing an existing system with VNC is straight
forward assuming the existing physical processes do not have side effects. The
benefit of prediction is gained at the cost of additional traffic and
processing. This research includes an analysis of VNC and suggestions for
optimization of the VNC algorithm and its parameters.

ABSTRACT_BEGIN
  Many applications want to use TCP congestion control to regulate the
transmission rate of a data packet stream. A natural way to achieve this goal
is to transport the data packet stream on a TCP connection. However, because
TCP implements both congestion and error control, transporting a data packet
stream directly using a TCP connection forces the data packet stream to be
subject to TCP's other properties caused by TCP error control, which may be
inappropriate for these applications.
  The TCP decoupling approach proposed in this thesis is a novel way of
applying TCP congestion control to a data packet stream without actually
transporting the data packet stream on a TCP connection. Instead, a TCP
connection using the same network path as the data packet stream is set up
separately and the transmission rate of the data packet stream is then
associated with that of the TCP packets. Since the transmission rate of these
TCP packets is under TCP congestion control, so is that of the data packet
stream. Furthermore, since the data packet stream is not transported on a TCP
connection, the regulated data packet stream is not subject to TCP error
control.
  Because of this flexibility, the TCP decoupling approach opens up many new
opportunities, solves old problems, and improves the performance of some
existing applications. All of these advantages will be demonstrated in the
thesis.
  This thesis presents the design, implementation, and analysis of the TCP
decoupling approach, and its successful applications in TCP trunking, wireless
communication, and multimedia streaming.

ABSTRACT_BEGIN
  We consider the adaptation of random early detection (RED) as an active queue
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two existing RED
variants and point out a weakness in both. The first variant where the drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high Packet Loss Ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm.

ABSTRACT_BEGIN
  We consider the adaptation of random early detection (RED) as a buffer
management algorithm for TCP traffic in Internet gateways where different
maximum transfer units (MTUs) are used. We studied the two RED variants
described in [4] and point out a weakness in both. The first variant where drop
probability is independent from the packet size discriminates connections with
smaller MTUs. The second variant results in a very high packet loss ratio
(PLR), and as a consequence low goodput, for connections with higher MTUs. We
show that fairness in terms of loss and goodput can be supplied through an
appropriate setting of the RED algorithm.

ABSTRACT_BEGIN
  We are moving toward a distributed, international, twenty-four hour,
electronic stock exchange. The exchange will use the global Internet, or
internet technology. This system is a natural application of multicast because
there are a large number of receivers that should receive the same information
simultaneously.
  The data requirements for the stock exchange are discussed. The current
multicast protocols lack the reliability, fairness, and scalability needed in
this application. We describe a distributed architecture together with a
reliable multicast protocol, a modification of the RMP protocol, that has
characteristics appropriate for this application.
  The architecture is used in three applications: In the first, we construct a
unified stock ticker of the transactions that are being conducted on the
various physical and electronic exchanges. Our objective is to deliver the the
same combined ticker reliably and simultaneously to all receivers, anywhere in
the world. In the second, we construct a unified sequence of buy and sell
offers that are delivered to a single exchange or a collection of exchanges.
Our objective is to give all traders the same fair access to an exchange
independent of their relative distances to the exchange or the loss
characteristics of the international network. In the third, we construct a
distributed, electronic trading floor that can replace the current exchanges.
This application uses the innovations from the first two applications to
combine their fairness attributes.

ABSTRACT_BEGIN
  The allocation of scarce spectral resources to support as many user
applications as possible while maintaining reasonable quality of service is a
fundamental problem in wireless communication. We argue that the problem is
best formulated in terms of decision theory. We propose a scheme that takes
decision-theoretic concerns (like preferences) into account and discuss the
difficulties and subtleties involved in applying standard techniques from the
theory of Markov Decision Processes (MDPs) in constructing an algorithm that is
decision-theoretically optimal. As an example of the proposed framework, we
construct such an algorithm under some simplifying assumptions. Additionally,
we present analysis and simulation results that show that our algorithm meets
its design goals. Finally, we investigate how far from optimal one well-known
heuristic is. The main contribution of our results is in providing insight and
guidance for the design of near-optimal admission-control policies.

ABSTRACT_BEGIN
  Network firewalls and routers use a rule database to decide which packets
will be allowed from one network onto another. By filtering packets the
firewalls and routers can improve security and performance. However, as the
size of the rule list increases, it becomes difficult to maintain and validate
the rules, and lookup latency may increase significantly. Ordered binary
decision diagrams (BDDs) - a compact method of representing and manipulating
boolean expressions - are a potential method of representing the rules. This
paper presents a new algorithm for representing such lists as a BDD and then
shows how the resulting boolean expression can be used to analyse rule sets.

ABSTRACT_BEGIN
  The three Power-Laws proposed by Faloutsos et al(1999) are important
discoveries among many recent works on finding hidden rules in the seemingly
chaotic Internet topology. In this note, we want to point out that the first
two laws discovered by Faloutsos et al(1999, hereafter, {\it Faloutsos' Power
Laws}) are in fact equivalent. That is, as long as any one of them is true, the
other can be derived from it, and {\it vice versa}. Although these two laws are
equivalent, they provide different ways to measure the exponents of their
corresponding power law relations. We also show that these two measures will
give equivalent results, but with different error bars. We argue that for nodes
of not very large out-degree($\leq 32$ in our simulation), the first Faloutsos'
Power Law is superior to the second one in giving a better estimate of the
exponent, while for nodes of very large out-degree($> 32$) the power law
relation may not be present, at least for the relation between the frequency of
out-degree and node out-degree.

ABSTRACT_BEGIN
  IP mobility addresses the problem of changing the network point-of-attachment
transparently during movement. Mobile IP is the proposed standard by IETF.
Several studies, however, have shown that Mobile IP has several drawbacks, such
as triangle routing and poor handoff performance. Multicast-based mobility has
been proposed as a promising solution to the above problems, incurring less
end-to-end delays and fast smooth handoff. Nonetheless, such architecture
suffers from multicast state scalability problems with the growth in number of
mobile nodes. This architecture also requires ubiquitous multicast deployment
and more complex security measures. To alleviate these problems, we propose an
intra-domain multicast-based mobility solution. A mobility proxy allocates a
multicast address for each mobile that moves to its domain. The mobile uses
this multicast address within a domain for micro mobility. Also, aggregation is
considered to reduce the multicast state. We conduct multicast state analysis
to study the efficiency of several aggregation techniques. We use extensive
simulation to evaluate our protocol's performance over a variety of real and
generated topologies. We take aggregation gain as metric for our evaluation.
  Our simulation results show that in general leaky aggregation obtains better
gains than perfect aggregation. Also, we notice that aggregation gain increases
with the increase in number of visiting mobile nodes and with the decrease in
number of mobility proxies within a domain.

ABSTRACT_BEGIN
  Emerging ad hoc networks are infrastructure-less networks consisting of
wireless devices with various power constraints, capabilities and mobility
characteristics. An essential capability in future ad hoc networks is the
ability to provide scalable multicast services. This paper presents a novel
adaptive architecture to support multicast services in large-scale wide-area ad
hoc networks. Existing works on multicast in ad hoc networks address only small
size networks. Our main design goals are scalability, robustness and
efficiency. We propose a self-configuring hierarchy extending zone-based
routing with the notion of contacts based on the small world graphs phenomenon
and new metrics of stability and mobility. We introduce a new geographic-based
multicast address allocation scheme coupled with adaptive anycast based on
group popularity. Our scheme is the first of its kind and promises efficient
and robust operation in the common case. Also, based on the new concept of
rendezvous regions, we provide a bootstrap mechanism for the multicast service;
a challenge generally ignored in previous work.

ABSTRACT_BEGIN
  The machines and beamlines controlled by VME industrial networks are very
popular in accelerator faculties. Recently new software technology, among of
which are Internet/Intranet application, Java language, and distributed
calculating environment, changes the control manner rapidly. A program based on
DCOM is composed to control of a variable included angle spherical grating
monochromator beamline at National Synchrotron Radiation Laboratory (NSRL) in
China. The control computer with a residential DCOM program is connected to
Intranet by LAN, over which the user-end-operating program located in another
computer sends driving beamline units' commands to the control computer. And
also a web page coded in Java, published by the WWW service running in the
control computer, is simply illustrated how to use web browser to query the
states of or to control the beamline units.

ABSTRACT_BEGIN
  The beamline network system at SPring-8 consists of three LANs; a BL-LAN for
beamline component control, a BL-USER-LAN for beamline experimental users and
an OA-LAN for the information services. These LANs are interconnected by a
firewall system. Since the network traffic and the number of beamlines have
increased, we upgraded the backbone of BL-USER-LAN from Fast Ethernet to
Gigabit Ethernet. And then, to establish the independency of a beamline and to
raise flexibility of every beamline, we also introduced the IEEE802.1Q Virtual
LAN (VLAN) technology into the BL-USER-LAN. We discuss here a future plan to
build the firewall system with hardware load balancers.

ABSTRACT_BEGIN
  To study mechanisms that cause the non-Gaussian nature of network traffic, we
analyzed IP flow statistics. For greedy flows in particular, we investigated
the hop counts between source and destination nodes, and classified
applications by the port number. We found that the main flows contributing to
the non-Gaussian nature of network traffic were HTTP flows with relatively
small hop counts compared with the average hop counts of all flows.

ABSTRACT_BEGIN
  Recently the problem of indexing and locating content in peer-to-peer
networks has received much attention. Previous work suggests caching index
entries at intermediate nodes that lie on the paths taken by search queries,
but until now there has been little focus on how to maintain these intermediate
caches. This paper proposes CUP, a new comprehensive architecture for
Controlled Update Propagation in peer-to-peer networks. CUP asynchronously
builds caches of index entries while answering search queries. It then
propagates updates of index entries to maintain these caches. Under unfavorable
conditions, when compared with standard caching based on expiration times, CUP
reduces the average miss latency by as much as a factor of three. Under
favorable conditions, CUP can reduce the average miss latency by more than a
factor of ten.
  CUP refreshes intermediate caches, reduces query latency, and reduces network
load by coalescing bursts of queries for the same item. CUP controls and
confines propagation to updates whose cost is likely to be recovered by
subsequent queries. CUP gives peer-to-peer nodes the flexibility to use their
own incentive-based policies to determine when to receive and when to propagate
updates. Finally, the small propagation overhead incurred by CUP is more than
compensated for by its savings in cache misses.

ABSTRACT_BEGIN
  In heterogeneous networks such as today's Internet, the differentiated
services architecture promises to provide QoS guarantees through scalable
service differentiation. Traffic marking is an important component of this
framework. In this paper, we propose two new aggregate markers that are
stateless, scalable and fair. We leverage stateless Active Queue Management
(AQM) algorithms to enable fair and efficient token distribution among
individual flows of an aggregate. The first marker, Probabilistic Aggregate
Marker (PAM), uses the Token Bucket burst size to probabilistically mark
incoming packets to ensure TCP-friendly and proportionally fair marking. The
second marker, Stateless Aggregate Fair Marker (F-SAM) approximates fair
queueing techniques to isolate flows while marking packets of the aggregate. It
distributes tokens evenly among the flows without maintaining per-flow state.
Our simulation results show that our marking strategies show upto 30%
improvement over other commonly used markers while marking flow aggregates.
These improvements are in terms of better average throughput and fairness
indices, in scenarios containing heterogeneous traffic consisting of TCP (both
long lived elephants and short lived mice) and misbehaving UDP flows. As a
bonus, F-SAM helps the mice to win the war against elephants.

ABSTRACT_BEGIN
  The deterministic network calculus offers an elegant framework for
determining delays and backlog in a network with deterministic service
guarantees to individual traffic flows. This paper addresses the problem of
extending the network calculus to a probabilistic framework with statistical
service guarantees. Here, the key difficulty relates to expressing, in a
statistical setting, an end-to-end (network) service curve as a concatenation
of per-node service curves. The notion of an effective service curve is
developed as a probabilistic bound on the service received by an individual
flow. It is shown that per-node effective service curves can be concatenated to
yield a network effective service curve.

ABSTRACT_BEGIN
  We propose centralized algorithm of data distribution in the unicast p2p
network. Good example of such networks are meshes of WWW and FTP mirrors.
Simulation of data propogation for different network topologies is performed
and it is shown that proposed method performs up to 200% better then common
apporaches

ABSTRACT_BEGIN
  In this study, the concept of small worlds is investigated in the context of
large-scale wireless ad hoc and sensor networks. Wireless networks are spatial
graphs that are usually much more clustered than random networks and have much
higher path length characteristics. We observe that by adding only few random
links, path length of wireless networks can be reduced drastically without
affecting clustering. What is even more interesting is that such links need not
be formed randomly but may be confined to a limited number of hops between the
connected nodes. This has an important practical implication, as now we can
introduce a distributed algorithm in large-scale wireless networks, based on
what we call contacts, to improve the performance of resource discovery in such
networks, without resorting to global flooding. We propose new contact-based
protocols for adding logical short cuts in wireless networks efficiently. The
new protocols take advantage of mobility in order to increase reachability of
the search. We study the performance of our proposed contact-based
architecture, and clarify the context in which large-scale wireless networks
can be turned into small world networks.

ABSTRACT_BEGIN
  In this paper we propose a novel architecture, CARD, for resource discovery
in large scale Mobile Ad hoc Networks (MANets) which, may scale up to thousands
of nodes and may span wide geographical regions. Unlike previously proposed
schemes, our architecture avoids expensive mechanisms such as global flooding
as well as complex coordination between nodes to form a hierarchy. CARD is also
independent of any external source of information such as GPS. In our
architecture nodes within a limited number of hops from each node form the
neighborhood of that node. Resources within the neighborhood can be readily
accessed with the help of a proactive scheme within the neighborhood. For
accessing resources beyond the neighborhood, each node also maintains a few
distant nodes called contacts. Contacts help in creating a small world in the
network and provide an efficient way to query for resources beyond the
neighborhood. As the number of contacts of a node increases, the network view
(reachability) of the node increases. Paths to contacts are validated
periodically to adapt to mobility. We present mechanisms for contact selection
and maintenance that attempt to increase reachability while minimizing
overhead. Our simulation results show a clear trade-off between increase in
reachability on one hand, and contact selection and maintenance overhead on the
other. Our results suggest that CARD can be configured to provide a desirable
reachability distribution for different network sizes. Comparisons with other
schemes for resource discovery, such as flooding and bordercasting, show our
architecture to be much more efficient and scalable.

ABSTRACT_BEGIN
  One of the most important metrics in the design of IP mobility protocols is
the handover performance. The current Mobile IP (MIP) standard has been shown
to exhibit poor handover performance. Most other work attempts to modify MIP to
slightly improve its efficiency, while others propose complex techniques to
replace MIP. Rather than taking these approaches, we instead propose a new
architecture for providing efficient and smooth handover, while being able to
co-exist and inter-operate with other technologies. Specifically, we propose an
intra-domain multicast-based mobility architecture, where a visiting mobile is
assigned a multicast address to use while moving within a domain. Efficient
handover is achieved using standard multicast join/prune mechanisms. Two
approaches are proposed and contrasted. The first introduces the concept
proxy-based mobility, while the other uses algorithmic mapping to obtain the
multicast address of visiting mobiles. We show that the algorithmic mapping
approach has several advantages over the proxy approach, and provide mechanisms
to support it. Network simulation (using NS-2) is used to evaluate our scheme
and compare it to other routing-based micro-mobility schemes - CIP and HAWAII.
The proactive handover results show that both M&M and CIP shows low handoff
delay and packet reordering depth as compared to HAWAII. The reason for M&M's
comparable performance with CIP is that both use bi-cast in proactive handover.
The M&M, however, handles multiple border routers in a domain, where CIP fails.
We also provide a handover algorithm leveraging the proactive path setup
capability of M&M, which is expected to outperform CIP in case of reactive
handover.

ABSTRACT_BEGIN
  We analyzed the non-Gaussian nature of network traffic using some Internet
traffic data. We found that (1) the non-Gaussian nature degrades network
performance, (2) it is caused by `greedy flows' that exist with non-negligible
probability, and (3) a large majority of `greedy flows' are TCP flows having
relatively small hop counts, which correspond to small round-trip times. We
conclude that in a network hat has greedy flows with non-negligible
probability, a traffic controlling scheme or bandwidth design that considers
non-Gaussian nature is essential.

ABSTRACT_BEGIN
  Conventional optical networks are based on SONET rings, but since rings are
known to use bandwidth inefficiently, there has been much research into shared
mesh protection, which promises significant bandwidth savings. Unfortunately,
most shared mesh protection schemes cannot guarantee that failed traffic will
be restored within the 50 ms timeframe that SONET standards specify. A notable
exception is the p-cycle scheme of Grover and Stamatelakis. We argue, however,
that p-cycles have certain limitations, e.g., there is no easy way to adapt
p-cycles to a path-based protection scheme, and p-cycles seem more suited to
static traffic than to dynamic traffic. In this paper we show that the key to
fast restoration times is not a ring-like topology per se, but rather the
ability to pre-cross-connect protection paths. This leads to the concept of a
pre-cross-connected trail or PXT, which is a structure that is more flexible
than rings and that adapts readily to both path-based and link-based schemes
and to both static and dynamic traffic. The PXT protection scheme achieves fast
restoration speeds, and our simulations, which have been carefully chosen using
ideas from experimental design theory, show that the bandwidth efficiency of
the PXT protection scheme is comparable to that of conventional shared mesh
protection schemes.

ABSTRACT_BEGIN
  Many ad hoc routing protocols are based on some variant of flooding. Despite
various optimizations, many routing messages are propagated unnecessarily. We
propose a gossiping-based approach, where each node forwards a message with
some probability, to reduce the overhead of the routing protocols. Gossiping
exhibits bimodal behavior in sufficiently large networks: in some executions,
the gossip dies out quickly and hardly any node gets the message; in the
remaining executions, a substantial fraction of the nodes gets the message. The
fraction of executions in which most nodes get the message depends on the
gossiping probability and the topology of the network. In the networks we have
considered, using gossiping probability between 0.6 and 0.8 suffices to ensure
that almost every node gets the message in almost every execution. For large
networks, this simple gossiping protocol uses up to 35% fewer messages than
flooding, with improved performance. Gossiping can also be combined with
various optimizations of flooding to yield further benefits. Simulations show
that adding gossiping to AODV results in significant performance improvement,
even in networks as small as 150 nodes. We expect that the improvement should
be even more significant in larger networks.

ABSTRACT_BEGIN
  The topology of a wireless multi-hop network can be controlled by varying the
transmission power at each node. In this paper, we give a detailed analysis of
a cone-based distributed topology control algorithm. This algorithm, introduced
in [16], does not assume that nodes have GPS information available; rather it
depends only on directional information. Roughly speaking, the basic idea of
the algorithm is that a node $u$ transmits with the minimum power
$p_{u,\alpha}$ required to ensure that in every cone of degree $\alpha$ around
$u$, there is some node that $u$ can reach with power $p_{u,\alpha}$. We show
that taking $\alpha = 5\pi/6$ is a necessary and sufficient condition to
guarantee that network connectivity is preserved. More precisely, if there is a
path from $s$ to $t$ when every node communicates at maximum power, then, if
$\alpha <= 5\pi/6$, there is still a path in the smallest symmetric graph
$G_\alpha$ containing all edges $(u,v)$ such that $u$ can communicate with $v$
using power $p_{u,\alpha}$. On the other hand, if $\alpha > 5\pi/6$,
connectivity is not necessarily preserved. We also propose a set of
optimizations that further reduce power consumption and prove that they retain
network connectivity. Dynamic reconfiguration in the presence of failures and
mobility is also discussed. Simulation results are presented to demonstrate the
effectiveness of the algorithm and the optimizations.

ABSTRACT_BEGIN
  We propose a protocol that, given a communication network, computes a
subnetwork such that, for every pair $(u,v)$ of nodes connected in the original
network, there is a minimum-energy path between $u$ and $v$ in the subnetwork
(where a minimum-energy path is one that allows messages to be transmitted with
a minimum use of energy). The network computed by our protocol is in general a
subnetwork of the one computed by the protocol given in [13]. Moreover, our
protocol is computationally simpler. We demonstrate the performance
improvements obtained by using the subnetwork computed by our protocol through
simulation.

ABSTRACT_BEGIN
  An Open Network Handle System (ONHS) provides an intermediate level of
service between IP numbers and domain names. A handle adheres permanently to an
owner, who may assign and reassign it to different addresses at will. But a
handle is a number, carrying no significance in natural language. Any user
desiring a handle may generate one from a public key. This memo describes a
simple implementation of an Open Network Handle System using the security
extensions to the Domain Name System (DNSSEC).

ABSTRACT_BEGIN
  Networked communications inherently depend on the ability of the sender of a
message to indicate through some token how the message should be delivered to a
particular recipient. The tokens that refer messages to recipients are
variously known as routes, addresses,handles, and names} ordered by their
relative nearness to network topology vs. human meaning. All four sorts of
token refer in some way to a recipient, but they are controlled by different
authorities and their meanings depend on different contextual parameters.
  Today's global Internet employs dynamically determined routes, IP addresses,
and domain names. Domain names combine the functions of handles and names. The
high value of domain names as names leads to substantial social and legal
dispute about their assignment, degrading their value as handles. The time has
come to provide a distinct open network handle system (ONHS), using handles
that are not meaningful in natural language and are therefore not subject to
the disputes surrounding the use of names.
  A handle service may be deployed easily as a handle domain within the current
Domain Name System. In order to minimize the administrative load, and maximize
their own autonomy, netizens may use public-key cryptography to assign their
own handles.

ABSTRACT_BEGIN
  The aim of this paper is an experimental study of cache systems in order to
optimize proxy cache systems and to modernize construction principles. Our
investigations lead to the criteria for the optimal use of storage capacity and
allow the description of the basic effects of the ratio between construction
parts, steady-state performance, optimal size, etc. We want to outline that the
results obtained and the plan of the experiment follow from the theoretical
model. Special consideration is given to the modification of the key formulas
supposed by Wolman at al.

ABSTRACT_BEGIN
  The aim of this paper is a theoretical study of a cache system in order to
optimize proxy cache systems and to modernize construction principles including
prefetching schemes. Two types of correlations, Zipf-like distribution and
normalizing conditions, play a role of the fundamental laws. A corresponding
system of equations allows to describe the basic effects like ratio between
construction parts, steady-state performance, optimal size, long-term
prefetching, etc. A modification of the fundamental laws leads to the
description of new effects of documents' renewal in the global network. An
internet traffic caching system based on Zipf-like distribution (ZBS) is
invented. The additional module to the cache construction gives an effective
prefetching by lifetime.

ABSTRACT_BEGIN
  The study of Complex Systems is considered by many to be a new scientific
field, and is distinguished by being a discipline that has applications within
many separate areas of scientific study. The study of Neural Networks, Traffic
Patterns, Artificial Intelligence, Social Systems, and many other scientific
areas can all be considered to fall within the realm of Complex Systems, and
can be studied from this new perspective. The advent of more capable computer
systems has allowed these systems to be simulated and modeled with far greater
ease, and new understanding of computer modeling approaches has allowed the
fledgling science to be studied as never before.
  The preliminary focus of this paper will be to provide a general overview of
the science of Complex Systems, including terminology, definitions, history,
and examples. I will attempt to look at some of the most important trends in
different areas of research, and give a general overview of research methods
that have been used in parallel with computer modeling. Also, I will further
define the areas of the science that concern themselves with computer modeling
and simulation, and I will attempt to make it clear why the science only came
into its own when the proper modeling and simulation tools were finally
available. In addition, although there seems to be general agreement between
different authors and institutes regarding the generalities of the study, there
are some differences in terminology and methodology. I have attempted in this
paper to bring as many elements together as possible, as far as the scope of
the subject is concerned, without losing focus by studying Complex System
techniques that are bound to one particular area of scientific study, unless
that area is that of computer modeling.

ABSTRACT_BEGIN
  A number of recent studies of the Internet topology at the autonomous systems
level (AS graph) are based on the BGP-based AS connectivity maps (original
maps). The so-called extended maps use additional data sources and contain more
complete pictures of the AS graph. In this paper, we compare an original map,
an extended map and a synthetic map generated by the Barabasi-Albert model. We
examine the recently reported rich-club phenomenon, alternative routing paths
and attack tolerance. We point out that the majority of the missing links of
the original maps are the connecting links between rich nodes (nodes with large
numbers of links) of the extended maps. We show that the missing links are
relevant because links between rich nodes can be crucial for the network
structure.

ABSTRACT_BEGIN
  The Internet topology at the Autonomous Systems level (AS graph) has a
power--law degree distribution and a tier structure. In this paper, we
introduce the Interactive Growth (IG) model based on the joint growth of new
nodes and new links. This simple and dynamic model compares favorable with
other Internet power--law topology generators because it not only closely
resembles the degree distribution of the AS graph, but also accurately matches
the hierarchical structure, which is measured by the recently reported
rich-club phenomenon.

ABSTRACT_BEGIN
  Recently we introduced the rich-club phenomenon as a quantitative metric to
characterize the tier structure of the Autonomous Systems level Internet
topology (AS graph) and we proposed the Interactive Growth (IG) model, which
closely matches the degree distribution and hierarchical structure of the AS
graph and compares favourble with other available Internet power-law topology
generators. Our research was based on the widely used BGP AS graph obtained
from the Oregon BGP routing tables. Researchers argue that Traceroute AS graph,
extracted from the traceroute data collected by the CAIDA's active probing
tool, Skitter, is more complete and reliable. To be prudent, in this paper we
analyze and compare topological structures of Traceroute AS graph and BGP AS
graph. Also we compare with two synthetic Internet topologies generated by the
IG model and the well-known Barabasi-Albert (BA) model. Result shows that both
AS graphs show the rich-club phenomenon and have similar tier structures, which
are closely matched by the IG model, however the BA model does not show the
rich-club phenomenon at all.

ABSTRACT_BEGIN
  The article analyzes a proposed network topology for the ATLAS DAQ DataFlow,
and identifies the Ethernet features required for a proper operation of the
network: MAC address table size, switch performance in terms of throughput and
latency, the use of Flow Control, Virtual LANs and Quality of Service. We
investigate these features on some Ethernet switches, and conclude on their
usefulness for the ATLAS DataFlow network.

ABSTRACT_BEGIN
  We propose flow-based analysis to estimate quality of an Internet connection.
Using results from the queuing theory we compare two expressions for backbone
traffic that have different scopes of applicability. A curve that shows
dependence of utilization of a link on a number of active flows in it describes
different states of the network. We propose a methodology for plotting such a
curve using data received from a Cisco router by NetFlow protocol, determining
the working area and the overloading point of the network. Our test is an easy
way to find a moment for upgrading the backbone.

ABSTRACT_BEGIN
  Ad hoc networks rely on the cooperation of the nodes participating in the
network to forward packets for each other. A node may decide not to cooperate
to save its resources while still using the network to relay its traffic. If
too many nodes exhibit this behavior, network performance degrades and
cooperating nodes may find themselves unfairly loaded. Most previous efforts to
counter this behavior have relied on further cooperation between nodes to
exchange reputation information about other nodes. If a node observes another
node not participating correctly, it reports this observation to other nodes
who then take action to avoid being affected and potentially punish the bad
node by refusing to forward its traffic. Unfortunately, such second-hand
reputation information is subject to false accusations and requires maintaining
trust relationships with other nodes. The objective of OCEAN is to avoid this
trust-management machinery and see how far we can get simply by using direct
first-hand observations of other nodes' behavior. We find that, in many
scenarios, OCEAN can do as well as, or even better than, schemes requiring
second-hand reputation exchanges. This encouraging result could possibly help
obviate solutions requiring trust-management for some contexts.

ABSTRACT_BEGIN
  Several tools exist that collect host-to-host connectivity measurements. To
improve the usability of such measurements, they should be mapped into a
framework consisting of complex subsystems, and the infrastructure that
connects them. We introduce one such framework, and analyze the architectural
implications on the network structure. In our framework, a complex subsystem
consists of several computing facilities and the infrastructure that connects
them: we call it a -monitoring domain-. The task of measuring the connectivity
between -monitoring domains- is considered distinct from the activity of
-storage- and -computing- elements. Therefore we introduce a new element in our
topology: we call it -theodolite- element, since its function is similar to
that of a transponder. Using these basic concepts, we analyze the architectural
implications on the network structure: in a nutshell, if we want that
-theodolites- serve as a reference, than the contribution to the relevant
network metrics due to the -monitoring domain- infrastructure must be
negligible with respect to contributions of the inter-domain infrastructure. In
addition all -theodolites- of a -monitoring domain- must give an image of the
inter-domain infrastructure that is consistent with that experienced by network
applications. We conclude giving a running SQL example of how information about
-monitoring domains- and -theodolites- could be organized, and we outline the
application of such framework in the GLUE schema activity for the network
element

ABSTRACT_BEGIN
  It starts out innocently enough - users want to monitor Online data and so
run their own copies of the detector control GUIs in their offices and at home.
But over time, the number of processes making requests for values to display on
GUIs, webpages and stripcharts can grow, and affect the performance of an
Input/Output Controller (IOC) such that it is unable to respond to requests
from requests critical to data-taking. At worst, an IOC can hang, its CPU
having been allocated 100% to responding to network requests.
  For the BaBar Online Detector Control System, we were able to eliminate this
problem and make great gains in security by moving all of the IOCs to a
non-routed, virtual LAN and by enlisting a workstation with two network
interface cards to act as the interface between the virtual LAN and the public
BaBar network. On the interface machine, we run the Experimental Physics
Industrial Control System (EPICS) Channel Access (CA) gateway software
(originating from Advanced Photon Source). This software accepts as inputs, all
the channels which are loaded into the EPICS databases on all the IOCs. It
polls them to update its copy of the values. It answers requests from
applications by sending them the currently cached value.
  We adopted the requirement that data-taking would be independent of the
gateway, so that, in the event of a gateway failure, data-taking would be
uninterrupted. In this way, we avoided introducing any new risk elements to
data-taking. Security rules already in use by the IOC were propagated to the
gateway's own security rules and the security of the IOCs themselves was
improved by removing them from the public BaBar network.

ABSTRACT_BEGIN
  We show that the Internet topology at the Autonomous System (AS) level has a
rich--club phenomenon. The rich nodes, which are a small number of nodes with
large numbers of links, are very well connected to each other. The rich--club
is a core tier that we measured using the rich--club connectivity and the
node--node link distribution. We obtained this core tier without any heuristic
assumption between the ASes. The rich--club phenomenon is a simple qualitative
way to differentiate between power law topologies and provides a criterion for
new network models. To show this, we compared the measured rich--club of the AS
graph with networks obtained using the Barab\'asi--Albert (BA) scale--free
network model, the Fitness BA model and the Inet--3.0 model.

ABSTRACT_BEGIN
  Denial of Service (DoS) attacks are one of the most challenging threats to
Internet security. An attacker typically compromises a large number of
vulnerable hosts and uses them to flood the victim's site with malicious
traffic, clogging its tail circuit and interfering with normal traffic. At
present, the network operator of a site under attack has no other resolution
but to respond manually by inserting filters in the appropriate edge routers to
drop attack traffic. However, as DoS attacks become increasingly sophisticated,
manual filter propagation becomes unacceptably slow or even infeasible.
  In this paper, we present Active Internet Traffic Filtering, a new automatic
filter propagation protocol. We argue that this system provides a guaranteed,
significant level of protection against DoS attacks in exchange for a
reasonable, bounded amount of router resources. We also argue that the proposed
system cannot be abused by a malicious node to interfere with normal Internet
operation. Finally, we argue that it retains its efficiency in the face of
continued Internet growth.

ABSTRACT_BEGIN
  Flooding provides important control and route establishment functionality for
a number of unicast and multicast protocols in Mobile Ad Hoc Networks.
Considering its wide use as a building block for other network layer protocols,
the flooding methodology should deliver a packet from one node to all other
network nodes using as few messages as possible. In this paper, we propose the
Optimized Flooding Protocol (OFP), based on a variation of The Covering Problem
that is encountered in geometry, to minimize the unnecessary transmissions
drastically and still be able to cover the whole region. OFP does not need
hello messages and hence OFP saves a significant amount of wireless bandwidth
and incurs lesser overhead. We present simulation results to show the
efficiency of OFP in both ideal cases and randomly distributed networks.
Moreover, OFP is scalable with respect to density; in fact OFP requires lesser
number of transmissions at higher densities. OFP is also resilient to
transmission errors.

ABSTRACT_BEGIN
  In this paper, we model the cost incurred by each peer participating in a
peer-to-peer network. Such a cost model allows to gauge potential disincentives
for peers to collaborate, and provides a measure of the ``total cost'' of a
network, which is a possible benchmark to distinguish between proposals. We
characterize the cost imposed on a node as a function of the experienced load
and the node connectivity, and show how our model applies to a few proposed
routing geometries for distributed hash tables (DHTs). We further outline a
number of open questions this research has raised.

ABSTRACT_BEGIN
  Based on measurements of the Internet topology data, we found out that there
are two mechanisms which are necessary for the correct modeling of the Internet
topology at the Autonomous Systems (AS) level: the Interactive Growth of new
nodes and new internal links, and a nonlinear preferential attachment, where
the preference probability is described by a positive-feedback mechanism. Based
on the above mechanisms, we introduce the Positive-Feedback Preference (PFP)
model which accurately reproduces many topological properties of the AS-level
Internet, including: degree distribution, rich-club connectivity, the maximum
degree, shortest path length, short cycles, disassortative mixing and
betweenness centrality. The PFP model is a phenomenological model which
provides a novel insight into the evolutionary dynamics of real complex
networks.

ABSTRACT_BEGIN
  A comparison between the topological properties of the measured Internet
topology, at the autonomous system level (AS graph), and the equivalent graphs
generated by two different power law topology generators is presented. Only one
of the synthetic generators reproduces the tier connectivity of the AS graph.

ABSTRACT_BEGIN
  A distributed denial-of-service (DDoS) attack can flood a victim site with
malicious traffic, causing service disruption or even complete failure.
Public-access sites like amazon or ebay are particularly vulnerable to such
attacks, because they have no way of a priori blocking unauthorized traffic.
  We present Active Internet Traffic Filtering (AITF), a mechanism that
protects public-access sites from highly distributed attacks by causing
undesired traffic to be blocked as close as possible to its sources. We
identify filters as a scarce resource and show that AITF protects a significant
amount of the victim's bandwidth, while requiring from each participating
router a number of filters that can be accommodated by today's routers. AITF is
incrementally deployable, because it offers a substantial benefit even to the
first sites that deploy it.

ABSTRACT_BEGIN
  In this paper considered question of using pattern recognition methods in
network equipment state identification.

ABSTRACT_BEGIN
  We consider the problem of providing service guarantees in a high-speed
packet switch. As basic requirements, the switch should be scalable to high
speeds per port, a large number of ports and a large number of traffic flows
with independent guarantees. Existing scalable solutions are based on Virtual
Output Queuing, which is computationally complex when required to provide
service guarantees for a large number of flows.
  We present a novel architecture for packet switching that provides support
for such service guarantees. A cost-effective fabric with small external
speedup is combined with a feedback mechanism that enables the fabric to be
virtually lossless, thus avoiding packet drops indiscriminate of flows. Through
analysis and simulation, we show that this architecture provides accurate
support for service guarantees, has low computational complexity and is
scalable to very high port speeds.

ABSTRACT_BEGIN
  In this paper, we study diagnosabilities of multiprocessor systems under two
diagnosis models: the PMC model and the comparison model. In each model, we
further consider two different diagnosis strategies: the precise diagnosis
strategy proposed by Preparata et al. and the pessimistic diagnosis strategy
proposed by Friedman. The main result of this paper is to determine
diagnosabilities of regular networks with certain conditions, which include
several widely used multiprocessor systems such as variants of hypercubes and
many others.

ABSTRACT_BEGIN
  Future smart environments will be characterized by multiple nodes that sense,
collect, and disseminate information about environmental phenomena through a
wireless network. In this paper, we define a set of applications that require a
new form of distributed knowledge about the environment, referred to as
non-uniform information granularity. By non-uniform information granularity we
mean that the required accuracy or precision of information is proportional to
the distance between a source node (information producer) and current sink node
(information consumer). That is, as the distance between the source node and
sink node increases, loss in information precision is acceptable. Applications
that can benefit from this type of knowledge range from battlefield scenarios
to rescue operations. The main objectives of this paper are two-fold: first, we
will precisely define non-uniform information granularity, and second, we will
describe different protocols that achieve non-uniform information dissemination
and analyze these protocols based on complexity, energy consumption, and
accuracy of information.

ABSTRACT_BEGIN
  The ability of a sensor node to determine its physical location within a
network (Localization) is of fundamental importance in sensor networks.
Interpretating data from sensors will not be possible unless the context of the
data is known; this is most often accomplished by tracking its physical
location. Existing research has focused on localization in static sensor
networks where localization is a one-time (or low frequency) activity. In
contrast, this paper considers localization for mobile sensors: when sensors
are mobile, localization must be invoked periodically to enable the sensors to
track their location. The higher the frequency of localization, the lower the
error introduced because of mobility. However, localization is a costly
operation since it involves both communication and computation. In this paper,
we propose and investigate adaptive and predictive protocols that control the
frequency of localization based on sensor mobility behavior to reduce the
energy requirements for localization while bounding the localization error. We
show that such protocols can significantly reduce the localization energy
without sacrificing accuracy (in fact, improving accuracy for most situations).
Using simulation and analysis we explore the tradeoff between energy efficiency
and localization error due to mobility for several protocols.

ABSTRACT_BEGIN
  In this paper, we introduce two deterministic models aimed at capturing the
dynamics of congested Internet connections. The first model is a
continuous-time model that combines a system of differential equations with a
sudden change in one of the state variables. The second model is a
discrete-time model with a time step that arises naturally from the system.
Results from these models show good agreement with the well-known ns network
simulator, better than the results of a previous, similar model. This is due in
large part to the use of the sudden change to reflect the impact of lost data
packets. We also discuss the potential use of this model in network traffic
state estimation.

ABSTRACT_BEGIN
  There is a growing interest in discovery of internet topology at the
interface level. A new generation of highly distributed measurement systems is
currently being deployed. Unfortunately, the research community has not
examined the problem of how to perform such measurements efficiently and in a
network-friendly manner. In this paper we make two contributions toward that
end. First, we show that standard topology discovery methods (e.g., skitter)
are quite inefficient, repeatedly probing the same interfaces. This is a
concern, because when scaled up, such methods will generate so much traffic
that they will begin to resemble DDoS attacks. We measure two kinds of
redundancy in probing (intra- and inter-monitor) and show that both kinds are
important. We show that straightforward approaches to addressing these two
kinds of redundancy must take opposite tacks, and are thus fundamentally in
conflict. Our second contribution is to propose and evaluate Doubletree, an
algorithm that reduces both types of redundancy simultaneously on routers and
end systems. The key ideas are to exploit the tree-like structure of routes to
and from a single point in order to guide when to stop probing, and to probe
each path by starting near its midpoint. Our results show that Doubletree can
reduce both types of measurement load on the network dramatically, while
permitting discovery of nearly the same set of nodes and links. We then show
how to enable efficient communication between monitors through the use of Bloom
filters.

ABSTRACT_BEGIN
  Wireless LANs have achieved a tremendous amount of growth in recent years.
Among various wireless LAN technologies, the IEEE 802.11b based wireless LAN
technology can be cited as the most prominent technology today. Despite being
widely deployed, 802.11b cannot be termed as a well matured technology.
Although 802.11b is adequate for basic connectivity and packet switching, It is
evident that there is ample scope for its improvement in areas like quality of
service, fairness, performance, security, etc. In this survey report, we
identify and argue that the Medium Access Controller for 802.11b networks is
the prime area for these improvements. To enunciate our claims we highlight
some of the quality of service, fairness, and performance issues related to
802.11b MAC. We also describe and analyze some of the current research aimed at
addressing these issues. We then propose a novel scheme called the Intelligent
Collision Avoidance, seeking to enhance the MAC to address some of the
performance issues in 802.11b and similar networks.

ABSTRACT_BEGIN
  Wireless sensor networks are finally becoming a reality. In this paper, we
present a scalable architecture for using wireless sensor networks in
combination with wireless Ethernet networks to provide a complete end-to-end
solution to narrow the gap between the low-level information and context
awareness. We developed and implemented a complete proximity detector in order
to give a wearable computer, such as a PDA, location context. Since location is
only one element of contextawareness, we pursued utilizing photo sensors and
temperature sensors in learning as much as possible about the environment. We
used the TinyOS RF Motes as our test bed WSN (Wireless Sensor Network), 802.11
compatible hardware as our wireless Ethernet network, and conventional PCs and
wired 802.3 networks to build the upper levels of the architecture.

ABSTRACT_BEGIN
  One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. We investigate a new routing algorithm that uses diffusion in
order to achieve relatively even power dissipation throughout a wireless sensor
network by making good local decisions. We leverage from concepts of
peer-to-peer networks in which the system acts completely decentralized and all
nodes in the network are equal peers. Our algorithm utilizes the node load,
power levels, and spatial information in order to make the optimal routing
decision. According to our preliminary experimental results, our proposed
algorithm performs well according to its goals.

ABSTRACT_BEGIN
  One of the limitations of wireless sensor nodes is their inherent limited
energy resource. Besides maximizing the lifetime of the sensor node, it is
preferable to distribute the energy dissipated throughout the wireless sensor
network in order to minimize maintenance and maximize overall system
performance. Any communication protocol that involves synchronization of peer
nodes incurs some overhead for setting up the communication. We introduce a new
algorithm, e3D (energy-efficient Distributed Dynamic Diffusion routing
algorithm), and compare it to two other algorithms, namely directed, and random
clustering communication. We take into account the setup costs and analyze the
energy-efficiency and the useful lifetime of the system. In order to better
understand the characteristics of each algorithm and how well e3D really
performs, we also compare e3D with its optimum counterpart and an optimum
clustering algorithm. The benefit of introducing these ideal algorithms is to
show the upper bound on performance at the cost of an astronomical prohibitive
synchronization costs. We compare the algorithms in terms of system lifetime,
power dissipation distribution, cost of synchronization, and simplicity of the
algorithm. Our simulation results show that e3D performs comparable to its
optimal counterpart while having significantly less overhead.

ABSTRACT_BEGIN
  Our contribution in this paper is e3D, a diffusion based routing protocol
that prolongs the system lifetime, evenly distributes the power dissipation
throughout the network, and incurs minimal overhead for synchronizing
communication. We compare e3D with other algorithms in terms of system
lifetime, power dissipation distribution, cost of synchronization, and
simplicity of the algorithm.

ABSTRACT_BEGIN
  This paper introduces relevant statistics for the description of routes in
the internet, seen as a graph at the interface level. Based on the observed
properties, we propose and evaluate methods for generating artificial routes
suitable for simulation purposes. The work in this paper is based upon a study
of over seven million route traces produced by CAIDA's skitter infrastructure.

ABSTRACT_BEGIN
  The multiplication of architecture description languages, component models
and platforms implies a serious dilemma for component based software
architects. On the one hand, they have to choose a language to describe
concrete configurations which will be automatically deployed on execution
platforms. On the other hand, they wish to capitalize their software
architectures independently of any description languages or platforms. To solve
this problem, we propose a multi personalities environment for the
configuration and the deployment of component based applications. This
environment is composed of a core capturing a canonical model of configuration
and deployment, and a set of personalities tailored to languages and platforms.
This paper details the architecture of such an environment and describes the
personalities for the CORBA and Fractal component models.

ABSTRACT_BEGIN
  La specification J2EE (Java 2 platform Enterprise Edition) definit une
architecture de serveur d'application Java. Jusqu'a J2EE 1.3, seuls les aspects
de deploiement concernant le developpeur d'applications etaient adresses. Avec
J2EE 1.4, les interfaces et les etapes de deploiement ont ete plus precisement
specifiees dans la specification "J2EE Deployment". JOnAS (Java Open
Application Server) est une plate-forme J2EE developpee au sein du consortium
ObjectWeb. Les aspects deploiement sont en cours de developpement. Cet article
decrit les concepts lies au deploiement dans J2EE, ainsi que les problematiques
levees lors de leur mise en oeuvre pour JOnAS. Il n'a pas pour but de presenter
un travail abouti, mais illustre le deploiement par un cas concret et ebauche
une liste de besoins non encore satisfaits dans le domaine.
  -----
  The J2EE (Java 2 platform Enterprise Edition) specification defines an
architecture for Java Application Servers. Until J2EE 1.3, the deployment
aspect was addressed from the developer point of view only. Since J2EE 1.4,
deployment APIs and steps have been more precisely specified within the "J2EE
Deployment Specification". JOnAS (Java Open Application Server) is a J2EE
platform implementation by ObjectWeb. The deployment aspects are under
development. This article describes the J2EE Deployment concepts, and the
issues raised when implementing deployment features within JOnAS. It does not
provide a complete solution, but illustrates deployment through a concrete
example and initiates a list of non fulfilled requirements.

ABSTRACT_BEGIN
  Les developpements logiciels sur les systemes UNIX font de plus en plus appel
aux logiciels libres. Nous proposons une solution de deploiement et de controle
de ces logiciels libres au sein d'une grande organisation. Nous nous attachons
particulierement a resoudre les problemes lies au deploiement multi-sites ainsi
qu'a la gestion de configuration de ces deploiements. L'originalite de notre
approche repose sur sa capacite a etre mise en oeuvre et controlee par les
utilisateurs plutot que par les administrateurs, sans necessiter d'expertise
particuliere, et par les possibilites de deploiement dans des environnements
heterogenes.
  -----
  Free and open source software is more and more used for software developments
on UNIX systems. We are proposing a solution to control the deployment of free
software in the context of a large corporation, focusing on multi-site
deployment and configuration management. The originality of our approach rests
on its ability to be implemented and controlled by users rather than
administrators, without requiring any particular expertise, and on its facility
to be deployed in heterogeneous environments.

ABSTRACT_BEGIN
  This paper proposes a software architecture for dynamical service adaptation.
The services are constituted by reusable software components. The adaptation's
goal is to optimize the service function of their execution context. For a
first step, the context will take into account just the user needs but other
elements will be added. A particular feature in our proposition is the profiles
that are used not only to describe the context's elements but also the
components itself. An Adapter analyzes the compatibility between all these
profiles and detects the points where the profiles are not compatibles. The
same Adapter search and apply the possible adaptation solutions: component
customization, insertion, extraction or replacement.

ABSTRACT_BEGIN
  Runtime reconfiguration considered as "applying required changes to a running
system" plays an important role for providing high availability not only of
safety- and mission-critical systems, but also for commercial web-applications
offering professional services. Hereby, the main concerns are maintaining the
consistency of the running system during reconfiguration and minimizing its
down-time caused by the reconfiguration. This paper focuses on the platform
independent subsystem that realises deployment and redeployment of J2EE modules
based on the new J2EE Deployment API as a part of the implementation of our
proposed system architecture enabling runtime reconfiguration of
component-based systems. Our "controlled runtime redeployment" comprises an
extension of hot deployment and dynamic reloading, complemented by allowing for
structural change

ABSTRACT_BEGIN
  Information and communication technologies are moving towards a new stage
where applications will be dynamically deployed, uninstalled, updated and
(re)configured. Several approaches have been followed with the goal of creating
a fully automated and context-aware deployment system. Ideally, this system
should be capable of handling the dynamics of this new situation, without
losing sight of other factors, such as performance, security, availability or
scalability. We will take some of the technologies that follow the principles
of Service Oriented Architectures, SOA, as a paradigm of dynamic environments.
SOA promote the breaking down of applications into sets of loosely coupled
elements, called services. Services can be dynamically bound, deployed,
reconfigured, uninstalled and updated. First of all, we will try to offer a
broad view on the specific deployment issues that arise in these environments.
Later on, we will present our approach to the problem. One of the essential
points that has to be tackled to develop an automated deployment engine will be
to have enough information to carry out tasks without human intervention. In
the article we will focus on the format and contents of deployment descriptors.
Additionally, we will go into the details of the deployment framework for OSGi
enabled gateways that has been developed by our research group. Finally we will
give some concluding remarks and some ideas for future work

ABSTRACT_BEGIN
  Deployment of software components for building distributed applications
consists of the coordination of a set of basic tasks like uploading component
binaries to the execution sites, loading them in memory, instantiating
components, interconnecting their ports, setting their business and technical
attributes. The automation of the deployment process then requires the presence
of a software infrastructure distributed itself on the different execution
sites. This paper presents the characteristics of such an infrastructure for
the deployment of CORBA component-based applications. This latter is designed
and implemented in the context of our OpenCCM platform, an open source
implementation of the CORBA Component Model. The main characteristic lays on
the fact that this infrastructure is itself designed as a set of CORBA
component assemblies. This allows its dynamic assembly during its deployment
over the execution sites

ABSTRACT_BEGIN
  The deployment of component-based applications relies on a centralized
directory to store the components. This paper describes an approach to
distribute software components to be deployed on a set of peers of a peer to
peer network in order to exploit some associated characteristics (load
balancing, fault-tolerance, self-organisation). The proposed architecture is
situated in the context of OSGI application deployment management. The software
components (bundles) are distributed among a set of nodes participating in the
execution of services. When a node wants to install a component which is not
deployed locally, the component is looked for and installed using a p2p
network.
  -----
  Le deploiement d'applications a composants repose sur une approche d'annuaire
centralise de stockage des composants. Cet article decrit une approche pour
distribuer les composants logiciels a deployer sur un ensemble de noeuds d'un
reseau pair-a-pair afin de pouvoir exploiter certaines caracteristiques
associees (equilibrage de charge, tolerance de panne, auto-organisation).
L'architecture proposee entre dans le cadre de la gestion du deploiement
d'applications sur le modele OSGi. Les composants logiciels (ou bundles) sont
repartis a travers un ensemble de noeuds participant a l'execution de services.
Lorsqu'un noeud veut installer un composant et si celui-ci n'est pas encore
deploye localement, il est recherche et installe en utilisant un reseau p2p

ABSTRACT_BEGIN
  Le deploiement est maintenant considere comme une activite a part entiere du
cycle de vie du logiciel. Les grandes entreprises souhaitent pouvoir
automatiser cette etape tout en prenant en compte les caracteristiques de
chaque machine cible. Pour repondre a ces besoins, nous avons defini un
environnement de deploiement : ORYA (Open enviRonment to deploY Applications).
Cet environnement utilise un meta-modele de deploiement, decrit dans ce papier.
Notre approche utilise aussi les technologies des federations et des procedes,
fournissant un environnement flexible et extensible pour l'utilisateur.
  -----
  The deployment is now a full activity of the software lifecycle. Large
enterprises want to automate this step, taking into account characteristics of
each target machine. To satisfy these needs, we have defined an environment for
the deployment phase: ORYA (Open enviRonment to deploY Applications). This
environment uses a deployment metamodel, described in this paper. Our approach
is based also on federation and process federations, providing a flexible and
extensible environment to the user

ABSTRACT_BEGIN
  Cet article presente FROGi, une proposition visant a introduire le modele a
composants Fractal a l'interieur de la plateforme de services OSGi. La
motivation derriere ce travail est double. D'un cote, FROGi offre aux
developpeurs de services OSGi un modele a composants extensibles qui facilite
le developpement des bundles ; ces derniers restent toutefois compatibles avec
les bundles "patrimoniaux". D'un autre cote, FROGi beneficie de
l'infrastructure de deploiement que represente OSGi et qui facilite la
realisation du conditionnement et du deploiement de composants Fractal. Dans
FROGi, une application Fractal est conditionnee sous la forme d'un ou plusieurs
bundles et elle peut etre deployee de facon partielle et les activites de
deploiement peuvent avoir lieu de facon continue.
  -- This paper presents FROGi, a proposal to introduce the Fractal component
model into the OSGi services platform. There are two motivations for this work.
The first one is to offer a flexible component model to the OSGi developers to
simplify bundle development. Bundles developed with FROGi are nevertheless
compatible with standard bundles. The second motivation is to leverage OSGi's
deployment capabilities to package and deploy Fractal components. In FROGi, a
Fractal application is packaged and delivered as a set of OSGi bundles; such an
application supports partial deployment and additionally, deployment activities
can occur continuously.

ABSTRACT_BEGIN
  Internet is growing at a fast pace. The link speeds are surging toward 40
Gbps with the emergence of faster link technologies. New applications are
coming up which require intelligent processing at the intermediate routers.
Switches and routers are becoming the bottlenecks in fast communication. On one
hand faster links deliver more packets every second and on the other hand
intelligent processing consumes more CPU cycles at the router. The conflicting
goals of providing faster but computationally expensive processing call for new
approaches in designing routers.
  This survey takes a look at the core functionalities, like packet
classification, buffer memory management, switch scheduling and output link
scheduling performed by a router in its data path processing and discusses the
algorithms that aim to reduce the performance bound for these operations. An
important requirement for the routers is to provide Quality of Service
guarantees. We propose an algorithm to guarantee QoS in Input Queued Routers.
The hardware solution to speed up router operation was Application Specific
Integrated Circuits (ASICs). But the inherent inflexibility of the method is a
demerit as network standards and application requirements are constantly
evolving, which seek a faster turnaround time to keep up with the changes. The
promise of Network Processors (NP) is the flexibility of general-purpose
processors together with the speed of ASICs. We will study the architectural
choices for the design of Network Processors and focus on some of the
commercially available NPs. There is a plethora of NP vendors in the market.
The discussion on the NP benchmarks sets the normalizing platform to evaluate
these NPs.

ABSTRACT_BEGIN
  The proliferation of IEEE 802.11-based wireless LANs opens up avenues for
creation of several tetherless and mobility oriented services. Most of these
services, like voice over WLAN, media streaming etc., generate delay and
bandwidth sensitive traffic. These traffic flows require undisrupted network
connectivity with some QoS guarantees. Unfortunately, there is no adequate
support built into these wireless LANs towards QoS provisioning. Further, the
network layer handoff latency incurred by mobile nodes in these wireless LANs
is too high for real-time applications to function properly. In this paper, we
describe a QoS mechanism, called Rether, to effectively support bandwidth
guarantee on wireless LANs. Rether is designed to support the current wireless
LAN technologies like 802.11b and 802.11a with a specific capability of being
tailored for QoS oriented technology like 802.11e. We also describe a
low-latency handoff mechanism which expedites network level handoff to provide
real-time applications with an added advantage of seamless mobility.

ABSTRACT_BEGIN
  Today, component oriented middlewares are used to design, develop and deploy
easily distributed applications, by ensuring the heterogeneity,
interoperability, and reuse of the software modules, and the separation between
the business code encapsulated in the components and the system code managed by
the containers. Several standards answer this definition such as: CCM (CORBA
Component Model), EJB (Enterprise Java Beans) and .Net. However these standards
offer a limited and fixed number of system services, removing any possibility
to add system services or to reconfigure dynamically the middleware. Our works
propose mechanisms to add and to adapt dynamically the system services, based
on a reconfiguration language which is dynamically adaptable to the need of the
reconfiguration, and on a tool of dynamic reconfiguration, a prototype was
achieved for the OpenCCM platform, that is an implementation of the CCM
specification. This work was partially financed by the european project
IST-COACH (2001-34445).

ABSTRACT_BEGIN
  Nowadays, numerous component models are used for various purposes: to build
applications, middleware or even operating systems. Those models commonly
support structure reconfiguration, that is modification of application's
architecture at runtime. On the other hand, very few allow implementation
reconfiguration, that is runtime modification of the code of components
building the application. In this article we present the work we performed on
JULIA, a Java-based implementation of the FRACTAL component model, in order for
it to support implementation reconfigurations. We show how we overcame the
limitations of Java class loading mechanism to allow runtime modifications of
components' implementation and interfaces. We also describe the integration of
our solution with the JULIA ADL.

ABSTRACT_BEGIN
  Software deployment can turn into a baffling problem when the components
being deployed exhibit non-functional requirements. If the platform on which
such components are deployed cannot satisfy their non-functional requirements,
then they may in turn fail to perform satisfactorily. In this paper, we present
a contract-based approach to take a specific category of non-functional
properties specified by components into account, that is those that pertain to
the resources that are necessary for their execution.

ABSTRACT_BEGIN
  With the development of the networks and the Internet, the problems of
automated deployment on broad scale became increasingly crucial. Software
deployment is a complex process covering several activities going from the
configuration to the retirement of a software product. During the execution of
a deployment process, exceptions can be met which put the site in an incoherent
state. To solve them, we propose an approach based on transactional concepts
which describes the actions to be undertaken when an exceptional situation is
met during the deployment process. The approach guaranties the respect of the
site's consistency by preserving part of the work already carried out by the
process. This article presents our approach and an experimentation made in an
academic deployment system.

ABSTRACT_BEGIN
  Bossa is a framework to develop new processes schedulers in commodity
operating systems. Although Bossa enables fine-grained management of the
processor through new scheduling policies, deploying an application with its
own scheduler raises some problems. In this paper we study the problems caused
when deploying an application and its scheduler and to adresse these, we
propose to establish Quality of Service contracts and mechanisms to reconfigure
the scheduler hierarchy.

ABSTRACT_BEGIN
  Software components turn out to be a convenient model to build complex
applications for scientific computing and to run them on a computational grid.
However, deploying complex, component-based applications in a grid environment
is particularly arduous. To prevent the user from directly dealing with a large
number of execution hosts and their heterogeneity within a grid, the
application deployment phase must be as automatic as possible. This paper
describes an architecture for automatic deployment of component-based
applications on computational grids. In the context of the CORBA Component
Model (CCM), this paper details all the steps to achieve an automatic
deployment of components as well as the entities involved: a grid access
middleware and its grid information service (like OGSI), a component deployment
model, as specified by CCM, an enriched application description and a
deployment planner in order to select resources and map components onto
computers.

ABSTRACT_BEGIN
  Autonomic computing has been proposed recently as a way to address the
difficult management of applications whose complexity is constantly increasing.
Autonomous applications will have to be especially flexible and be able to
monitor themselves permanently. This work presents a framework, Pandora, which
eases the construction of applications that satisfy this double goal. Pandora
relies on an original application programming pattern - based on stackable
layers and message passing - to obtain minimalist model and architecture that
allows to control the overhead imposed by the full reflexivity of the
framework. Besides, a prototype of the framework has been implemented in C++. A
detailed performance study, together with examples of use, complement this
presentation

ABSTRACT_BEGIN
  The ever growing software complexity suggests that they will never be bugfree
and therefore secure. Software compagnies regulary publish updates. But maybe
because of lack of time or care or maybe because stopping application is
annoying, such updates are rarely if ever deployed on users' machines. We
propose an integrated tool allowing system administrators to deploy critical
security updates on the fly on applications running remotly without end-user
intervention. Our approach is based on an aspect weaving system, Arachne, that
dynamicaly rewrites binary code. Hence updated applications are still running
while they are updated. Our second tool Minerve integrates Arachne within the
standart updating process: Minerve takes a patch produced by dif and eventually
builds a dynamic patch that can later be woven to update the application on the
fly. In addition, Minerve allows to consult patches translated in a dedicated
language and hence eases auditing tasks.

ABSTRACT_BEGIN
  The new applications being intended for more and more heterogeneous
environments, it is necessary to propose solutions of development which answer
in best the necessities of adaptation of new services. Component-based
programming partially answers this aim, allowing easy replacement of software
blocks in order to provide the most adapted version of a component.
Nevertheless, most of the industrial component-based model implementations do
not allow to provide to components the most adapted technical services (naming,
trading, security, transaction, etc.). In this paper, we suggest defining
technical services themselves under the shape of components. We shall detail
our proposition, by basing it on the Fractal component model of Objectweb.
Then, we shall bring solutions for the use of these new component-based
technical services and shall propose a set of management components which allow
to administer in a dynamic and stand-alone way the obtained components. Finally
we present the prototype of the proposed solution.

ABSTRACT_BEGIN
  We study the use of local heuristics to determine spanning subgraphs for use
in the dissemination of information in complex networks. We introduce two
different heuristics and analyze their behavior in giving rise to spanning
subgraphs that perform well in terms of allowing every node of the network to
be reached, of requiring relatively few messages and small node bandwidth for
information dissemination, and also of stretching paths with respect to the
underlying network only modestly. We contribute a detailed mathematical
analysis of one of the heuristics and provide extensive simulation results on
random graphs for both of them. These results indicate that, within certain
limits, spanning subgraphs are indeed expected to emerge that perform well in
respect to all requirements. We also discuss the spanning subgraphs' inherent
resilience to failures and adaptability to topological changes.

ABSTRACT_BEGIN
  Network-Wide Broadcast (NWB) is a common operation in Mobile Ad hoc Networks
(MANETs) used by routing protocols to discover routes and in group
communication operations. NWB is commonly performed via flooding, which has
been shown to be expensive in dense MANETs because of its high redundancy.
Several efforts have targeted reducing the redundancy of floods. In this work,
we target another problem that can substantially impact the success of NWBs:
since MAC level broadcasts are unreliable, it is possible for critical
rebroadcasts to be lost, leading to a significant drop in the node coverage.
This is especially true under heavy load and in sparse topologies. We show that
the techniques that target reducing the overhead of flooding, reduce its
inherent redundancy and harm its reliability. In addition, we show that static
approaches are more vulnerable to this problem. We then present a selective
rebroadcast approach to improve the robustness of NWBs. We show that our
approach leads to considerable improvement in NWB coverage relative to a
recently proposed solution to this problem, with a small increase in overhead.
The proposed approaches do not require proactive neighbor discovery and are
therefore resilient to mobility. Finally, the solution can be added to
virtually all NWB approaches to improve their reliability.

ABSTRACT_BEGIN
  A revolution is taking place in telecommunication networks. New services are
appearing on platforms such as third generation cellular phones (3G) and
broadband Internet access. This motivates the transition from mostly switched
to all-IP networks. The replacement of the traditional shallow and well-defined
interface to telephony networks brings accrued flexibility, but also makes the
network accordingly difficult to properly secure. This paper surveys the
implications of this transition on security issues in telecom applications. It
does not give an exhaustive list of security tools or security protocols. Its
goal is rather to initiate the reader to the security issues brought to carrier
class servers by this revolution.

ABSTRACT_BEGIN
  Spam costs US corporations upwards of $8.9 billion a year, and comprises as
much as 40% of all email received. Solutions exist to reduce the amount of spam
seen by end users, but cannot withstand sophisticated attacks. Worse yet, many
will occasionally misclassify and silently drop legitimate email. Spammers take
advantage of the near-zero cost of sending email to flood the network, knowing
that success even a tiny fraction of the time means a profit. End users,
however, have proven unwilling to pay money to send email to friends and
family.
  We show that it is feasible to extend the existing mail system to reduce the
amount of unwanted email, without misclassifying email, and without charging
well-behaved users. We require that bulk email senders accurately classify each
email message they send as an advertisement with an area of interest or else be
charged a small negative incentive per message delivered. Recipients are able
to filter out email outside their scope of interest, while senders are able to
focus their sendings to the appropriate audience.

ABSTRACT_BEGIN
  Inspired by the Statistical Physics of complex networks, wireless multihop ad
hoc communication networks are considered in abstracted form. Since such
engineered networks are able to modify their structure via topology control, we
search for optimized network structures, which maximize the end-to-end
throughput performance. A modified version of betweenness centrality is
introduced and shown to be very relevant for the respective modeling. The
calculated optimized network structures lead to a significant increase of the
end-to-end throughput. The discussion of the resulting structural properties
reveals that it will be almost impossible to construct these optimized
topologies in a technologically efficient distributive manner. However, the
modified betweenness centrality also allows to propose a new routing metric for
the end-to-end communication traffic. This approach leads to an even larger
increase of throughput capacity and is easily implementable in a
technologically relevant manner.

ABSTRACT_BEGIN
  The concept of small worlds is introduced into the physical topology of
wireless networks in this work. A. Helmy provided two con- struction schemes of
small worlds for the wireless networks, link rewiring and link addition, but he
mainly focused on the virtual topology. Based on the broadcasting nature of the
radio transmission, we propose a con- struction scheme of small worlds for the
physical topology of Multiple- Input Multiple-Output (MIMO) wireless networks.
Besides the topology- related topics, we also evaluate the reduction of the
power required by a request.

ABSTRACT_BEGIN
  This paper is concerned with the characterization of the relationship between
topology and traffic dynamics. We use a model of network generation that allows
the transition from random to scale free networks. Specifically, we consider
three different topological types of network: random, scale-free with \gamma =
3, scale-free with \gamma = 2. By using a novel LRD traffic generator, we
observe best performance, in terms of transmission rates and delivered packets,
in the case of random networks. We show that, even if scale-free networks are
characterized by shorter characteristic-path- length (the lower the exponent,
the lower the path-length), they show worst performances in terms of
communication. We conjecture this could be explained in terms of changes in the
load distribution, defined here as the number of shortest paths going through a
given vertex. In fact, that distribu- tion is characterized by (i) a decreasing
mean (ii) an increas- ing standard deviation, as the networks becomes
scale-free (especially scale-free networks with low exponents). The use of a
degree-independent server also discriminates against a scale-free structure. As
a result, since the model is un- controlled, most packets will go through the
same vertices, favoring the onset of congestion.

ABSTRACT_BEGIN
  Most MANET (Mobile Ad hoc NETwork) research assumes idealized propagation
models. Experimental results have shown significant divergence from simulation
results due to the effect of signal fading in realistic wireless communication
channels. In this paper, we characterize the impact of fading on protocol
performance. We first study the effect of fading on MAC performance and show
that its effect can be dominating. One of our important conclusions is that
eliminating RTS/CTS packets results in more effective operation under fading.
We also identify an unfairness problem that arises due to backoffs in the
presence of fading. Moreover, fading results in several subtle interactions
between the MAC and routing layers. We identify several of these problems and
make observations about effective approaches for addressing them. For example,
the criteria for determining the best path should not only consider the link
status but also the link order. In addition, because routing protocols rely on
MAC level transmission failure (when the retry limit is exceeded), route
failure errors are often generated unnecessarily. Finally, because MAC level
broadcasts are unreliable, they are especially vulnerable to fading. We analyze
these effects and outline preliminary solutions to them.

ABSTRACT_BEGIN
  Routing in Delay Tolerant Networks (DTNs) benefits considerably if one can
take advantage of knowledge concerning node mobility. The main contribution of
this paper is the definition of a generic routing scheme for DTNs using a
high-dimensional Euclidean space constructed upon nodes' mobility patterns. For
example, nodes are represented as points having as coordinates their
probability of being found in each possible location. We present simulation
results indicating that such a scheme can be beneficial in a scenario inspired
by studies done on real mobility traces. This work should open the way to
further use of the virtual space formalism in DTN routing.

ABSTRACT_BEGIN
  In large-scale wireless networks such as mobile ad hoc and sensor networks,
efficient and robust service discovery and data-access mechanisms are both
essential and challenging. Rendezvous-based mechanisms provide a valuable
solution for provisioning a wide range of services. In this paper, we describe
Rendezvous Regions (RRs) - a novel scalable rendezvous-based architecture for
wireless networks. RR is a general architecture proposed for service location
and bootstrapping in ad hoc networks, in addition to data-centric storage,
configuration, and task assignment in sensor networks. In RR the network
topology is divided into geographical regions, where each region is responsible
for a set of keys representing the services or data of interest. Each key is
mapped to a region based on a hash-table-like mapping scheme. A few elected
nodes inside each region are responsible for maintaining the mapped
information. The service or data provider stores the information in the
corresponding region and the seekers retrieve it from there. We run extensive
detailed simulations, and high-level simulations and analysis, to investigate
the design space, and study the architecture in various environments including
node mobility and failures. We evaluate it against other approaches to identify
its merits and limitations. The results show high success rate and low overhead
even with dynamics. RR scales to large number of nodes and is highly robust and
efficient to node failures. It is also robust to node mobility and location
inaccuracy with a significant advantage over point-based rendezvous mechanisms.

ABSTRACT_BEGIN
  Geocasting is the delivery of packets to nodes within a certain geographic
area. For many applications in wireless ad hoc and sensor networks, geocasting
is an important and frequent communication service. The challenging problem in
geocasting is distributing the packets to all the nodes within the geocast
region with high probability but with low overhead. According to our study we
notice a clear tradeoff between the proportion of nodes in the geocast region
that receive the packet and the overhead incurred by the geocast packet
especially at low densities and irregular distributions. We present two novel
protocols for geocasting that achieve high delivery rate and low overhead by
utilizing the local location information of nodes to combine geographic routing
mechanisms with region flooding. We show that the first protocol
Geographic-Forwarding-Geocast (GFG) has close-to-minimum overhead in dense
networks and that the second protocol Geographic-Forwarding-Perimeter-Geocast
(GFPG) provides guaranteed delivery without global flooding or global network
information even at low densities and with the existence of region gaps or
obstacles. An adaptive version of the second protocol (GFPG*) has the desirable
property of perfect delivery at all densities and close-to-minimum overhead at
high densities. We evaluate our mechanisms and compare them using simulation to
other proposed geocasting mechanisms. The results show the significant
improvement in delivery rate (up to 63% higher delivery percentage in low
density networks) and reduction in overhead (up to 80% reduction) achieved by
our mechanisms. We hope for our protocols to become building block mechanisms
for dependable sensor network architectures that require robust efficient
geocast services.

ABSTRACT_BEGIN
  The transmission of electric signals on a coupled line with distributed
RLC-parameters is considered as a propagation of a dissipative quasi particle.
A calculation technique is developed, alternative to the one, accepted for
lumped lines. The relativistic wave equation for the transient response is
deduced following the common Ohm-low-type considerations. The exact expressions
for the Green function, for information transfer velocity and for time delay
are obtained on this base. The fundamental restrictions on the measurement
accuracy of the time delay are pointed out. The obtained results are naturally
generalized for the multilevel networks of the arbitrary dimension.

ABSTRACT_BEGIN
  Today's Internet maps, which are all collected from a small number of vantage
points, are falling short of being accurate. We suggest here a paradigm shift
for this task. DIMES is a distributed measurement infrastructure for the
Internet that is based on the deployment of thousands of light weight
measurement agents around the globe.
  We describe the rationale behind DIMES deployment, discuss its design
trade-offs and algorithmic challenges, and analyze the structure of the
Internet as it seen with DIMES.

ABSTRACT_BEGIN
  Energy is one of the most important resources in wireless sensor networks.
Recently, the mobility of base station has been exploited to preserve the
energy. But in event driven networks, the mobility issue is quite different
from the continuous monitoring one because only a small portion of sensor node
has data to send at one time. The number of sensor node that forward traffic
should be minimized to prolong the network lifetime. In this paper, we propose
a movement-assisted energy conserving method which tries to reduce the amount
of forwarding sensor node by directing the base station to move close to the
hotspots. This method achieves good performance especially when applied to a
network with a set of cooperative mobile base station. Extensive simulation has
been done to verify the effectiveness of the propose schema.

ABSTRACT_BEGIN
  We consider networks of anonymous sensors and address the problem of
constructing routes for the delivery of information from a group of sensors in
response to a query by a sink. In order to circumvent the restrictions imposed
by anonymity, we rely on using the power level perceived by the sensors in the
query from the sink. We introduce a simple distributed algorithm to achieve the
building of routes to the sink and evaluate its performance by means of
simulations.

ABSTRACT_BEGIN
  The development of veracious models of the Internet topology has received a
lot of attention in the last few years. Many proposed models are based on
topologies derived from RouteViews BGP table dumps (BTDs). However, BTDs do not
capture all AS-links of the Internet topology and most importantly the number
of the hidden AS-links is unknown, resulting in AS-graphs of questionable
quality. As a first step to address this problem, we introduce a new
AS-topology discovery methodology that results in more complete and accurate
graphs. Moreover, we use data available from existing measurement facilities,
circumventing the burden of additional measurement infrastructure. We deploy
our methodology and construct an AS-topology that has at least 61.5% more
AS-links than BTD-derived AS-topologies we examined. Finally, we analyze the
temporal and topological properties of the augmented graph and pinpoint the
differences from BTD-derived AS-topologies.

ABSTRACT_BEGIN
  We conduct the most comprehensive study of WLAN traces to date. Measurements
collected from four major university campuses are analyzed with the aim of
developing fundamental understanding of realistic user behavior in wireless
networks. Both individual user and inter-node (group) behaviors are
investigated and two classes of metrics are devised to capture the underlying
structure of such behaviors.
  For individual user behavior we observe distinct patterns in which most users
are 'on' for a small fraction of the time, the number of access points visited
is very small and the overall on-line user mobility is quite low. We clearly
identify categories of heavy and light users. In general, users exhibit high
degree of similarity over days and weeks.
  For group behavior, we define metrics for encounter patterns and friendship.
Surprisingly, we find that a user, on average, encounters less than 6% of the
network user population within a month, and that encounter and friendship
relations are highly asymmetric. We establish that number of encounters follows
a biPareto distribution, while friendship indexes follow an exponential
distribution. We capture the encounter graph using a small world model, the
characteristics of which reach steady state after only one day.
  We hope for our study to have a great impact on realistic modeling of network
usage and mobility patterns in wireless networks.

ABSTRACT_BEGIN
  Denial-of-Service (DoS) and Distributed DoS (DDoS) attacks can cause serious
problems in wireless networks due to limited network and host resources.
Attacker traceback is a promising solution to take a proper countermeasure near
the attack origins, to discourage attackers from launching attacks, and for
forensics. However, attacker traceback in Mobile Ad-hoc Networks (MANETs) is a
challenging problem due to the dynamic topology, and limited network resources.
It is especially difficult to trace back attacker(s) when they are moving to
avoid traceback. In this paper, we introduce the ATTENTION protocol framework,
which pays special attention to MAC layer abnormal activity under attack.
ATTENTION consists of three classes, namely, coarse-grained traceback,
fine-grained traceback and spatio-temporal fusion architecture. For
energy-efficient attacker searching in MANETs, we also utilize small-world
model. Our simulation analysis shows 79% of success rate in DoS attacker
traceback with coarse-grained attack signature. In addition, with fine-grained
attack signature, it shows 97% of success rate in DoS attacker traceback and
83% of success rate in DDoS attacker traceback. We also show that ATTENTION has
robustness against node collusion and mobility.

ABSTRACT_BEGIN
  The current framework of network utility maximization for distributed rate
allocation assumes fixed channel code rates. However, by adapting the physical
layer channel coding, different rate-reliability tradeoffs can be achieved on
each link and for each end user. Consider a network where each user has a
utility function that depends on both signal quality and data rate, and each
link may provide a `fatter' (`thinner') information `pipe' by allowing a higher
(lower) decoding error probability. We propose two distributed, pricing-based
algorithms to attain optimal rate-reliability tradeoff, with an interpretation
that each user provides its willingness to pay for reliability to the network
and the network feeds back congestion prices to users. The proposed algorithms
converge to a tradeoff point between rate and reliability, which is proved to
be globally optimal for codes with sufficiently large codeword lengths and user
utilities with sufficiently negative curvatures.

ABSTRACT_BEGIN
  Despite prevailing concerns that the current Internet interdomain routing
system will not scale to meet the needs of the 21st century global Internet,
networking research has not yet led to the construction of a new routing
architecture with satisfactory and mathematically provable scalability
characteristics. Worse, continuing empirical trends of the existing routing and
topology structure of the Internet are alarming: the foundational principles of
the current routing and addressing architecture are an inherently bad match for
the naturally evolving structure of Internet interdomain topology. We are
fortunate that a sister discipline, theory of distributed computation, has
developed routing algorithms that offer promising potential for genuinely
scalable routing on realistic Internet-like topologies. Indeed, there are many
recent breakthroughs in the area of compact routing, which has been shown to
drastically outperform, in terms of efficiency and scalability, even the
boldest proposals developed in networking research. Many open questions remain,
but we believe the applicability of compact routing techniques to Internet
interdomain routing is a research area whose potential payoff for the future of
networking is too high to ignore.

ABSTRACT_BEGIN
  We present a simple model reproducing the long-range autocorrelations and the
power spectrum of the web traffic. The model assumes the traffic as Poisson
flow of files with size distributed according to the power-law. In this model
the long-range autocorrelations are independent of the network properties as
well as of inter-packet time distribution.

ABSTRACT_BEGIN
  Internet traffic exhibits self-similarity and long-range dependence (LRD) on
various time scales. A well studied issue is the estimation of statistical
parameters characterizing traffic self-similarity and LRD, such as the Hurst
parameter H. In this paper, we propose to adapt the Modified Allan Variance
(MAVAR), a time-domain quantity originally conceived to discriminate fractional
noise in frequency stability measurement, to estimate the Hurst parameter of
LRD traffic traces and, more generally, to identify fractional noise components
in network traffic. This novel method is validated by comparison to one of the
best techniques for analyzing self-similar and LRD traffic: the logscale
diagram based on wavelet analysis. Both methods are applied to pseudo-random
LRD data series, generated with assigned values of H. The superior spectral
sensitivity of MAVAR achieves outstanding accuracy in estimating H, even better
than the logscale method. The behaviour of MAVAR with most common deterministic
signals that yield nonstationarity in data under analysis is also studied.
Finally, both techniques are applied to a real IP traffic trace, providing a
sound example of the usefulness of MAVAR also in traffic characterization, to
complement other established techniques as the logscale method.

ABSTRACT_BEGIN
  This article first addresses applicability of Euclidean models to the domain
of Internet routing. Those models are found (limitedly) applicable. Then a
simplistic model of routing is constructed for Euclidean plane densely covered
with points-routers. The model guarantees low stretch and logarithmical size of
routing tables at any node. The paper concludes with a discussion on
applicability of the model to real-world Internet routing.

ABSTRACT_BEGIN
  This paper critically examines some propositions and arguments of
cs.NI/0508021 regarding applicability of hierarchical routing and perspectives
of compact routing. Arguments against the former are found to be inaccurate
while the latter is found to be equivalent to well-known deployed solutions.
Also, multiple (stacked) application of compact-routing solutions is found to
be equivalent to hierarchical routing.

ABSTRACT_BEGIN
  Decoupling the permanent identifier of a node from the node's
topology-dependent address is a promising approach toward completely scalable
self-organizing networks. A group of proposals that have adopted such an
approach use the same structure to: address nodes, perform routing, and
implement location service. In this way, the consistency of the routing
protocol relies on the coherent sharing of the addressing space among all nodes
in the network. Such proposals use a logical tree-like structure where routes
in this space correspond to routes in the physical level. The advantage of
tree-like spaces is that it allows for simple address assignment and
management. Nevertheless, it has low route selection flexibility, which results
in low routing performance and poor resilience to failures. In this paper, we
propose to increase the number of paths using incomplete hypercubes. The design
of more complex structures, like multi-dimensional Cartesian spaces, improves
the resilience and routing performance due to the flexibility in route
selection. We present a framework for using hypercubes to implement indirect
routing. This framework allows to give a solution adapted to the dynamics of
the network, providing a proactive and reactive routing protocols, our major
contributions. We show that, contrary to traditional approaches, our proposal
supports more dynamic networks and is more robust to node failures.

ABSTRACT_BEGIN
  The paper is taken out.

ABSTRACT_BEGIN
  Current directions in network routing research have not kept pace with the
latest developments in network architectures, such as peer-to-peer networks,
sensor networks, ad-hoc wireless networks, and overlay networks. A common
characteristic among all of these new technologies is the presence of highly
dynamic network topologies. Currently deployed single-path routing protocols
cannot adequately cope with this dynamism, and existing multi-path algorithms
make trade-offs which lead to less than optimal performance on these networks.
This drives the need for routing protocols designed with the unique
characteristics of these networks in mind.
  In this paper we propose the notion of reachability routing as a solution to
the challenges posed by routing on such dynamic networks. In particular, our
formulation of reachability routing provides cost-sensitive multi-path
forwarding along with loop avoidance within the confines of the Internet
Protocol (IP) architecture. This is achieved through the application of
reinforcement learning within a probabilistic routing framework. Following an
explanation of our design decisions and a description of the algorithm, we
provide an evaluation of the performance of the algorithm on a variety of
network topologies. The results show consistently superior performance compared
to other reinforcement learning based routing algorithms.

ABSTRACT_BEGIN
  Geographical routing protocols have several desirable features for use in ad
hoc and sensor networks but are susceptible to voids and localization errors.
Virtual coordinate systems are an alternative solution to geographically based
routing protocols that works by overlaying a coordinate system on the sensors
relative to well chosen reference points. VC is resilient to localization
errors; however, we show that it is vulnerable to different forms of the void
problem and have no viable complementary approach to overcome them.
Specifically, we show that there are instances when packets reach nodes with no
viable next hop nodes in the forwarding set. In addition, it is possible for
nodes with the same coordinates to arise at different points in the network in
the presence of voids. This paper identifies and analyzes these problems. It
also compares several existing routing protocols based on Virtual Coordinate
systems. Finally, it presents a new routing algorithm that uses backtracking to
overcome voids to achieve high connectivity in the greedy phase, higher overall
path quality and more resilience to localization errors. We show these
properties using extensive simulation analysis.

ABSTRACT_BEGIN
  In the wide area master-slave PLC (powerline communication) system, the
source node cannot reach the destination node without packet relay. Due to the
time-variable attenuation in the powerline, the communication distance cannot
be defined. Two kind of dynamic repeater algorithms are developed, dynamic
source routing and flooding based routing. In this paper, we use analytic
approach to compare the performance of those two routing protocols. We give
formulas to calculate the average duration of a polling cycle for each
protocols. Then we present simulation results to bolster the results of our
analysis. We use three metrics, which are bandwidth consumed for routing
signaling, normalized routing load and average duration of a polling cycle to
evaluate those routing protocols.

ABSTRACT_BEGIN
  We consider the problem of distributing a vaccine for immunizing a scale-free
network against a given virus or worm. We introduce a new method, based on
vaccine dissemination, that seems to reflect more accurately what is expected
to occur in real-world networks. Also, since the dissemination is performed
using only local information, the method can be easily employed in practice.
Using a random-graph framework, we analyze our method both mathematically and
by means of simulations. We demonstrate its efficacy regarding the trade-off
between the expected number of nodes that receive the vaccine and the network's
resulting vulnerability to develop an epidemic as the virus or worm attempts to
infect one of its nodes. For some scenarios, the new method is seen to render
the network practically invulnerable to attacks while requiring only a small
fraction of the nodes to receive the vaccine.

ABSTRACT_BEGIN
  We present the first complete measurement of the Chinese Internet topology at
the autonomous systems (AS) level based on traceroute data probed from servers
of major ISPs in mainland China. We show that both the Chinese Internet AS
graph and the global Internet AS graph can be accurately reproduced by the
Positive-Feedback Preference (PFP) model with the same parameters. This result
suggests that the Chinese Internet preserves well the topological
characteristics of the global Internet. This is the first demonstration of the
Internet's topological fractality, or self-similarity, performed at the level
of topology evolution modeling.

ABSTRACT_BEGIN
  Because a delay tolerant network (DTN) can often be partitioned, the problem
of routing is very challenging. However, routing benefits considerably if one
can take advantage of knowledge concerning node mobility. This paper addresses
this problem with a generic algorithm based on the use of a high-dimensional
Euclidean space, that we call MobySpace, constructed upon nodes' mobility
patterns. We provide here an analysis and the large scale evaluation of this
routing scheme in the context of ambient networking by replaying real mobility
traces. The specific MobySpace evaluated is based on the frequency of visit of
nodes for each possible location. We show that the MobySpace can achieve good
performance compared to that of the other algorithms we implemented, especially
when we perform routing on the nodes that have a high connection time. We
determine that the degree of homogeneity of mobility patterns of nodes has a
high impact on routing. And finally, we study the ability of nodes to learn
their own mobility patterns.

ABSTRACT_BEGIN
  The internet structure is extremely complex. The Positive-Feedback Preference
(PFP) model is a recently introduced internet topology generator. The model
uses two generic algorithms to replicate the evolution dynamics observed on the
internet historic data. The phenomenological model was originally designed to
match only two topology properties of the internet, i.e. the rich-club
connectivity and the exact form of degree distribution. Whereas numerical
evaluation has shown that the PFP model accurately reproduces a large set of
other nontrivial characteristics as well. This paper aims to investigate why
and how this generative model captures so many diverse properties of the
internet. Based on comprehensive simulation results, the paper presents a
detailed analysis on the exact origin of each of the topology properties
produced by the model. This work reveals how network evolution mechanisms
control the obtained topology properties and it also provides insights on
correlations between various structural characteristics of complex networks.

ABSTRACT_BEGIN
  A fundamental understanding of gain provided by motion prediction in wireless
ad hoc routing is currently lacking. This paper examines benefits in routing
obtainable via prediction. A theoretical best-case non-predictive routing model
is quantified in terms of both message overhead and update time for
non-predictive routing. This best- case model of existing routing performance
is compared with predictive routing. Several specific instances of predictive
improvements in routing are examined. The primary contribution of this paper is
quantification of predictive gain for wireless ad hoc routing.

ABSTRACT_BEGIN
  The primary contribution of this work is to examine the energy efficiency of
pulse coupled oscillation for time synchronization in a realistic wireless
network environment and to explore the impact of mobility on convergence rate.
Energy coupled oscillation is susceptible to interference; this approach uses
reception and decoding of short packet bursts to eliminate this problem. The
energy efficiency of a commonly used timestamp broadcast algorithm is compared
and contrasted with pulse-coupled oscillation. The emergent pulse coupled
oscillation technique shows greater energy efficiency as well as robustness
with mobility. A proportion of the sensors may be integrated with GPS receivers
in order to obtain a master clock time.

ABSTRACT_BEGIN
  Motivated by the problem of the coexistence on transmission links of
telecommunication networks of elastic and unresponsive traffic, we study in
this paper the impact on the busy period of an M/M/1 queue of a small
perturbation in the server rate. The perturbation depends upon an independent
stationary process (X(t)) and is quantified by means of a parameter \eps \ll 1.
We specifically compute the two first terms of the power series expansion in
\eps of the mean value of the busy period duration. This allows us to study the
validity of the Reduced Service Rate (RSR) approximation, which consists in
comparing the perturbed M/M/1 queue with the M/M/1 queue where the service rate
is constant and equal to the mean value of the perturbation. For the first term
of the expansion, the two systems are equivalent. For the second term, the
situation is more complex and it is shown that the correlations of the
environment process (X(t)) play a key role.

ABSTRACT_BEGIN
  We study in this paper the integration of elastic and streaming traffic on a
same link in an IP network. We are specifically interested in the computation
of the mean bit rate obtained by a data transfer. For this purpose, we consider
that the bit rate offered by streaming traffic is low, of the order of
magnitude of a small parameter \eps \ll 1 and related to an auxiliary
stationary Markovian process (X(t)). Under the assumption that data transfers
are exponentially distributed, arrive according to a Poisson process, and share
the available bandwidth according to the ideal processor sharing discipline, we
derive the mean bit rate of a data transfer as a power series expansion in
\eps. Since the system can be described by means of an M/M/1 queue with a
time-varying server rate, which depends upon the parameter \eps and process
(X(t)), the key issue is to compute an expansion of the area swept under the
occupation process of this queue in a busy period. We obtain closed formulas
for the power series expansion in \eps of the mean bit rate, which allow us to
verify the validity of the so-called reduced service rate at the first order.
The second order term yields more insight into the negative impact of the
variability of streaming flows.

ABSTRACT_BEGIN
  A visualisation tool is presented to facilitate the study on large-scale
communications networks. This tool provides a simple and effective way to
summarise the topology of a complex network at a coarse level.

ABSTRACT_BEGIN
  It is now well known that Internet traffic exhibits self-similarity, which
cannot be described by traditional Markovian models such as the Poisson
process. The causes of self-similarity of network traffic must be identified
because understanding the nature of network traffic is critical in order to
properly design and implement computer networks and network services like the
World Wide Web. While some researchers have argued self similarity is generated
by the typical applications or caused by Transport layer Protocols, it is also
possible that the CSMA/CD protocol may cause or at least contribute to this
phenomenon. In this paper, we use NS simulator to study the effect of CSMA/CD
Exponential Backoff retransmission algorithm on Traffic Self similarity.

ABSTRACT_BEGIN
  In the past few years, the network measurement community has been interested
in the problem of internet topology discovery using a large number (hundreds or
thousands) of measurement monitors. The standard way to obtain information
about the internet topology is to use the traceroute tool from a small number
of monitors. Recent papers have made the case that increasing the number of
monitors will give a more accurate view of the topology. However, scaling up
the number of monitors is not a trivial process. Duplication of effort close to
the monitors wastes time by reexploring well-known parts of the network, and
close to destinations might appear to be a distributed denial-of-service (DDoS)
attack as the probes converge from a set of sources towards a given
destination. In prior work, authors of this report proposed Doubletree, an
algorithm for cooperative topology discovery, that reduces the load on the
network, i.e., router IP interfaces and end-hosts, while discovering almost as
many nodes and links as standard approaches based on traceroute. This report
presents our open-source and freely downloadable implementation of Doubletree
in a tool we call traceroute@home. We describe the deployment and validation of
traceroute@home on the PlanetLab testbed and we report on the lessons learned
from this experience. We discuss how traceroute@home can be developed further
and discuss ideas for future improvements.

ABSTRACT_BEGIN
  Research on performance, robustness, and evolution of the global Internet is
fundamentally handicapped without accurate and thorough knowledge of the nature
and structure of the contractual relationships between Autonomous Systems
(ASs). In this work we introduce novel heuristics for inferring AS
relationships. Our heuristics improve upon previous works in several technical
aspects, which we outline in detail and demonstrate with several examples.
Seeking to increase the value and reliability of our inference results, we then
focus on validation of inferred AS relationships. We perform a survey with ASs'
network administrators to collect information on the actual connectivity and
policies of the surveyed ASs. Based on the survey results, we find that our new
AS relationship inference techniques achieve high levels of accuracy: we
correctly infer 96.5% customer to provider (c2p), 82.8% peer to peer (p2p), and
90.3% sibling to sibling (s2s) relationships. We then cross-compare the
reported AS connectivity with the AS connectivity data contained in BGP tables.
We find that BGP tables miss up to 86.2% of the true adjacencies of the
surveyed ASs. The majority of the missing links are of the p2p type, which
highlights the limitations of present measuring techniques to capture links of
this type. Finally, to make our results easily accessible and practically
useful for the community, we open an AS relationship repository where we
archive, on a weekly basis, and make publicly available the complete Internet
AS-level topology annotated with AS relationship information for every pair of
AS neighbors.

ABSTRACT_BEGIN
  Failure restoration at the IP layer in IP-over-WDM networks requires to map
the IP topology on the WDM topology in such a way that a failure at the WDM
layer leaves the IP topology connected. Such a mapping is called $survivable$.
As finding a survivable mapping is known to be NP-complete, in practice it
requires a heuristic approach. We have introduced in [1] a novel algorithm
called ``SMART'', that is more effective and scalable than the heuristics known
to date. Moreover, the formal analysis of SMART [2] has led to new
applications: the formal verification of the existence of a survivable mapping,
and a tool tracing and repairing the vulnerable areas of the network. In this
paper we extend the theoretical analysis in [2] by considering $multiple
failures$.

ABSTRACT_BEGIN
  We consider the problem of network coding across multiple unicasts. We give,
for wired and wireless networks, efficient polynomial time algorithms for
finding optimal network codes within the class of network codes restricted to
XOR coding between pairs of flows.

ABSTRACT_BEGIN
  Positioning systems in self-organizing networks generally rely on
measurements such as delay and received signal strength, which may be difficult
to obtain and often require dedicated equipment. An alternative to such
approaches is to use simple connectivity information, that is, the presence or
absence of a link between any pair of nodes, and to extend it to hop-counts, in
order to obtain an approximate coordinate system. Such an approximation is
sufficient for a large number of applications, such as routing. In this paper,
we propose Jumps, a positioning system for those self-organizing networks in
which other types of (exact) positioning systems cannot be used or are deemed
to be too costly. Jumps builds a multiple coordinate system based solely on
nodes neighborhood knowledge. Jumps is interesting in the context of wireless
sensor networks, as it neither requires additional embedded equipment nor
relies on any nodes capabilities. While other approaches use only three
hop-count measurements to infer the position of a node, Jumps uses an arbitrary
number. We observe that an increase in the number of measurements leads to an
improvement in the localization process, without requiring a high dense
environment. We show through simulations that Jumps, when compared with
existing approaches, reduces the number of nodes sharing the same coordinates,
which paves the way for functions such as position-based routing.

ABSTRACT_BEGIN
  This paper has been withdrawn by the author due to the need for further
revision.

ABSTRACT_BEGIN
  We present a unified analytical framework within which power control, rate
allocation, routing, and congestion control for wireless networks can be
optimized in a coherent and integrated manner. We consider a multi-commodity
flow model with an interference-limited physical-layer scheme in which power
control and routing variables are chosen to minimize the sum of convex link
costs reflecting, for instance, queuing delay. Distributed network algorithms
where joint power control and routing are performed on a node-by-node basis are
presented. We show that with appropriately chosen parameters, these algorithms
iteratively converge to the global optimum from any initial point with finite
cost. Next, we study refinements of the algorithms for more accurate link
capacity models, and extend the results to wireless networks where the
physical-layer achievable rate region is given by an arbitrary convex set, and
the link costs are strictly quasiconvex. Finally, we demonstrate that
congestion control can be seamlessly incorporated into our framework, so that
algorithms developed for power control and routing can naturally be extended to
optimize user input rates.

ABSTRACT_BEGIN
  To bring coherence between Wireless Access Protocol (WAP) and Hyper Text
Transfer Protocol (HTTP), in this paper, we have proposed an enhanced Internet
framework, which incorporates a new markup language and a browser compatible
with both of the access control protocols. This Markup Language and the browser
enables co-existence of both Hyper Text Markup Language (HTML) and Wireless
Markup Language (WML) contents in a single source file, whereas the browser
incorporates the ability to hold contents compliant with both HTTP and WAP. The
proposed framework also bridges the security gap that is present in the
existing mobile Internet framework.
  Keywords: WAP, WML, HTTP, HTML, browser, parser, wireless devices.

ABSTRACT_BEGIN
  Conventional IP routing protocols are not suitable for multimedia
applications which have very stringent Quality-of-Service (QoS) demands and
they require a connection oriented service. For multimedia applications it is
expected that the router should be able to forward the packet according to the
demand of the packet and it is necessary to find a path that satisfies the
specific demands of a particular application. In order to address these issues,
in this paper, we have presented a QoS aware IP routing protocol where a router
stores information about the QoS parameters and routes the packet accordingly.
  Keywords: IP Routing Protocol, Quality of Service (QoS) parameter, QoSIP,
Selective Flooding.

ABSTRACT_BEGIN
  Recent proposals in multicast overlay construction have demonstrated the
importance of exploiting underlying network topology. However, these
topology-aware proposals often rely on incremental and periodic refinements to
improve the system performance. These approaches are therefore neither
scalable, as they induce high communication cost due to refinement overhead,
nor efficient because long convergence time is necessary to obtain a stabilized
structure. In this paper, we propose a highly scalable locating algorithm that
gradually directs newcomers to their a set of their closest nodes without
inducing high overhead. On the basis of this locating process, we build a
robust and scalable topology-aware clustered hierarchical overlay scheme,
called LCC. We conducted both simulations and PlanetLab experiments to evaluate
the performance of LCC. Results show that the locating process entails modest
resources in terms of time and bandwidth. Moreover, LCC demonstrates promising
performance to support large scale multicast applications.

ABSTRACT_BEGIN
  Geographic routing provides relatively good performance at a much lower
overhead than conventional routing protocols such as AODV. However, the
performance of these protocols is impacted by physical voids, and localization
errors. Accordingly, virtual coordinate systems (VCS) were proposed as an
alternative approach that is resilient to localization errors and that
naturally routes around physical voids. However, we show that VCS is vulnerable
to different forms of the void problem and the performance of greedy routing on
VCS is worse than that of geographic forwarding. We show that these anomalies
are due to the integral nature of VCS, which causes quantization noise in the
estimate of connectivity and node location. We propose an aligned virtual
coordinate system (AVCS) on which the greedy routing success can be
significantly improved. With our approach, and for the first time, we show that
greedy routing on VCS out-performs that on physical coordinate systems even in
the absence of localization errors. We compare AVCS against some of the most
popular geographical routing protocols both on physical coordinate system and
the virtual coordinate systems and show that AVCS significantly improves
performance over the best known solutions.

ABSTRACT_BEGIN
  Traceroute is a networking tool that allows one to discover the path that
packets take from a source machine, through the network, to a destination
machine. It is widely used as an engineering tool, and also as a scientific
tool, such as for discovery of the network topology at the IP level. In prior
work, authors on this technical report have shown how to improve the efficiency
of route tracing from multiple cooperating monitors. However, it is not unusual
for a route tracing monitor to operate in isolation. Somewhat different
strategies are required for this case, and this report is the first systematic
study of those requirements. Standard traceroute is inefficient when used
repeatedly towards multiple destinations, as it repeatedly probes the same
interfaces close to the source. Others have recognized this inefficiency and
have proposed tracing backwards from the destinations and stopping probing upon
encounter with a previously-seen interface. One of this technical report's
contributions is to quantify for the first time the efficiency of this
approach. Another contribution is to describe the effect of non-responding
destinations on this efficiency. Since a large portion of destination machines
do not reply to probe packets, backwards probing from the destination is often
infeasible. We propose an algorithm to tackle non-responding destinations, and
we find that our algorithm can strongly decrease probing redundancy at the cost
of a small reduction in node and link discovery.

ABSTRACT_BEGIN
  This paper revisits the issue of route discovery in dynamic source routing
(DSR) for mobile ad hoc networks (MANETs), and puts forward a proposal of a
lightweight non-optimal route suppression technique based on the observation of
a rarely noted but commonly occurring phenomenon in route discovery. The
technique exploits the observed phenomenon to extract query state information
that permits intermediate nodes to identify and suppress the initiation of
route replies with non-optimal routes, even if the route query is received for
the first time. A detailed evaluation of DSR with non-optimal route suppression
is found to yield significant improvements in both protocol efficiency and
performance.

ABSTRACT_BEGIN
  One vision of future wireless networks is that they will be deeply integrated
and embedded in our lives and will involve the use of personalized mobile
devices. User behavior in such networks is bound to affect the network
performance. It is imperative to study and characterize the fundamental
structure of wireless user behavior in order to model, manage, leverage and
design efficient mobile networks. It is also important to make such study as
realistic as possible, based on extensive measurements collected from existing
deployed wireless networks.
  In this study, using our systematic TRACE approach, we analyze wireless
users' behavioral patterns by extensively mining wireless network logs from two
major university campuses. We represent the data using location preference
vectors, and utilize unsupervised learning (clustering) to classify trends in
user behavior using novel similarity metrics. Matrix decomposition techniques
are used to identify (and differentiate between) major patterns. While our
findings validate intuitive repetitive behavioral trends and user grouping, it
is surprising to find the qualitative commonalities of user behaviors from the
two universities. We discover multi-modal user behavior for more than 60% of
the users, and there are hundreds of distinct groups with unique behavioral
patterns in both campuses. The sizes of the major groups follow a power-law
distribution. Our methods and findings provide an essential step towards
network management and behavior-aware network protocols and applications, to
name a few.

ABSTRACT_BEGIN
  In the context of ambient networks where each small device must trust its
neighborhood rather than a fixed network, we propose in this paper a
\textit{trust management framework} inspired by known social patterns and based
on the following statements: each mobile constructs itself a local level of
trust what means that it does not accept recommendation by other peers, and the
only relevant parameter, beyond some special cases discussed later, to evaluate
the level of trust is the number of common trusted mobiles. These trusted
mobiles are considered as entries in a local database called history for each
device and we use identity-based cryptography to ensure strong security:
history must be a non-tansferable object.

ABSTRACT_BEGIN
  We investigate cascade dynamics in threshold-controlled (multiplex)
propagation on random geometric networks. We find that such local dynamics can
serve as an efficient, robust, and reliable prototypical activation protocol in
sensor networks in responding to various alarm scenarios. We also consider the
same dynamics on a modified network by adding a few long-range communication
links, resulting in a small-world network. We find that such construction can
further enhance and optimize the speed of the network's response, while keeping
energy consumption at a manageable level.

ABSTRACT_BEGIN
  In grid networks, distributed resources are interconnected by wide area
network to support compute and data-intensive applications, which require
reliable and efficient transfer of gigabits (even terabits) of data. Different
from best-effort traffic in Internet, bulk data transfer in grid requires
bandwidth reservation as a fundamental service. Existing reservation schemes
such as RSVP are designed for real-time traffic specified by reservation rate,
transfer start time but with unknown lifetime. In comparison, bulk data
transfer requests are defined in terms of volume and deadline, which provide
more information, and allow more flexibility in reservation schemes, i.e.,
transfer start time can be flexibly chosen, and reservation for a single
request can be divided into multiple intervals with different reservation
rates. We define a flexible reservation framework using time-rate function
algebra, and identify a series of practical reservation scheme families with
increasing generality and potential performance, namely, FixTime-FixRate,
FixTime-FlexRate, FlexTime-FlexRate, and Multi-Interval. Simple heuristics are
used to select representative scheme from each family for performance
comparison. Simulation results show that the increasing flexibility can
potentially improve system performance, minimizing both blocking probability
and mean flow time. We also discuss the distributed implementation of proposed
framework.

ABSTRACT_BEGIN
  Where distributed agents must share voluminous set membership information,
Bloom filters provide a compact, though lossy, way for them to do so. Numerous
recent networking papers have examined the trade-offs between the bandwidth
consumed by the transmission of Bloom filters, and the error rate, which takes
the form of false positives, and which rises the more the filters are
compressed. In this paper, we introduce the retouched Bloom filter (RBF), an
extension that makes the Bloom filter more flexible by permitting the removal
of selected false positives at the expense of generating random false
negatives. We analytically show that RBFs created through a random process
maintain an overall error rate, expressed as a combination of the false
positive rate and the false negative rate, that is equal to the false positive
rate of the corresponding Bloom filters. We further provide some simple
heuristics and improved algorithms that decrease the false positive rate more
than than the corresponding increase in the false negative rate, when creating
RBFs. Finally, we demonstrate the advantages of an RBF over a Bloom filter in a
distributed network topology measurement application, where information about
large stop sets must be shared among route tracing monitors.

ABSTRACT_BEGIN
  In the widely used 802.11 standard, the so called performance anomaly is a
well known issue. Several works have tried to solve this problem by introducing
mechanisms such as packet fragmentation, backoff adaptation, or packet
aggregation during a fixed time interval. In this paper, we propose a novel
approach solving the performance anomaly problem by packet aggregation using a
dynamic time interval, which depends on the busy time of the wireless medium.
Our solution differs from other proposition in the literature because of this
dynamic time interval, which allows increasing fairness, reactivity, and in
some cases efficiency. In this article, we emphasize the performance evaluation
of our proposal.

ABSTRACT_BEGIN
  In future mobility support will require handling roaming in heterogeneous
access networks. In order to enable seamless roaming it is necessary to
minimize the impact of the vertical handoffs. Localized mobility management
schemes such as FMIPv6 and HMIPv6 do not provide sufficient handoff
performance, since they have been designed for horizontal handoffs. In this
paper, we propose the SafetyNet protocol, which allows a Mobile Node to perform
seamless vertical handoffs. Further, we propose a handoff timing algorithm
which allows a Mobile Node to delay or even completely avoid upward vertical
handoffs. We implement the SafetyNet protocol and compare its performance with
the Fast Handovers for Mobile IPv6 protocol in our wireless test bed and
analyze the results. The experimental results indicate that the proposed
SafetyNet protocol can provide an improvement of up to 95% for TCP performance
in vertical handoffs, when compared with FMIPv6 and an improvement of 64% over
FMIPv6 with bicasting. We use numerical analysis of the protocol to show that
its signaling and data transmission overhead is comparable to Fast Mobile IPv6
and significantly smaller than that of FMIPv6 with bicasting.

ABSTRACT_BEGIN
  We study routing for massively dense wireless networks, i.e., wireless
networks that contain so many nodes that, in addition to their usual
microscopic description, a novel macroscopic description becomes possible. The
macroscopic description is not detailed, but nevertheless contains enough
information to permit a meaningful study and performance optimization of the
network. Within this context, we continue and significantly expand previous
work on the analogy between optimal routing and the propagation of light
according to the laws of Geometrical Optics. Firstly, we pose the analogy in a
more general framework than previously, notably showing how the eikonal
equation, which is the central equation of Geometrical Optics, also appears in
the networking context. Secondly, we develop a methodology for calculating the
cost function, which is the function describing the network at the macroscopic
level. We apply this methodology for two important types of networks: bandwidth
limited and energy limited.

ABSTRACT_BEGIN
  We study optimal user-network association in an integrated 802.11 WLAN and
3G-UMTS hybrid cell. Assuming saturated resource allocation on the downlink of
WLAN and UMTS networks and a single QoS class of mobiles arriving at an average
location in the hybrid cell, we formulate the problem with two different
approaches: Global and Individual optimality. The Globally optimal association
is formulated as an SMDP (Semi Markov Decision Process) connection routing
decision problem where rewards comprise a financial gain component and an
aggregate network throughput component. The corresponding Dynamic Programming
equations are solved using Value Iteration method and a stationary optimal
policy with neither convex nor concave type switching curve structure is
obtained. Threshold type and symmetric switching curves are observed for the
analogous homogenous network cases. The Individual optimality is studied under
a non-cooperative dynamic game framework with expected service time of a mobile
as the decision cost criteria. It is shown that individual optimality in a
WLAN-UMTS hybrid cell, results in a threshold policy curve of descending
staircase form with increasing Poisson arrival rate of mobiles.

ABSTRACT_BEGIN
  In 3G UMTS, two main transport channels have been provided for downlink data
transmission: a common FACH channel and a dedicated DCH channel. The
performance of TCP in UMTS depends much on the channel switching policy used.
In this paper, we propose and analyze three new basic threshold-based channel
switching policies for UMTS that we name as QS (Queue Size), FS (Flow Size) and
QSFS (QS & FS combined) policy. These policies significantly improve over a
modified threshold policy in [1] by about 17% in response time metrics. We
further propose and evaluate a new improved switching policy that we call
FS-DCH (at-least flow-size threshold on DCH) policy. This policy is biased
towards short TCP flows of few packets and is thus a cross-layer policy that
improves the performance of TCP by giving priority to the initial few packets
of a flow on the fast DCH channel. Extensive simulation results confirm this
improvement for the case when number of TCP connections is low.

ABSTRACT_BEGIN
  This article describes our strategy for deploying self-forming ad hoc
networks based on the Internet Protocol version 6 and evaluates the dynamics of
this proposal. Among others, we suggest a technique called adaptive routing
that provides secure intelligent routing capabilities to computer communication
networks. This technique uses the flow label, supports hybrid metrics, network
load sharing, and is not restricted to evaluation of performance on first hop
routers when making routing decisions. Selective anycasting is an extension to
the anycast addressing model that supports exclusion of members of groups that
perform poorly or inappropriately on a per-host basis. Distributed name lookup
is suggested for integrating self-forming and global networks where they
coexist. At last, we pose an address hierarchy to support unmanaged discovery
of services in unknown networks.

ABSTRACT_BEGIN
  The role of competition and monetary benefits in the design of Content
Delivery Networks (CDNs) is largely an unexplored area. In this paper, we
investigate the effect of competition among the competitive web based CDNs and
show that little difference in their performance may cause significant
financial gain/loss. It turns out that the economy of scale effect is very
significant for the success of a CDN in a competitive market. So CDN peering
might be a good idea. Since performance and conforming to the service level
agreement (SLA) with content providers is very important, we then focus on
designing CDN from this perspective. We provide an asymptotically optimal
static request routing policy for a CDN under a model where a CDN company
guarantees a certain level of user latency to the content providers in the SLA.

ABSTRACT_BEGIN
  In this paper, we examine the cause of the border effect observed in many
mobility models used to construct simulations of ad hoc networking protocol
performance. We specify conditions under which a node mobility model must
produce spatial mobile node distribution functions that obey the diffusion
equation. In particular demonstrate that these conditions are satisfied by the
random direction (RD) model. We show that it is possible to construct mobility
models that attain uniform steady-state distributions without resorting to
reflection or ``wrapping'' of nodes at the border of a test region. Finally, we
show that the random waypoint (RWP) model may be reproduced by the application
of a ``volume rule'' to an RD model. This volume rule violates the assumptions
that lead to the diffusion equation. We suggest a generalization of the RWP
model that can provide more uniform mobile node distributions.

ABSTRACT_BEGIN
  The performance of peer-to-peer file replication comes from its piece and
peer selection strategies. Two such strategies have been introduced by the
BitTorrent protocol: the rarest first and choke algorithms. Whereas it is
commonly admitted that BitTorrent performs well, recent studies have proposed
the replacement of the rarest first and choke algorithms in order to improve
efficiency and fairness. In this paper, we use results from real experiments to
advocate that the replacement of the rarest first and choke algorithms cannot
be justified in the context of peer-to-peer file replication in the Internet.
We instrumented a BitTorrent client and ran experiments on real torrents with
different characteristics. Our experimental evaluation is peer oriented,
instead of tracker oriented, which allows us to get detailed information on all
exchanged messages and protocol events. We go beyond the mere observation of
the good efficiency of both algorithms. We show that the rarest first algorithm
guarantees close to ideal diversity of the pieces among peers. In particular,
on our experiments, replacing the rarest first algorithm with source or network
coding solutions cannot be justified. We also show that the choke algorithm in
its latest version fosters reciprocation and is robust to free riders. In
particular, the choke algorithm is fair and its replacement with a bit level
tit-for-tat solution is not appropriate. Finally, we identify new areas of
improvements for efficient peer-to-peer file replication protocols.

ABSTRACT_BEGIN
  Prior work on routing in delay tolerant networks (DTNs) has commonly made the
assumption that each pair of nodes shares the same inter-contact time
distribution as every other pair. The main argument in this paper is that
researchers should also be looking at heterogeneous inter-contact time
distributions. We demonstrate the presence of such heterogeneity in the
often-used Dartmouth Wi-Fi data set. We also show that DTN routing can benefit
from knowing these distributions. We first introduce a new stochastic model
focusing on the inter-contact time distributions between all pairs of nodes,
which we validate on real connectivity patterns. We then analytically derive
the mean delivery time for a bundle of information traversing the network for
simple single copy routing schemes. The purpose is to examine the theoretic
impact of heterogeneous inter-contact time distributions. Finally, we show that
we can exploit this user diversity to improve routing performance.

ABSTRACT_BEGIN
  Most wireless terrestrial networks are designed based on the assumption that
the nodes are deployed on a two-dimensional (2D) plane. However, this 2D
assumption is not valid in underwater, atmospheric, or space communications. In
fact, recent interest in underwater acoustic ad hoc and sensor networks hints
at the need to understand how to design networks in 3D. Unfortunately, the
design of 3D networks is surprisingly more difficult than the design of 2D
networks. For example, proofs of Kelvin's conjecture and Kepler's conjecture
required centuries of research to achieve breakthroughs, whereas their 2D
counterparts are trivial to solve. In this paper, we consider the coverage and
connectivity issues of 3D networks, where the goal is to find a node placement
strategy with 100% sensing coverage of a 3D space, while minimizing the number
of nodes required for surveillance. Our results indicate that the use of the
Voronoi tessellation of 3D space to create truncated octahedral cells results
in the best strategy. In this truncated octahedron placement strategy, the
transmission range must be at least 1.7889 times the sensing range in order to
maintain connectivity among nodes. If the transmission range is between 1.4142
and 1.7889 times the sensing range, then a hexagonal prism placement strategy
or a rhombic dodecahedron placement strategy should be used. Although the
required number of nodes in the hexagonal prism and the rhombic dodecahedron
placement strategies is the same, this number is 43.25% higher than the number
of nodes required by the truncated octahedron placement strategy. We verify by
simulation that our placement strategies indeed guarantee ubiquitous coverage.
We believe that our approach and our results presented in this paper could be
used for extending the processes of 2D network design to 3D networks.

ABSTRACT_BEGIN
  Being motivated by recent developments in the theory of complex networks, we
examine the robustness of communication networks under intentional attack that
takes down network nodes in a decreasing order of their nodal degrees. In this
paper, we study two different effects that have been largely missed in the
existing results: (i) some communication networks, like Internet, are too large
for anyone to have global information of their topologies, which makes the
accurate intentional attack practically impossible; and (ii) most attacks in
communication networks are propagated from one node to its neighborhood
node(s), utilizing local network-topology information only. We show that
incomplete global information has different impacts to the intentional attack
in different circumstances, while local information-based attacks can be
actually highly efficient. Such insights would be helpful for the future
developments of efficient network attack/protection schemes.

ABSTRACT_BEGIN
  Ad hoc networking specific challenges foster a strong research effort on
efficient protocols design. Routing protocols based on a self-organized
structure have been studied principally for the robustness and the scalability
they provide. On the other hand, self-organization schemes may decrease the
network capacity since they concentrate the traffic on privileged links. This
paper presents four models for evaluating the capacity of a routing schemes on
802.11 like networks. Our approach consists in modeling the radio resource
sharing principles of 802.11 like MAC protocols as a set of linear constraints.
We have implemented two models of fairness. The first one assumes that nodes
have a fair access to the channel, while the second one assumes that on the
radio links. We then develop a pessimistic and an optimistic scenarii of
spatial re-utilization of the medium, yielding a lower bound and an upper bound
on the network capacity for each fairness case. Our models are independent of
the routing protocols and provide therefore a relevant framework for their
comparison. We apply our models to a comparative analysis of the well-known
shortest path base flat routing protocol OLSR against two main self-organized
structure approaches, VSR, and Wu & Li's protocols. This study concludes on the
relevance of self-organized approaches from the network capacity point of view.

ABSTRACT_BEGIN
  In hierarchical reliable multicast environment, makespan is the time that is
required to fully and successfully transmit a packet from the sender to all
receivers. Low makespan is vital for achieving high throughput with a TCP-like
window based sending scheme. In hierarchical reliable multicast methods, the
number of repair servers and their locations influence the makespan. In this
paper we propose a new method to decide the locations of repair servers that
can reduce the makespan in hierarchical reliable multicast networks. Our method
has a formulation based on mixed integer programming to analyze the makespan
minimization problem. A notable aspect of the formulation is that heterogeneous
links and packet losses are taken into account in the formulation. Three
different heuristics are presented to find the locations of repair servers in
reasonable time in the formulation. Through simulations, three heuristics are
carefully analyzed and compared on networks with different sizes. We also
evaluate our proposals on PGM (Pragmatic General Multicast) reliable multicast
protocol using ns-2 simulation. The results show that the our best heuristic is
close to the lower bound by a factor of 2.3 in terms of makespan and by a
factor of 5.5 in terms of the number of repair servers.

ABSTRACT_BEGIN
  The Maximum Differential Backlog (MDB) control policy of Tassiulas and
Ephremides has been shown to adaptively maximize the stable throughput of
multi-hop wireless networks with random traffic arrivals and queueing. The
practical implementation of the MDB policy in wireless networks with mutually
interfering links, however, requires the development of distributed
optimization algorithms. Within the context of CDMA-based multi-hop wireless
networks, we develop a set of node-based scaled gradient projection power
control algorithms which solves the MDB optimization problem in a distributed
manner using low communication overhead. As these algorithms require time to
converge to a neighborhood of the optimum, the optimal rates determined by the
MDB policy can only be found iteratively over time. For this, we show that the
iterative MDB policy with convergence time remains throughput optimal.

ABSTRACT_BEGIN
  In this article, we first provide a taxonomy of dynamic spectrum access. We
then focus on opportunistic spectrum access, the overlay approach under the
hierarchical access model of dynamic spectrum access. we aim to provide an
overview of challenges and recent developments in both technological and
regulatory aspects of opportunistic spectrum access.

ABSTRACT_BEGIN
  The objective of this paper is to propose models enabling to study the
behaviour of Ethernet switch for Networked Control Systems. Two scheduler
policies are analyzed: the static priority and the WRR (Weighted Round Robin).
The modelling work is based on Coloured Petri Nets. A temporal validation step
based on the simulation of these modelling, shows that the obtained results are
near to the expected behaviour of these scheduler policies.

ABSTRACT_BEGIN
  Recent interest in networked control systems (NCS) has instigated research in
both communication networks and control. Analysis of NCSs has usually been
performed from either the network or the control point of view, but not many
papers exist where the analysis of both is done in the same context. In this
paper an overall analysis of the networked control system is presented. First,
the procedure of obtaining the upper bound delay value for packet transmission
in the switched Ethernet network is presented. Next, the obtained delay
estimate is utilised in delay compensation for improving the Quality of
Performance (QoP) of the control systems. The presented upper bound delay
algorithm applies ideas from network calculus theory. For the improvement of
QoP, two delay compensation strategies, the Smith predictor based and the
robust control based delay compensation strategies, are presented and compared.

ABSTRACT_BEGIN
  Recent interest in networked control systems (NCS) has instigated research in
various areas of both communication networks and control. The analysis of NCS
has often been performed either from the network, or the control point of view
and not many papers exist were the analysis of both is done in the same
context. Here a simple overall analysis is presented. In the paper the
procedure of obtaining the upper bound delay value in the switched Ethernet
network is proposed and the obtained delay estimate is used in stability
analysis of the feedback loop and in the control compensation. The upper bound
delay algorithm is based on the network calculus theory, the stability analysis
uses the small gain theorem, and control compensating strategy is based on
Smith predictor, where however the upper bound delay is utilised in obtaining
the delay estimate.

ABSTRACT_BEGIN
  The work described in this paper is a contribution to the problems of
managing in data-intensive scientific applications. First, we discuss
scientific workflows and motivate there use in scientific applications. Then,
we introduce the concept of cooperative processes and describe their
interactions and uses in a flexible cooperative workflow system called
\textit{Bonita}. Finally, we propose an approach to integrate and synthesize
the data exchanged by the mapping of data-intensive science into Bonita, using
a binary approach, and illustrate the endeavors done to enhance the performance
computations within a dynamic environment.

ABSTRACT_BEGIN
  The Networked Control Systems (NCS) are complex systems which integrate
information provided by several domians such as automatic control, computer
science, communication network. The work presented in this paper concerns fault
detection, isolation and compensation of communication network. The proposed
method is based on the classical approach of Fault Detection and Isolation and
Fault Tolerant Control (FDI/FTC) currently used in diagnosis. The modelling of
the network to be supervised is based on both couloured petri nets and network
calculus theory often used to represent and analyse the network behaviour. The
goal is to implement inside network devices algorithms enabling to detect,
isolate and compensate communication faults in an autonomous way.

ABSTRACT_BEGIN
  This paper discusses a method for pipelining the calculation of CRC's, such
as ITU/CCITT CRC32, into a mostly feed-forward architecture. This method allows
several benefits such as independent scaling of circuit frequency and data
throughput. Additionally it allows calculation over packet tails (packet length
not a multiple of CRC input width). Finally it offers the ability to update a
CRC where a subset of data in the packet has changed.

ABSTRACT_BEGIN
  High-speed photonic switching networks can switch optical signals at the rate
of several terabits per second. However, they suffer from an intrinsic
crosstalk problem when two optical signals cross at the same switch element. To
avoid crosstalk, active connections must be node-disjoint in the switching
network. In this paper, we propose a sequence of decomposition and merge
operations, called conjugate transformation, performed on each switch element
to tackle this problem. The network resulting from this transformation is
called conjugate network. By using the numbering-schemes of networks, we prove
that if the route assignments in the original network are link-disjoint, their
corresponding ones in the conjugate network would be node-disjoint. Thus,
traditional nonblocking switching networks can be transformed into
crosstalk-free optical switches in a routine manner. Furthermore, we show that
crosstalk-free multicast switches can also be obtained from existing
nonblocking multicast switches via the same conjugate transformation.

ABSTRACT_BEGIN
  Le standard IEEE 802.11 est inefficace pour la transmission multim\'{e}dia en
multipoint. En particulier, les paquets multipoints sont envoy\'{e}s en boucle
ouverte de la m\^{e}me mani\`{e}re que les paquets broadcast. L'absence
d'acquittements rend impossible la mise en oeuvre de m\'{e}canismes de
contr\^{o}le de congestion, de m\'{e}canisme de fiabilisation de la
transmission ainsi que d'algorithmes d'adaptation du d\'{e}bit de transmission
physique. Dans ce rapport, nous proposons de nouveaux m\'{e}canismes de
ransmission multipoint qui se basent sur une approche leader pour renvoyer des
acquittements. Nous nous interessons \`{a} des solutions pratiques qui sont
suceptibles d'\^{e}tre implant\'{e}s dans les cartes r\'{e}seaux sans fil
actuelles et futures et qui restent compatibles avec les stations IEEE 802.11
standards. Nous proposons deux m\'{e}canismes pour adapter le d\'{e}bit de
transmission physique des flots multipoints: un m\'{e}canisme simplifi\'{e}
appel\'{e} LB-ARF et un m\'{e}canisme plus robuste appel\'{e} RRAM. Nos
simulations montrent que pour des environnements statiques, un m\'{e}canisme
aussi simple que LB-ARF suffit pour obtenir de bonnes performances. Le
m\'{e}canisme RRAM est quant \`{a} lui aussi efficace dans des environnements
statiques que lorsque les stations sont mobiles.

ABSTRACT_BEGIN
  This paper introduces Prawn, a tool for prototyping communication protocols
over IEEE 802.11 networks. Prawn allows researchers to conduct both functional
assessment and performance evaluation as an inherent part of the protocol
design process. Since Prawn runs on real IEEE 802.11 nodes, prototypes can be
evaluated and adjusted under realistic conditions. Once the prototype has been
extensively tested and thoroughly validated, and its functional design tuned
accordingly, it is then ready for implementation. Prawn facilitates prototype
development by providing: (i) a set of building blocks that implement common
functions needed by a wide range of wireless protocols (e.g., neighbor
discovery, link quality assessment, message transmission and reception), and
(ii) an API that allows protocol designers to access Prawn primitives. We show
through a number of case studies how Prawn supports prototyping as part of
protocol design and, as a result of enabling deployment and testing under
real-world scenarios, how Prawn provides useful feedback on protocol operation
and performance.

ABSTRACT_BEGIN
  We introduce an application of a mobile transient network architecture on top
of the current Internet. This paper is an application extension to a conceptual
mobile network architecture. It attempts to specifically reinforce some of the
powerful notions exposed by the architecture from an application perspective.
Of these notions, we explore the network expansion layer, an overlay of
components and services, that enables a persistent identification network and
other required services. The overlay abstraction introduces several benefits of
which mobility and communication across heterogenous network structures are of
interest to this paper. We present implementations of several components and
protocols including gateways, Agents and the Open Device Access Protocol. Our
present identification network implementation exploits the current
implementation of the Handle System through the use of distributed, global and
persistent identifiers called handles. Handles are used to identify and locate
devices and services abstracting any physical location or network association
from the communicating ends. A communication framework is finally demonstrated
that would allow for mobile devices on the public Internet to have persistent
identifiers and thus be persistently accessible either directly or indirectly.
This application expands IP inter-operability beyond its current boundaries.

ABSTRACT_BEGIN
  VoIP applications are emerging today as an important component in business
and communication industry. In this paper, we address the intrusion detection
and prevention in VoIP networks and describe how a conceptual solution based on
the Bayes inference approach can be used to reinforce the existent security
mechanisms. Our approach is based on network monitoring and analyzing of the
VoIP-specific traffic. We give a detailed example on attack detection using the
SIP signaling protocol.

ABSTRACT_BEGIN
  The advances in WDM technology lead to the great interest in traffic grooming
problems. As traffic often changes from time to time, the problem of grooming
dynamic traffic is of great practical value. In this paper, we discuss dynamic
grooming of traffic in star and tree networks. A genetic algorithm (GA) based
approach is proposed to support arbitrary dynamic traffic patterns, which
minimizes the number of ADM's and wavelengths. To evaluate the algorithm,
tighter bounds are derived. Computer simulation results show that our algorithm
is efficient in reducing both the numbers of ADM's and wavelengths in tree and
star networks.

ABSTRACT_BEGIN
  Traffic grooming is widely employed to reduce the number of ADM's and
wavelengths. We consider the problem of grooming of dynamic traffic in WDM tree
and star networks in this paper. To achieve better results, we used the
bifurcation techniques to the grooming of arbitrary dynamic traffic in a
strictly non-blocking manner in networks. Three splitting methods, including
Traffic-Cutting, Traffic-Dividing and Synthesized-Splitting were proposed. A
genetic algorithm (GA) approach based on these methods was proposed to tackle
such grooming problems in tree and star networks. The performance of these
algorithms was tested under different conditions in star and tree networks.
Computer simulation results showed that our algorithm is efficient in reducing
both the numbers of ADM's and wavelengths.

ABSTRACT_BEGIN
  Whoever has had his cell phone stolen knows how frustrating it is to be
unable to get his contact list back. To avoid data loss when losing or
destroying a mobile device like a PDA or a cell phone, data is usually
backed-up to a fixed station. However, in the time between the last backup and
the failure, important data can have been produced and then lost. To handle
this issue, we propose a transparent collaborative backup system. Indeed, by
saving data on other mobile devices between two connections to a global
infrastructure, we can resist to such scenarios. In this paper, after a general
description of such a system, we present a way to replicate data on mobile
devices to attain a prerequired resilience for the backup.

ABSTRACT_BEGIN
  In this document, we compare three existing simulation platforms (OPNET
Modeler, Network Simulator 2, Georgia Tech Sensor Network Simulator). Our
comparative study focuses on ease of use, scalability, ease of implementing
power consumption model and physical layer modeling accuracy, mainly.
Conclusions of this study are presented, and will help us decide which
simulating environment to use for evaluating power-aware self-organizing sensor
networks protocols.

ABSTRACT_BEGIN
  Peer-to-peer protocols play an increasingly instrumental role in Internet
content distribution. Consequently, it is important to gain a full
understanding of how these protocols behave in practice and how their
parameters impact overall performance. We present the first experimental
investigation of the peer selection strategy of the popular BitTorrent protocol
in an instrumented private torrent. By observing the decisions of more than 40
nodes, we validate three BitTorrent properties that, though widely believed to
hold, have not been demonstrated experimentally. These include the clustering
of similar-bandwidth peers, the effectiveness of BitTorrent's sharing
incentives, and the peers' high average upload utilization. In addition, our
results show that BitTorrent's new choking algorithm in seed state provides
uniform service to all peers, and that an underprovisioned initial seed leads
to the absence of peer clustering and less effective sharing incentives. Based
on our observations, we provide guidelines for seed provisioning by content
providers, and discuss a tracker protocol extension that addresses an
identified limitation of the protocol.

ABSTRACT_BEGIN
  The distributed control systems are more and more used in many industrial
applications. These systems are often referred as "Networked control systems".
The goal of this paper is to show the network influence on feedback control
systems. Two networks are considered: Switched Ethernet network and CAN
fieldbus. The first one represents the non deterministic network and second one
represents the deterministic one. Several scenarii are studied to analyse the
stability of system according to different network parameters (packets losses,
congestion and frame priority). The Truetime simulator is used in this work.

ABSTRACT_BEGIN
  It is widely believed that the Internet's AS-graph degree distribution obeys
a power-law form. Most of the evidence showing the power-law distribution is
based on BGP data. However, it was recently argued that since BGP collects data
in a tree-like fashion, it only produces a sample of the degree distribution,
and this sample may be biased. This argument was backed by simulation data and
mathematical analysis, which demonstrated that under certain conditions a tree
sampling procedure can produce an artificail power-law in the degree
distribution. Thus, although the observed degree distribution of the AS-graph
follows a power-law, this phenomenon may be an artifact of the sampling
process. In this work we provide some evidence to the contrary. We show, by
analysis and simulation, that when the underlying graph degree distribution
obeys a power-law with an exponent larger than 2, a tree-like sampling process
produces a negligible bias in the sampled degree distribution. Furthermore,
recent data collected from the DIMES project, which is not based on BGP
sampling, indicates that the underlying AS-graph indeed obeys a power-law
degree distribution with an exponent larger than 2. By combining this empirical
data with our analysis, we conclude that the bias in the degree distribution
calculated from BGP data is negligible.

ABSTRACT_BEGIN
  This article studies disruption tolerant networks (DTNs) where each node
knows the probabilistic distribution of contacts with other nodes. It proposes
a framework that allows one to formalize the behaviour of such a network. It
generalizes extreme cases that have been studied before where (a) either nodes
only know their contact frequency with each other or (b) they have a perfect
knowledge of who meets who and when. This paper then gives an example of how
this framework can be used; it shows how one can find a packet forwarding
algorithm optimized to meet the 'delay/bandwidth consumption' trade-off:
packets are duplicated so as to (statistically) guarantee a given delay or
delivery probability, but not too much so as to reduce the bandwidth, energy,
and memory consumption.

ABSTRACT_BEGIN
  Internet topology analysis has recently experienced a surge of interest in
computer science, physics, and the mathematical sciences. However, researchers
from these different disciplines tend to approach the same problem from
different angles. As a result, the field of Internet topology analysis and
modeling must untangle sets of inconsistent findings, conflicting claims, and
contradicting statements.
  On May 10-12, 2006, CAIDA hosted the Workshop on Internet topology (WIT). By
bringing together a group of researchers spanning the areas of computer
science, physics, and the mathematical sciences, the workshop aimed to improve
communication across these scientific disciplines, enable interdisciplinary
crossfertilization, identify commonalities in the different approaches, promote
synergy where it exists, and utilize the richness that results from exploring
similar problems from multiple perspectives.
  This report describes the findings of the workshop, outlines a set of
relevant open research problems identified by participants, and concludes with
recommendations that can benefit all scientific communities interested in
Internet topology research.

ABSTRACT_BEGIN
  Assessing mobility in a thorough fashion is a crucial step toward more
efficient mobile network design. Recent research on mobility has focused on two
main points: analyzing models and studying their impact on data transport.
These works investigate the consequences of mobility. In this paper, instead,
we focus on the causes of mobility. Starting from established research in
sociology, we propose SIMPS, a mobility model of human crowd motion. This model
defines two complimentary behaviors, namely socialize and isolate, that
regulate an individual with regard to her/his own sociability level. SIMPS
leads to results that agree with scaling laws observed both in small-scale and
large-scale human motion. Although our model defines only two simple individual
behaviors, we observe many emerging collective behaviors (group
formation/splitting, path formation, and evolution). To our knowledge, SIMPS is
the first model in the networking community that tackles the roots governing
mobility.

ABSTRACT_BEGIN
  Distributed Denial-of-Service (DDoS) attacks are a major problem in the
Internet today. In one form of a DDoS attack, a large number of compromised
hosts send unwanted traffic to the victim, thus exhausting the resources of the
victim and preventing it from serving its legitimate clients. One of the main
mechanisms that have been proposed to deal with DDoS is filtering, which allows
routers to selectively block unwanted traffic. Given the magnitude of DDoS
attacks and the high cost of filters in the routers today, the successful
mitigation of a DDoS attack using filtering crucially depends on the efficient
allocation of filtering resources. In this paper, we consider a single router,
typically the gateway of the victim, with a limited number of available
filters. We study how to optimally allocate filters to attack sources, or
entire domains of attack sources, so as to maximize the amount of good traffic
preserved, under a constraint on the number of filters. We formulate the
problem as an optimization problem and solve it optimally using dynamic
programming, study the properties of the optimal allocation, experiment with a
simple heuristic and evaluate our solutions for a range of realistic
attack-scenarios. First, we look at a single-tier where the collateral damage
is high due to the filtering at the granularity of domains. Second, we look at
the two-tier problem where we have an additional constraint on the number of
filters and the filtering is performed on the granularity of attackers and
domains.

ABSTRACT_BEGIN
  We introduce a model for decentralized networks with collaborating peers. The
model is based on the stable matching theory which is applied to systems with a
global ranking utility function. We consider the dynamics of peers searching
for efficient collaborators and we prove that a unique stable solution exists.
We prove that the system converges towards the stable solution and analyze its
speed of convergence. We also study the stratification properties of the model,
both when all collaborations are possible and for random possible
collaborations. We present the corresponding fluid limit on the choice of
collaborators in the random case. As a practical example, we study the
BitTorrent Tit-for-Tat policy. For this system, our model provides an
interesting insight on peer download rates and a possible way to optimize peer
strategy.

ABSTRACT_BEGIN
  Networked Controlled Systems (NCSs) are more and more used in industrial
applications. They are strongly connected to real-time constraints because
important delays induced by the network can lead to an unstable process
control. Usually, the network used in NCSs is shared with many others
applications requiring different Quality of Service. The objective of this
paper is to optimize the tuning of the network scheduling mechanisms in taking
into account the level of Quality of Control. The goal is to maximize the
bandwidth allocation for unconstrained frames in guarantying that the control
constraints are respected. In this paper, we focus on switched Ethernet network
implementing the Classification of Service (IEEE 802.1p) based on a Weighted
Round Robin policy.

ABSTRACT_BEGIN
  In a wireless network, mobile nodes (MNs) repeatedly perform tasks such as
layer 2 (L2) handoff, layer 3 (L3) handoff and authentication. These tasks are
critical, particularly for real-time applications such as VoIP. We propose a
novel approach, namely Cooperative Roaming (CR), in which MNs can collaborate
with each other and share useful information about the network in which they
move. We show how we can achieve seamless L2 and L3 handoffs regardless of the
authentication mechanism used and without any changes to either the
infrastructure or the protocol. In particular, we provide a working
implementation of CR and show how, with CR, MNs can achieve a total L2+L3
handoff time of less than 16 ms in an open network and of about 21 ms in an
IEEE 802.11i network. We consider behaviors typical of IEEE 802.11 networks,
although many of the concepts and problems addressed here apply to any kind of
mobile network.

ABSTRACT_BEGIN
  It is now commonly accepted that the unit disk graph used to model the
physical layer in wireless networks does not reflect real radio transmissions,
and that the lognormal shadowing model better suits to experimental
simulations. Previous work on realistic scenarios focused on unicast, while
broadcast requirements are fundamentally different and cannot be derived from
unicast case. Therefore, broadcast protocols must be adapted in order to still
be efficient under realistic assumptions. In this paper, we study the
well-known multipoint relay protocol (MPR). In the latter, each node has to
choose a set of neighbors to act as relays in order to cover the whole 2-hop
neighborhood. We give experimental results showing that the original method
provided to select the set of relays does not give good results with the
realistic model. We also provide three new heuristics in replacement and their
performances which demonstrate that they better suit to the considered model.
The first one maximizes the probability of correct reception between the node
and the considered relays multiplied by their coverage in the 2-hop
neighborhood. The second one replaces the coverage by the average of the
probabilities of correct reception between the considered neighbor and the
2-hop neighbors it covers. Finally, the third heuristic keeps the same concept
as the second one, but tries to maximize the coverage level of the 2-hop
neighborhood: 2-hop neighbors are still being considered as uncovered while
their coverage level is not higher than a given coverage threshold, many
neighbors may thus be selected to cover the same 2-hop neighbors.

ABSTRACT_BEGIN
  It is well-known that wide-area networks face today several performance and
reliability problems. In this work, we propose to solve these problems by
connecting two or more local-area networks together via a Redundant Array of
Internet Links (or RAIL) and by proactively replicating each packet over these
links. In that sense, RAIL is for networks what RAID (Redundant Array of
Inexpensive Disks) was for disks. In this paper, we describe the RAIL approach,
present our prototype (called the RAILedge), and evaluate its performance.
First, we demonstrate that using multiple Internet links significantly improves
the end-to-end performance in terms of network-level as well as
application-level metrics for Voice-over-IP and TCP. Second, we show that a
delay padding mechanism is needed to complement RAIL when there is significant
delay disparity between the paths. Third, we show that two paths provide most
of the benefit, if carefully managed. Finally, we discuss a RAIL-network
architecture, where RAILedges make use of path redundancy, route control and
application-specific mechanisms, to improve WAN performance.

ABSTRACT_BEGIN
  We consider the RIPE WHOIS Internet data as characterized by the Cooperative
Association for Internet Data Analysis (CAIDA), and show that the Tempered
Preferential Attachment model [1] provides an excellent fit to this data.
  [1] D'Souza, Borgs, Chayes, Berger and Kleinberg, to appear PNAS USA, 2007.

ABSTRACT_BEGIN
  In this paper, the topology dynamic of Gnutella is studied through phase
space. The dynamic changes in peer degree are studied as a time series in two
dimensional phase space which is defined as the number of connected leaves and
the number of connected ultras. The reported degrees are concentrated in three
special Software related regions that we named as Ultra Stable Region, Leaf
Stable Region and Transition Belt. A method is proposed to classify degree
traces in phase space into different classes. Connection churn then is studied
along with the churn in degree. It shows that the topological structure of
Gnutella is rather stable in its connection degree but not the topology itself.
The connection drop rate is estimated and the live time of connections is
inferred afterwards. M/M/m/m loss queue system is introduced to model the
degree keeping process in Gnutella. This model revealed that the degree stable
is ensured by large new connection efforts. In other words the stable in
topological structure of Gnutella is a results of essential unstable in its
topology. That opens a challenge to the basic design philosophy of this
network.

ABSTRACT_BEGIN
  This dissertation has extensively looked into all aspects of VoIP
commu-nications technology, and information presented in preceding chapters,
which build up a solid framework to discuss the conceptual design model, and
investigate features that could be incorporated for actual Pro-jects, with
parameters that are tested on field values. The dissertation follows a
five-course model, for answering different questions, both tech-nical and
businesslike, around central issues, that have been crucial to explanation of
the topic; starting with a general overview of VoIP tech-nology, analyzing
current VoIP encryption methods, identifying security threats, designing a
robust VoIP system based on particulars discussed in preceding chapters, and
finally, a VoIP simulation.

ABSTRACT_BEGIN
  The aim of this technical report is to give a short overview of known
techniques for network tomography (introduced in the paper of Vardi (1996)),
extended by a Bayesian approach originating Tebaldi and West (1998). Since the
studies of A.K. Erlang (1878-1929) on telephone networks in the last
millennium, lots of needs are seen in todays applications of networks and
network tomography, so for instance networks are a critical component of the
information structure supporting finance, commerce and even civil and national
defence. An attack on a network can be performed as an intrusion in the network
or as sending a lot of fault information and disturbing the network flow. Such
attacks can be detected by modelling the traffic flows in a network, by
counting the source destination packets and even by measuring counts over time
and by drawing a comparison with this 'time series' for instance.

ABSTRACT_BEGIN
  We address the design of opportunistic spectrum access (OSA) strategies that
allow secondary users to independently search for and exploit instantaneous
spectrum availability. Integrated in the joint design are three basic
components: a spectrum sensor that identifies spectrum opportunities, a sensing
strategy that determines which channels in the spectrum to sense, and an access
strategy that decides whether to access based on imperfect sensing outcomes.
  We formulate the joint PHY-MAC design of OSA as a constrained partially
observable Markov decision process (POMDP). Constrained POMDPs generally
require randomized policies to achieve optimality, which are often intractable.
By exploiting the rich structure of the underlying problem, we establish a
separation principle for the joint design of OSA. This separation principle
reveals the optimality of myopic policies for the design of the spectrum sensor
and the access strategy, leading to closed-form optimal solutions. Furthermore,
decoupling the design of the sensing strategy from that of the spectrum sensor
and the access strategy, the separation principle reduces the constrained POMDP
to an unconstrained one, which admits deterministic optimal policies. Numerical
examples are provided to study the design tradeoffs, the interaction between
the spectrum sensor and the sensing and access strategies, and the robustness
of the ensuing design to model mismatch.

ABSTRACT_BEGIN
  We develop bounds on the capacity of wireless networks when the traffic is
non-uniform, i.e., not all nodes are required to receive and send similar
volumes of traffic. Our results are asymptotic, i.e., they hold with
probability going to unity as the number of nodes goes to infinity. We study
\emph{(i)} asymmetric networks, where the numbers of sources and destinations
of traffic are unequal, \emph{(ii)} multicast networks, in which each created
packet has multiple destinations, \emph{(iii)} cluster networks, that consist
of clients and a limited number of cluster heads, and each client wants to
communicate with any of the cluster heads, and \emph{(iv)} hybrid networks, in
which the nodes are supported by a limited infrastructure. Our findings
quantify the fundamental capabilities of these wireless networks to handle
traffic bottlenecks, and point to correct design principles that achieve the
capacity without resorting to overly complicated protocols.

ABSTRACT_BEGIN
  We study the interaction between the AIMD (Additive Increase Multiplicative
Decrease) congestion control and a bottleneck router with Drop Tail buffer. We
consider the problem in the framework of deterministic hybrid models. First, we
show that the hybrid model of the interaction between the AIMD congestion
control and bottleneck router always converges to a cyclic behavior. We
characterize the cycles. Necessary and sufficient conditions for the absence of
multiple jumps of congestion window in the same cycle are obtained. Then, we
propose an analytical framework for the optimal choice of the router buffer
size. We formulate the problem of the optimal router buffer size as a
multi-criteria optimization problem, in which the Lagrange function corresponds
to a linear combination of the average goodput and the average delay in the
queue. The solution to the optimization problem provides further evidence that
the buffer size should be reduced in the presence of traffic aggregation. Our
analytical results are confirmed by simulations performed with Simulink and the
NS simulator.

ABSTRACT_BEGIN
  Peer-to-peer protocols play an increasingly instrumental role in Internet
content distribution. It is therefore important to gain a complete
understanding of how these protocols behave in practice and how their operating
parameters affect overall system performance. This paper presents the first
detailed experimental investigation of the peer selection strategy in the
popular BitTorrent protocol. By observing more than 40 nodes in instrumented
private torrents, we validate three protocol properties that, though believed
to hold, have not been previously demonstrated experimentally: the clustering
of similar-bandwidth peers, the effectiveness of BitTorrent's sharing
incentives, and the peers' high uplink utilization. In addition, we observe
that BitTorrent's modified choking algorithm in seed state provides uniform
service to all peers, and that an underprovisioned initial seed leads to
absence of peer clustering and less effective sharing incentives. Based on our
results, we provide guidelines for seed provisioning by content providers, and
discuss a tracker protocol extension that addresses an identified limitation of
the protocol.

ABSTRACT_BEGIN
  Wireless LAN is currently enjoying rapid deployment in University
departments, business offices, hospitals and homes. It becomes an inexpensive
technology and allows multiple numbers of the households to simultaneously
access the internet while roaming about the house. In the present work, the
design and development of a wireless LAN is highlighted which utilizes direct
sequence spread spectrum (DSSS) technology at 900MHz RF carrier frequency in
its physical layer. This provides enormous security in the physical layer and
hence it is very difficult to hack or jam the network. The installation cost is
also less due to the use of 900 MHz RF carrier frequency..

ABSTRACT_BEGIN
  Since the proposition of Quality of Service architectures by the IETF, the
interaction between TCP and the QoS services has been intensively studied. This
paper proposes to look forward to the results obtained in terms of TCP
throughput guarantee in the DiffServ Assured Forwarding (DiffServ/AF) service
and to present an overview of the different proposals to solve the problem. It
has been demonstrated that the standardized IETF DiffServ conditioners such as
the token bucket color marker and the time sliding window color maker were not
good TCP traffic descriptors. Starting with this point, several propositions
have been made and most of them presents new marking schemes in order to
replace or improve the traditional token bucket color marker. The main problem
is that TCP congestion control is not designed to work with the AF service.
Indeed, both mechanisms are antagonists. TCP has the property to share in a
fair manner the bottleneck bandwidth between flows while DiffServ network
provides a level of service controllable and predictable. In this paper, we
build a classification of all the propositions made during these last years and
compare them. As a result, we will see that these conditioning schemes can be
separated in three sets of action level and that the conditioning at the
network edge level is the most accepted one. We conclude that the problem is
still unsolved and that TCP, conditioned or not conditioned, remains
inappropriate to the DiffServ/AF service.

ABSTRACT_BEGIN
  The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of
the former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the
Wireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access
(EDCA) function of 802.11e defines multiple Access Categories (AC) with
AC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS)
values, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and
prioritization. We propose an analytical model for the EDCA function which
incorporates an accurate CW, AIFS, and TXOP differentiation at any traffic
load. The proposed model is also shown to capture the effect of MAC layer
buffer size on the performance. Analytical and simulation results are compared
to demonstrate the accuracy of the proposed approach for varying traffic loads,
EDCA parameters, and MAC layer buffer space.

ABSTRACT_BEGIN
  We develop distributed algorithms to allocate resources in multi-hop wireless
networks with the aim of minimizing total cost. In order to observe the
fundamental duplexing constraint that co-located transmitters and receivers
cannot operate simultaneously on the same frequency band, we first devise a
spectrum allocation scheme that divides the whole spectrum into multiple
sub-bands and activates conflict-free links on each sub-band. We show that the
minimum number of required sub-bands grows asymptotically at a logarithmic rate
with the chromatic number of network connectivity graph. A simple distributed
and asynchronous algorithm is developed to feasibly activate links on the
available sub-bands. Given a feasible spectrum allocation, we then design
node-based distributed algorithms for optimally controlling the transmission
powers on active links for each sub-band, jointly with traffic routes and user
input rates in response to channel states and traffic demands. We show that
under specified conditions, the algorithms asymptotically converge to the
optimal operating point.

ABSTRACT_BEGIN
  Resources in a distributed system can be identified using identifiers based
on random numbers. When using a distributed hash table to resolve such
identifiers to network locations, the straightforward approach is to store the
network location directly in the hash table entry associated with an
identifier. When a mobile host contains a large number of resources, this
requires that all of the associated hash table entries must be updated when its
network address changes.
  We propose an alternative approach where we store a host identifier in the
entry associated with a resource identifier and the actual network address of
the host in a separate host entry. This can drastically reduce the time
required for updating the distributed hash table when a mobile host changes its
network address. We also investigate under which circumstances our approach
should or should not be used. We evaluate and confirm the usefulness of our
approach with experiments run on top of OpenDHT.

ABSTRACT_BEGIN
  Mobile multi-hop ad hoc networks allow establishing local groups of
communicating devices in a self-organizing way. However, in a global setting
such networks fail to work properly due to network partitioning. Providing that
devices are capable of communicating both locally-e.g. using Wi-Fi or
Bluetooth-and additionally also with arbitrary remote devices-e.g. using
GSM/UMTS links-the objective is to find efficient ways of inter-linking
multiple network partitions. Tackling this problem of topology control, we
focus on the class of small-world networks that obey two distinguishing
characteristics: they have a strong local clustering while still retaining a
small average distance between two nodes. This paper reports on results gained
investigating the question if small-world properties are indicative for an
efficient link management in multiple multi-hop ad hoc network partitions.

ABSTRACT_BEGIN
  In this paper, we study video streaming over wireless networks with network
coding capabilities. We build upon recent work, which demonstrated that network
coding can increase throughput over a broadcast medium, by mixing packets from
different flows into a single packet, thus increasing the information content
per transmission. Our key insight is that, when the transmitted flows are video
streams, network codes should be selected so as to maximize not only the
network throughput but also the video quality. We propose video-aware
opportunistic network coding schemes that take into account both (i) the
decodability of network codes by several receivers and (ii) the importance and
deadlines of video packets. Simulation results show that our schemes
significantly improve both video quality and throughput.

ABSTRACT_BEGIN
  We analyze the Two Level Processor Sharing (TLPS) scheduling discipline with
the hyper-exponential job size distribution and with the Poisson arrival
process. TLPS is a convenient model to study the benefit of the file size based
differentiation in TCP/IP networks. In the case of the hyper-exponential job
size distribution with two phases, we find a closed form analytic expression
for the expected sojourn time and an approximation for the optimal value of the
threshold that minimizes the expected sojourn time. In the case of the
hyper-exponential job size distribution with more than two phases, we derive a
tight upper bound for the expected sojourn time conditioned on the job size. We
show that when the variance of the job size distribution increases, the gain in
system performance increases and the sensitivity to the choice of the threshold
near its optimal value decreases.

ABSTRACT_BEGIN
  The traffic behavior of University of Louisville network with the
interconnected backbone routers and the number of Virtual Local Area Network
(VLAN) subnets is investigated using the Random Matrix Theory (RMT) approach.
We employ the system of equal interval time series of traffic counts at all
router to router and router to subnet connections as a representation of the
inter-VLAN traffic. The cross-correlation matrix C of the traffic rate changes
between different traffic time series is calculated and tested against
null-hypothesis of random interactions.
  The majority of the eigenvalues \lambda_{i} of matrix C fall within the
bounds predicted by the RMT for the eigenvalues of random correlation matrices.
The distribution of eigenvalues and eigenvectors outside of the RMT bounds
displays prominent and systematic deviations from the RMT predictions.
Moreover, these deviations are stable in time.
  The method we use provides a unique possibility to accomplish three
concurrent tasks of traffic analysis. The method verifies the uncongested state
of the network, by establishing the profile of random interactions. It
recognizes the system-specific large-scale interactions, by establishing the
profile of stable in time non-random interactions. Finally, by looking into the
eigenstatistics we are able to detect and allocate anomalies of network traffic
interactions.

ABSTRACT_BEGIN
  In recent years, there has been a strong interest in measuring the available
bandwidth of network paths. Several methods and techniques have been proposed
and various measurement tools have been developed and evaluated. However, there
have been few comparative studies with regards to the actual performance of
these tools. This paper presents a study of available bandwidth measurement
techniques and undertakes a comparative analysis in terms of accuracy,
intrusiveness and response time of active probing tools. Finally, measurement
errors and the uncertainty of the tools are analysed and overall conclusions
made.

ABSTRACT_BEGIN
  Multicast is a central challenge for emerging multi-hop wireless
architectures such as wireless mesh networks, because of its substantial cost
in terms of bandwidth. In this report, we study one specific case of multicast:
broadcasting, sending data from one source to all nodes, in a multi-hop
wireless network. The broadcast we focus on is based on network coding, a
promising avenue for reducing cost; previous work of ours showed that the
performance of network coding with simple heuristics is asymptotically optimal:
each transmission is beneficial to nearly every receiver. This is for
homogenous and large networks of the plan. But for small, sparse or for
inhomogeneous networks, some additional heuristics are required. This report
proposes such additional new heuristics (for selecting rates) for broadcasting
with network coding. Our heuristics are intended to use only simple local
topology information. We detail the logic of the heuristics, and with
experimental results, we illustrate the behavior of the heuristics, and
demonstrate their excellent performance.

ABSTRACT_BEGIN
  In this paper, we conduct extensive simulations to understand the properties
of the overlay generated by BitTorrent. We start by analyzing how the overlay
properties impact the efficiency of BitTorrent. We focus on the average peer
set size (i.e., average number of neighbors), the time for a peer to reach its
maximum peer set size, and the diameter of the overlay. In particular, we show
that the later a peer arrives in a torrent, the longer it takes to reach its
maximum peer set size. Then, we evaluate the impact of the maximum peer set
size, the maximum number of outgoing connections per peer, and the number of
NATed peers on the overlay properties. We show that BitTorrent generates a
robust overlay, but that this overlay is not a random graph. In particular, the
connectivity of a peer to its neighbors depends on its arriving order in the
torrent. We also show that a large number of NATed peers significantly
compromise the robustness of the overlay to attacks. Finally, we evaluate the
impact of peer exchange on the overlay properties, and we show that it
generates a chain-like overlay with a large diameter, which will adversely
impact the efficiency of large torrents.

ABSTRACT_BEGIN
  Wireless sensor networks (WSN) have recently received an increasing interest.
They are now expected to be deployed for long periods of time, thus requiring
software updates. Updating the software code automatically on a huge number of
sensors is a tremendous task, as ''by hand'' updates can obviously not be
considered, especially when all participating sensors are embedded on mobile
entities. In this paper, we investigate an approach to automatically update
software in mobile sensor-based application when no localization mechanism is
available. We leverage the peer-to-peer cooperation paradigm to achieve a good
trade-off between reliability and scalability of code propagation. More
specifically, we present the design and evaluation of GCP ({\emph Gossip-based
Code Propagation}), a distributed software update algorithm for mobile wireless
sensor networks. GCP relies on two different mechanisms (piggy-backing and
forwarding control) to improve significantly the load balance without
sacrificing on the propagation speed. We compare GCP against traditional
dissemination approaches. Simulation results based on both synthetic and
realistic workloads show that GCP achieves a good convergence speed while
balancing the load evenly between sensors.

ABSTRACT_BEGIN
  Random walk can be used as a centrality measure of a directed graph. However,
if the graph is reducible the random walk will be absorbed in some subset of
nodes and will never visit the rest of the graph. In Google PageRank the
problem was solved by introduction of uniform random jumps with some
probability. Up to the present, there is no clear criterion for the choice this
parameter. We propose to use parameter-free centrality measure which is based
on the notion of quasi-stationary distribution. Specifically we suggest four
quasi-stationary based centrality measures, analyze them and conclude that they
produce approximately the same ranking. The new centrality measures can be
applied in spam detection to detect ``link farms'' and in image search to find
photo albums.

ABSTRACT_BEGIN
  We study efficient broadcasting for wireless sensor networks, with network
coding. We address this issue for homogeneous sensor networks in the plane. Our
results are based on a simple principle (IREN/IRON), which sets the same rate
on most of the nodes (wireless links) of the network. With this rate selection,
we give a value of the maximum achievable broadcast rate of the source: our
central result is a proof of the value of the min-cut for such networks, viewed
as hypergraphs. Our metric for efficiency is the number of transmissions
necessary to transmit one packet from the source to every destination: we show
that IREN/IRON achieves near optimality for large networks; that is,
asymptotically, nearly every transmission brings new information from the
source to the receiver. As a consequence, network coding asymptotically
outperforms any scheme that does not use network coding.

ABSTRACT_BEGIN
  We consider the decoding of bit interleaved coded modulation (BICM) applied
to multiband OFDM for practical scenarios where only a noisy (possibly very
bad) estimate of the channel is available at the receiver. First, a decoding
metric based on the channel it a posteriori probability density, conditioned on
the channel estimate is derived and used for decoding BICM multiband OFDM.
Then, we characterize the limits of reliable information rates in terms of the
maximal achievable outage rates associated to the proposed metric. We also
compare our results with the outage rates of a system using a theoretical
decoder. Our results are useful for designing a communication system where a
prescribed quality of service (QoS), in terms of achievable target rates with
small error probability, must be satisfied even in the presence of imperfect
channel estimation. Numerical results over both realistic UWB and theoretical
Rayleigh fading channels show that the proposed method provides significant
gain in terms of BER and outage rates compared to the classical mismatched
detector, without introducing any additional complexity.

ABSTRACT_BEGIN
  We consider the decoding of bit interleaved coded modulation (BICM) applied
to both multiband and MIMO OFDM systems for typical scenarios where only a
noisy (possibly very bad) estimate of the channel is provided by sending a
limited number of pilot symbols. First, by using a Bayesian framework involving
the channel a posteriori density, we adopt a practical decoding metric that is
robust to the presence of channel estimation errors. Then this metric is used
in the demapping part of BICM multiband and MIMO OFDM receivers. We also
compare our results with the performance of a mismatched decoder that replaces
the channel by its estimate in the decoding metric. Numerical results over both
realistic UWB and theoretical Rayleigh fading channels show that the proposed
method provides significant gain in terms of bit error rate compared to the
classical mismatched detector, without introducing any additional complexity.

ABSTRACT_BEGIN
  This paper introduces an expectation-maximization (EM) algorithm within a
wavelet domain Bayesian framework for semi-blind channel estimation of
multiband OFDM based UWB communications. A prior distribution is chosen for the
wavelet coefficients of the unknown channel impulse response in order to model
a sparseness property of the wavelet representation. This prior yields, in
maximum a posteriori estimation, a thresholding rule within the EM algorithm.
We particularly focus on reducing the number of estimated parameters by
iteratively discarding ``unsignificant'' wavelet coefficients from the
estimation process. Simulation results using UWB channels issued from both
models and measurements show that under sparsity conditions, the proposed
algorithm outperforms pilot based channel estimation in terms of mean square
error and bit error rate and enhances the estimation accuracy with less
computational complexity than traditional semi-blind methods.

ABSTRACT_BEGIN
  Optimal decoding of bit interleaved coded modulation (BICM) MIMO-OFDM where
an imperfect channel estimate is available at the receiver is investigated.
First, by using a Bayesian approach involving the channel a posteriori density,
we derive a practical decoding metric for general memoryless channels that is
robust to the presence of channel estimation errors. Then, we evaluate the
outage rates achieved by a decoder that uses our proposed metric. The
performance of the proposed decoder is compared to the classical mismatched
decoder and a theoretical decoder defined as the best decoder in the presence
of imperfect channel estimation. Numerical results over Rayleigh block fading
MIMO-OFDM channels show that the proposed decoder outperforms mismatched
decoding in terms of bit error rate and outage capacity without introducing any
additional complexity.

ABSTRACT_BEGIN
  The many-to-many social communication activity on the popular technology-news
website Slashdot has been studied. We have concentrated on the dynamics of
message production without considering semantic relations and have found
regular temporal patterns in the reaction time of the community to a news-post
as well as in single user behavior. The statistics of these activities follow
log-normal distributions. Daily and weekly oscillatory cycles, which cause
slight variations of this simple behavior, are identified. A superposition of
two log-normal distributions can account for these variations. The findings are
remarkable since the distribution of the number of comments per users, which is
also analyzed, indicates a great amount of heterogeneity in the community. The
reader may find surprising that only a few parameters allow a detailed
description, or even prediction, of social many-to-many information exchange in
this kind of popular public spaces.

ABSTRACT_BEGIN
  While there exist compact routing schemes designed for grids, trees, and
Internet-like topologies that offer routing tables of sizes that scale
logarithmically with the network size, we demonstrate in this paper that in
view of recent results in compact routing research, such logarithmic scaling on
Internet-like topologies is fundamentally impossible in the presence of
topology dynamics or topology-independent (flat) addressing. We use analytic
arguments to show that the number of routing control messages per topology
change cannot scale better than linearly on Internet-like topologies. We also
employ simulations to confirm that logarithmic routing table size scaling gets
broken by topology-independent addressing, a cornerstone of popular
locator-identifier split proposals aiming at improving routing scaling in the
presence of network topology dynamics or host mobility. These pessimistic
findings lead us to the conclusion that a fundamental re-examination of
assumptions behind routing models and abstractions is needed in order to find a
routing architecture that would be able to scale ``indefinitely.''

ABSTRACT_BEGIN
  As the web grows and the amount of traffics on the web server increase,
problems related to performance begin to appear. Some of the problems, such as
the number of users that can access the server simultaneously, the number of
requests that can be handled by the server per second (requests per second) to
bandwidth consumption and hardware utilization like memories and CPU. To give
better quality of service (\textbf{\textit{QoS}}), web hosting providers and
also the system administrators and network administrators who manage the server
need a benchmark application to measure the capabilities of their servers.
Later, the application intends to work under Linux/Unix -- like platforms and
built using Erlang/OTP R11 as a concurrent oriented language under Fedora Core
Linux 5.0. \textbf{\textit{WiiBench}} is divided into two main parts, the
controller section and the launcher section. Controller is the core of the
application. It has several duties, such as read the benchmark scenario file,
configure the program based on the scenario, initialize the launcher section,
gather the benchmark results from local and remote Erlang node where the
launcher runs and write them in a log file (later the log file will be used to
generate a report page for the sysadmin). Controller also has function as a
timer which act as timing for user inters arrival to the server. Launcher
generates a number of users based on the scenario, initialize them and start
the benchmark by sending requests to the web server. The clients also gather
the benchmark result and send them to the controller.

ABSTRACT_BEGIN
  Each node in a wireless multi-hop network can adjust the power level at which
it transmits and thus change the topology of the network to save energy by
choosing the neighbors with which it directly communicates. Many previous
algorithms for distributed topology control have assumed an ability at each
node to deduce some location-based information such as the direction and the
distance of its neighbor nodes with respect to itself. Such a deduction of
location-based information, however, cannot be relied upon in real environments
where the path loss exponents vary greatly leading to significant errors in
distance estimates. Also, multipath effects may result in different signal
paths with different loss characteristics, and none of these paths may be
line-of-sight, making it difficult to estimate the direction of a neighboring
node. In this paper, we present Step Topology Control (STC), a simple
distributed topology control algorithm which reduces energy consumption while
preserving the connectivity of a heterogeneous sensor network without use of
any location-based information. We show that the STC algorithm achieves the
same or better order of communication and computational complexity when
compared to other known algorithms that also preserve connectivity without the
use of location-based information. We also present a detailed simulation-based
comparative analysis of the energy savings and interference reduction achieved
by the algorithms. The results show that, in spite of not incurring a higher
communication or computational complexity, the STC algorithm performs better
than other algorithms in uniform wireless environments and especially better
when path loss characteristics are non-uniform.

ABSTRACT_BEGIN
  In this report, we consider maximal solutions to the induced bounded-degree
subgraph problem and relate it to issues concerning stream control in
multiple-input multiple-output (MIMO) networks. We present a new distributed
algorithm that completes in logarithmic time with high probability and is
guaranteed to complete in linear time. We conclude the report with simulation
results that address the effectiveness of stream control and the relative
impact of receiver overloading and flexible interference suppression.

ABSTRACT_BEGIN
  We propose Otiy, a node-centric location service that limits the impact of
location updates generate by mobile nodes in IEEE802.11-based wireless mesh
networks. Existing location services use node identifiers to determine the
locator (aka anchor) that is responsible for keeping track of a node's
location. Such a strategy can be inefficient because: (i) identifiers give no
clue on the node's mobility and (ii) locators can be far from the
source/destination shortest path, which increases both location delays and
bandwidth consumption. To solve these issues, Otiy introduces a new strategy
that identifies nodes to play the role of locators based on the likelihood of a
destination to be close to these nodes- i.e., locators are identified depending
on the mobility pattern of nodes. Otiy relies on the cyclic mobility patterns
of nodes and creates a slotted agenda composed of a set of predicted locations,
defined according to the past and present patterns of mobility. Correspondent
nodes fetch this agenda only once and use it as a reference for identifying
which locators are responsible for the node at different points in time. Over a
period of about one year, the weekly proportion of nodes having at least 50% of
exact location predictions is in average about 75%. This proportion increases
by 10% when nodes also consider their closeness to the locator from only what
they know about the network.

ABSTRACT_BEGIN
  Wireless Sensor Networks research and demand are now in full expansion, since
people came to understand these are the key to a large number of issues in
industry, commerce, home automation, healthcare, agriculture and environment,
monitoring, public safety etc. One of the most challenging research problems in
sensor networks research is power awareness and power-saving techniques. In
this master's thesis, we have studied one particular power-saving technique,
i.e. frequency scaling. In particular, we analysed the close relationship
between clock frequencies in a microcontroller and several types of constraints
imposed on these frequencies, e.g. by other components of the microcontroller,
by protocol specifications, by external factors etc. Among these constraints,
we were especially interested in the ones imposed by the timer service and by
the serial ports' transmission rates. Our efforts resulted in a microcontroller
configuration management tool which aims at assisting application programmers
in choosing microcontroller configurations, in function of the particular needs
and constraints of their application.

ABSTRACT_BEGIN
  Considering a wireless sensor network whose nodes are distributed randomly
over a given area, a probability model for the network lifetime is provided.
Using this model and assuming that packet generation follows a Poisson
distribution, an analytical expression for the complementary cumulative density
function (ccdf) of the lifetime is obtained. Using this ccdf, one can
accurately find the probability that the network achieves a given lifetime. It
is also shown that when the number of sensors, $N$, is large, with an error
exponentially decaying with $N$, one can predict whether or not a certain
lifetime can be achieved. The results of this work are obtained for both
multi-hop and single-hop wireless sensor networks and are verified with
computer simulation. The approaches of this paper are shown to be applicable to
other packet generation models and the effect of the area shape is also
investigated.

ABSTRACT_BEGIN
  In this paper, the problem of pinning control for synchronization of complex
dynamical networks is discussed. A cost function of the controlled network is
defined by the feedback gain and the coupling strength of the network. An
interesting result is that lower cost is achieved by the control scheme of
pinning nodes with smaller degrees. Some rigorous mathematical analysis is
presented for achieving lower cost in the synchronization of different
star-shaped networks. Numerical simulations on some non-regular complex
networks generated by the Barabasi-Albert model and various star-shaped
networks are shown for verification and illustration.

ABSTRACT_BEGIN
  In this paper, the synchronizability problem of dynamical networks is
addressed, where better synchronizability means that the network synchronizes
faster with lower-overshoot. The L2 norm of the error vector e is taken as a
performance index to measure this kind of synchronizability. For the
equilibrium synchronization case, it is shown that there is a close
relationship between the L2 norm of the error vector e and the H2 norm of the
transfer function G of the linearized network about the equilibrium point.
Consequently, the effect of the network coupling topology on the H2 norm of the
transfer function G is analyzed. Finally, an optimal controller is designed,
according to the so-called LQR problem in modern control theory, which can
drive the whole network to its equilibrium point and meanwhile minimize the L2
norm of the output of the linearized network.

ABSTRACT_BEGIN
  For most wireless services with variable rate transmission, both average rate
and rate oscillation are important performance metrics. The traditional
performance criterion, utility of average transmission rate, boosts the average
rate but also results in high rate oscillations. We introduce a utility
function of instantaneous transmission rates. It is capable of facilitating the
resource allocation with flexible combinations of average rate and rate
oscillation. Based on the new utility, we consider the time and power
allocation in a time-shared wireless network. Two adaptation policies are
developed, namely, time sharing (TS) and joint time sharing and power control
(JTPC). An extension to quantized time sharing with limited channel feedback
(QTSL) for practical systems is also discussed. Simulation results show that by
controlling the concavity of the utility function, a tradeoff between the
average rate and rate oscillation can be easily made.

ABSTRACT_BEGIN
  Optical WDM mesh networks are able to transport huge amount of information.
The use of such technology however poses the problem of protection against
failures such as fibre cuts. One of the principal methods for link protection
used in optical WDM networks is pre-configured protection cycle (p-cycle). The
major problem of this method of protection resides in finding the optimal set
of p-cycles which protect the network for a given distribution of working
capacity. Existing heuristics generate a large set of p-cycle candidates which
are entirely independent of the network state, and from then the good sub-set
of p-cycles which will protect the network is selected. In this paper, we
propose a new algorithm of generation of p-cycles based on the incremental
aggregation of the shortest cycles. Our generation of p-cycles depends on the
state of the network. This enables us to choose an efficient set of p-cycles
which will protect the network. The set of p-cycles that we generate is the
final set which will protect the network, in other words our heuristic does not
go through the additional step of p-cycle selection

ABSTRACT_BEGIN
  Redundant sensing capabilities are often required in sensor network
applications due to various reasons, e.g. robustness, fault tolerance, or
increased accuracy. At the same time high sensor redundancy offers the
possibility of increasing network lifetime by scheduling sleep intervals for
some sensors and still providing continuous service with help of the remaining
active sensors. In this paper centralized and distributed algorithms are
proposed to solve the k-coverage sensing problem and maximize network lifetime.
When physically possible, the proposed robust Controlled Greedy Sleep Algorithm
provides guaranteed service independently of node and communication errors in
the network. The performance of the algorithm is illustrated and compared to
results of a random solution by simulation examples.

ABSTRACT_BEGIN
  Wireless microsensor networks, which have been the topic of intensive
research in recent years, are now emerging in industrial applications. An
important milestone in this transition has been the release of the IEEE
802.15.4 standard that specifies interoperable wireless physical and medium
access control layers targeted to sensor node radios. In this paper, we
evaluate the potential of an 802.15.4 radio for use in an ultra low power
sensor node operating in a dense network. Starting from measurements carried
out on the off-the-shelf radio, effective radio activation and link adaptation
policies are derived. It is shown that, in a typical sensor network scenario,
the average power per node can be reduced down to 211m mm mW. Next, the energy
consumption breakdown between the different phases of a packet transmission is
presented, indicating which part of the transceiver architecture can most
effectively be optimized in order to further reduce the radio power, enabling
self-powered wireless microsensor networks.

ABSTRACT_BEGIN
  Ultra-wideband (UWB) communication is an emerging wireless technology that
promises high data rates over short distances and precise locationing. The
large available bandwidth and the constraint of a maximum power spectral
density drives a unique set of system challenges. This paper addresses these
challenges using two UWB transceivers and a discrete prototype platform.

ABSTRACT_BEGIN
  Fast wireless access has rapidly become commonplace. Wireless access points
and Hotspot servers are sprouting everywhere. Battery lifetime continues to be
a critical issue in mobile computing. This paper first gives an overview of
WLAN energy saving strategies, followed by an illustration of a system-level
methodology for saving power in heterogeneous wireless environments.

ABSTRACT_BEGIN
  This paper retraces the historical development of wireless LAN technology in
the context of the pursuit of ever higher data rate, describes the significant
technical breakthroughs that are now occurring, and speculates on future
directions that the technology may take over the remainder of the decade. The
challenges that these developments have created for low power operation are
considered, as well as some of the opportunities that are presented to mitigate
them. The importance of MIMO as an emerging technology for 802.11 is
specifically highlighted, both in terms of the significant increase in data
rate and range that it enables as well as the considerable challenge that it
presents for the development of low power wireless LAN products.

ABSTRACT_BEGIN
  In this paper, we provide a throughput analysis of the IEEE 802.11 protocol
at the data link layer in non-saturated traffic conditions taking into account
the impact of both transmission channel and capture effects in Rayleigh fading
environment. The impact of both non-ideal channel and capture become important
in terms of the actual observed throughput in typical network conditions
whereby traffic is mainly unsaturated, especially in an environment of high
interference.
  We extend the multi-dimensional Markovian state transition model
characterizing the behavior at the MAC layer by including transmission states
that account for packet transmission failures due to errors caused by
propagation through the channel, along with a state characterizing the system
when there are no packets to be transmitted in the buffer of a station.
Finally, we derive a linear model of the throughput along with its interval of
validity.
  Simulation results closely match the theoretical derivations confirming the
effectiveness of the proposed model.

ABSTRACT_BEGIN
  In this paper, we provide a saturation throughput analysis of the IEEE 802.11
protocol at the data link layer by including the impact of both transmission
channel and capture effects in Rayleigh fading environment. Impacts of both
non-ideal channel and capture effects, specially in an environment of high
interference, become important in terms of the actual observed throughput. As
far as the 4-way handshaking mechanism is concerned, we extend the
multi-dimensional Markovian state transition model characterizing the behavior
at the MAC layer by including transmission states that account for packet
transmission failures due to errors caused by propagation through the channel.
This way, any channel model characterizing the physical transmission medium can
be accommodated, including AWGN and fading channels. We also extend the Markov
model in order to consider the behavior of the contention window when employing
the basic 2-way handshaking mechanism.
  Under the usual assumptions regarding the traffic generated per node and
independence of packet collisions, we solve for the stationary probabilities of
the Markov chain and develop expressions for the saturation throughput as a
function of the number of terminals, packet sizes, raw channel error rates,
capture probability, and other key system parameters. The theoretical
derivations are then compared to simulation results confirming the
effectiveness of the proposed models.

ABSTRACT_BEGIN
  We propose a linear model of the throughput of the IEEE 802.11 Distributed
Coordination Function (DCF) protocol at the data link layer in non-saturated
traffic conditions. We show that the throughput is a linear function of the
packet arrival rate (PAR) $\lambda$ with a slope depending on both the number
of contending stations and the average payload length. We also derive the
interval of validity of the proposed model by showing the presence of a
critical $\lambda$, above which the station begins operating in saturated
traffic conditions.
  The analysis is based on the multi-dimensional Markovian state transition
model proposed by Liaw \textit{et al.} with the aim of describing the behaviour
of the MAC layer in unsaturated traffic conditions. Simulation results closely
match the theoretical derivations, confirming the effectiveness of the proposed
linear model.

ABSTRACT_BEGIN
  In this paper, we provide a throughput analysis of the IEEE 802.11 protocol
at the data link layer in non-saturated traffic conditions taking into account
the impact of both transmission channel and capture effects in Rayleigh fading
environment. Impacts of both non-ideal channel and capture become important in
terms of the actual observed throughput in typical network conditions whereby
traffic is mainly unsaturated, specially in an environment of high
interference.
  We extend the multi-dimensional Markovian state transition model
characterizing the behavior at the MAC layer by including transmission states
that account for packet transmission failures due to errors caused by
propagation through the channel, along with a state characterizing the system
when there are no packets to be transmitted in the buffer of a station.

ABSTRACT_BEGIN
  In this note we analyse various stability properties of the max-min fair Rate
Control Protocol (RCP) operating with small buffers. We first tackle the issue
of stability for networks with arbitrary topologies. We prove that the max-min
fair RCP fluid model is globally stable in the absence of propagation delays,
and also derive a set of conditions for local stability when arbitrary
heterogeneous propagation delays are present. The network delay stability
result assumes that, at equilibrium, there is only one bottleneck link along
each route. Lastly, in the simpler setting of a single link, single delay
model, we investigate the impact of the loss of local stability via a Hopf
bifurcation.

ABSTRACT_BEGIN
  This paper considers the requirements to ensure bounded mean queuing delay
and non-starvation in a slotted Aloha network operating the exponential backoff
protocol. It is well-known that the maximum possible throughput of a slotted
Aloha system with a large number of nodes is 1/e = 0.3679. Indeed, a saturation
throughput of 1/e can be achieved with an exponential backoff factor of r =
e/(e-1)=1.5820. The binary backoff factor of r = 2 is assumed in the majority
of prior work, and in many practical multiple-access networks such as the
Ethernet and WiFi. For slotted Aloha, the saturation throughput 0.3466 for r =
2 is reasonably close to the maximum of 1/e, and one could hardly raise
objection to adopting r = 2 in the system. However, this paper shows that if
mean queuing delay is to be bounded, then the sustainable throughput when r = 2
is only 0.2158, a drastic 41% drop from 1/e . Fortunately, the optimal setting
of r = 1.3757 under the bounded mean-delay requirement allows us to achieve
sustainable throughput of 0.3545, a penalty of only less than 4% relative to
1/e. A general conclusion is that the value of r may significantly affect the
queuing delay performance. Besides analyzing mean queuing delay, this paper
also delves into the phenomenon of starvation, wherein some nodes are deprived
of service for an extended period of time while other nodes hog the system.
Specifically, we propose a quantitative definition for starvation and show that
the conditions to guarantee bounded mean delay and non-starved operation are
one of the same, thus uniting these two notions. Finally, we show that when
mean delay is large and starvation occurs, the performance results obtained
from simulation experiments may not converge. A quantitative discussion of this
issue is provided in this paper.

ABSTRACT_BEGIN
  The main goal of routing protocol is to efficiency delivers data from source
to destination. All routing protocols are the same in this goal, but the way
they adopt to achieve it is different, so routing strategy has an egregious
role on the performance of an ad hoc network. Most of routing protocols
proposed for ad hoc networks have a flat structure. These protocols expand the
control overhead packets to discover or maintain a route. On the other hand a
number of hierarchical-based routing protocols have been developed, mostly are
based on layered design. These protocols improve network performances
especially when the network size grows up since details about remote portion of
network can be handled in an aggregate manner. Although, there is another
approach to design a protocol called cross-layer design. Using this approach
information can exchange between different layer of protocol stack, result in
optimizing network performances.
  In this paper, we intend to exert cross-layer design to optimize Cluster
Based Routing Protocol (Cross-CBRP). Using NS-2 network simulator we evaluate
rate of cluster head changes, throughput and packet delivery ratio. Comparisons
denote that Cross-CBRP has better performances with respect to the original
CBRP.

ABSTRACT_BEGIN
  The aim of this paper is twofold. On one hand, it presents a
multi-dimensional Markovian state transition model characterizing the behavior
at the Medium Access Control (MAC) layer by including transmission states that
account for packet transmission failures due to errors caused by propagation
through the channel, along with a state characterizing the system when there
are no packets to be transmitted in the queue of a station (to model
non-saturated traffic conditions). On the other hand, it provides a throughput
analysis of the IEEE 802.11 protocol at the data link layer in both saturated
and non-saturated traffic conditions taking into account the impact of both
transmission channel and multirate transmission in Rayleigh fading environment.
Simulation results closely match the theoretical derivations confirming the
effectiveness of the proposed model.

ABSTRACT_BEGIN
  In this article, we present a multi-tool method for the development and the
analysis of a new medium access method. IEEE 802.15.4 / ZigBee technology has
been used as a basis for this new determinist MAC layer which enables a high
level of QoS. This WPAN can be typically used for wireless sensor networks
which require strong temporal constraints. To validate the proposed protocol,
three complementary and adequate tools are used: Petri Nets for the formal
validation of the algorithm, a dedicated simulator for the temporal aspects,
and some measures on a real prototype based on a couple of ZigBee FREESCALE
components for the hardware characterization of layers #1 and #2.

ABSTRACT_BEGIN
  Peer-to-peer content distribution systems have been enjoying great
popularity, and are now gaining momentum as a means of disseminating video
streams over the Internet. In many of these protocols, including the popular
BitTorrent, content is split into mostly fixed-size pieces, allowing a client
to download data from many peers simultaneously. This makes piece size
potentially critical for performance. However, previous research efforts have
largely overlooked this parameter, opting to focus on others instead. This
paper presents the results of real experiments with varying piece sizes on a
controlled BitTorrent testbed. We demonstrate that this parameter is indeed
critical, as it determines the degree of parallelism in the system, and we
investigate optimal piece sizes for distributing small and large content. We
also pinpoint a related design trade-off, and explain how BitTorrent's choice
of dividing pieces into subpieces attempts to address it.

ABSTRACT_BEGIN
  A restless multi-armed bandit problem that arises in multichannel
opportunistic communications is considered, where channels are modeled as
independent and identical Gilbert-Elliot channels and channel state
observations are subject to errors. A simple structure of the myopic policy is
established under a certain condition on the false alarm probability of the
channel state detector. It is shown that the myopic policy has a semi-universal
structure that reduces channel selection to a simple round-robin procedure and
obviates the need to know the underlying Markov transition probabilities. The
optimality of the myopic policy is proved for the case of two channels and
conjectured for the general case based on numerical examples.

ABSTRACT_BEGIN
  Modeling Internet growth is important both for understanding the current
network and to predict and improve its future. To date, Internet models have
typically attempted to explain a subset of the following characteristics:
network structure, traffic flow, geography, and economy. In this paper we
present a discrete, agent-based model, that integrates all of them. We show
that the model generates networks with topologies, dynamics, and (more
speculatively) spatial distributions that are similar to the Internet.

ABSTRACT_BEGIN
  To date, most analysis of WLANs has been focused on their operation under
saturation condition. This work is an attempt to understand the fundamental
performance of WLANs under unsaturated condition. In particular, we are
interested in the delay performance when collisions of packets are resolved by
an exponential backoff mechanism. Using a multiple-vacation queueing model, we
derive an explicit expression for packet delay distribution, from which
necessary conditions for finite mean delay and delay jitter are established. It
is found that under some circumstances, mean delay and delay jitter may
approach infinity even when the traffic load is way below the saturation
throughput. Saturation throughput is therefore not a sound measure of WLAN
capacity when the underlying applications are delay sensitive. To bridge the
gap, we define safe-bounded-mean-delay (SBMD) throughput and
safe-bounded-delay-jitter (SBDJ) throughput that reflect the actual network
capacity users can enjoy when they require bounded mean delay and delay jitter,
respectively. The analytical model in this paper is general enough to cover
both single-packet reception (SPR) and multi-packet reception (MPR) WLANs, as
well as carrier-sensing and non-carrier-sensing networks. We show that the SBMD
and SBDJ throughputs scale super-linearly with the MPR capability of a network.
Together with our earlier work that proves super-linear throughput scaling
under saturation condition, our results here complete the demonstration of MPR
as a powerful capacity-enhancement technique for both delay-sensitive and
delay-tolerant applications.

ABSTRACT_BEGIN
  We introduce four algorithms for packet transport in complex networks. These
algorithms use deterministic rules which depend, in different ways, on the
degree of the node, the number of packets posted down each edge, the mean
delivery time of packets sent down each edge to each destination and the time
since an edge last transmitted a packet. On scale-free networks all our
algorithms are considerably more efficient and can handle a larger load than
the random walk algorithm. We consider in detail various attributes of our
algorithms, for instance we show that an algorithm that bases its decisions on
the mean delivery time jams unless it incorporates information about the degree
of the destination node.

ABSTRACT_BEGIN
  Due to emerging real-time and multimedia applications, efficient routing of
information packets in dynamically changing communication network requires that
as the load levels, traffic patterns and topology of the network change, the
routing policy also adapts. We focused in this paper on QoS based routing by
developing a neuro-dynamic programming to construct dynamic state dependent
routing policies. We propose an approach based on adaptive algorithm for packet
routing using reinforcement learning which optimizes two criteria: cumulative
cost path and end-to-end delay. Numerical results obtained with OPNET simulator
for different packet interarrival times statistical distributions with
different levels of traffic's load show that the proposed approach gives better
results compared to standard optimal path routing algorithms.

ABSTRACT_BEGIN
  In this proposition, we present a link management technique for pro-active
routing protocols for ad-hoc networks. This new mechanism is based on signal
strength hence cross layer approach is used. The hysteresis mechanism provided
by OLSR is improved upon by using signal strength in combination with the hello
loss based hysteresis. The signal power is used to determine if the
link-quality is improving or deteriorating while packet losses are handled
through the hysteresis mechanism specified in OLSR RFC. This not only makes the
link management more robust but also helps in anticipating link breakages
thereby greatly improving the performance.

ABSTRACT_BEGIN
  The surest way to increase the system capacity of a wireless link is by
getting the transmitter and receiver closer to each other, which creates the
dual benefits of higher quality links and more spatial reuse. In a network with
nomadic users, this inevitably involves deploying more infrastructure,
typically in the form of microcells, hotspots, distributed antennas, or relays.
A less expensive alternative is the recent concept of femtocells, also called
home base-stations, which are data access points installed by home users get
better indoor voice and data coverage. In this article, we overview the
technical and business arguments for femtocells, and describe the
state-of-the-art on each front. We also describe the technical challenges
facing femtocell networks, and give some preliminary ideas for how to overcome
them.

ABSTRACT_BEGIN
  We investigate in this paper the performance of a simple file sharing
principle. For this purpose, we consider a system composed of N peers becoming
active at exponential random times; the system is initiated with only one
server offering the desired file and the other peers after becoming active try
to download it. Once the file has been downloaded by a peer, this one
immediately becomes a server. To investigate the transient behavior of this
file sharing system, we study the instant when the system shifts from a
congested state where all servers available are saturated by incoming demands
to a state where a growing number of servers are idle. In spite of its apparent
simplicity, this queueing model (with a random number of servers) turns out to
be quite difficult to analyze. A formulation in terms of an urn and ball model
is proposed and corresponding scaling results are derived. These asymptotic
results are then compared against simulations.

ABSTRACT_BEGIN
  Experimentation is important when designing communication protocols for
Wireless Sensor Networks. Lower-layers have a major impact on upper-layer
performance, and the complexity of the phenomena can not be entirely captured
by analysis or simulation. In this report, we go through the complete process,
from designing an energy-efficient self-organizing communication architecture
(MAC, routing and application layers) to real-life experimentation roll-outs.
The presented communication architecture includes a MAC protocol which avoids
building and maintaining neighborhood tables, and a geographically-inspired
routing protocol over virtual coordinates. The application consists of a mobile
sink interrogating a wireless sensor network based on the requests issued by a
disconnected base station. After the design process of this architecture, we
verify it functions correctly by simulation, and we perform a temporal
verification. This study is needed to calculate the maximum speed the mobile
sink can take. We detail the implementation, and the results of the off-site
experimentation (energy consumption at PHY layer, collision probability at MAC
layer, and routing). Finally, we report on the real-world deployment where we
have mounted the mobile sink node on a radio-controlled airplane.

ABSTRACT_BEGIN
  This paper presents a modified proportional fairness (PF) criterion suitable
for mitigating the \textit{rate anomaly} problem of multirate IEEE 802.11
Wireless LANs employing the mandatory Distributed Coordination Function (DCF)
option. Compared to the widely adopted assumption of saturated network, the
proposed criterion can be applied to general networks whereby the contending
stations are characterized by specific packet arrival rates, $\lambda_s$, and
transmission rates $R_d^{s}$.
  The throughput allocation resulting from the proposed algorithm is able to
greatly increase the aggregate throughput of the DCF while ensuring fairness
levels among the stations of the same order of the ones available with the
classical PF criterion. Put simply, each station is allocated a throughput that
depends on a suitable normalization of its packet rate, which, to some extent,
measures the frequency by which the station tries to gain access to the
channel. Simulation results are presented for some sample scenarios, confirming
the effectiveness of the proposed criterion.

ABSTRACT_BEGIN
  Recent literature has proved that stable dynamic routing algorithms have
solid theoretical foundation that makes them suitable to be implemented in a
real protocol, and used in practice in many different operational network
contexts. Such algorithms inherit much of the properties of congestion
controllers implementing one of the possible combination of AQM/ECN schemes at
nodes and flow control at sources. In this paper we propose a linear program
formulation of the multi-commodity flow problem with congestion control, under
max-min fairness, comprising demands with or without exogenous peak rates. Our
evaluations of the gain, using path diversity, in scenarios as intra-domain
traffic engineering and wireless mesh networks encourages real implementations,
especially in presence of hot spots demands and non uniform traffic matrices.
We propose a flow aware perspective of the subject by using a natural
multi-path extension to current congestion controllers and show its performance
with respect to current proposals. Since flow aware architectures exploiting
path diversity are feasible, scalable, robust and nearly optimal in presence of
flows with exogenous peak rates, we claim that our solution rethinked in the
context of realistic traffic assumptions performs as better as an optimal
approach with all the additional benefits of the flow aware paradigm.

ABSTRACT_BEGIN
  Discriminatory Processor Sharing policy introduced by Kleinrock is of a great
interest in many application areas, including telecommunications, web
applications and TCP flow modelling. Under the DPS policy the job priority is
controlled by the vector of weights. Verifying the vector of weights it is
possible to modify the service rates of the jobs and optimize system
characteristics. In the present paper we present the results concerning the
comparison of two DPS policies with different weight vectors. We show the
monotonicity of the expected sojourn time of the system depending on the weight
vector under certain condition on the system. Namely, the system has to consist
of classes with means which are quite different from each other. The classes
with similar means can be organized together and considered as one class, so
the given restriction can be overcame.

ABSTRACT_BEGIN
  A well-known approach to intradomain traffic engineering consists in finding
the set of link weights that minimizes a network-wide objective function for a
given intradomain traffic matrix. This approach is inadequate because it
ignores a potential impact on interdomain routing. Indeed, the resulting set of
link weights may trigger BGP to change the BGP next hop for some destination
prefixes, to enforce hot-potato routing policies. In turn, this results in
changes in the intradomain traffic matrix that have not been anticipated by the
link weights optimizer, possibly leading to degraded network performance.
  We propose a BGP-aware link weights optimization method that takes these
effects into account, and even turns them into an advantage. This method uses
the interdomain traffic matrix and other available BGP data, to extend the
intradomain topology with external virtual nodes and links, on which all the
well-tuned heuristics of a classical link weights optimizer can be applied. A
key innovative asset of our method is its ability to also optimize the traffic
on the interdomain peering links. We show, using an operational network as a
case study, that our approach does so efficiently at almost no extra
computational cost.

ABSTRACT_BEGIN
  The locator/identifier split approach assumes separating functions of a
locator (i.e. topology--dependent attachment point address) and identifier
(topology-independent unique identifier) currently both served by an IP
address. This work is an attempt to redefine semantics of MAC address to make
it a pure layer-2 locator instead of a pure globally-unique identifier. Such an
exercise might be interesting from the standpoint of Ethernet scaling and Metro
Ethernet technologies. From the global routing perspective, introduction of
multihoming, traffic engineering and failover at the 2nd layer may reduce
pressure on the 3rd layer.

ABSTRACT_BEGIN
  Unlike traditional routing procedures that, at the best, single out a unique
route, multi-path routing protocols discover proactively several alternative
routes. It has been recognized that multi-path routing can be more efficient
than traditional one mainly for mobile ad hoc networks, where route failure
events are frequent. Most studies in the area of multi-path routing focus on
heuristic methods, and the performances of these strategies are commonly
evaluated by numerical simulations. The need of a theoretical analysis
motivates such a paper, which proposes to resort to the terminal-pair routing
reliability as performance metric. This metric allows one to assess the
performance gain due to the availability of route diversity. By resorting to
graph theory, we propose an analytical framework to evaluate the tolerance of
multi-path route discovery processes against route failures for mobile ad hoc
networks. Moreover, we derive a useful bound to easily estimate the performance
improvements achieved by multi-path routing with respect to any traditional
routing protocol. Finally, numerical simulation results show the effectiveness
of this performance analysis.

ABSTRACT_BEGIN
  In Kleinrock and Kamoun's paper, the inverse relation of routing table length
index and routing path length index in hierarchical routing model is
illustrated. In this paper we give the analytical correlation of routing table
length index and routing path length index in hierarchical routing model.

ABSTRACT_BEGIN
  Direct application of network coding at the physical layer - physical layer
network coding (PNC) - is a promising technique for two-way relay wireless
networks. In a two-way relay network, relay nodes are used to relay two-way
information flows between pairs of end nodes. This paper proposes a precise
definition for PNC. Specifically, in PNC, a relay node does not decode the
source information from the two ends separately, but rather directly maps the
combined signals received simultaneously to a signal to be relayed. Based on
this definition, PNC can be further sub-classed into two categories - PNCF (PNC
over finite field) and PNCI (PNC over infinite field) - according to whether
the network-code field (or groups, rings) adopted is finite or infinite. For
each of PNCF and PNCI, we consider two specific estimation techniques for
dealing with noise in the mapping process. The performance of the four schemes
is investigated by means of analysis and simulation, assuming symbol-level
synchronization only.

ABSTRACT_BEGIN
  This paper studies the buffered Aloha with K-exponential backoff collision
resolution algorithms. The buffered Aloha network is modeled as a multi-queue
single-server system. We adopt a widely used approach in packet switching
systems to decompose the multi-queue system into independent first-in-first-out
(FIFO) queues, which are hinged together by the probability of success of
head-of-line (HOL) packets. A unified method is devised to tackle the stability
and throughput problems of K-exponential backoff with any cutoff phase K. For
networks with a finite number of nodes, we show that the K-exponential backoff
is stable if the retransmission factor is properly chosen from the stable
region. The maximum stable throughput is derived and demonstrated via examples
of geometric retransmission (K=1) and exponential backoff (K=infinity). For
networks with an infinite number of nodes, we show that geometric
retransmission is unstable, and the stable network throughput of exponential
backoff can only be achieved at the cost of potential unbounded delay in each
input queue. Furthermore, we address the stability issue of the systems at the
undesired stable point. All analytical results presented in this paper are
verified and confirmed by simulations.

ABSTRACT_BEGIN
  Web Service orchestrations are compositions of different Web Services to form
a new service. The services called during the orchestration guarantee a given
performance to the orchestrater, usually in the form of contracts. These
contracts can be used by the orchestrater to deduce the contract it can offer
to its own clients, by performing contract composition. An implicit assumption
in contract based QoS management is: "the better the component services
perform, the better the orchestration's performance will be". Thus, contract
based QoS management for Web services orchestrations implicitly assumes
monotony. In some orchestrations, however, monotony can be violated, i.e., the
performance of the orchestration improves when the performance of a component
service degrades. This is highly undesirable since it can render the process of
contract composition inconsistent. In this paper we define monotony for
orchestrations modelled by Colored Occurrence Nets (CO-nets) and we
characterize the classes of monotonic orchestrations. We show that few
orchestrations are indeed monotonic, mostly since latency can be traded for
quality of data. We also propose a sound refinement of monotony, called
conditional monotony, which forbids this kind of cheating and show that
conditional monotony is widely satisfied by orchestrations. This finding leads
to reconsidering the way SLAs should be formulated.

ABSTRACT_BEGIN
  A wireless sensor/actuator network (WSAN) is a group of sensors and actuators
that are geographically distributed and interconnected by wireless networks.
Sensors gather information about the state of physical world. Actuators react
to this information by performing appropriate actions. WSANs thus enable cyber
systems to monitor and manipulate the behavior of the physical world. WSANs are
growing at a tremendous pace, just like the exploding evolution of Internet.
Supporting quality of service (QoS) will be of critical importance for
pervasive WSANs that serve as the network infrastructure of diverse
applications. To spark new research and development interests in this field,
this paper examines and discusses the requirements, critical challenges, and
open research issues on QoS management in WSANs. A brief overview of recent
progress is given.

ABSTRACT_BEGIN
  With traditional open-loop scheduling of network resources, the
quality-of-control (QoC) of networked control systems (NCSs) may degrade
significantly in the presence of limited bandwidth and variable workload. The
goal of this work is to maximize the overall QoC of NCSs through dynamically
allocating available network bandwidth. Based on codesign of control and
scheduling, an integrated feedback scheduler is developed to enable flexible
QoC management in dynamic environments. It encompasses a cascaded feedback
scheduling module for sampling period adjustment and a direct feedback
scheduling module for priority modification. The inherent characteristics of
priority-driven control networks make it feasible to implement the proposed
feedback scheduler in real-world systems. Extensive simulations show that the
proposed approach leads to significant QoC improvement over the traditional
open-loop scheduling scheme under both underloaded and overloaded network
conditions.

ABSTRACT_BEGIN
  Wireless sensor/actuator networks (WSANs) are emerging rapidly as a new
generation of sensor networks. Despite intensive research in wireless sensor
networks (WSNs), limited work has been found in the open literature in the
field of WSANs. In particular, quality-of-service (QoS) management in WSANs
remains an important issue yet to be investigated. As an attempt in this
direction, this paper develops a fuzzy logic control based QoS management
(FLC-QM) scheme for WSANs with constrained resources and in dynamic and
unpredictable environments. Taking advantage of the feedback control
technology, this scheme deals with the impact of unpredictable changes in
traffic load on the QoS of WSANs. It utilizes a fuzzy logic controller inside
each source sensor node to adapt sampling period to the deadline miss ratio
associated with data transmission from the sensor to the actuator. The deadline
miss ratio is maintained at a pre-determined desired level so that the required
QoS can be achieved. The FLC-QM has the advantages of generality, scalability,
and simplicity. Simulation results show that the FLC-QM can provide WSANs with
QoS support.

ABSTRACT_BEGIN
  When the stations in an IEEE 802.11 infrastructure Basic Service Set (BSS)
employ Transmission Control Protocol (TCP) in the transport layer, this
exacerbates per-flow unfair access which is a direct result of uplink/downlink
bandwidth asymmetry in the BSS. We propose a novel and simple analytical model
to approximately calculate the per-flow TCP congestion window limit that
provides fair and efficient TCP access in a heterogeneous wired-wireless
scenario. The proposed analysis is unique in that it considers the effects of
varying number of uplink and downlink TCP flows, differing Round Trip Times
(RTTs) among TCP connections, and the use of delayed TCP Acknowledgment (ACK)
mechanism. Motivated by the findings of this analysis, we design a link layer
access control block to be employed only at the Access Point (AP) in order to
resolve the unfair access problem. The novel and simple idea of the proposed
link layer access control block is employing a congestion control and filtering
algorithm on TCP ACK packets of uplink flows, thereby prioritizing the access
of TCP data packets of downlink flows at the AP. Via simulations, we show that
short- and long-term fair access can be provisioned with the introduction of
the proposed link layer access control block to the protocol stack of the AP
while improving channel utilization and access delay.

ABSTRACT_BEGIN
  We present the station-based unfair access problem among the uplink and the
downlink stations in the IEEE 802.11e infrastructure Basic Service Set (BSS)
when the default settings of the Enhanced Distributed Channel Access (EDCA)
parameters are used. We discuss how the transport layer protocol
characteristics alleviate the unfairness problem. We design a simple,
practical, and standard-compliant framework to be employed at the Access Point
(AP) for fair and efficient access provisioning. A dynamic measurement-based
EDCA parameter adaptation block lies in the core of this framework. The
proposed framework is unique in the sense that it considers the characteristic
differences of Transmission Control Protocol (TCP) and User Datagram Protocol
(UDP) flows and the coexistence of stations with varying bandwidth or
Quality-of-Service (QoS) requirements. Via simulations, we show that our
solution provides short- and long-term fair access for all stations in the
uplink and downlink employing TCP and UDP flows with non-uniform packet rates
in a wired-wireless heterogeneous network. In the meantime, the QoS
requirements of coexisting real-time flows are also maintained.

ABSTRACT_BEGIN
  Suppose that wirelessly communicating sensors are placed in a regular fashion
on the points of a lattice. Common communication protocols allow the sensors to
broadcast messages at arbitrary times, which can lead to problems should two
sensors broadcast at the same time. It is shown that one can exploit a tiling
of the lattice to derive a deterministic periodic schedule for the broadcast
communication of sensors that is guaranteed to be collision-free. The proposed
schedule is shown to be optimal in the number of time slots.

ABSTRACT_BEGIN
  Wireless control systems (WCSs) often have to operate in dynamic environments
where the network traffic load may vary unpredictably over time. The sampling
in sensors is conventionally time triggered with fixed periods. In this
context, only worse-than-possible quality of control (QoC) can be achieved when
the network is underloaded, while overloaded conditions may significantly
degrade the QoC, even causing system instability. This is particularly true
when the bandwidth of the wireless network is limited and shared by multiple
control loops. To address these problems, a flexible time-triggered sampling
scheme is presented in this work. Smart sensors are used to facilitate dynamic
adjustment of sampling periods, which enhances the flexibility and resource
efficiency of the system based on time-triggered sampling. Feedback control
technology is exploited for adapting sampling periods in a periodic manner. The
deadline miss ratio in each control loop is maintained at/around a desired
level, regardless of workload variations. Simulation results show that the
proposed sampling scheme is able to deal with dynamic and unpredictable
variations in network traffic load. Compared to conventional time-triggered
sampling, it leads to much better QoC in WCSs operating in dynamic
environments.

ABSTRACT_BEGIN
  Wireless sensor/actuator networks (WSANs) are emerging as a new generation of
sensor networks. Serving as the backbone of control applications, WSANs will
enable an unprecedented degree of distributed and mobile control. However, the
unreliability of wireless communications and the real-time requirements of
control applications raise great challenges for WSAN design. With emphasis on
the reliability issue, this paper presents an application-level design
methodology for WSANs in mobile control applications. The solution is generic
in that it is independent of the underlying platforms, environment, control
system models, and controller design. To capture the link quality
characteristics in terms of packet loss rate, experiments are conducted on a
real WSAN system. From the experimental observations, a simple yet efficient
method is proposed to deal with unpredictable packet loss on actuator nodes.
Trace-based simulations give promising results, which demonstrate the
effectiveness of the proposed approach.

ABSTRACT_BEGIN
  We present an efficient routing approach for delivering packets in complex
networks. On delivering a message from a node to a destination, a node forwards
the message to a neighbor by estimating the waiting time along the shortest
path from each of its neighbors to the destination. This projected waiting time
is dynamical in nature and the path through which a message is delivered would
be adapted to the distribution of messages in the network. Implementing the
approach on scale-free networks, we show that the present approach performs
better than the shortest-path approach and another approach that takes into
account of the waiting time only at the neighboring nodes. Key features in
numerical results are explained by a mean field theory. The approach has the
merit that messages are distributed among the nodes according to the
capabilities of the nodes in handling messages.

ABSTRACT_BEGIN
  We propose and study a model of traffic in communication networks. The
underlying network has a structure that is tunable between a scale-free growing
network with preferential attachments and a random growing network. To model
realistic situations where different nodes in a network may have different
capabilities, the message or packet creation and delivering rates at a node are
assumed to depend on the degree of the node. Noting that congestions are more
likely to take place at the nodes with high degrees in networks with scale-free
character, an efficient approach of selectively enhancing the
message-processing capability of a small fraction (e.g. 3%) of the nodes is
shown to perform just as good as enhancing the capability of all nodes. The
interplay between the creation rate and the delivering rate in determining
non-congested or congested traffic in a network is studied more numerically and
analytically.

ABSTRACT_BEGIN
  Different routing strategies may result in different behaviors of traffic on
internet. We analyze the correlation of traffic data for three typical routing
strategies by the detrended fluctuation analysis (DFA) and find that the degree
of correlation of the data can be divided into three regions, i.e., weak,
medium, and strong correlation. The DFA scalings are constants in both the
regions of weak and strong correlation but monotonously increase in the region
of medium correlation. We suggest that it is better to consider the traffic on
complex network as three phases, i.e., the free, buffer, and congestion phase,
than just as two phases believed before, i.e., the free and congestion phase.

ABSTRACT_BEGIN
  The strong growth of low rate wireless personal area networks (LR-WPAN),
leads us to consider the autonomy problems, thus node lifetime in a network,
knowing that the power supplies replacement is often difficult to realize. The
inherent mobility in this type of equipment is an essential element. It will
provide routing constraints, so a complex problem to solve. This article
provides work lines to assess the performance of such a network in terms of
energy consumption and mobility. The objectives are contradictory; it will
necessarily find a compromise. In addition, if we want to guarantee a maximum
delay for the transmitted messages, possibility offered by the IEEE 802.15-4
standard, another compromise necessitate a strictly fixed structure and a fully
mobile structure. Therefore, we present a quantization of the energy cost
related to the desired data rate and compared to the sleep duration of nodes in
the network. Then, we open reflexion lines to find the best compromise:
consumption / mobility / guaranteed deadlines, in suggesting an adaptive
network structure from a concept of MANET.

ABSTRACT_BEGIN
  We offer a platform for database consultations and/or biomedical images
exchanges, adapted to the low rate wireless transmission, and intended for
general practitioners or specialists. The goal can be preventive, diagnostic
and therapeutic. it Concerns specialties such as radiology, ultrasound, the
anatomical pathology or endoscopy. The main features required in such a context
are to adjust the data compression of both the specific needs of telemedicine
and limited capabilities of wireless communication networks. We present our
approach in which we have set out criteria on Biomedical images quality,
compressed by the wavelet method to retain all the necessary information for an
accurate diagnosis, and determined the characteristics of a wireless network
with minimal performances for the transmission of these images within
constraints related to the modality and the data flow, in this case Wifi based
on the IEEE 802.11 standard. Our results will assess the capacity of this
standard in terms of speed, to transmit images at a rate of 10 frames per
second. It will be necessary to quantify the amount of information to add to
the image datas to enable a transmission in good conditions and the appropriate
modus operandi.

ABSTRACT_BEGIN
  Today, many network applications require shorter react time. Robotic field is
an excellent example of these needs: robot react time has a direct effect on
its task's complexity. Here, we propose a full deterministic medium access
method for a wireless robotic application. This contribution is based on some
low-power wireless personal area networks, like ZigBee standard. Indeed, ZigBee
has identified limits with Quality of Service due to non-determinist medium
access and probable collisions during medium reservation requests. In this
paper, two major improvements are proposed: an efficient polling of the star
nodes and a temporal deterministic distribution of peer-to-peer messages. This
new MAC protocol with no collision offers some QoS faculties.

ABSTRACT_BEGIN
  Today's industrial sensor networks require strong reliability and guarantees
on messages delivery. These needs are even more important in real time
applications like control/command, such as robotic wireless communications
where strong temporal constraints are critical. For these reasons, classical
random-based Medium Access Control (MAC) protocols present a non-null frame
collision probability. In this paper we present an original full deterministic
MAC-layer for industrial wireless network and its performance evaluation thanks
to the development of a material prototype.

ABSTRACT_BEGIN
  Control/command processes require a transmission system with some
characteristics like high reliability, low latency and strong guarantees on
messages delivery. Concerning wire networks, field buses technologies like FIP
offer this kind of service (periodic tasks, real time constraints...).
Unfortunately, few wireless technologies can propose a communication system
which respects such constraints. Indeed, wireless transmissions must deal with
medium characteristics which make impossible the direct translation of
mechanisms used with wire networks. The purpose of this paper is to present an
original Medium Access Control (MAC) layer for a real time Low Power-Wireless
Personal Area Network (LP-WPAN). The proposed MAC-layer has been validated by
several complementary methods; in this paper, we focus on the specific
Simultaneous Guaranteed Time Slot (SGTS) part.

ABSTRACT_BEGIN
  This paper is focused on the problem of optimizing the aggregate throughput
of the Distributed Coordination Function (DCF) employing the basic access
mechanism at the data link layer of IEEE 802.11 protocols. In order to broaden
the applicability of the proposed analysis, we consider general operating
conditions accounting for both non-saturated and saturated traffic in the
presence of transmission channel errors, as exemplified by the packet error
rate $P_e$.
  The main clue of this work stems from the relation that links the aggregate
throughput of the network to the packet rate $\lambda$ of the contending
stations. In particular, we show that the aggregate throughput $S(\lambda)$
presents two clearly distinct operating regions that depend on the actual value
of the packet rate $\lambda$ with respect to a critical value $\lambda_c$,
theoretically derived in this work.
  The behavior of $S(\lambda)$ paves the way to a cross-layer optimization
algorithm, which proved to be effective for maximizing the aggregate throughput
in a variety of network operating conditions. A nice consequence of the
proposed optimization framework relies on the fact that the aggregate
throughput can be predicted quite accurately with a simple, yet effective,
closed-form expression, which is also derived in the article.
  Finally, theoretical and simulation results are presented throughout the work
in order to unveil, as well as verify, the key ideas.

ABSTRACT_BEGIN
  This paper proposes and analyzes a distributed MAC protocol that achieves
zero collision with no control message exchange nor synchronization. ZC
(ZeroCollision) is neither reservation-based nor dynamic TDMA; the protocol
supports variable-length packets and does not lose efficiency when some of the
stations do not transmit. At the same time, ZC is not a CSMA; in its steady
state, it is completely collision-free. The stations transmit repeatedly in a
round-robin order once the convergence state is reached. If some stations skip
their turn, their transmissions are replaced by idle $20 \mu$-second mini-slots
that enable the other stations to keep track of their order. Because of its
short medium access delay and its efficiency, the protocol supports both
real-time and elastic applications. The protocol allows for nodes leaving and
joining the network; it can allocate more throughput to specific nodes (such as
an access point). The protocol is robust against carrier sensing errors or
clock drift. While collision avoidance is guaranteed in a single collision
domain, it is not the case in a multiple collision one. However, experiments
show ZC supports a comparable amount of goodput to CSMA in a multiple collision
domain environment. The paper presents an analysis and extensive simulations of
the protocol, confirming that ZC outperforms both CSMA and TDMA at high and low
load.

ABSTRACT_BEGIN
  Merging wireless traces is a fundamental step in measurement-based studies
involving multiple packet sniffers. Existing merging tools either require a
wired infrastructure or are limited in their usability. We propose WiPal, an
offline merging tool for IEEE 802.11 traces that has been designed to be
efficient and simple to use. WiPal is flexible in the sense that it does not
require any specific services, neither from monitors (like synchronization,
access to a wired network, or embedding specific software) nor from its
software environment (e.g. an SQL server). We present WiPal's operation and
show how its features - notably, its modular design - improve both ease of use
and efficiency. Experiments on real traces show that WiPal is an order of
magnitude faster than other tools providing the same features. To our
knowledge, WiPal is the only offline trace merger that can be used by the
research community in a straightforward fashion.

ABSTRACT_BEGIN
  This paper presents a modified proportional fairness (PF) criterion suitable
for mitigating the \textit{rate anomaly} problem of multirate IEEE 802.11
Wireless LANs employing the mandatory Distributed Coordination Function (DCF)
option. Compared to the widely adopted assumption of saturated network, the
proposed criterion can be applied to general networks whereby the contending
stations are characterized by specific packet arrival rates, $\lambda_s$, and
transmission rates $R_d^{s}$.
  The throughput allocation resulting from the proposed algorithm is able to
greatly increase the aggregate throughput of the DCF while ensuring fairness
levels among the stations of the same order of the ones available with the
classical PF criterion. Put simply, each station is allocated a throughput that
depends on a suitable normalization of its packet rate, which, to some extent,
measures the frequency by which the station tries to gain access to the
channel. Simulation results are presented for some sample scenarios, confirming
the effectiveness of the proposed criterion.

ABSTRACT_BEGIN
  Network coding is a recently proposed method for transmitting data, which has
been shown to have potential to improve wireless network performance. We study
network coding for one specific case of multicast, broadcasting, from one
source to all nodes of the network. We use network coding as a loss tolerant,
energy-efficient, method for broadcast. Our emphasis is on mobile networks. Our
contribution is the proposal of DRAGONCAST, a protocol to perform network
coding in such a dynamically evolving environment. It is based on three
building blocks: a method to permit real-time decoding of network coding, a
method to adjust the network coding transmission rates, and a method for
ensuring the termination of the broadcast. The performance and behavior of the
method are explored experimentally by simulations; they illustrate the
excellent performance of the protocol.

ABSTRACT_BEGIN
  Several users of our AS relationship inference data
(http://www.caida.org/data/active/as-relationships/), released with cs/0604017,
asked us why it contained AS relationship cycles, e.g., cases where AS A is a
provider of AS B, B is a provider of C, and C is a provider of A, or other
cycle types. Having been answering these questions in private communications,
we have eventually decided to write down our answers here for future reference.

ABSTRACT_BEGIN
  We propose behavior-oriented services as a new paradigm of communication in
mobile human networks. Our study is motivated by the tight user-network
coupling in future mobile societies. In such a paradigm, messages are sent to
inferred behavioral profiles, instead of explicit IDs. Our paper provides a
systematic framework in providing such services. First, user behavioral
profiles are constructed based on traces collected from two large wireless
networks, and their spatio-temporal stability is analyzed. The implicit
relationship discovered between mobile users could be utilized to provide a
service for message delivery and discovery in various network environments. As
an example application, we provide a detailed design of such a service in
challenged opportunistic network architecture, named CSI. We provide a fully
distributed solution using behavioral profile space gradients and small world
structures.
  Our analysis shows that user behavioral profiles are surprisingly stable,
i.e., the similarity of the behavioral profile of a user to its future
behavioral profile is above 0.8 for two days and 0.75 for one week, and remains
above 0.6 for five weeks. The correlation coefficient of the similarity metrics
between a user pair at different time instants is above 0.7 for four days, 0.62
for a week, and remains above 0.5 for two weeks. Leveraging such a stability in
user behaviors, the CSI service achieves delivery rate very close to the
delay-optimal strategy (above 94%), with minimal overhead (less than 84% of the
optimal). We believe that this new paradigm will act as an enabler of multiple
new services in mobile societies, and is potentially applicable in
server-based, heterogeneous or infrastructure-less wireless environments.

ABSTRACT_BEGIN
  Voice Service Providers (VSPs) participating in VoIP peering frequently want
to withhold their identity and related privacy-sensitive information from other
parties during the VoIP communication. A number of existing documents on VoIP
privacy exist, but most of them focus on end user privacy. By summarizing and
extending existing work, we present a unified privacy mechanism for both VoIP
users and service providers. We also show a case study on how VSPs can use this
mechanism for identity and topology hiding in VoIP peering.

ABSTRACT_BEGIN
  In contrast with most internet topology measurement research, our concern
here is not to obtain a map as complete and precise as possible of the whole
internet. Instead, we claim that each machine's view of this topology, which we
call ego-centered view, is an object worth of study in itself. We design and
implement an ego-centered measurement tool, and perform radar-like measurements
consisting of repeated measurements of such views of the internet topology. We
conduct long-term (several weeks) and high-speed (one round every few minutes)
measurements of this kind from more than one hundred monitors, and we provide
the obtained data. We also show that these data may be used to detect events in
the dynamics of internet topology.

ABSTRACT_BEGIN
  Many models have been proposed to generate Internet Autonomous System (AS)
topologies, most of which make structural assumptions about the AS graph. In
this paper we compare AS topology generation models with several observed AS
topologies. In contrast to most previous works, we avoid making assumptions
about which topological properties are important to characterize the AS
topology. Our analysis shows that, although matching degree-based properties,
the existing AS topology generation models fail to capture the complexity of
the local interconnection structure between ASs. Furthermore, we use BGP data
from multiple vantage points to show that additional measurement locations
significantly affect local structure properties, such as clustering and node
centrality. Degree-based properties, however, are not notably affected by
additional measurements locations. These observations are particularly valid in
the core. The shortcomings of AS topology generation models stems from an
underestimation of the complexity of the connectivity in the core caused by
inappropriate use of BGP data.

ABSTRACT_BEGIN
  Vehicle-to-vehicle communications can be used effectively for intelligent
transport systems (ITS) and location-aware services. The ability to disseminate
information in an ad-hoc fashion allows pertinent information to propagate
faster through the network. In the realm of ITS, the ability to spread warning
information faster and further is of great advantage to the receivers of this
information. In this paper we propose and present a message-dissemination
procedure that uses vehicular wireless protocols for influencing traffic flow,
reducing congestion in road networks. The computational experiments presented
in this paper show how an intelligent driver model (IDM) and car-following
model can be adapted to 'react' to the reception of information. This model
also presents the advantages of coupling together traffic modelling tools and
network simulation tools.

ABSTRACT_BEGIN
  This paper has been withdrawn due to errors in the analysis of data with
Carrier Access Rate control and statistical methodologies.

ABSTRACT_BEGIN
  Energy efficiency and transmission delay are very important parameters for
wireless multi-hop networks. Previous works that study energy efficiency and
delay are based on the assumption of reliable links. However, the unreliability
of the channel is inevitable in wireless multi-hop networks. This paper
investigates the trade-off between the energy consumption and the end-to-end
delay of multi-hop communications in a wireless network using an unreliable
link model. It provides a closed form expression of the lower bound on the
energy-delay trade-off for different channel models (AWGN, Raleigh flat fading
and Nakagami block-fading) in a linear network. These analytical results are
also verified in 2-dimensional Poisson networks using simulations. The main
contribution of this work is the use of a probabilistic link model to define
the energy efficiency of the system and capture the energy-delay trade-offs.
Hence, it provides a more realistic lower bound on both the energy efficiency
and the energy-delay trade-off since it does not restrict the study to the set
of perfect links as proposed in earlier works.

ABSTRACT_BEGIN
  We consider power control in cognitive radio networks where secondary users
identify and exploit instantaneous and local spectrum opportunities without
causing unacceptable interference to primary users. We qualitatively
characterize the impact of the transmission power of secondary users on the
occurrence of spectrum opportunities and the reliability of opportunity
detection. Based on a Poisson model of the primary network, we quantify these
impacts by showing that (i) the probability of spectrum opportunity decreases
exponentially with respect to the transmission power of secondary users, where
the exponential decay constant is given by the traffic load of primary users;
(ii) reliable opportunity detection is achieved in the two extreme regimes in
terms of the ratio between the transmission power of secondary users and that
of primary users. Such analytical characterizations allow us to study power
control for optimal transport throughput under constraints on the interference
to primary users. Furthermore, we reveal the difference between detecting
primary signals and detecting spectrum opportunities, and demonstrate the
complex relationship between physical layer spectrum sensing and MAC layer
throughput. The dependency of this PHY-MAC interaction on the application type
and the use of handshake signaling such as RTS/CTS is illustrated.

ABSTRACT_BEGIN
  Acyclic preferences recently appeared as an elegant way to model many
distributed systems. An acyclic instance admits a unique stable configuration,
which can reveal the performance of the system. In this paper, we give the
statistical properties of the stable configuration for three classes of acyclic
preferences: node-based preferences, distance-based preferences, and random
acyclic systems. Using random overlay graphs, we prove using mean-field and
fluid-limit techniques that these systems have an asymptotically continuous
independent rank distribution for a proper scaling, and the analytical solution
is compared to simulations. These results provide a theoretical ground for
validating the performance of bandwidth-based or proximity-based unstructured
systems.

ABSTRACT_BEGIN
  This paper focuses on multirate IEEE 802.11 Wireless LAN employing the
mandatory Distributed Coordination Function (DCF) option. Its aim is threefold.
Upon starting from the multi-dimensional Markovian state transition model
proposed by Malone \textit{et.al.} for characterizing the behavior of the IEEE
802.11 protocol at the Medium Access Control layer, it presents an extension
accounting for packet transmission failures due to channel errors. Second, it
establishes the conditions under which a network constituted by $N$ stations,
each station transmitting with its own bit rate, $R^{(s)}_d$, and packet rate,
$\lambda_s$, can be assumed loaded. Finally, it proposes a modified
Proportional Fairness (PF) criterion, suitable for mitigating the \textit{rate
anomaly} problem of multirate loaded IEEE 802.11 Wireless LANs, employing the
mandatory DCF option. Compared to the widely adopted assumption of saturated
network, the proposed fairness criterion can be applied to general loaded
networks.
  The throughput allocation resulting from the proposed algorithm is able to
greatly increase the aggregate throughput of the DCF, while ensuring fairness
levels among the stations of the same order as the ones guaranteed by the
classical PF criterion.
  Simulation results are presented for some sample scenarios, confirming the
effectiveness of the proposed criterion for optimized throughput allocation.

ABSTRACT_BEGIN
  An ad-hoc wireless network is a collection of nodes that come together to
dynamically create a network, with no fixed infrastructure or centralized
administration. An ad-hoc network is characterized by energy constrained nodes,
bandwidth constrained links and dynamic topology. With the growing use of
wireless networks (including ad-hoc networks) for real-time applications, such
as voice, video, and real-time data, the need for Quality of Service (QoS)
guarantees in terms of delay, bandwidth, and packet loss is becoming
increasingly important. Providing QoS in ad-hoc networks is a challenging task
because of dynamic nature of network topology and imprecise state information.
Hence, it is important to have a dynamic routing protocol with fast re-routing
capability, which also provides stable route during the life-time of the flows.
  In this thesis, we have proposed a novel, energy aware, stable routing
protocol named, Stability-based QoS-capable Ad-hoc On-demand Distance Vector
(SQ-AODV), which is an enhancement of the well-known Ad-hoc On-demand Distance
Vector (AODV) routing protocol for ad-hoc wireless networks. SQ-AODV utilizes a
cross-layer design approach in which information about the residual energy of a
node is used for route selection and maintenance. An important feature of
SQ-AODV protocol is that it uses only local information and requires no
additional communication or co-operation between the network nodes. SQ-AODV
possesses a make-before-break re-routing capability that enables near-zero
packet drops and is compatible with the basic AODV data formats and operation,
making it easy to adopt in ad-hoc networks.

ABSTRACT_BEGIN
  In this paper, we investigate the network utility maximization problem in
FDMA systems. We summarize with a suite of criteria on designing utility
functions so as to achieve the global optimization convex. After proposing the
general form of the utility functions, we present examples of commonly used
utility function forms that are consistent with the criteria proposed in this
paper, which include the well-known proportional fairness function and the
sigmoidal-like functions. In the second part of this paper, we use numerical
results to demonstrate a case study based on the criteria mentioned above,
which deals with the subcarrier scheduling problem with dynamic rate allocation
in FDMA system.

ABSTRACT_BEGIN
  A distributed spiral algorithm for distributed optimization in WSN is
proposed. By forming a spiral-shape message passing scheme among clusters,
without loss of estimation accuracy and convergence speed, the algorithm is
proved to converge with a lower total transport cost than the distributed
in-cluster algorithm.

ABSTRACT_BEGIN
  Conventional heterogeneous-traffic scheduling schemes utilize zero-delay
constraint for real-time services, which aims to minimize the average packet
delay among real-time users. However, in light or moderate load networks this
strategy is unnecessary and leads to low data throughput for non-real-time
users. In this paper, we propose a heuristic scheduling scheme to solve this
problem. The scheme measures and assigns scheduling priorities to both
real-time and non-real-time users, and schedules the radio resources for the
two user classes simultaneously. Simulation results show that the proposed
scheme efficiently handles the heterogeneous-traffic scheduling with diverse
QoS requirements and alleviates the unfairness between real-time and
non-real-time services under various traffic loads.

ABSTRACT_BEGIN
  In this paper, we compare the performances of cooperative and distributed
spectrum sensing in wireless sensor networks. After introducing the basic
problem, we describe two strategies: 1) a cooperative sensing strategy, which
takes advantage of cooperation diversity gain to increase probability of
detection and 2) a distributed sensing strategy, which by passing the results
in an inter-node manner increases energy efficiency and fairness among nodes.
Then, we compare the performances of the strategies in terms of three criteria:
agility, energy efficiency, and robustness against SNR changes, and summarize
the comparison. It shows that: 1) the non-cooperative strategy has the best
fairness of energy consumption, 2) the cooperative strategy leads to the best
agility, and 3) the distributed strategy leads to the lowest energy consumption
and the best robustness against SNR changes.

ABSTRACT_BEGIN
  This paper first presents a parallel solution for the Flowshop Scheduling
Problem in parallel environment, and then proposes a novel load balancing
strategy. The proposed Proportional Fairness Strategy (PFS) takes computational
performance of computing process sets into account, and assigns additional load
to computing nodes proportionally to their evaluated performance. In order to
efficiently utilize the power of parallel resource, we also discuss the data
structure used in communications among computational nodes and design an
optimized data transfer strategy. This data transfer strategy combined with the
proposed load balancing strategy have been implemented and tested on a super
computer consisted of 86 CPUs using MPI as the middleware. The results show
that the proposed PFS achieves better performance in terms of computing time
than the existing Adaptive Contracting Within Neighborhood Strategy. We also
show that the combination of both the Proportional Fairness Strategy and the
proposed data transferring strategy achieves additional 13~15% improvement in
efficiency of parallelism.

ABSTRACT_BEGIN
  This paper presents a capture of the queries managed by an eDonkey server
during almost 10 weeks, leading to the observation of almost 9 billion messages
involving almost 90 million users and more than 275 million distinct files.
Acquisition and management of such data raises several challenges, which we
discuss as well as the solutions we developed. We obtain a very rich dataset,
orders of magnitude larger than previously avalaible ones, which we provide for
public use. We finally present basic analysis of the obtained data, which
already gives evidence of non-trivial features.

ABSTRACT_BEGIN
  We study a sensor node with an energy harvesting source. The generated energy
can be stored in a buffer. The sensor node periodically senses a random field
and generates a packet. These packets are stored in a queue and transmitted
using the energy available at that time. We obtain energy management policies
that are throughput optimal, i.e., the data queue stays stable for the largest
possible data rate. Next we obtain energy management policies which minimize
the mean delay in the queue.We also compare performance of several easily
implementable sub-optimal energy management policies. A greedy policy is
identified which, in low SNR regime, is throughput optimal and also minimizes
mean delay.

ABSTRACT_BEGIN
  This paper presents a new rate based call gapping method. The main advantage
is that it provides maximal throughput, priority handling and fairness for
traffic classes without queues, unlike Token Bucket which provides only the
first two or Weighted Fair Queuing that uses queues. The Token Bucket is used
for call gapping because it has good throughput characteristics. For this
reason we present a mixture of the two methods keeping the good properties of
both. A mathematical model has been developed to support our proposal. It
defines the three requirements and proves theorems about if they are satisfied
with the different call gapping mechanisms. Simulation, numerical results and
statistical discussion are also presented to underpin the findings.

ABSTRACT_BEGIN
  This work calls into question a substantial body of past work on CSMA
wireless networks. In the majority of studies on CSMA wireless networks, a
contention graph is used to model the carrier sensing relationships (CS) among
links. This is a 0-1 model in which two links can either sense each other
completely or not. In real experiments, we observed that this is generally not
the case: the CS relationship between the links are often probabilistic and can
vary dynamically over time. This is the case even if the distance between the
links is fixed and there is no drastic change in the environment. Furthermore,
this partial carrier sensing relationship is prevalent and occurs over a wide
range of distances between the links. This observation is not consistent with
the 0-1 contention graph and implies that many results and conclusions drawn
from previous theoretical studies need to be re-examined. This paper
establishes a more accurate CS model with the objective of laying down a
foundation for future theoretical studies that reflect reality. Towards that
end, we set up detailed experiments to investigate the partial carrier sensing
phenomenon. We discuss the implications and the use of our partial carrier
sensing model in network analysis.

ABSTRACT_BEGIN
  This paper proposes a new reliability algorithm specifically useful when
retransmission is either problematic or not possible. In case of multimedia or
multicast communications and in the context of the Delay Tolerant Networking
(DTN), the classical retransmission schemes can be counterproductive in terms
of data transfer performance or not possible when the acknowledgment path is
not always available. Indeed, over long delay links, packets retransmission has
a meaning of cost and must be minimized.In this paper, we detail a novel
reliability mechanism with an implicit acknowledgment strategy that could be
used either within these new DTN proposals, for multimedia traffic or in the
context of multicast transport protocols. This proposal is based on a new
on-the-fly erasure coding concept specifically designed to operate efficient
reliable transfer over bi-directional links. This proposal, named Tetrys,
allows to unify a full reliability with an error correction scheme. In this
paper, we model the performance of this proposal and demonstrate with a
prototype, that we can achieve a full reliability without acknowledgment path
confirmation. Indeed, the main findings are that Tetrys is not sensitive to the
loss of acknowledgments while ensuring a faster data availability to the
application compared to other traditional acknowledgment schemes. Finally, we
pave the first step of the integration of such algorithm inside a congestion
controlled protocol.

ABSTRACT_BEGIN
  There is a trend towards using wireless technologies in networked control
systems. However, the adverse properties of the radio channels make it
difficult to design and implement control systems in wireless environments. To
attack the uncertainty in available communication resources in wireless control
systems closed over WLAN, a cross-layer adaptive feedback scheduling (CLAFS)
scheme is developed, which takes advantage of the co-design of control and
wireless communications. By exploiting cross-layer design, CLAFS adjusts the
sampling periods of control systems at the application layer based on
information about deadline miss ratio and transmission rate from the physical
layer. Within the framework of feedback scheduling, the control performance is
maximized through controlling the deadline miss ratio. Key design parameters of
the feedback scheduler are adapted to dynamic changes in the channel condition.
An event-driven invocation mechanism for the feedback scheduler is also
developed. Simulation results show that the proposed approach is efficient in
dealing with channel capacity variations and noise interference, thus providing
an enabling technology for control over WLAN.

ABSTRACT_BEGIN
  This paper investigates the impact of carrier frequency offset (CFO) and
sampling frequency offset (SFO) on the performance of different MIMO-OFDM
schemes with high spectral efficiency for next generation of terrestrial
digital TV. We analyze particularly orthogonal Alamouti scheme, and
non-orthogonal (NO) schemes like VBLAST, linear dispersion (LD) code and Golden
code. This analysis gives a global view on the best suitable MIMO-OFDM scheme
with respect to CFO and SFO. We show that for high spectral efficiency,
Alamouti is more sensitive to CFO and SFO. Moreover, we show that all studied
MIMO-OFDM schemes are sensitive to CFO when it is greater than 1% of
inter-carrier spacing. Their sensitivity due to SFO is less than that due to
CFO.

ABSTRACT_BEGIN
  This article investigates the effect of equal and unequal received powers on
the performances of different MIMO-OFDM schemes for terrestrial digital TV.
More precisely, we focus on three types of non-orthogonal schemes: the BLAST
scheme, the Linear Dispersion (LD) code and the Golden code, and we compare
their performances to that of Alamouti scheme. Using two receiving antennas, we
show that for moderate attenuation on the second antenna and high spectral
efficiency, Golden code outperforms other schemes. However, Alamouti scheme
presents the best performance for low spectral efficiency and equal received
powers or when one antenna is dramatically damaged. When three antennas are
used, we show that Golden code offers the highest robustness to power unbalance
at the receiving side

ABSTRACT_BEGIN
  This article introduces a 3D space-time-space block code for future
terrestrial digital TV in single frequency networks. The proposed 3D code is
based on a double layer structure designed for inter-cell and intra-cell space
time coded transmissions. We show that this new structure is particularly
efficient for SFN environments regardless of the location of the receiver. It
is then suitable for fixed, portable and mobile receptions.

ABSTRACT_BEGIN
  Bit error rate (BER) prediction over channel realisations has emerged as an
active research area. In this paper, we give analytical signal to interference
and noise ratio (SINR) evaluation of MIMO-OFDM systems using an iterative
receiver. Using this analytical SINR expression, we propose an accurate BER
prediction method based on effective exponential SINR mapping (EESM) method. We
show by simulations that our method is independent of the channel realisation
and of the MIMO scheme. It is only dependent on the modulation and coding
scheme.

ABSTRACT_BEGIN
  This paper investigates the impact of carrier frequency offset (CFO) on the
performance of different MIMO-OFDM schemes with high spectral efficiency for
next generation of terrestrial digital TV. We show that all studied MIMO-OFDM
schemes are sensitive to CFO when it is greater than 1% of inter-carrier
spacing. We show also that the Alamouti scheme is the most sensitive MIMO
scheme to CFO

ABSTRACT_BEGIN
  This letter introduces a 3D space-time-space block code for future digital TV
systems. The code is based on a double layer structure for inter-cell and
intra-cell transmission mode in single frequency networks. Without increasing
the complexity of the receiver, the proposed code is very efficient for
different transmission scenarios.

ABSTRACT_BEGIN
  Orthogonal frequency-division multiplexing (OFDM) is the most popular
transmission technology in digital terrestrial broadcasting (DTTB), adopted by
many DTTB standards. In this paper, the bit error rate (BER) performance of two
DTTB systems, namely cyclic prefix OFDM (CP-OFDM) based DVB-T and time domain
synchronous OFDM (TDS-OFDM) based DTMB, is evaluated in different channel
conditions. Spectrum utilization and power efficiency are also discussed to
demonstrate the transmission overhead of both systems. Simulation results show
that the performances of the two systems are much close. Given the same ratio
of guard interval (GI), the DVB-T outperforms DTMB in terms of signal to noise
ratio (SNR) in Gaussian and Ricean channels, while DTMB behaves better
performance in Rayleigh channel in higher code rates and higher orders of
constellation thanks to its efficient channel coding and interleaving scheme.

ABSTRACT_BEGIN
  In this paper, we propose a novel channel estimation technique based on
spread pilots for digital video broadcasting. This technique consists in adding
a linear preceding function before the OFDM modulation and dedicating one of
the preceding sequence to transmit the pilot symbols for the channel
estimation. The merits of this technique are its simplicity, its flexibility,
and the gains in terms of spectral efficiency and useful bit rate obtained
compared to the classical pilot based estimation schemes used in DVB standards.
The performance evaluated over realistic channel models, shows the efficiency
of this technique which turns out to be a promising channel estimation
technique for the future terrestrial video broadcasting systems.

ABSTRACT_BEGIN
  In this paper, we propose a novel channel estimation technique based on 2D
spread pilots. The merits of this technique are its simplicity, its flexibility
regarding the transmission scenarios, and the spectral efficiency gain obtained
compared to the classical pilot based estimation schemes used in DVB standards.
We derive the analytical expression of the mean square error of the estimator
and show it is a function of the autocorrelation of the channel in both time
and frequency domains. The performance evaluated over a realistic channel model
shows the efficiency of this technique which turns out to be a promising
channel estimation for the future mobile video broadcasting systems.

ABSTRACT_BEGIN
  In wireless sensor networks, bandwidth is one of precious resources to
multimedia applications. To get more bandwidth, multipath routing is one
appropriate solution provided that inter-path interferences are minimized. In
this paper, we address the problem of interfering paths in the context of
wireless multimedia sensor networks and consider both intra-session as well as
inter-session interferences. Our main objective is to provide necessary
bandwidth to multimedia applications through non-interfering paths while
increasing the network lifetime. To do so, we adopt an incremental approach
where for a given session, only one path is built at once. Additional paths are
built when required, typically in case of congestion or bandwidth shortage.
Interference awareness and energy saving are achieved by switching a subset of
sensor nodes in a {\em passive state} in which they do not take part in the
routing process. Despite the routing overhead introduced by the incremental
approach we adopt, our simulations show that this can be compensated by the
overall achieved throughput and the amount of consumed energy per correctly
received packet especially for relatively long sessions such as multimedia
ones. This is mainly due to the fact that a small number of non-interfering
paths allows for better performances than a large number of interfering ones.

ABSTRACT_BEGIN
  Wireless sensor networks hold a great potential in the deployment of several
applications of a paramount importance in our daily life. Video sensors are
able to improve a number of these applications where new approaches adapted to
both wireless sensor networks and video transport specific characteristics are
required. The aim of this work is to provide the necessary bandwidth and to
alleviate the congestion problem to video streaming. In this paper, we
investigate various load repartition strategies for congestion control
mechanism on top of a multipath routing feature. Simulations are performed in
order to get insight into the performances of our proposals.

ABSTRACT_BEGIN
  Using time domain measurements, we assess the feasibility of time-reversal
technique in ultra-wideband (UWB) communication. A typical indoor propagation
channel is selected for the exploration. The channel response between receive
and transmit antenna pairs is measured using time domain equipments which
include an arbitrary wave generator (AWG) and a digital storage oscilloscope
(DSO). The time-reversed version of the channel response is constructed with
AWG and re-transmitted in the channel. The equivalent time reversed channel
response is recorded. The properties of the time reversal technique in the line
of sight (LOS) co-polar and cross-polar scenarios are measured.

ABSTRACT_BEGIN
  UWB communication is a recent research area for indoor propagation channels.
Time Reversal (TR) communication in UWB has shown promising results for
improving the system performance. In multiuser environment, the system
performance is significantly degraded due to the interference among different
users. TR reduces the interference caused by multiusers due to its spatial
focusing property. The performance of a multiuser TR communication system is
further improved if the TR filter is modified. In this paper, multiuser TR in
UWB communication is investigated using simple TR filter and a modified TR
filter with circular shift operation. The concept of circular shift in TR is
analytically studied. Thereafter, the channel impulse responses (CIR) of a
typical indoor laboratory environment are measured. The measured CIRs are used
to analyze the received signal peak power and signal to interference ratio
(SIR) with and without performing the circular shift operation in a multiuser
environment.

ABSTRACT_BEGIN
  Existing transport protocols, be it TCP, SCTP or DCCP, do not provide an
efficient congestion control mechanism for heterogeneous wired-cum-wireless
networks. Solutions involving implicit loss discrimination schemes have been
proposed but were never implemented. Appropriate mechanisms can dramatically
improve bandwidth usage over the Internet, especially for multimedia transport
based on partial reliability. In this paper we have implemented and evaluated a
congestion control mechanism that implicitly discriminates congestion and
wireless losses in the datagram congestion control protocol (DCCP) congestion
control identification (CCID) framework. The new CCID was implemented as a NS-2
module. Comparisons were made with the TCP-like CCID and showed that the
bandwidth utilization was improved by more than 30% and up to 50% in
significant setups.

ABSTRACT_BEGIN
  The sharing of network traces is an important prerequisite for the
development and evaluation of efficient anomaly detection mechanisms.
Unfortunately, privacy concerns and data protection laws prevent network
operators from sharing these data. Anonymization is a promising solution in
this context; however, it is unclear if the sanitization of data preserves the
traffic characteristics or introduces artifacts that may falsify traffic
analysis results. In this paper, we examine the utility of anonymized flow
traces for anomaly detection. We quantitatively evaluate the impact of IP
address anonymization, namely variations of permutation and truncation, on the
detectability of large-scale anomalies. Specifically, we analyze three weeks of
un-sampled and non-anonymized network traces from a medium-sized backbone
network. We find that all anonymization techniques, except prefix-preserving
permutation, degrade the utility of data for anomaly detection. We show that
the degree of degradation depends to a large extent on the nature and mix of
anomalies present in a trace. Moreover, we present a case study that
illustrates how traffic characteristics of individual hosts are distorted by
anonymization.

ABSTRACT_BEGIN
  A protocol named Threshold Bipolar (TB) is proposed as a fetching strategy at
the startup stage of p2p live streaming systems. In this protocol, chunks are
fetched consecutively from buffer head at the beginning. After the buffer is
filled into a threshold, chunks at the buffer tail will be fetched first while
keeping the contiguously filled part in the buffer above the threshold even
when the buffer is drained at a playback rate. High download rate, small
startup latency and natural strategy handover can be reached at the same time
by this protocol. Important parameters in this protocol are identified. The
buffer progress under this protocol is then expressed as piecewise lines
specified by those parameters. Startup traces of peers measured from PPLive are
studied to show the real performance of TB protocol in a real system. A simple
design model of TB protocol is proposed to reveal important considerations in a
practical design.

ABSTRACT_BEGIN
  In a radio network with single source-destination pair and some relays, a
link between any two nodes is considered to have same or zero path loss.
However in practice some links may have considerably high path loss than others
but still being useful. In this report, we take into account signals received
from these links also. \indent Our system model consists of a
source-destination pair with two layers of relays in which weaker links between
source and second layer and between the first layer and destination are also
considered. We propose some protocols in this system model, run simulations
under optimum power allocation, and compare these protocols. We show that under
reasonable channel strength of these weaker links, the proposed protocols
perform ($ \approx 2$ dB) better than the existing basic protocol. As expected,
the degree of improvement increases with the strength of the weaker links. We
also show that with the receive channel knowledge in relays, the reliability
and data rate are improved.

ABSTRACT_BEGIN
  Burst contention is a well known challenging problem in Optical Burst
Switching (OBS) networks. Deflection routing is used to resolve contention.
Burst retransmission is used to reduce the Burst Loss Ratio (BLR) by
retransmitting dropped bursts. Previous works show that combining deflection
and retransmission outperforms both pure deflection and pure retransmission
approaches. This paper proposes a new Adaptive Hybrid Deflection and
Retransmission (AHDR) approach that dynamically combines deflection and
retransmission approaches based on network conditions such as BLR and link
utilization. Network Simulator 2 (ns-2) is used to simulate the proposed
approach on different network topologies. Simulation results show that the
proposed approach outperforms static approaches in terms of BLR and goodput.

ABSTRACT_BEGIN
  In a two tier cellular network -- comprised of a central macrocell underlaid
with shorter range femtocell hotspots -- cross-tier interference limits overall
capacity with universal frequency reuse. To quantify near-far effects with
universal frequency reuse, this paper derives a fundamental relation providing
the largest feasible cellular Signal-to-Interference-Plus-Noise Ratio (SINR),
given any set of feasible femtocell SINRs. We provide a link budget analysis
which enables simple and accurate performance insights in a two-tier network. A
distributed utility-based SINR adaptation at femtocells is proposed in order to
alleviate cross-tier interference at the macrocell from cochannel femtocells.
The Foschini-Miljanic (FM) algorithm is a special case of the adaptation. Each
femtocell maximizes their individual utility consisting of a SINR based reward
less an incurred cost (interference to the macrocell). Numerical results show
greater than 30% improvement in mean femtocell SINRs relative to FM. In the
event that cross-tier interference prevents a cellular user from obtaining its
SINR target, an algorithm is proposed that reduces transmission powers of the
strongest femtocell interferers. The algorithm ensures that a cellular user
achieves its SINR target even with 100 femtocells/cell-site, and requires a
worst case SINR reduction of only 16% at femtocells. These results motivate
design of power control schemes requiring minimal network overhead in two-tier
networks with shared spectrum.

ABSTRACT_BEGIN
  Realistic mobility models are fundamental to evaluate the performance of
protocols in mobile ad hoc networks. Unfortunately, there are no mobility
models that capture the non-homogeneous behaviors in both space and time
commonly found in reality, while at the same time being easy to use and
analyze. Motivated by this, we propose a time-variant community mobility model,
referred to as the TVC model, which realistically captures spatial and temporal
correlations. We devise the communities that lead to skewed location visiting
preferences, and time periods that allow us to model time dependent behaviors
and periodic re-appearances of nodes at specific locations.
  To demonstrate the power and flexibility of the TVC model, we use it to
generate synthetic traces that match the characteristics of a number of
qualitatively different mobility traces, including wireless LAN traces,
vehicular mobility traces, and human encounter traces. More importantly, we
show that, despite the high level of realism achieved, our TVC model is still
theoretically tractable. To establish this, we derive a number of important
quantities related to protocol performance, such as the average node degree,
the hitting time, and the meeting time, and provide examples of how to utilize
this theory to guide design decisions in routing protocols.

ABSTRACT_BEGIN
  The lack of extensive research in the application of inexpensive wireless
sensor nodes for the early detection of wildfires motivated us to investigate
the cost of such a network. As a first step, in this paper we present several
results which relate the time to detection and the burned area to the number of
sensor nodes in the region which is protected. We prove that the probability
distribution of the burned area at the moment of detection is approximately
exponential, given that some hypotheses hold: the positions of the sensor nodes
are independent random variables uniformly distributed and the number of sensor
nodes is large. This conclusion depends neither on the number of ignition
points nor on the propagation model of the fire.

ABSTRACT_BEGIN
  We study sensor networks with energy harvesting nodes. The generated energy
at a node can be stored in a buffer. A sensor node periodically senses a random
field and generates a packet. These packets are stored in a queue and
transmitted using the energy available at that time at the node. For such
networks we develop efficient energy management policies. First, for a single
node, we obtain policies that are throughput optimal, i.e., the data queue
stays stable for the largest possible data rate. Next we obtain energy
management policies which minimize the mean delay in the queue. We also compare
performance of several easily implementable suboptimal policies. A greedy
policy is identified which, in low SNR regime, is throughput optimal and also
minimizes mean delay. Next using the results for a single node, we develop
efficient MAC policies.

ABSTRACT_BEGIN
  Both IEEE 802.15.4 and 802.15.4a standards allow for dynamic channel
allocation and use of multiple channels available at their physical layers but
its MAC protocols are designed only for single channel. Also, sensor's
transceivers such as CC2420 provide multiple channels and as shown in [1], [2]
and [3] channel switch latency of CC2420 transceiver is just about 200$\mu$s.
In order to enhance both energy efficiency and to shorten end to end delay, we
propose, in this report, a spectrum-efficient frequency allocation schemes that
are able to statically assign control channels and dynamically reuse data
channels for Personal Area Networks (PANs) inside a Large-Scale WSN based on
UWB technology.

ABSTRACT_BEGIN
  How can we protect the network infrastructure from malicious traffic, such as
scanning, malicious code propagation, and distributed denial-of-service (DDoS)
attacks? One mechanism for blocking malicious traffic is filtering: access
control lists (ACLs) can selectively block traffic based on fields of the IP
header. Filters (ACLs) are already available in the routers today but are a
scarce resource because they are stored in the expensive ternary content
addressable memory (TCAM).
  In this paper, we develop, for the first time, a framework for studying
filter selection as a resource allocation problem. Within this framework, we
study five practical cases of source address/prefix filtering, which correspond
to different attack scenarios and operator's policies. We show that filter
selection optimization leads to novel variations of the multidimensional
knapsack problem and we design optimal, yet computationally efficient,
algorithms to solve them. We also evaluate our approach using data from
Dshield.org and demonstrate that it brings significant benefits in practice.
Our set of algorithms is a building block that can be immediately used by
operators and manufacturers to block malicious traffic in a cost-efficient way.

ABSTRACT_BEGIN
  This paper proposes an analytical framework for peer-to-peer (P2P) networks
and introduces schemes for building P2P networks to approach the minimum
weighted average download time (WADT). In the considered P2P framework, the
server, which has the information of all the download bandwidths and upload
bandwidths of the peers, minimizes the weighted average download time by
determining the optimal transmission rate from the server to the peers and from
the peers to the other peers. This paper first defines the static P2P network,
the hierarchical P2P network and the strictly hierarchical P2P network. Any
static P2P network can be decomposed into an equivalent network of sub-peers
that is strictly hierarchical. This paper shows that convex optimization can
minimize the WADT for P2P networks by equivalently minimizing the WADT for
strictly hierarchical networks of sub-peers. This paper then gives an upper
bound for minimizing WADT by constructing a hierarchical P2P network, and lower
bound by weakening the constraints of the convex problem. Both the upper bound
and the lower bound are very tight. This paper also provides several suboptimal
solutions for minimizing the WADT for strictly hierarchical networks, in which
peer selection algorithms and chunk selection algorithm can be locally
designed.

ABSTRACT_BEGIN
  This paper has been withdrawn by the authors.

ABSTRACT_BEGIN
  Peer-to-peer (P2P) locality has recently raised a lot of interest in the
community. Indeed, whereas P2P content distribution enables financial savings
for the content providers, it dramatically increases the traffic on inter-ISP
links. To solve this issue, the idea to keep a fraction of the P2P traffic
local to each ISP was introduced a few years ago. Since then, P2P solutions
exploiting locality have been introduced. However, several fundamental issues
on locality still need to be explored. In particular, how far can we push
locality, and what is, at the scale of the Internet, the reduction of traffic
that can be achieved with locality? In this paper, we perform extensive
experiments on a controlled environment with up to 10 000 BitTorrent clients to
evaluate the impact of high locality on inter-ISP links traffic and peers
download completion time. We introduce two simple mechanisms that make high
locality possible in challenging scenarios and we show that we save up to
several orders of magnitude inter-ISP traffic compared to traditional locality
without adversely impacting peers download completion time. In addition, we
crawled 214 443 torrents representing 6 113 224 unique peers spread among 9 605
ASes. We show that whereas the torrents we crawled generated 11.6 petabytes of
inter-ISP traffic, our locality policy implemented for all torrents would have
reduced the global inter-ISP traffic by 40%.

ABSTRACT_BEGIN
  In this paper, we address the problem of K-out-of-L exclusion, a
generalization of the mutual exclusion problem, in which there are $\ell$ units
of a shared resource, and any process can request up to $\mathtt k$ units
($1\leq\mathtt k\leq\ell$). We propose the first deterministic self-stabilizing
distributed K-out-of-L exclusion protocol in message-passing systems for
asynchronous oriented tree networks which assumes bounded local memory for each
process.

ABSTRACT_BEGIN
  Heterogeneous applications could be assimilated within the same wireless
sensor network with the aid of modern motes that have multiple sensor boards on
a single radio board. Different types of data generated from such types of
motes might have different transmission characteristics in terms of priority,
transmission rate, required bandwidth, tolerable packet loss, delay demands
etc. Considering a sensor network consisting of such multi-purpose nodes, in
this paper we propose Prioritized Heterogeneous Traffic-oriented Congestion
Control Protocol (PHTCCP) which ensures efficient rate control for prioritized
heterogeneous traffic. Our protocol uses intra-queue and inter-queue priorities
for ensuring feasible transmission rates of heterogeneous data. It also
guarantees efficient link utilization by using dynamic transmission rate
adjustment. Detailed analysis and simulation results are presented along with
the description of our protocol to demonstrate its effectiveness in handling
prioritized heterogeneous traffic in wireless sensor networks.

ABSTRACT_BEGIN
  We study in this paper an $M/M/1$ queue whose server rate depends upon the
state of an independent Ornstein-Uhlenbeck diffusion process $(X(t))$ so that
its value at time $t$ is $\mu \phi(X(t))$, where $\phi(x)$ is some bounded
function and $\mu>0$. We first establish the differential system for the
conditional probability density functions of the couple $(L(t),X(t))$ in the
stationary regime, where $L(t)$ is the number of customers in the system at
time $t$. By assuming that $\phi(x)$ is defined by $\phi(x) = 1-\varepsilon
((x\wedge a/\varepsilon)\vee(-b/\varepsilon))$ for some positive real numbers
$a$, $b$ and $\varepsilon$, we show that the above differential system has a
unique solution under some condition on $a$ and $b$. We then show that this
solution is close, in some appropriate sense, to the solution to the
differential system obtained when $\phi$ is replaced with
$\Phi(x)=1-\varepsilon x$ for sufficiently small $\varepsilon$. We finally
perform a perturbation analysis of this latter solution for small
$\varepsilon$. This allows us to check at the first order the validity of the
so-called reduced service rate approximation, stating that everything happens
as if the server rate were constant and equal to $\mu(1-\eps\E(X(t)))$.

ABSTRACT_BEGIN
  We show in this note that by deterministic packet sampling, the tail of the
distribution of the original flow size can be obtained by rescaling that of the
sampled flow size. To recover information on the flow size distribution lost
through packet sampling, we propose some heuristics based on measurements from
different backbone IP networks. These heuristic arguments allow us to recover
the complete flow size distribution.

ABSTRACT_BEGIN
  We consider in this paper an urn and ball problem with replacement, where
balls are with different colors and are drawn uniformly from a unique urn. The
numbers of balls with a given color are i.i.d. random variables with a heavy
tailed probability distribution, for instance a Pareto or a Weibull
distribution. We draw a small fraction $p\ll 1$ of the total number of balls.
The basic problem addressed in this paper is to know to which extent we can
infer the total number of colors and the distribution of the number of balls
with a given color. By means of Le Cam's inequality and Chen-Stein method,
bounds for the total variation norm between the distribution of the number of
balls drawn with a given color and the Poisson distribution with the same mean
are obtained. We then show that the distribution of the number of balls drawn
with a given color has the same tail as that of the original number of balls.
We finally establish explicit bounds between the two distributions when each
ball is drawn with fixed probability $p$.

ABSTRACT_BEGIN
  In this paper we investigate the value of a social network with respect to
the probability mechanism underlying its structure. Specifically, we compute
the value for small world and scale free networks. We provide evidence in
support of the value to be given by Zipfs law.

ABSTRACT_BEGIN
  When we represent a network of sensors in Euclidean space by a graph, there
are two distances between any two nodes that we may consider. One of them is
the Euclidean distance. The other is the distance between the two nodes in the
graph, defined to be the number of edges on a shortest path between them. In
this paper, we consider a network of sensors placed uniformly at random in a
two-dimensional region and study two conditional distributions related to these
distances. The first is the probability distribution of distances in the graph,
conditioned on Euclidean distances; the other is the probability density
function associated with Euclidean distances, conditioned on distances in the
graph. We study these distributions both analytically (when feasible) and by
means of simulations. To the best of our knowledge, our results constitute the
first of their kind and open up the possibility of discovering improved
solutions to certain sensor-network problems, as for example sensor
localization.

ABSTRACT_BEGIN
  Researchers have proposed a variety of Internet topology models. However
almost all of them focus on generating one graph based on one single static
source graph. On the other hand, Internet topology is evolving over time
continuously with the addition and deletion of nodes and edges. If a model is
based on all the topologies in the past, instead of one of them, it will be
more accurate and closer to the real world topology. In this paper, we study
the Internet As-level topology time-series from two different sources and find
that both of them obey four same dynamic graph patterns. Then we propose a mode
that can infer the topology in the future based on all the topologies in the
past. Through theoretical and experimental analysis, we prove the topology that
our model generates can match both the static and dynamic graph patterns. In
addition, the parameters in the model are meaningful. Finally, we theoretically
and experimentally prove that these parameters are directly related to some
important graph characteristics.

ABSTRACT_BEGIN
  This paper presents a brief overview of several studies concerning the indoor
wireless communications at 60 GHz performed by the IETR. The characterization
and the modeling of the radio propagation channel are based on several
measurement campaigns realized with the channel sounder developed at IETR. Some
typical residential environments were also simulated by ray tracing and
Gaussian Beam Tracking. The obtained results show a good agreement with the
similar experimental results. Currently, the IETR is developing a high data
rate wireless communication system operating at 60 GHz. The single-carrier
architecture of this system is also presented.

ABSTRACT_BEGIN
  This thesis focuses on link scheduling in wireless mesh networks by taking
into account physical layer characteristics. The assumption made throughout is
that a packet is received successfully only if the Signal to Interference and
Noise Ratio (SINR) at the receiver exceeds the communication threshold. The
thesis also discusses the complementary problem of flow control. (1) We
consider various problems on centralized link scheduling in Spatial Time
Division Multiple Access (STDMA) wireless mesh networks. We motivate the use of
spatial reuse as performance metric and provide an explicit characterization of
spatial reuse. We propose link scheduling algorithms based on certain graph
models (communication graph, SINR graph) of the network. Our algorithms achieve
higher spatial reuse than that of existing algorithms, with only a slight
increase in computational complexity. (2) We investigate random access
algorithms in wireless networks. We assume that the receiver is capable of
power-based capture and propose a splitting algorithm that varies transmission
powers of users on the basis of quaternary channel feedback. We model the
algorithm dynamics by a Discrete Time Markov Chain and consequently show that
its maximum stable throughput is 0.5518. Our algorithm achieves higher maximum
stable throughput and significantly lower delay than the First Come First Serve
(FCFS) splitting algorithm with uniform transmission power. (3) We consider the
problem of flow control in packet networks from an information-theoretic
perspective. We derive the maximum entropy of a flow which conforms to traffic
constraints imposed by a generalized token bucket regulator (GTBR), by taking
into account the covert information present in randomness of packet lengths.

ABSTRACT_BEGIN
  It has been known that heterogeneous networks are vulnerable to the
intentional removal of a small fraction of highly connected or loaded nodes,
which implies that, to protect a network effectively, a few important nodes
should be allocated with more defense resources than the others. However, if
too many resources are allocated to the few important nodes, the numerous
less-important nodes will be less protected, which, when attacked all together,
still capable of causing a devastating damage. A natural question therefore is
how to efficiently distribute the limited defense resources among the network
nodes such that the network damage is minimized whatever attack strategy the
attacker may take. In this paper, taking into account the factor of attack
cost, we will revisit the problem of network security and search for efficient
network defense against the cost-based attacks. The study shows that, for a
general complex network, there will exist an optimal distribution of the
defense resources, with which the network is well protected from cost-based
attacks. Furthermore, it is found that the configuration of the optimal defense
is dependent on the network parameters. Specifically, network that has a larger
size, sparser connection and more heterogeneous structure will be more
benefited from the defense optimization.

ABSTRACT_BEGIN
  The back-pressure algorithm is a well-known throughput-optimal algorithm.
However, its delay performance may be quite poor even when the traffic load is
not close to network capacity due to the following two reasons. First, each
node has to maintain a separate queue for each commodity in the network, and
only one queue is served at a time. Second, the back-pressure routing algorithm
may route some packets along very long routes. In this paper, we present
solutions to address both of the above issues, and hence, improve the delay
performance of the back-pressure algorithm. One of the suggested solutions also
decreases the complexity of the queueing data structures to be maintained at
each node.

ABSTRACT_BEGIN
  We consider a transmission of a delay-sensitive data stream from a single
source to a single destination. The reliability of this transmission may suffer
from bursty packet losses - the predominant type of failures in today's
Internet. An effective and well studied solution to this problem is to protect
the data by a Forward Error Correction (FEC) code and send the FEC packets over
multiple paths.
  In this paper we show that the performance of such a multipath FEC scheme can
often be further improved. Our key observation is that the propagation times on
the available paths often significantly differ, typically by 10-100ms.
  We propose to exploit these differences by appropriate packet scheduling that
we call `Spread'. We evaluate our solution with a precise, analytical
formulation and trace-driven simulations. Our studies show that Spread
substantially outperforms the state-of-the-art solutions. It typically achieves
two- to five-fold improvement (reduction) in the effective loss rate. Or
conversely, keeping the same level of effective loss rate, Spread significantly
decreases the observed delays and helps fighting the delay jitter.

ABSTRACT_BEGIN
  Burst contention is a well-known challenging problem in Optical Burst
Switching (OBS) networks. Deflection routing is used to resolve contention.
Burst retransmission is used to reduce the Burst Loss Ratio (BLR) by
retransmitting dropped bursts. Previous works show that combining deflection
and retransmission outperforms both pure deflection and pure retransmission
approaches. This paper proposes a new Adaptive Hybrid Deflection and
Retransmission (AHDR) approach that dynamically combines deflection and
retransmission approaches based on network conditions such as BLR and link
utilization. Network Simulator 2 (ns-2) is used to simulate the proposed
approach on different network topologies. Simulation results show that the
proposed approach outperforms static approaches in terms of BLR by using an
adaptive decision threshold.

ABSTRACT_BEGIN
  We propose a transceiver architecture for automatic beamforming and
instantaneous setup of a multigigabit-per-second wireless link between two
millimeter wave radios. The retro-directive architecture eliminates necessity
of slow and complex digital algorithms required for searching and tracking the
directions of opposite end radios. Simulations predict <5 micro-seconds setup
time for a 2-Gbps bidirectional 60-GHz communication link between two 10-meters
apart radios. The radios have 4-element arrayed antennas, and use QPSK
modulation with 1.5 GHz analog bandwidth.

ABSTRACT_BEGIN
  In this paper, we propose and evaluate a distributed protocol to manage trust
diffusion in ad hoc networks. In this protocol, each node i maintains a \trust
value" about an other node j which is computed both as a result of the
exchanges with node j itself and as a function of the opinion that other nodes
have about j. These two aspects are respectively weighted by a trust index that
measures the trust quality the node has in its own experiences and by a trust
index representing the trust the node has in the opinions of the other nodes.
Simulations have been realized to validate the robustness of this protocol
against three kinds of attacks: simple coalitions, Trojan attacks and detonator
attacks.

ABSTRACT_BEGIN
  Mobile wireless network research focuses on scenarios at the extremes of the
network connectivity continuum where the probability of all nodes being
connected is either close to unity, assuming connected paths between all nodes
(mobile ad hoc networks), or it is close to zero, assuming no multi-hop paths
exist at all (delay-tolerant networks). In this paper, we argue that a sizable
fraction of networks lies between these extremes and is characterized by the
existence of partial paths, i.e. multi-hop path segments that allow forwarding
data closer to the destination even when no end-to-end path is available. A
fundamental issue in such networks is dealing with disruptions of end-to-end
paths. Under a stochastic model, we compare the performance of the established
end-to-end retransmission (ignoring partial paths), against a forwarding
mechanism that leverages partial paths to forward data closer to the
destination even during disruption periods. Perhaps surprisingly, the
alternative mechanism is not necessarily superior. However, under a stochastic
monotonicity condition between current v.s. future path length, which we
demonstrate to hold in typical network models, we manage to prove superiority
of the alternative mechanism in stochastic dominance terms. We believe that
this study could serve as a foundation to design more efficient data transfer
protocols for partially-connected networks, which could potentially help
reducing the gap between applications that can be supported over disconnected
networks and those requiring full connectivity.

ABSTRACT_BEGIN
  In this paper, we study the setting of carrier-sensing range in 802.11
networks under the (cumulative) physical interference model. Specifically, we
identify a carrier-sensing range that will prevent collisions in 802.11
networks due to carrier-sensing failure under the physical interference model.
We find that the carrier-sensing range required under the physical interference
model must be larger than that required under the protocol (pairwise)
interference model by a multiplicative factor. For example, if the SINR
requirement is 10dB and the path-loss exponent is 4, the factor is 1.4.
Furthermore, given a fixed pathloss exponent of 4, the factor increases as the
SINR requirement increases. However, the limit of the factor is 1.84 as the
SINR requirement goes to infinity.

ABSTRACT_BEGIN
  Minimizing handoff latency and achieving near-zero packet loss is critical
for delivering multimedia infotainment applications to fast-moving vehicles
that are likely to encounter frequent handoffs. In this paper, we propose a
dual-radio cross-layer handoff scheme for infrastructure-mode 802.11 Wireless
Networks that achieve this goal. We present performance results of an
implementation of our algorithm in a Linux-based On-Board-Unit prototype.

ABSTRACT_BEGIN
  We propose in this paper an on-line algorithm based on Bloom filters for
identifying large flows in IP traffic (a.k.a. elephants). Because of the large
number of small flows, hash tables of these algorithms have to be regularly
refreshed. Recognizing that the periodic erasure scheme usually used in the
technical literature turns out to be quite inefficient when using real traffic
traces over a long period of time, we introduce a simple adaptive scheme that
closely follows the variations of traffic. When tested against real traffic
traces, the proposed on-line algorithm performs well in the sense that the
detection ratio of long flows by the algorithm over a long time period is quite
high. Beyond the identification of elephants, this same class of algorithms is
applied to the closely related problem of detection of anomalies in IP traffic,
e.g., SYN flood due for instance to attacks. An algorithm for detecting SYN and
volume flood anomalies in Internet traffic is designed. Experiments show that
an anomaly is detected in less than one minute and the targeted destinations
are identified at the same time.

ABSTRACT_BEGIN
  This article is focused on mobile development using Visual Studio 2005, web
services and their connection to Oracle server, willing to help programmers to
realize simple and useful mobile applications.

ABSTRACT_BEGIN
  We introduce hierarchical neighbor graphs, a new architecture for connecting
ad hoc wireless nodes distributed in a plane. The structure has the flavor of
hierarchical clustering and requires only local knowledge and minimal
computation at each node to be formed and repaired. Hence, it is a suitable
interconnection model for an ad hoc wireless sensor network. The structure is
able to use energy efficiently by reorganizing dynamically when the battery
power of heavily utilized nodes degrades and is able to achieve throughput,
energy efficiency and network lifetimes that compare favorably with the leading
proposals for data collation in sensor networks such as LEACH (Heinzelman et.
al., 2002). Additionally, hierarchical neighbor graphs have low power stretch
i.e. the power required to connect nodes through the network is a small factor
higher than the power required to connect them directly. Our structure also
compares favorably to mathematical structures proposed for connecting points in
a plane e.g. nearest-neighbor graphs (Ballister et. al., 2005), $\theta$-graphs
(Ruppert and Seidel, 1991), in that it has expected constant degree and does
not require any significant computation or global information to be formed.

ABSTRACT_BEGIN
  In a Multi-hop Wireless Networks (MHWN), packets are routed between source
and destination using a chain of intermediate nodes; chains are a fundamental
communication structure in MHWNs whose behavior must be understood to enable
building effective protocols. The behavior of chains is determined by a number
of complex and interdependent processes that arise as the sources of different
chain hops compete to transmit their packets on the shared medium. In this
paper, we show that MAC level interactions play the primary role in determining
the behavior of chains. We evaluate the types of chains that occur based on the
MAC interactions between different links using realistic propagation and packet
forwarding models. We discover that the presence of destructive interactions,
due to different forms of hidden terminals, does not impact the throughput of
an isolated chain significantly. However, due to the increased number of
retransmissions required, the amount of bandwidth consumed is significantly
higher in chains exhibiting destructive interactions, substantially influencing
the overall network performance. These results are validated by testbed
experiments. We finally study how different types of chains interfere with each
other and discover that well behaved chains in terms of self-interference are
more resilient to interference from other chains.

ABSTRACT_BEGIN
  In this paper we describe the channel impairments and equalization methods
currently used in WiFi, WiMax and WCDMA. After a review of channel model for
Intelligent Transportation System (ITS), we proposed an equalization method
which will be useful for the estimation of strong multipath channel at a high
velocity.

ABSTRACT_BEGIN
  A proper remote sensing device is required for automatic cruise control (ACC)
to avoid collision in transportation system. In this paper we proposed a direct
sequence spread spectrum (DSSS) radar for remote sensing in intelligent
transporation system(ITS). We have successfully detected single target and
through 1D radar imaging we are capable to separate multiple targets. We have
also implemented DSSS radar using software defined radio (SDR) and successfully
detected a single target.

ABSTRACT_BEGIN
  High data rate is required for multimedia communication. But the
communication at high data rate is always challenging. In this work we have
successfully performed data chatting, Voice chatting and high quality video
transmission between two distant units using MIMO adapter, Direct sequence
spread spectrum system and MATLAB/SIMULINK platform.

ABSTRACT_BEGIN
  We address the connectivity of large-scale ad hoc heterogeneous wireless
networks, where secondary users exploit channels temporarily unused by primary
users and the existence of a communication link between two secondary users
depends on not only the distance between them but also the transmitting and
receiving activities of nearby primary users. We introduce the concept of
connectivity region defined as the set of density pairs -- the density of
secondary users and the density of primary transmitters -- under which the
secondary network is connected. Using theories and techniques from continuum
percolation, we analytically characterize the connectivity region of the
secondary network and reveal the tradeoff between proximity (the number of
neighbors) and the occurrence of spectrum opportunities. Specifically, we
establish three basic properties of the connectivity region -- contiguity,
monotonicity of the boundary, and uniqueness of the infinite connected
component, where the uniqueness implies the occurrence of a phase transition
phenomenon in terms of the almost sure existence of either zero or one infinite
connected component; we identify and analyze two critical densities which
jointly specify the profile as well as an outer bound on the connectivity
region; we study the impacts of secondary users' transmission power on the
connectivity region and the conditional average degree of a secondary user, and
demonstrate that matching the interference ranges of the primary and the
secondary networks maximizes the tolerance of the secondary network to the
primary traffic load. Furthermore, we establish a necessary condition and a
sufficient condition for connectivity, which lead to an outer bound and an
inner bound on the connectivity region.

ABSTRACT_BEGIN
  This paper presents a review for the development of Intelligent
Transportation System (ITS) world wide and the use of Smart Antennas in ITS.
This review work also discusses the usual problems in ITS and proposes the
solution of such problems using smart antennas.

ABSTRACT_BEGIN
  this is a review paper. this describes how DGPS is helpful for lane detection
and to avoid collission.

ABSTRACT_BEGIN
  Network operators are reluctant to share traffic data due to security and
privacy concerns. Consequently, there is a lack of publicly available traces
for validating and generalizing the latest results in network and security
research. Anonymization is a possible solution in this context; however, it is
unclear how the sanitization of data preserves characteristics important for
traffic analysis. In addition, the privacy-preserving property of
state-of-the-art IP address anonymization techniques has come into question by
recent attacks that successfully identified a large number of hosts in
anonymized traces.
  In this paper, we examine the tradeoff between data utility for anomaly
detection and the risk of host identification for IP address truncation.
Specifically, we analyze three weeks of unsampled and non-anonymized network
traces from a medium-sized backbone network to assess data utility. The risk of
de-anonymizing individual IP addresses is formally evaluated, using a metric
based on conditional entropy.
  Our results indicate that truncation effectively prevents host identification
but degrades the utility of data for anomaly detection. However, the degree of
degradation depends on the metric used and whether network-internal or external
addresses are considered. Entropy metrics are more resistant to truncation than
unique counts and the detection quality of anomalies degrades much faster in
internal addresses than in external addresses. In particular, the usefulness of
internal address counts is lost even for truncation of only 4 bits whereas
utility of external address entropy is virtually unchanged even for truncation
of 20 bits.

ABSTRACT_BEGIN
  Ring topology is a simple configuration used to connect processes that
communicate among themselves. A number of network standards such as token ring,
token bus, and FDDI are based on the ring connectivity. This article will
develop an implementation of a ring of processes that communicate among
themselves via pipe links. The processes are nodes in the ring. Each process
reads from its standard input and writes in its standard output. N-1 process
redirects the its standard output to a standard input of the process through a
pipe. When the ring-structure is designed, the project can be extended to
simulate networks or to implement algorithms for mutual exclusion.

ABSTRACT_BEGIN
  This review paper presents analytical information regarding the transfer of
TCP data flows on paths towards interconnected wireless systems, with emphasis
on 3G cellular networks. The focus is on protocol modifications in face of
problems arising from terminal mobility and wireless transmission. The
objective of this paper is not to present an exhaustive review of the
literature, but to filter out the causes of poor TCP performance in such
systems and give a rationalized view of measures that can be taken against
them.

ABSTRACT_BEGIN
  Link state routing protocols such as OSPF or IS-IS currently use only best
paths to forward IP packets throughout a domain. The optimality of sub-paths
ensures consistency of hop by hop forwarding although paths, calculated using
Dijkstra algorithm, are recursively composed. According to the link metric, the
diversity of existing paths can be underestimated using only best paths. Hence,
it reduces potential benefits of multipath applications such as load balancing
and fast rerouting. In this paper, we propose a low time complexity multipath
computation algorithm able to calculate at least two paths with a different
first hop between all pairs of nodes in the network if such next hops exist.
Using real and generated topologies, we evaluate and compare the complexity of
our proposition with several techniques. Simulation results suggest that the
path diversity achieved with our proposition is approximatively the same that
the one obtained using consecutive Dijsktra computations, but with a lower time
complexity.

ABSTRACT_BEGIN
  This paper presents a statistically sound method for measuring the accuracy
with which a probabilistic model reflects the growth of a network, and a method
for optimising parameters in such a model. The technique is data-driven, and
can be used for the modeling and simulation of any kind of evolving network.
  The overall framework, a Framework for Evolving Topology Analysis (FETA), is
tested on data sets collected from the Internet AS-level topology, social
networking websites and a co-authorship network. Statistical models of the
growth of these networks are produced and tested using a likelihood-based
method. The models are then used to generate artificial topologies with the
same statistical properties as the originals. This work can be used to predict
future growth patterns for a known network, or to generate artificial models of
graph topology evolution for simulation purposes. Particular application
examples include strategic network planning, user profiling in social networks
or infrastructure deployment in managed overlay-based services.

ABSTRACT_BEGIN
  Full exploitation of the bandwidth resources of Wireless Networks is
challenging because of the sharing of the radio medium among neighboring nodes.
Practical algorithms and distributed schemes that tries to optimising the use
of the network radio resources. In this technical report we present the proof
that maximising the network capacity is is an APX Complete problem (not
approximable within 1/(1 - 2^(-k)) - eps for eps > 0).

ABSTRACT_BEGIN
  We address the problem of multiuser scheduling with partial channel
information in a multi-cell environment. The scheduling problem is formulated
jointly with the ARQ based channel learning process and the intercell
interference mitigating cell breathing protocol. The optimal joint scheduling
policy under various system constraints is established. The general problem is
posed as a generalized Restless Multiarmed Bandit process and the notion of
indexability is studied. We conjecture, with numerical support, that the
multicell multiuser scheduling problem is indexable and obtain a partial
structure of the index policy.

ABSTRACT_BEGIN
  We consider the downlink of a cellular system and address the problem of
multiuser scheduling with partial channel information. In our setting, the
channel of each user is modeled by a three-state Markov chain. The scheduler
indirectly estimates the channel via accumulated Automatic Repeat Request (ARQ)
feedback from the scheduled users and uses this information in future
scheduling decisions. Using a Partially Observable Markov Decision Process
(POMDP), we formulate a throughput maximization problem that is an extension of
our previous work where the channels were modeled using two states. We recall
the greedy policy that was shown to be optimal and easy to implement in the two
state case and study the implementation structure of the greedy policy in the
considered downlink. We classify the system into two types based on the channel
statistics and obtain round robin structures for the greedy policy for each
system type. We obtain performance bounds for the downlink system using these
structures and study the conditions under which the greedy policy is optimal.

ABSTRACT_BEGIN
  As radio spectrum usage paradigm moving from the traditional command and
control allocation scheme to the open spectrum allocation scheme, wireless
networks meet new opportunities and challenges. In this article we introduce
the concept of cognitive wireless mesh (CogMesh) networks and address the
unique problem in such a network. CogMesh is a self-organized distributed
network architecture combining cognitive technologies with the mesh structure
in order to provide a uniform service platform over a wide range of networks.
It is based on dynamic spectrum access (DSA) and featured by self-organization,
self-configuration and self-healing. The unique problem in CogMesh is the
common control channel problem, which is caused by the opportunistic spectrum
sharing nature of secondary users (SU) in the network. More precisely, since
the channels of SUs are fluctuating according to the radio environment, it is
difficult to find always available global common control channels. This puts a
significant challenge on the network design. We develop the control cloud based
control channel selection and cluster based network formation techniques to
tackle this problem. Moreover, we show in this article that the swarm
intelligence is a good candidate to deal with the control channel problem in
CogMesh. Since the study of cognitive wireless networks (CWN) is still in its
early phase, the ideas provided in this article act as a catalyst to inspire
new solutions in this field.

ABSTRACT_BEGIN
  Maximizing network lifetime is a very challenging issue in routing protocol
design for Mobile Ad-hoc NETworks (MANETs), since mobile nodes are powered by
limited-capacity batteries. Furthermore, replacing or recharging batteries is
often impossible in critical environments (e.g. battlefields, disaster areas,
etc.) The proposed MEA-DSR (Multipath Energy-Aware on Demand Source Routing)
protocol uses a load distribution policy in order to maximize network lifetime.
The simulation results have shown the efficiency of the proposed protocol in
comparison to DSR routing protocol in many difficult scenarios

ABSTRACT_BEGIN
  Indoor multpropagation channel is modeled by the Kaiser electromagnetic
wavelet. A method for channel characterization is proposed by modeling all the
reflections of indoor propagation in a kernel function instead of its impulse
response. This lead us to consider a fractal modulation scheme in which Kaiser
wavelets substitute the traditional sinusoidal carrier.

ABSTRACT_BEGIN
  By focusing on what can be observed by running traceroute-like measurements
at a high frequency from a single monitor to a fixed destination set, we show
that the observed view of the topology is constantly evolving at a pace much
higher than expected. Repeated measurements discover new IP addresses at a
constant rate, for long period of times (up to several months). In order to
provide explanations, we study this phenomenon both at the IP, and at the
Autonomous System levels. We show that this renewal of IP addresses is
partially caused by a BGP routing dynamics, altering paths between existing
ASes. Furthermore, we conjecture that an intra AS routing dynamics is another
cause of this phenomenon.

ABSTRACT_BEGIN
  Traceroute is widely used: from the diagnosis of network problems to the
assemblage of internet maps. Unfortu- nately, there are a number of problems
with traceroute methodology, which lead to the inference of erroneous routes.
This paper studies particular structures arising in nearly all traceroute
measurements. We characterize them as "loops", "cycles", and "diamonds". We
iden- tify load balancing as a possible cause for the appear- ance of false
loops, cycles and diamonds, i.e., artifacts that do not represent the internet
topology. We pro- vide a new publicly-available traceroute, called Paris
traceroute, which, by controlling the packet header con- tents, provides a
truer picture of the actual routes that packets follow. We performed
measurements, from the perspective of a single source tracing towards multiple
destinations, and Paris traceroute allowed us to show that many of the
particular structures we observe are indeed traceroute measurement artifacts.

ABSTRACT_BEGIN
  Collecting information about user activity in peer-to-peer systems is a key
but challenging task. We describe here a distributed platform for doing so on
the eDonkey network, relying on a group of honeypot peers which claim to have
certain files and log queries they receive for these files. We then conduct
some measurements with typical scenarios and use the obtained data to analyze
the impact of key parameters like measurement duration, number of honeypots
involved, and number of advertised files. This illustrates both the possible
uses of our measurement system, and the kind of data one may collect using it.

ABSTRACT_BEGIN
  Complex networks are at the core of an intense research activity. However, in
most cases, intricate and costly measurement procedures are needed to explore
their structure. In some cases, these measurements rely on link queries: given
two nodes, it is possible to test the existence of a link between them. These
tests may be costly, and thus minimizing their number while maximizing the
number of discovered links is a key issue. This paper studies this problem: we
observe that properties classically observed on real-world complex networks
give hints for their efficient measurement; we derive simple principles and
several measurement strategies based on this, and experimentally evaluate their
efficiency on real-world cases. In order to do so, we introduce methods to
evaluate the efficiency of strategies. We also explore the bias that different
measurement strategies may induce.

ABSTRACT_BEGIN
  In order to make full use of geographic routing techniques developed for
large scale networks, nodes must be localized. However, localization and
virtual localization techniques in sensor networks are dependent either on
expensive and sometimes unavailable hardware (e.g. GPS) or on sophisticated
localization calculus (e.g. triangulation) which are both error-prone and with
a costly overhead.
  Instead of localizing nodes in a traditional 2-dimensional space, we intend
to use directly the raw distance to a set of anchors to route messages in the
multi-dimensional space. This should enable us to use any geographic routing
protocol in a robust and efficient manner in a very large range of scenarios.

ABSTRACT_BEGIN
  Internet traffic displays many persistent periodicities (oscillations) on a
large range of time scales. This paper describes the measurement methodology to
detect Internet traffic periodicities and also describes the main periodicities
in Internet traffic.

ABSTRACT_BEGIN
  Maximizing network lifetime is a very challenging issue in routing protocol
design for Mobile Ad-hoc NETworks (MANETs), since mobile nodes are powered by
limited-capacity batteries. Furthermore, replacing or recharging batteries is
often impossible in critical environments (e.g. battlefields, disaster areas,
etc.). Energy consumption was considered for a long time equivalent to
bandwidth consumption. However, recent works have shown that "energy" and
"bandwidth" are substantially different metrics. Moreover, it was found that
traditional routing policies such as "the shortest path" one can have a
negative impact on energy consumption balance. Therefore, several new
approaches have been proposed addressing energy efficiency explicitly. Our work
is related to energy efficient routing for MANETs' problem. The proposed
MEA-DSR (Multipath Energy-Aware on Demand Source Routing) protocol is based on
a load sharing strategy between mobile nodes in order to maximize network
lifetime. To achieve this goal, we used multipath routing; nodes' residual
energies and paths length were also considered when making routing decisions.
Simulation results have shown the efficiency of the proposed protocol under
difficult scenarios characterised by high mobility, high density and important
traffic load.

ABSTRACT_BEGIN
  This paper presents the realization of a wireless Gigabit Ethernet
communication system operating in the 60 GHz band. The system architecture uses
a single carrier modulation. A differential encoded binary phase shift keying
modulation and a differential demodulation scheme are adopted for the
intermediate frequency blocks. The baseband blocks use Reed- Solomon RS (255,
239) coding and decoding for channel forward error correction (FEC). First
results of bit error rate (BER) measurements at 875 Mbps, without channel
coding, are presented for different antennas.

ABSTRACT_BEGIN
  This paper proposes a complete IFoF system architecture derived from
simplified IEEE802.15.3c PHY layer proposal to successfully ensure near 1 Gbps
on the air interface. The system architecture utilizes low complexity baseband
processing modules. The byte/frame synchronization technique is designed to
provide a high value of preamble detection probability and a very small value
of the false detection probability. Conventional Reed-Solomon RS (255, 239)
coding is used for Channel Forward Error Correction (FEC). Good communication
link quality and Bit Error Rate (BER) results at 875 Mbps are achieved with
directional antennas.

ABSTRACT_BEGIN
  This paper presents the design and the realization of a 60 GHz wireless
Gigabit Ethernet communication system. A differential encoded binary phase
shift keying modulation (DBPSK) and differential demodulation schemes are
adopted for the IF blocks. The Gigabit Ethernet interface allows a high speed
transfer of multimedia files via a 60 GHz wireless link. First measurement
results are shown for 875 Mbps data rate.

ABSTRACT_BEGIN
  Underwater communication is a challenging topic due to its singular channel
characteristics. Most protocols used in terrestrial wireless communication can
not be directly applied in the underwater world. In this paper, we focus on the
issue of energy efficient transmission in underwater sensor networks (UWSNs)
and analyze this problem in a rigorous and theoretical way. We formalize an
optimization problem which aims to minimize energy consumption and
simultaneously accounts for other performance metrics such as the data
reliability and the communication delay. With the help of Karush-Kuhn-Tucker
conditions (KKT conditions), we derive a simple and explicit, but nevertheless
accurate, approximate solution under reasonable assumptions. This approximate
solution provides theoretical guidelines for designing durable and reliable
UWSNs. Our result also shows that reliability and communication delay are
crucial factors to the energy consumption for transmission.

ABSTRACT_BEGIN
  In optimizing the topology of wireless networks built of a dynamic set of
spatially embedded agents, there are many trade-offs to be dealt with. The
network should preferably be as small (in the sense that the average, or
maximal, pathlength is short) as possible, it should be robust to failures, not
consume too much power, and so on. In this paper, we investigate simple models
of how agents can choose their neighbors in such an environment. In our model
of attachment, we can tune from one situation where agents prefer to attach to
others in closest proximity, to a situation where distance is ignored (and thus
attachments can be made to agents further away). We evaluate this scenario with
several performance measures and find that the optimal topologies, for most of
the quantities, is obtained for strategies resulting in a mix of most local and
a few random connections.

ABSTRACT_BEGIN
  Swarming peer-to-peer systems play an increasingly instrumental role in
Internet content distribution. It is therefore important to better understand
how these systems behave in practice. Recent research efforts have looked at
various protocol parameters and have measured how they affect system
performance and robustness. However, the importance of the strategy based on
which peers establish connections has been largely overlooked. This work
utilizes extensive simulations to examine the default overlay construction
strategy in BitTorrent systems. Based on the results, we identify a critical
parameter, the maximum allowable number of outgoing connections at each peer,
and evaluate its impact on the robustness of the generated overlay. We find
that there is no single optimal value for this parameter using the default
strategy. We then propose an alternative strategy that allows certain new peer
connection requests to replace existing connections. Further experiments with
the new strategy demonstrate that it outperforms the default one for all
considered metrics by creating an overlay more robust to churn. Additionally,
our proposed strategy exhibits optimal behavior for a well-defined value of the
maximum number of outgoing connections, thereby removing the need to set this
parameter in an ad-hoc manner.

ABSTRACT_BEGIN
  Using an approach developed in physics, we propose a new framework for the
study of cellular networks. The key idea of the physical network model we
propose is to replace the discrete base stations (BS) entities by a continuum
of transmitters which are spatially distributed in the network. This allows us
to establish a closed form formula of the other-cell downlink interference
factor f, as a function of the location of the mobile. We define here f as the
ratio of outer cell received power (i.e. the power received from other cells)
to the inner cell received power. This physical model allows calculating the
influence of interference on any mobile in a cell, whatever its position.
Results obtained with that closed-form formula are close to the ones obtained
by simulations using a traditional hexagonal network model. Since the physical
model allows to establish a closed form formula of the interference factor, it
allows to do analytical studies of wireless networks such as outage
probability, quality of service, capacity.

ABSTRACT_BEGIN
  A multi-channel MAC seems to be an interesting approach for improving network
throughput by multiplexing transmissions over orthogonal channels. In
particular, Molecular MAC has recently proposed to modify the standard IEEE
802.11 DCF access method to use dynamic channel switching for efficient packet
forwarding over multiple hops. However, this MAC layer requires role and
channel assignment to nodes: some of them use a static channel, while others
dynamically switch to neighbor channels on-demand. To assign roles and
channels, we extend the notion of the Weakly Connected Dominating Set, the
structure already used in clustering. More precisely, we adapt the WCDS
structure and introduce new constraints to define what we call a reversible
WCDS (r-WCDS), which is particularly suitable for wireless mesh networks
operating under Molecular MAC. We propose a divide-and-conquer scheme that
partitions the network into clusters with one leader per cluster solving a MILP
formulation to assign roles in its cluster. By appropriately defining the roles
at the border of clusters, we maintain global connectivity in the r-wcds.
Finally, our simulations show that the performance of the propose scheme is
close to a centralized algorithm.

ABSTRACT_BEGIN
  In this paper we address the problem of discovering network topology in
proprietary networks. Namely, we investigate topology discovery in Cisco-based
networks. Cisco devices run Cisco Discovery Protocol (CDP) which holds
information about these devices. We first compare properties of topologies that
can be obtained from networks deploying CDP versus Spanning Tree Protocol (STP)
and Management Information Base (MIB) Forwarding Database (FDB). Then we
describe a method of discovering topology of CDP-based networks. Our
experiments show that the physical topology of the network including links that
are in Forwarding Block state can be discovered.

ABSTRACT_BEGIN
  This paper suggests a system architecture for wireless widearea- networking
access using adhoc networking between a mobile Client node without direct
connectivity to a wirelesswide- area-network and a mobile Service Provider node
with connectivity to a wireless-wide-area-network. It provides a means for
securely providing such adhoc wireless networking services using a Server for
tunneling and routing, registration and authentication. The architecture also
provides support for handoff of a Client node from one Service Provider to
another with persistence of a tunnel between the Client and the Server enabling
a soft-handoff. Different wireless protocols may be used for adhoc networking,
with filtered interconnection of authenticated Clients implemented at a Service
Provider node. The architecture is applicable across different wide-areanetwork
protocols, and provides simultaneous support for multiple wide-area-network
protocols.

ABSTRACT_BEGIN
  With the major wireless service providers planning to start deployment of 4G
wireless networks by mid 2010, research and industry communities are racing
against time to find solutions for some of the prominent still open issues in
4G networks. The growing interest in 4G networks is driven by the set of new
services will be made available for the first time such as accessing the
Internet anytime from anywhere, global roaming, and wider support for
multimedia applications. In this paper describe some of the key opportunities
will be made available by 4G networks, present key challenges and point to some
proposed solutions.

ABSTRACT_BEGIN
  Ethernet networks have undergone impressive growth since the past few
decades. This growth can be appreciated in terms of the equipment, such as
switches and links, that have been added, as well as in the number of users
that it supports. In parallel to this expansion, over the past decade the
networking research community has shown a growing interest in discovering and
analyzing the Ethernet topology. Research in this area has concentrated on the
theoretical analysis of Ethernet topology as well as developing tools and
methods for mapping the network layout. These efforts have brought us to a
crucial juncture for Ethernet topology measurement infrastructures: while,
previously, these were both small (in terms of number of measurement points),
we are starting to see the deployment of large-scale distributed systems
composed of hundreds or thousands of monitors. As we look forward to this next
generation of systems, we take stock of what has been achieved so far. In this
survey, we discuss past and current mechanisms for discovering the Ethernet
topology from theoretical and practical prospective. In addition to discovery
techniques, we provide insights into some of the well known open issues related
to Ethernet topology discovery.

ABSTRACT_BEGIN
  This chapter will focus on the multiobjective formulation of an optimization
problem and highlight the assets of a multiobjective Tabu implementation for
such problems. An illustration of a specific Multiobjective Tabu heuristic
(referred to as MO Tabu in the following) will be given for 2 particular
problems arising in wireless systems. The first problem addresses the planning
of access points for a WLAN network with some Quality of Service requirements
and the second one provides an evaluation mean to assess the performance
evaluation of a wireless sensor network. The chapter will begin with an
overview of multiobjective (MO) optimization featuring the definitions and
concepts of the domain (e.g. Dominance, Pareto front,...) and the main MO
search heuristics available so far. We will then emphasize on the definition of
a problem as a multiobjective optimization problem and illustrate it by the two
examples from the field of wireless networking. The next part will focus on MO
Tabu, a Tabu-inspired multiobjective heuristic and describe its assets compared
to other MO heuristics. The last part of the chapter will show the results
obtained with this MO Tabu strategy on the 2 wireless networks related
problems. Conclusion on the use of Tabu as a multiobjective heuristic will be
drawn based on the results presented so far.

ABSTRACT_BEGIN
  In this paper, we investigate the use of a cross-layer allocation mechanism
for the high-rate ultra-wideband (UWB) systems. The aim of this paper is
twofold. First, through the cross-layer approach that provides a new service
differentiation approach to the fully distributed UWB systems, we support
traffic with quality of service (QoS) guarantee in a multi-user context.
Second, we exploit the effective SINR method that represents the
characteristics of multiple sub-carrier SINRs in the multi-band WiMedia
solution proposed for UWB systems, in order to provide the channel state
information needed for the multi-user sub-band allocation. This new approach
improves the system performance and optimizes the spectrum utilization with a
low cost data exchange between the different users while guaranteeing the
required QoS. In addition, this new approach solves the problem of the
cohabitation of more than three users in the same WiMedia channel.

ABSTRACT_BEGIN
  We consider multiuser scheduling in wireless networks with channel variations
and flow-level dynamics. Recently, it has been shown that the MaxWeight
algorithm, which is throughput-optimal in networks with a fixed number users,
fails to achieve the maximum throughput in the presence of flow-level dynamics.
In this paper, we propose a new algorithm, called workload-based scheduling
with learning, which is provably throughput-optimal, requires no prior
knowledge of channels and user demands, and performs significantly better than
previously suggested algorithms.

ABSTRACT_BEGIN
  This two-part paper series studies the performance of buffered Aloha networks
with K-Exponential Backoff collision resolution algorithms. Part I focuses on
stability and throughput analysis and Part II presents the delay analysis.
  In Part I, the buffered Aloha network is modeled as a multi-queue
single-server system. We adopt a widely used approach in packet switching
systems to decompose the multi-queue system into independent first-in-first-out
(FIFO) queues, which are hinged together by the probability of success of
head-of-line (HOL) packets. A unified method is devised to tackle the stability
and throughput problems of K-Exponential Backoff with any cutoff phase K. We
demonstrate that a network with K-Exponential Backoff can be stabilized if the
retransmission factor q is properly selected. The stable region of q is
characterized and illustrated via examples of Geometric Retransmission (K=1)
and Exponential Backoff (K=infinity). With an increasing number of nodes n, we
show that the stable region of Geometric Retransmission rapidly shrinks, and
vanishes as n goes to infinity. In contrast, the stable region of Exponential
Backoff does not vary with the network population n, implying that a stable
throughput can be achieved in networks with Exponential Backoff even with an
infinite number of nodes. All the analytical results presented in this paper
series are verified by simulations.

ABSTRACT_BEGIN
  This paper presents the delay analysis for buffered Aloha networks with
K-Exponential Backoff. Mean access delay and mean queueing delay are derived
and demonstrated via the examples of Geometric Retransmission (K=1) and
Exponential Backoff (K=infinity). The comparison shows that higher delay is
incurred with Geometric Retransmission when the aggregate input rate is small,
and the delay gap is enlarged as the number of nodes n increases. With a high
traffic input rate, however, the delay performance with Exponential Backoff
severely deteriorates. The mean queueing delay will be unbounded if the
aggregate input rate exceeds 0.3.
  We also extend the analysis to the contention-window-based backoff model
which is widely adopted in practical MAC protocols. It will be revealed that
both the retransmission-probability-based and the contention-window-based
models exhibit the same stable region and achieve similar queueing performance
in most cases, which justifies the intuition that was taken but remained
unverified in previous studies: the retransmission-probability-based backoff
model can serve as a good approximation of the contention-window-based one.

ABSTRACT_BEGIN
  In this paper correspondence between experimental data for packet delay and
two theoretical types of distribution is investigated. Calculations have shown
that the exponential distribution describes the data on network delay better,
than truncated normal distribution. Precision experimental data to within
microseconds are gathered by means of the RIPE Test Box. In addition to exact
measurements the data gathered by means of the utility {\em ping} has been
parsed that has not changed the main result. As a result, the equation for an
exponential distribution, in the best way describing process of packet delay in
a TCP/IP based network is written. The search algorithm for key parameters as
for normal, and an exponential distribution is resulted.

ABSTRACT_BEGIN
  BGP is the de facto protocol used for inter-autonomous system routing in the
Internet. Generally speaking, BGP has been proven to be secure, efficient,
scalable, and robust. However, with the rapid evolving of the Internet in the
past few decades, there are increasing concerns about BGS's ability to meet the
needs of the Internet routing. There are two major limitations of BGP which are
its failure to address several key security issues, and some operational
related problems. The design and ubiquity of BGP have complicated past efforts
at securing inter-domain routing. This paper surveys the past work related to
BGP security and operational issues. We explore the limitations and advantages
of proposed solutions in these two limitations.

ABSTRACT_BEGIN
  We consider a wireless sensor network whose main function is to detect
certain infrequent alarm events, and to forward alarm packets to a base
station, using geographical forwarding. The nodes know their locations, and
they sleep-wake cycle, waking up periodically but not synchronously. In this
situation, when a node has a packet to forward to the sink, there is a
trade-off between how long this node waits for a suitable neighbor to wake up
and the progress the packet makes towards the sink once it is forwarded to this
neighbr. Hence, in choosing a relay node, we consider the problem of minimizing
average delay subject to a constraint on the average progress. By constraint
relaxation, involving a Lagrange multiplier, we formulate this next hop relay
selection problem as a Markov decision process (MDP). The exact optimal
solution (BF (Best Forward)) can be found, but is computationally intensive.
Next, we consider a mathematically simplified model for which the optimal
policy (SF (Simplified Forward)) turns out to be a simple one-step-look-ahead
rule. Simulations show that SF is very close in performance to BF, even for
reasonably small node density. We then study the end-to-end performance of SF
in comparison with two extremal policies: Max Forward (MF), which makes the
maximum possible progress per hop and thus reduces network energy consumption,
and First Forward (FF), which forwards the packet to the first node to wake up,
and thus tends to make end-to-end forwarding delays small. We find that, with
appropriate choice of one hop average progress constraint, SF can be tuned to
provide a favorable trade-off between end-to-end packet delay and the number of
hops in the forwarding path.

ABSTRACT_BEGIN
  Although there exist very accurate hardware systems for measuring traffic on
the internet, their widespread use for analysis tasks is limited by their high
cost. On the other hand, less expensive, software-based systems exist that are
widely available and can be used to perform a number of simple analysis tasks.
The caveat with using such software systems is that application of standard
analysis methods cannot proceed blindly because inherent distortions exist in
the measurements obtained from software systems. The goal of this paper is to
analyze common Internet measurement systems to discover the effect of these
distortions on common analysis tasks. Then by selecting one specific task,
periodic signal detection, a more in-depth analysis is conducted which derives
a signal representation to capture the salient features of the measurement and
develops a periodic detection mechanism designed for the measurement system
which outperforms an existing detection method not optimized for the
measurement system. Finally, through experiments the importance of
understanding the relationship between the input traffic, measurement system
configuration and detection method performance is emphasized.

ABSTRACT_BEGIN
  This paper studies the problem of congestion control and scheduling in ad hoc
wireless networks that have to support a mixture of best-effort and real-time
traffic. Optimization and stochastic network theory have been successful in
designing architectures for fair resource allocation to meet long-term
throughput demands. However, to the best of our knowledge, strict packet delay
deadlines were not considered in this framework previously. In this paper, we
propose a model for incorporating the quality of service (QoS) requirements of
packets with deadlines in the optimization framework. The solution to the
problem results in a joint congestion control and scheduling algorithm which
fairly allocates resources to meet the fairness objectives of both elastic and
inelastic flows, and per-packet delay requirements of inelastic flows.

ABSTRACT_BEGIN
  Delay tolerant Networks (DTNs) leverage the mobility of relay nodes to
compensate for lack of permanent connectivity and thus enable communication
between nodes that are out of range of each other. To decrease message delivery
delay, the information to be transmitted is replicated in the network. We study
replication mechanisms that include Reed-Solomon type codes as well as network
coding in order to improve the probability of successful delivery within a
given time limit. We propose an analytical approach that allows us to compute
the probability of successful delivery. We study the effect of coding on the
performance of the network while optimizing parameters that govern routing.

ABSTRACT_BEGIN
  In Heterogeneous mobile ad hoc networks (MANETs) congestion occurs with
limited resources. Due to the shared wireless channel and dynamic topology,
packet transmissions suffer from interference and fading. In heterogeneous ad
hoc networks, throughput via a given route is depending on the minimum data
rate of all its links. In a route of links with various data rates, if a high
data rate node forwards more traffic to a low data rate node, there is a chance
of congestion, which leads to long queuing delays in such routes. Since hop
count is used as a routing metric in traditional routing, it do not adapt well
to mobile nodes. A congestion-aware routing metric for MANETs should
incorporate transmission capability, reliability, and congestion around a link.
In this paper, we propose to develop a hop-by-hop congestion aware routing
protocol which employs a combined weight value as a routing metric, based on
the data rate, queuing delay, link quality and MAC overhead. Among the
discovered routes, the route with minimum cost index is selected, which is
based on the node weight of all the in-network nodes. Simulation results prove
that our proposed routing protocol attains high throughput and packet delivery
ratio, by reducing the packet drop and delay.

ABSTRACT_BEGIN
  This article has been withdrawn by the authors

ABSTRACT_BEGIN
  This paper studies the delay constrained multicast capacity of large scale
mobile ad hoc networks (MANETs). We consider a MANET consists of $n_s$
multicast sessions. Each multicast session has one source and $p$ destinations.
The wireless mobiles move according to a two-dimensional i.i.d. mobility model.
Each source sends identical information to the $p$ destinations in its
multicast session, and the information is required to be delivered to all the
$p$ destinations within $D$ time-slots. Given the delay constraint $D,$ we
first prove that the capacity per multicast session is $O(\min\{1, (\log
p)(\log (n_sp)) \sqrt{\frac{D}{n_s}}\}).$ Given non-negative functions $f(n)$
and $g(n)$: $f(n)=O(g(n))$ means there exist positive constants $c$ and $m$
such that $f(n) \leq cg(n)$ for all $ n\geq m;$ $f(n)=\Omega(g(n))$ means there
exist positive constants $c$ and $m$ such that $f(n)\geq cg(n)$ for all $n\geq
m;$ $f(n)=\Theta(g(n))$ means that both $f(n)=\Omega(g(n))$ and $f(n)=O(g(n))$
hold; $f(n)=o(g(n))$ means that $\lim_{n\to \infty} f(n)/g(n)=0;$ and
$f(n)=\omega(g(n))$ means that $\lim_{n\to \infty} g(n)/f(n)=0.$ We then
propose a joint coding/scheduling algorithm achieving a throughput of
$\Theta(\min\{1,\sqrt{\frac{D}{n_s}}\}).$ Our simulations show that the joint
coding/scheduling algorithm achieves a throughput of the same order
($\Theta(\min\{1, \sqrt{\frac{D}{n_s}}\})$) under random walk model and random
waypoint model.

ABSTRACT_BEGIN
  A Mobile Ad-Hoc Network (MANET) is a collection of wireless mobile nodes
forming a temporary network without using any centralized access point,
infrastructure, or centralized administration. In this paper we introduce an
Energy Efficient Location Aided Routing (EELAR) Protocol for MANETs that is
based on the Location Aided Routing (LAR). EELAR makes significant reduction in
the energy consumption of the mobile nodes batteries by limiting the area of
discovering a new route to a smaller zone. Thus, control packets overhead is
significantly reduced. In EELAR a reference wireless base station is used and
the network's circular area centered at the base station is divided into six
equal sub-areas. At route discovery instead of flooding control packets to the
whole network area, they are flooded to only the sub-area of the destination
mobile node. The base station stores locations of the mobile nodes in a
position table. To show the efficiency of the proposed protocol we present
simulations using NS-2. Simulation results show that EELAR protocol makes an
improvement in control packet overhead and delivery ratio compared to AODV,
LAR, and DSR protocols.

ABSTRACT_BEGIN
  Our objective is to provide guaranteed packet delivery service in time
constrained sensor networks. The wireless network is a highly variable
environment, where available link bandwidth may vary with network load. Since
multimedia applications require higher bandwidth so we use FSO links for their
transmission. The main advantage of FSO links is that they offer higher
bandwidth and security, while RF links offer more reliability. The routing in
this multitier network is based on directional geographic routing protocol, in
which sensors route their data via multihop paths, to a powerful base station,
through a cluster head. Some modifications have also been incorporated in the
MAC layer to improve the QoS of such systems.

ABSTRACT_BEGIN
  The growth of real-time content streaming over the Internet has resulted in
the use of peer-to-peer (P2P) approaches for scalable content delivery. In such
P2P streaming systems, each peer maintains a playout buffer of content chunks
which it attempts to fill by contacting other peers in the network. The
objective is to ensure that the chunk to be played out is available with high
probability while keeping the buffer size small. Given that a particular peer
has been selected, a \emph{policy} is a rule that suggests which chunks should
be requested by the peer from other peers.. We consider consider a number of
recently suggested policies consistent with buffer minimization for a given
target of skip free playout. We first study a \emph{rarest-first} policy that
attempts to obtain chunks farthest from playout, and a \emph{greedy} policy
that attempts to obtain chunks nearest to playout. We show that they both have
similar buffer scalings (as a function of the number of peers of target
probability of skip-free probability). We then study a hybrid policy which
achieves order sense improvements over both policies and can achieve order
optimal performance. We validate our results using simulations.

ABSTRACT_BEGIN
  GSM networks are very expensive. The network design process requires too many
decisions in a combinatorial explosion. For this reason, the larger is the
network, the harder is to achieve a totally human based optimized solution. The
BSC (Base Station Control) nodes have to be geographically well allocated to
reduce the transmission costs. There are decisions of association between BTS
and BSC those impacts in the correct dimensioning of these BSC. The choice of
BSC quantity and model capable of carrying the cumulated traffic of its
affiliated BTS nodes in turn reflects on the total cost. In addition, the last
component of the total cost is due to transmission for linking BSC nodes to
MSC. These trunks have a major significance since the number of required E1
lines is larger than BTS to BSC link. This work presents an integer programming
model and a computational tool for designing GSM (Global System for Mobile
Communications) networks, regarding BSS (Base Station Subsystem) with optimized
cost.

ABSTRACT_BEGIN
  Timer-based mechanisms are often used to help a given (sink) node select the
best helper node among many available nodes. Specifically, a node transmits a
packet when its timer expires, and the timer value is a monotone non-increasing
function of its local suitability metric. The best node is selected
successfully if no other node's timer expires within a 'vulnerability' window
after its timer expiry, and so long as the sink can hear the available nodes.
In this paper, we show that the optimal metric-to-timer mapping that (i)
maximizes the probability of success or (ii) minimizes the average selection
time subject to a minimum constraint on the probability of success, maps the
metric into a set of discrete timer values. We specify, in closed-form, the
optimal scheme as a function of the maximum selection duration, the
vulnerability window, and the number of nodes. An asymptotic characterization
of the optimal scheme turns out to be elegant and insightful. For any
probability distribution function of the metric, the optimal scheme is
scalable, distributed, and performs much better than the popular inverse metric
timer mapping. It even compares favorably with splitting-based selection, when
the latter's feedback overhead is accounted for.

ABSTRACT_BEGIN
  Optimal dissemination schemes have previously been studied for peer-to-peer
live streaming applications. Live streaming being a delay-sensitive
application, fine tuning of dissemination parameters is crucial. In this
report, we investigate optimal sizing of chunks, the units of data exchange,
and probe sets, the number peers a given node probes before transmitting
chunks. Chunk size can have significant impact on diffusion rate (chunk miss
ratio), diffusion delay, and overhead. The size of the probe set can also
affect these metrics, primarily through the choices available for chunk
dissemination. We perform extensive simulations on the so-called random-peer,
latest-useful dissemination scheme. Our results show that size does matter,
with the optimal size being not too small in both cases.

ABSTRACT_BEGIN
  Epidemic-style diffusion schemes have been previously proposed for achieving
peer-to-peer live streaming. Their performance trade-offs have been deeply
analyzed for homogeneous systems, where all peers have the same upload
capacity. However, epidemic schemes designed for heterogeneous systems have not
been completely understood yet. In this report we focus on the peer selection
process and propose a generic model that encompasses a large class of
algorithms. The process is modeled as a combination of two functions, an aware
one and an agnostic one. By means of simulations, we analyze the
awareness-agnostism trade-offs on the peer selection process and the impact of
the source distribution policy in non-homogeneous networks. We highlight that
the early diffusion of a given chunk is crucial for its overall diffusion
performance, and a fairness trade-off arises between the performance of
heterogeneous peers, as a function of the level of awareness.

ABSTRACT_BEGIN
  Distributed live streaming has brought a lot of interest in the past few
years. In the homogeneous case (all nodes having the same capacity), many
algorithms have been proposed, which have been proven almost optimal or
optimal. On the other hand, the performance of heterogeneous systems is not
completely understood yet. In this paper, we investigate the impact of
heterogeneity on the achievable delay of chunk-based live streaming systems. We
propose several models for taking the atomicity of a chunk into account. For
all these models, when considering the transmission of a single chunk,
heterogeneity is indeed a ``blessing'', in the sense that the achievable delay
is always faster than an equivalent homogeneous system. But for a stream of
chunks, we show that it can be a ``curse'': there is systems where the
achievable delay can be arbitrary greater compared to equivalent homogeneous
systems. However, if the system is slightly bandwidth-overprovisioned, optimal
single chunk diffusion schemes can be adapted to a stream of chunks, leading to
near-optimal, faster than homogeneous systems, heterogeneous live streaming
systems.

ABSTRACT_BEGIN
  Connectivity patterns in intermittently-connected mobile networks (ICMN) can
be modeled as edge-Markovian dynamic graphs. We propose a new model for
epidemic propagation on such graphs and calculate a closed-form expression that
links the best achievable delivery ratio to common ICMN parameters such as
message size, maximum tolerated delay, and link lifetime. These theoretical
results are compared to those obtained by replaying a real-life contact trace.

ABSTRACT_BEGIN
  Structure pathology detection is an important security task in building
construction, which is performed by an operator by looking manually for damages
on the materials. This activity could be dangerous if the structure is hidden
or difficult to reach. On the other hand, embedded devices and wireless sensor
networks (WSN) are becoming popular and cheap, enabling the design of an
alternative pathology detection system to monitor structures based on these
technologies. This article introduces a ZigBee WSN system, intending to be
autonomous, easy to use and with low power consumption. Its functional parts
are fully discussed with diagrams, as well as the protocol used to collect
samples from sensor nodes. Finally, several tests focused on range and power
consumption of our prototype are shown, analysing whether the results obtained
were as expected or not.

ABSTRACT_BEGIN
  The democratization of wireless networks combined to the emergence of mobile
devices increasingly autonomous and efficient lead to new services. Positioning
services become overcrowded. Accuracy is the main quality criteria in
positioning. But to better appreciate this one a coefficient is needed. In this
paper we present Geometric and Signal Strength Dilution of Precision (DOP) for
positioning systems based on Wi-Fi and Signal Strength measurements.

ABSTRACT_BEGIN
  Emergence of online content voting networks allows users to share and rate
content including social news, photos and videos. The basic idea behind online
content voting networks is that aggregate user activities (e.g., submitting and
rating content) makes high-quality content thrive through the unprecedented
scale, high dynamics and divergent quality of user generated content (UGC). To
better understand the nature and impact of online content voting networks, we
have analyzed Digg, a popular online social news aggregator and rating website.
Based on a large amount of data collected, we provide an in-depth study of
Digg. In particular, we study structural properties of Digg social network,
impact of social network properties on user digging activities and vice versa,
distribution of user diggs, content promotion, and information filtering. We
also provide insight into design of content promotion algorithms and
recommendation-assisted content discovery. Overall, we believe that the results
presented in this paper are crucial in understanding online content rating
networks.

ABSTRACT_BEGIN
  A cross-layer scheme, namely ALOHA With Collision Resolution (ALOHA-CR), is
proposed for high throughput wireless communications in a cellular scenario.
Transmissions occur in a time-slotted ALOHA-type fashion but with an important
difference: simultaneous transmissions of two users can be successful. If more
than two users transmit in the same slot the collision cannot be resolved and
retransmission is required. If only one user transmits, the transmitted packet
is recovered with some probability, depending on the state of the channel. If
two users transmit the collision is resolved and the packets are recovered by
first over-sampling the collision signal and then exploiting independent
information about the two users that is contained in the signal polyphase
components. The ALOHA-CR throughput is derived under the infinite backlog
assumption and also under the assumption of finite backlog. The contention
probability is determined under these two assumptions in order to maximize the
network throughput and maintain stability. Queuing delay analysis for network
users is also conducted. The performance of ALOHA-CR is demonstrated on the
Wireless Open Access Research Platform (WARP) test-bed containing five software
defined radio nodes. Analysis and test-bed results indicate that ALOHA-CR leads
to significant increase in throughput and reduction of service delays.

ABSTRACT_BEGIN
  Mobile IPv6 will be an integral part of the next generation Internet
protocol. The importance of mobility in the Internet gets keep on increasing.
Current specification of Mobile IPv6 does not provide proper support for
reliability in the mobile network and there are other problems associated with
it. In this paper, we propose Virtual Private Network (VPN) based Home Agent
Reliability Protocol (VHAHA) as a complete system architecture and extension to
Mobile IPv6 that supports reliability and offers solutions to the security
problems that are found in Mobile IP registration part. The key features of
this protocol over other protocols are: better survivability, transparent
failure detection and recovery, reduced complexity of the system and workload,
secure data transfer and improved overall performance.

ABSTRACT_BEGIN
  Accurate and fast packet delivery rate (PDR) estimation, used in evaluating
wireless link quality, is a prerequisite to increase the performance of mobile,
multi-hop and multi-rate wireless ad hoc networks. Unfortunately, contemporary
PDR estimation methods, i.e. beacon-based packet counting in Estimated
Transmission Time and Expected Transmission Count metrics, have unsatisfactory
performance. Therefore, in this paper we propose a novel PDR estimation method
based on SNR profiles. We classify all possible link quality estimation methods
and compare them analytically against our design. Results show that it leads to
a more efficient link quality estimation. Further investigations with the
prototype implementation of our method in IEEE 802.11b/g testbeds reveal that
the accuracy of PDR estimation in mobile scenarios can be improved up to 50% in
comparison to generic packet-based PDR. Experiments with the same prototype on
link and routing layers for different measurement scenarios show that it leads
to a better rate adaptation and route selection in the form of end-to-end
throughput increase compared to traditional packet counting methods.

ABSTRACT_BEGIN
  This paper proposes a novel, highly effective spectrum sensing algorithm for
cognitive radio and whitespace applications. The proposed spectral covariance
sensing (SCS) algorithm exploits the different statistical correlations of the
received signal and noise in the frequency domain. Test statistics are computed
from the covariance matrix of a partial spectrogram and compared with a
decision threshold to determine whether a primary signal or arbitrary type is
present or not. This detector is analyzed theoretically and verified through
realistic open-source simulations using actual digital television signals
captured in the US. Compared to the state of the art in the literature, SCS
improves sensitivity by 3 dB for the same dwell time, which is a very
significant improvement for this application. Further, it is shown that SCS is
highly robust to noise uncertainty, whereas many other spectrum sensors are
not.

ABSTRACT_BEGIN
  This paper studies the impact of interference asynchrony among different
links in a wireless network. Without deliberate coordination and cooperation
among the active links, there is a naturally occurring misalignment between the
symbols of the targeted signal of a receiver and the symbols of the interfering
signals. Interestingly, we show that the interference asynchrony can actually
improve the BER performance, compared with the situation in which symbols of al
signals ay aligned. In particular, we show that symbol misalignment can
decrease the "effective interference power" and change the distribution of the
interfering signals, in a way that results in lower BER. To ensure that symbol
misalignment can be consistently attained, we propose two simple schemes that
introduce time-varying symbol offsets to obtain an "average" performance of
random symbol misalignment. Notably, our schemes do not change the simple
receiver design structure; only the transmitters are modified in a minor way.

ABSTRACT_BEGIN
  This paper takes a critical look at the usefulness of power law models of the
Internet. The twin focuses of the paper are Internet traffic and topology
generation. The aim of the paper is twofold. Firstly it summarises the state of
the art in power law modelling particularly giving attention to existing open
research questions. Secondly it provides insight into the failings of such
models and where progress needs to be made for power law research to feed
through to actual improvements in network performance.

ABSTRACT_BEGIN
  This paper proposes a self-optimized coverage coordination scheme for
two-tier femtocell networks, in which a femtocell base station adjusts the
transmit power based on the statistics of the signal and the interference power
that is measured at a femtocell downlink. Furthermore, an analytic expression
is derived for the coverage leakage probability that a femtocell coverage area
leaks into an outdoor macrocell. The coverage analysis is verified by
simulation, which shows that the proposed scheme provides sufficient indoor
femtocell coverage and that the femtocell coverage does not leak into an
outdoor macrocell.

ABSTRACT_BEGIN
  This paper proposes two interference mitigation strategies that adjust the
maximum transmit power of femtocell users to suppress the cross-tier
interference at a macrocell base station (BS). The open-loop and the
closed-loop control suppress the cross-tier interference less than a fixed
threshold and an adaptive threshold based on the noise and interference (NI)
level at the macrocell BS, respectively. Simulation results show that both
schemes effectively compensate the uplink throughput degradation of the
macrocell BS due to the cross-tier interference and that the closed-loop
control provides better femtocell throughput than the open-loop control at a
minimal cost of macrocell throughput.

ABSTRACT_BEGIN
  A network's transmission capacity is the maximal rate of traffic inflow that
the network can handle without causing congestion. Here we study how to enhance
this quantity by redistributing the capability of individual nodes while
preserving the total sum of node capability. We propose a practical and
effective node-capability allocation scheme which allocates a node's capability
based on the local knowledge of the node's connectivity. We show the scheme
enhances the transmission capacity by two orders of magnitude for networks with
heterogenous structures.

ABSTRACT_BEGIN
  The two-phase MIMO NC (network coding) scheme can be used to boost the
throughput in a two-way relay channel in which nodes are equipped with multiple
antennas. The obvious strategy is for the relay node to extract the individual
packets from the two end nodes and mix the two packets to form a network-coded
packet. In this paper, we propose a new scheme called MIMO PNC (physical
network coding), in which the relay extracts the summation and difference of
the two end packets and then converts them to the network-coded form. MIMO PNC
is a natural combination of the single-antenna PNC scheme and the linear MIMO
detection scheme. The advantages of MIMO PNC are many. First, it removes the
stringent carrier-phase requirement in single-antenna PNC. Second, it is linear
in complexity with respect to the constellation size and the number of
simultaneous data streams in MIMO. Simulation shows that MIMO PNC outperforms
the straightforward MIMO NC significantly under random Rayleigh fading channel.
Based on our analysis, we further conjecture that MIMO PNC outperforms MIMO NC
under all possible realizations of the channel.

ABSTRACT_BEGIN
  A main distinguishing feature of a wireless network compared with a wired
network is its broadcast nature, in which the signal transmitted by a node may
reach several other nodes, and a node may receive signals from several other
nodes, simultaneously. Rather than a blessing, this feature is treated more as
an interference-inducing nuisance in most wireless networks today (e.g., IEEE
802.11). This paper shows that the concept of network coding can be applied at
the physical layer to turn the broadcast property into a capacity-boosting
advantage in wireless ad hoc networks. Specifically, we propose a
physical-layer network coding (PNC) scheme to coordinate transmissions among
nodes. In contrast to "straightforward" network coding which performs coding
arithmetic on digital bit streams after they have been received, PNC makes use
of the additive nature of simultaneously arriving electromagnetic (EM) waves
for equivalent coding operation. And in doing so, PNC can potentially achieve
100% and 50% throughput increases compared with traditional transmission and
straightforward network coding, respectively, in 1-D regular linear networks
with multiple random flows. The throughput improvements are even larger in 2-D
regular networks: 200% and 100%, respectively.

ABSTRACT_BEGIN
  Different cross layer design for mobile adhoc network focuses on different
optimization purpose, different Quality of Service (QoS) metric and the
functions like delay, priority handling, security, etc. Existing cross layer
designs provide individual solution for congestion control, fault tolerance,
power conservation, energy minimization and flow control and the major drawback
is of high cost and overhead. In this paper, we propose to design multiple
cross layer design based architecture to provide a combined solution for link
failure management, power conservation, congestion control and admission
control. By simulation results, we show that the average end to end delay,
average energy consumption and the packet loss are considerably reduced with
the increase in high throughput and good delivery ratio.

ABSTRACT_BEGIN
  This article introduces a simple and effective methodology to determine the
level of congestion in a network with an ECN-like marking scheme. The purpose
of the ECN bit is to notify TCP sources of an imminent congestion in order to
react before losses occur. However, ECN is a binary indicator which does not
reflect the congestion level (i.e. the percentage of queued packets) of the
bottleneck, thus preventing any adapted reaction. In this study, we use a
counter in place of the traditional ECN marking scheme to assess the number of
times a packet has crossed a congested router. Thanks to this simple counter,
we drive a statistical analysis to accurately estimate the congestion level of
each router on a network path. We detail in this paper an analytical method
validated by some preliminary simulations which demonstrate the feasibility and
the accuracy of the concept proposed. We conclude this paper with possible
applications and expected future work.

ABSTRACT_BEGIN
  We present an analytical framework to assess the link layer throughput of
multichannel Opportunistic Spectrum Access (OSA) ad hoc networks. Specifically,
we focus on analyzing various combinations of collaborative spectrum sensing
and Medium Access Control (MAC) protocol abstractions. We decompose
collaborative spectrum sensing into layers, parametrize each layer, classify
existing solutions, and propose a new protocol called Truncated Time Division
Multiple Access (TTDMA) that supports efficient distribution of sensing results
in "k out of N" fusion rule. In case of multichannel MAC protocols we evaluate
two main approaches of control channel design with (i) dedicated and (ii)
hopping channel. We propose to augment these protocols with options of handling
secondary user (SU) connections preempted by primary user (PU) by (i)
connection buffering until PU departure and (ii) connection switching to a
vacant PU channel. By comparing and optimizing different design combinations we
show that (i) it is generally better to buffer preempted SU connections than to
switch them to PU vacant channels and (ii) TTDMA is a promising design option
for collaborative spectrum sensing process when k does not change over time.

ABSTRACT_BEGIN
  The link scheduling in wireless multi-hop networks is addressed. Different
from most of work that adopt the protocol interference model which merely take
consideration of packet collisions, our proposed algorithms use the physical
interference model to reflect the aggregated signal to interference and noise
ratio (SINR), which is a more accurate abstraction of the real scenario. We
first propose a centralized scheduling method based on the Integer Linear
Programming (ILP) and resolve it by an approximate solution based on the
randomized rounding method. The probability bound of getting a guaranteed
approximate factor is given. We then extend the centralized algorithm to a
distributed solution, which is favorable in wireless networks. It is proven
that with the distributed scheduling method, all links can transmit without
interference, and the approximate ratio of the algorithm is also given.

ABSTRACT_BEGIN
  TBecause of limited energy of nodes, an important issue for sensor network is
efficient use of the energy. The clustering technique reduces energy
consumption as cluster head sends sensed information to a sink node. Because of
such character of clustering technique, electing cluster head is an important
element for networks. This paper proposes RCFT (ReClustering Formation
Technique) that reconstruct clusters in hierarchical sensor networks. RCFT is a
protocol that reconstructed clusters considering position of a cluster head and
nodes in randomly constructed clusters. And this paper demonstrated that
clusters are composed evenly through simulation, accordingly this simulation
shows the result reducing energy consumption.

ABSTRACT_BEGIN
  What is the impact of obstacles on the graphs of connections between stations
in Mobile Ad hoc Networks? In order to answer, at least partially, this
question, the first step is to define both an environment with obstacles and a
mobility model for the stations in such an environment. The present paper
focuses on a new way of considering the mobility within environments with
obstacles, while keeping the core ideas of the well-known Random WayPoint
mobility model (a.k.a RWP). Based on a mesh-partitioning of the space, we
propose a new model called RSP-O-G for which we compute the spatial
distribution of stations and analyse how the presence of obstacles impacts this
distribution compared to the distribution when no obstacles are present.
Coupled with a simple model of radio propagation, and according to the density
of stations in the environment, we study the mean degree of the connection
graphs corresponding to such mobile ad hoc networks.

ABSTRACT_BEGIN
  The evolution of the Internet during the last years, has lead to a dramatic
increase of the size of its graph at the Autonomous System (AS) level. Soon -
if not already - its size will make the latter impractical for use from the
research community, e.g. for protocol testing. Reproducing a smaller size,
snapshot of the AS graph is thus important. However, the first step towards
this direction is to obtain the ability to faithfully reproduce the full AS
topology. The objective of our work, is to create a generator able to
accurately emulate and reproduce the distinctive properties of the Internet
graph. Our approach is based on (a) the identification of the jellyfish-like
structure [1] of the Internet and (b) the consideration of the peer-to-peer and
customer-provider relations between ASs. We are the first to exploit the
distinctive structure of the Internet graph together with utilizing the
information provided by the AS relationships in order to create a tool with the
aforementioned capabilities. Comparing our generator with the existing ones in
the literature, the main difference is found on the fact that our tool does not
try to satisfy specific metrics, but tries to remain faithful to the conceptual
model of the Internet structure. In addition, our approach can lead to (i) the
identification of important attributes and patterns in the Internet AS
topology, as well as, (ii) the extraction of valuable information on the
various relationships between ASs and their effect on the formulation of the
Internet structure. We implement our graph generator and we evaluate it using
the largest and most recent available dataset for the AS topology. Our
evaluations, clearly show the ability of our tool to capture the structural
properties of the Internet topology at the AS level with high accuracy.

ABSTRACT_BEGIN
  This paper discusses the advent of new technologies which have emerged under
the area of Location Based Services (LBS). An innovative implementation and
approach has been presented for design of applications which are inventive and
attractive towards the user. Spatial Trigger is one of the most promising
additions to the LBS technologies. This paper describes ways in which mobile
advertisement services can be introduced effectively in the cellular market by
bringing innovation in them through effective usage of Spatial Triggers. Hence,
opening new horizons to make the consumer cellular networks, commercially, more
effective and informative.

ABSTRACT_BEGIN
  This paper presents the design and the realization of a hybrid wireless
Gigabit Ethernet indoor communications system operating at 60 GHz. As the 60
GHz radio link operates only in a single-room configuration, an additional
Radio over Fiber (RoF) link is used to ensure the communications within all the
rooms of a residential environment. The system uses low complexity baseband
processing modules. A byte synchronization technique is designed to provide a
high value of the preamble detection probability and a very small value of the
false detection probability. Conventional RS (255, 239) encoder and decoder are
used for channel forward error correction (FEC). The FEC parameters are
determined by the tradeoff between higher coding gain and hardware complexity.
The results of bit error rate measurements at 875 Mbps are presented for
various antennas configurations.

ABSTRACT_BEGIN
  This paper presents the study and the realization of a hybrid 60 GHz wireless
communications system. As the 60 GHz radio link operates only in a single-room
configuration, an additional Radio over Fibre (RoF) link is used to ensure the
communications in all the rooms of a residential environment. A single carrier
architecture is adopted. The system uses low complexity baseband processing
modules. A byte/frame synchronization technique is designed to provide a high
value of the preamble detection probability and a very small value of the false
alarm probability. Conventional RS (255, 239) encoder and decoder are used to
correct errors in the transmission channel. Results of Bit Error Rate (BER)
measurements are presented for various antennas configurations.

ABSTRACT_BEGIN
  This paper presents the study and the realization at IETR of a high data rate
60 GHz wireless communications system. The system uses a simple single carrier
architecture. The receiver architecture is based on a differential demodulation
which minimizes the intersymbol interference (ISI) effect and a signal
processing unit composed of a joint frame and byte synchronization block and a
conventional RS (255, 239) decoder. The byte synchronization technique provides
a high preamble detection probability and a very small value of the false
detection probability. First measurement results show a good communication link
quality in line of sight environments with directional antennas.

ABSTRACT_BEGIN
  This report considers the class of applications of sensor networks in which
each sensor node makes measurements, such as temperature or humidity, at the
precise location of the node. Such spot-sensing applications approximate the
physical condition of the entire region of interest by the measurements made at
only the points where the sensor nodes are located. Given a certain density of
nodes in a region, a more spatially uniform distribution of the nodes leads to
a better approximation of the physical condition of the region. This report
considers the error in this approximation and seeks to improve the quality of
representation of the physical condition of the points in the region in the
data collected by the sensor network. We develop two essential metrics which
together allow a rigorous quantitative assessment of the quality of
representation achieved: the average representation error and the unevenness of
representation error, the latter based on a well-accepted measure of inequality
used in economics. We present the rationale behind the use of these metrics and
derive relevant theoretical bounds on them in the common scenario of a planar
region of arbitrary shape covered by a sensor network deployment. A simple new
heuristic algorithm is presented for each node to determine if and when it
should sense or sleep to conserve energy while also preserving the quality of
representation. Simulation results show that it achieves a significant
improvement in the quality of representation compared to other related
distributed algorithms. Interestingly, our results also show that improved
spatial uniformity has the welcome side-effect of a significant increase in the
network lifetime.

ABSTRACT_BEGIN
  In this paper, we analyze a round-based pricing scheme that encourages
favorable behavior from users of real-time P2P applications like P2PTV. In the
design of pricing schemes, we consider price to be a function of usage and
capacity of download/upload streams, and quality of content served. Users are
consumers and servers at the same time in such networks, and often exhibit
behavior that is unfavorable towards maximization of social benefits.
Traditionally, network designers have overcome this difficulty by building-in
traffic latencies. However, using simulations, we show that appropriate pricing
schemes and usage terms can enable designers to limit required traffic
latencies, and be able to earn nearly 30% extra revenue from providing P2PTV
services. The service provider adjusts the prices of individual programs
incrementally within rounds, while making relatively large-scale adjustments at
the end of each round. Through simulations, we show that it is most beneficial
for the service provider to carry out 5 such rounds of price adjustments for
maximizing his average profit and minimizing the associated standard deviation
at the same time.

ABSTRACT_BEGIN
  The last few decades have seen considerable research progress in
microelectronics and integrated circuits, system-on-chip design, wireless
communication, and sensor technology. This progress has enabled the seamless
integration of autonomous wireless sensor nodes around a human body to create a
Body Sensor Network (BSN). The development of a proactive and ambulatory BSN
induces a number of enormous issues and challenges. This paper presents the
technical hurdles during the design and implementation of a low-power Medium
Access Control (MAC) protocol for in-body and on-body sensor networks. We
analyze the performance of IEEE 802.15.4 protocol for the on-body sensor
network. We also provide a comprehensive insight into the heterogeneous
characteristics of the in-body sensor network. A low-power technique called
Pattern-Based Wake-up Table is proposed to handle the normal traffic in a BSN.
The proposed technique provides a reliable solution towards low-power
communication in the in-body sensor network.

ABSTRACT_BEGIN
  Wireless relaying is one of the promising solutions to overcome the channel
impairments and provide high data rate coverage that appears for beyond 3G
mobile communications. In this paper we present an end to end BER performance
analysis of dual hop wireless communication systems equipped with multiple
decode and forward relays over the Rayleigh fading channel with relay
selection. We select the best relay based on end to end channel conditions. We
apply orthogonal space time block coding (OSTBC) at source, and also present
how the multiple antennas at the source terminal affects the end to end BER
performance. This intermediate relay technique will cover long distance where
destination is out of reach from source.

ABSTRACT_BEGIN
  Current advances in wireless communication, microelectronics, semiconductor
technologies, and intelligent sensors have contributed to the development of
unobtrusive WBANs. These networks provide long term health monitoring of
patients without any constraint in their normal activities. Traditional MAC
protocols do not accommodate the assorted WBAN traffic requirements in a power
efficient manner. In this paper, we present a brief discussion on the
development process of a low power MAC protocol for WBANs. We observe the
behavior of a beacon-enabled IEEE 802.15.4 for on-body sensor networks. We
further propose a low power technique called traffic based wakeup mechanism for
a WBAN that exploits the traffic patterns of the BAN Nodes to ensure power
efficient and reliable communication.

ABSTRACT_BEGIN
  The advent of fourth generation technologies in wireless networks and the
rapid growth of 3G have heralded an era that will require researchers to find
reliable and easily implement-able solutions to the problem of poor TCP
performance in the wireless environment. Since a large part of the Internet is
TCP-based, solving this problem will be instrumental in determining if the move
from wired to wireless will be seamless or not. This paper proposes a scheme
that uses the base station's ability to predict the time at which the link may
be going down and to estimate the period for which the mobile would be
unreachable due to conditions like fading. By using cross-layer and ACK pacing
algorithms, the base station prevents the fixed host from timing out while
waiting for ACKs from the mobile. This in turn prevents TCP on the fixed host
from bringing down the throughput drastically due to temporary network
conditions, caused by mobility or the unreliability of wireless links.
Experimental results indicate a reasonable increase in throughput when the ACK
holding scheme is used.

ABSTRACT_BEGIN
  The dual-cross scenario of the hybrid wireless sensor networks (WSNs) is
studied and a novel MIMO Cluster Cooperative Assignment Cross Layer Scheduling
Scheme (MCCA-CLSS) is proposed in this paper. The comparison and the
predominance of the proposed scheme are demonstrated, the clusters are
optimized. With the help of the simulations, the relative energy consumption
and the end-to-end blocking probability are all improved. The addressing ratio
of success in the condition of the unchanged parameters and external
information can be increased and the network can tolerate more hops to support
reliable transportation by the proposed scheme.

ABSTRACT_BEGIN
  A special scenario of the topology in the hybrid Cognitive Ad-hoc networks is
studied and a novel cross layer scheme is proposed in this paper. The proposed
scheme integrated the attributes both of the new performance evaluation machine
check time metric and the topology space in special scenario. The topology and
power consumption of each node can all be optimized due to the minimum link
occupation with the help of this scheme. Simulation results show that the novel
scheme can give schedule guarantee to the multi-channel networks in the
variable node loads and transmission powers, and make the node stable to
support multi-hops at the same time.

ABSTRACT_BEGIN
  This paper presents an empirical discussion on the design and implementation
of a power-efficient Medium Access Control (MAC) protocol for in-body and
on-body sensor networks. We analyze the performance of a beacon-enabled IEEE
802.15.4, PB-TDMA, and S-MAC protocols for on-body sensor networks. We further
present a Traffic Based Wakeup Mechanism that utilizes the traffic patterns of
the BAN Nodes (BNs) to accommodate the entire BSN traffic. To enable a logical
connection between different BNs working on different frequency bands, a method
called Bridging function is proposed. The Bridging function integrates all BNs
working on different bands into a complete BSN.

ABSTRACT_BEGIN
  Recent advances in intelligent sensors, microelectronics and integrated
circuit, system-on-chip design and low power wireless communication introduced
the development of miniaturised and autonomous sensor nodes. These tiny sensor
nodes can be deployed to develop a proactive Body Sensor Network (BSN). The
rapid advancement in ultra low-power RF (radio frequency) technology enables
invasive and non-invasive devices to communicate with a remote station. This
communication revolutionizes healthcare system by enabling long term health
monitoring of a patient and providing real time feedback to the medical
experts. In this paper, we present In-body and On-body communication networks
with a special focus on the methodologies of wireless communication between
implanted medical devices with external monitoring equipment and recent
technological growth in both areas. We also discuss open issues and challenges
in a BSN.

ABSTRACT_BEGIN
  Recent advances in wideband impulse technology, low power communication along
with unlicensed band have enabled ultra wide band (UWB) as a leading technology
for future wireless applications. This paper outlines the applications of
emerging UWB technology in a private and commercial sector. We further talk
about UWB technology for a wireless body area network (WBAN).

ABSTRACT_BEGIN
  An "RF sensor" network can monitor RSS values on links in the network and
perform device-free localization, i.e., locating a person or object moving in
the area in which the network is deployed. This paper provides a statistical
model for the RSS variance as a function of the person's position w.r.t. the
transmitter (TX) and receiver (RX). We show that the ensemble mean of the RSS
variance has an approximately linear relationship with the expected total
affected power (ETAP). We then use analysis to derive approximate expressions
for the ETAP as a function of the person's position, for both scattering and
reflection. Counterintuitively, we show that reflection, not scattering, causes
the RSS variance contours to be shaped like Cassini ovals. Experimental tests
reported here and in past literature are shown to validate the analysis.

ABSTRACT_BEGIN
  As the Internet becomes severely overburdened with exponentially growing
traffic demand, it becomes a general belief that a new generation data network
is in urgent need today. However, standing at this crossroad, we find that we
are in a situation that lacks a theory of network designing. This issue becomes
even more serious as the recent progress of network measurement and modeling
challenges the foundation of network research in the past decades.
  This paper tries to set up a scientific foundation for network designing by
formalizing it as a multi-objective optimization process and quantifying the
way different designing choices independently and collectively influence these
objectives. A cartesian coordinate system is introduced to map the effect of
each designing scheme to a coordinate. We investigated the achievable area of
the network designing space and proved some boundary conditions. It is shown
that different kind of networks display different shapes of achievable areas in
the cartesian coordinate and exhibit different abilities to achieve
cost-effective and scalable designing. In particular, we found that the
philosophy underlying current empirical network designing and engineering fails
to meet the cost-effective and evolvable requirements of network designing. We
demonstrated that the efficient routing combined with effective betweenness
based link bandwidth allocation scheme is a cost-effective and scalable design
for BA-like scale-free networks, whereas if other designing choices cannot be
determined beforehand, ER network is a markedly good candidate for
cost-effective and scalable design.

ABSTRACT_BEGIN
  Many Web-based data sources and services are available as feeds, a model that
provides consumers with a loosely coupled way of interacting with providers.
The current feed model is limited in its capabilities, however. Though it is
simple to implement and scales well, it cannot be transferred to a wider range
of application scenarios. This paper conceptualizes feeds as a way to serialize
query results, describes the current hardcoded query semantics of such a
perspective, and surveys the ways in which extensions of this hardcoded model
have been proposed or implemented. Our generalized view of feeds as query
result serializations has implications for the applicability of feeds as a
generic Web service for any collection that is providing access to individual
information items. As one interesting and compelling class of applications, we
describe a simple way in which a query-based approach to feeds can be used to
support location-based services.

ABSTRACT_BEGIN
  This paper proposes a mathematical justification of the phenomenon of extreme
congestion at a very limited number of nodes in very large networks. It is
argued that this phenomenon occurs as a combination of the negative curvature
property of the network together with minimum length routing. More
specifically, it is shown that, in a large n-dimensional hyperbolic ball B of
radius R viewed as a roughly similar model of a Gromov hyperbolic network, the
proportion of traffic paths transiting through a small ball near the center is
independent of the radius R whereas, in a Euclidean ball, the same proportion
scales as 1/R^{n-1}. This discrepancy persists for the traffic load, which at
the center of the hyperbolic ball scales as the square of the volume, whereas
the same traffic load scales as the volume to the power (n+1)/n in the
Euclidean ball. This provides a theoretical justification of the experimental
exponent discrepancy observed by Narayan and Saniee between traffic loads in
Gromov-hyperbolic networks from the Rocketfuel data base and synthetic
Euclidean lattice networks. It is further conjectured that for networks that do
not enjoy the obvious symmetry of hyperbolic and Euclidean balls, the point of
maximum traffic is near the center of mass of the network.

ABSTRACT_BEGIN
  This paper aims to identify the operational region of a link in terms of its
utilization and alert operators at the point where the link becomes overloaded
and requires a capacity upgrade. The number of active flows is considered the
real network state and is proposed to use a proxy for utilization. The Gaussian
approximation gives the expression for the confidence interval on an
operational region. The easy rule has been formulated to display the network
defects by means of measurements of router loading and number of active flows.
Mean flow performance is considered as the basic universal index characterized
quality of network services provided to single user.

ABSTRACT_BEGIN
  We propose an architecture, Flare, that is a structured and easy way to
develop applications rapidly, in a multitude of languages, which make use of
online storage of data and management of users. The architecture eliminates the
need for server-side programming in most cases, creation and management of
online database storage servers, re-creation of user management schemes and
writing a lot of unnecessary code for accessing different web-based services
using their APIs. A Web API provides a common API for various web-based
services like Blogger [2], Wordpress, MSN Live, Facebook [3] etc. Access
Libraries provided for major programming languages and platforms make it easy
to develop applications using the Flare Web Service. We demonstrate a simple
micro-blogging service developed using these APIs in two modes: a graphical
browser-based mode, and a command-line mode in C++, which provide two different
interfaces to the same account and data.

ABSTRACT_BEGIN
  Packet-dispersion based measurement tools insert pairs of probe packets with
a known separation into the network for transmission over a unicast path or a
multicast tree. Samples of the separation between the probe pairs at the
destination(s) are observed. Heuristic techniques are then used by these tools
to estimate the path characteristics from the observations. In this paper we
present a queueing theoretic setting for packet-dispersion based probing.
Analogous to network tomography, we develop techniques to estimate the
parameters of the arrival process to the individual links from the samples of
the output separations, i.e., from the end-to-end measurements. The links are
modeled as independent discrete time queues with i.i.d. arrivals. We first
obtain an algorithm to obtain the (joint) distribution of the separation
between the probes at the destination(s) for a given distribution of the
spacing at the input. The parameter estimates of the arrival process are
obtained as the minimizer of a cost function between the empirical and
calculated distributions. We also carry out extensive simulations and numerical
experiments to study the performance of the estimation algorithm under the
fairly `harsh' conditions of non stationarity of the arrival process. We find
that the estimations work fairly well for two queues in series and for
multicast.

ABSTRACT_BEGIN
  The goal of congestion control is to avoid congestion in network elements. A
network element is congested if it is being offered more traffic than it can
process. To detect such situations and to neutralize them we should monitor
traffic in the network. In this paper, we propose using Cisco's NetFlow
technology, which allows collecting statistics about traffic in the network by
generating special NetFlow packets. Cisco's routers can send NetFlow packets to
a special node, so we can collect these packets, analyze its content and detect
network congestion. We use Cisco's feature as example, some other vendors
(Juniper, 3COM, Alcatel, etc.) provide similar features for their routers. We
also consider a simple system, which collects statistical information about
network elements, determines overloaded elements and identifies flows, which
congest them.

ABSTRACT_BEGIN
  Due to the advancement of computing and communication technology, networked
control systems may soon become prevalent in many control applications. While
the capability of employing the communication network in the control loop
certainly provides many benefits, it also raises several challenges which need
to be overcome to utilize the benefits.
  In this chapter, we focus on one major challenge: a middleware framework that
enables a networked control system to be implemented. Indeed our thesis is that
a middleware for networked control sys important for the future of networked
control systems.
  We discuss the fundamental issues which need to be considered in the design
and development of an appropriate middleware for networked control systems. We
describe \emph{Etherware}, a middleware for networked control system which has
been developed at the University of Illinois, as an example of such a
middleware framework, to illustrate how these issues can be addressed in the
design of a middleware. Using a networked inverted pendulum control system as
an example, we demonstrate the powerful capabilities provided by Etherware for
a networked control system.

ABSTRACT_BEGIN
  Sensor networks are particularly applicable to the tracking of objects in
motion. For such applications, it may not necessary that the whole region be
covered by sensors as long as the uncovered region is not too large. This
notion has been formalized by Balasubramanian et.al. as the problem of
$\kappa$-weak coverage. This model of coverage provides guarantees about the
regions in which the objects may move undetected. In this paper, we analyse the
theoretical aspects of the problem and provide guarantees about the lifetime
achievable. We introduce a number of practical algorithms and analyse their
significance. The main contribution is a novel linear programming based
algorithm which provides near-optimal lifetime. Through extensive
experimentation, we analyse the performance of these algorithms based on
several parameters defined.

ABSTRACT_BEGIN
  Relay selection for cooperative communications promises significant
performance improvements, and is, therefore, attracting considerable attention.
While several criteria have been proposed for selecting one or more relays,
distributed mechanisms that perform the selection have received relatively less
attention. In this paper, we develop a novel, yet simple, asymptotic analysis
of a splitting-based multiple access selection algorithm to find the single
best relay. The analysis leads to simpler and alternate expressions for the
average number of slots required to find the best user. By introducing a new
`contention load' parameter, the analysis shows that the parameter settings
used in the existing literature can be improved upon. New and simple bounds are
also derived. Furthermore, we propose a new algorithm that addresses the
general problem of selecting the best $Q \ge 1$ relays, and analyze and
optimize it. Even for a large number of relays, the algorithm selects the best
two relays within 4.406 slots and the best three within 6.491 slots, on
average. We also propose a new and simple scheme for the practically relevant
case of discrete metrics. Altogether, our results develop a unifying
perspective about the general problem of distributed selection in cooperative
systems and several other multi-node systems.

ABSTRACT_BEGIN
  Energy efficiency is a corner stone of sustainability in data center and
high-performance networking. However, at present there is a notable structural
mismatch between network silicon development targets and network equipment
utilization patterns in the field. In particular, some aspects of network
energy utilization (eg load-proportional energy consumption) routinely stay out
of focus during system design and implementation. Drawing from hands-on
research and development in high-speed and grid networking, we identify a novel
approach to energy efficiency in network engineering. In this paper, we
demonstrate how the problem of efficient network system design can be dissected
into smaller sections based on timescales of traffic processing. The newly
proposed approach allows R&D efforts to be tightly paired to resources and
sustainability targets to improve energy efficiency in many classes of network
and telecom devices.

ABSTRACT_BEGIN
  Towards employing low complexity transceivers for signal reception in
Ultra-Wideband (UWB) systems, Transmitted Reference (TR) and Differential TR
(DTR) schemes have attracted researchers attention. In this letter, we
introduce an alternative, less complex scheme, called Self Reference (SR) UWB
transceiver, which uses a modified replica of the received signal itself as
reference pulse, resulting in double data rates compared to TR schemes.
Moreover, SR eliminates the need for delay lines at the receiver side, which
constitute a major drawback of the conventional TR and DTR schemes, while it
also requires no channel estimations, resulting in lower complexity
implementations and power savings. The performance of the SR scheme is
investigated in high-frequency (HF) channels, showing that it offers a better
or comparable performance to that of DTR, depending on the channel conditions.

ABSTRACT_BEGIN
  This paper addresses an overview of the wireless sensor networks. It is shown
that MEMS/NEMS technologies and SIP concept are well suited for advanced
architectures. It is also shown analog architectures have to be compatible with
digital signal techniques to develop smart network of microsystem.

ABSTRACT_BEGIN
  This paper addresses the radio interface problematic for MANET (Mobile Ad-hoc
NETwork) applications. Here we propose to study the radio reconfigurability in
order to provide a unique physical layer which is able to deal with all MANET
applications. For implementing this reconfigurable physical layer, we propose
to use Impulse Radio Ultra WideBand (IRUWB). This paper presents also our two
level design approach for obtaining our reconfigurable IR-UWB receiver on FPGA
(Field Programmable Gate Array).

ABSTRACT_BEGIN
  We introduce the radio reconfigurability thanks to IRUWB mostly digital
architecture for MANET context. This particular context implies some
constraints on the radio interface such as low cost, low power, small
dimensions and simplicity. Here, we propose an implementation of dynamic
reconfigurable receiver on ASIC, and FPGA, after having explained the
advantages of mostly digital radio for reconfigurability. In this paper, by
studying our prototypes, we could prove that reconfigurability is on the
contrary with MANET constraints needs. The proposed solution allows data rate,
radio range, energy and spectrum occupation reconfigurability.

ABSTRACT_BEGIN
  This paper presents our approach of the radio interface problematic for
Wireless Sensor Network. We introduce the WSN context and constraints
associated. We propose an IR-UWB solution and illustrate why it could be a
viable solution for WSN. A high level modelling and simulation platform for
IR-UWB radio interface is proposed on Matlab. It allows us to determine
according to BER versus Eb/N0 criteria and the WSN constraints what kind of
design is more adequate. Moreover, a co-design co-simulation platform Matlab
VHDL is proposed here. Using this platform we designed IR-UWB transceiver
having reconfigurable capabilities, such as data rate reconfiguration, time
hopping code, spectrum occupation and radio range reconfiguration.

ABSTRACT_BEGIN
  Impulse Radio Ultra Wide Band (IR-UWB) is a promising technology to address
Wireless Sensor Network (WSN) constraints. However, existing network simulation
tools do not provide a complete WSN simulation architecture, with the IR-UWB
specificities at the PHYsical (PHY) and the Medium Access Control (MAC) layers.
In this paper, we propose a WSN simulation architecture based on the IR-UWB
technique. At the PHY layer, we take into account the pulse collision by
dealing with the pulse propagation delay. We also modelled MAC protocols
specific to IRUWB, for WSN applications. To completely fit the WSN simulation
requirements, we propose a generic and reusable sensor and sensing channel
model. Most of the WSN application performances can be evaluated thanks to the
proposed simulation architecture. The proposed models are implemented on a
scalable and well known network simulator: Global Mobile Information System
Simulator (GloMoSim). However, they can be reused for all other packet based
simulation platforms.

ABSTRACT_BEGIN
  This paper describes a detailed performance evaluation of distributed Medium
Access Control (MAC) protocols for Wireless Sensor Networks based on Impulse
Radio Ultra Wideband (IR-UWB) Physical layer (PHY). Two main classes of Medium
Access Control protocol have been considered: Slotted and UnSlotted with
reliability. The reliability is based on Automatic Repeat ReQuest (ARQ). The
performance evaluation is performed using a complete Wireless Sensor Networks
(WSN) simulator built on the Global Mobile Information System Simulator
(GloMoSim). The optimal operating parameters are first discussed for IR-UWB in
terms of slot size, retransmission delay and the number of retransmission, then
a comparison between IR-UWB and other transmission techniques in terms of
reliability latency and power efficiency.

ABSTRACT_BEGIN
  Starting from the Shannon channel capacity, we propose an IR-UWB channel
capacity based on the delay spread for multipath time variant channels. This
IR-UWB channel capacity is obtained from the no ISI (Inter Symbol Interference)
assumption and for binary modulations. The impact of the kind of implementation
is considered on the IR-UWB channel capacity. This study is lead for mixed and
mostly digital implementation. The key parameters and theirs impacts on the
channel capacity are exposed in each case: the data converters for mostly
digital implementations and the pulse generator capabilities for mixed
implementations. Finally, these two implementations are compared from a data
rate point of view. Their behaviors regarding an increase of the operating
frequency are also studied.

ABSTRACT_BEGIN
  This paper presents a performance evaluation of Wireless Sensor Networks
(WSN) based on Impulse Radio Ultra Wideband (IR-UWB) over a new simulation
platform developed for this purpose. The simulation platform is built on an
existing network simulator: Global Mobile Information System Simulator
(GloMoSim). It mainly focuses on the accurately modeling of IR-UWB PHYsical
(PHY) and Medium Access Control (MAC) layer. Pulse collision is modeled
according to the used time hopping sequence (THS) and the pulse propagation
delay in order to increase the simulation fidelity. It also includes a
detection and identification application based on a new sensing channel and new
sensor device models. The proposed architecture is generic so it can be reused
for any simulation platform. The performance evaluation is based on one of the
typical WSN applications: local area protection, where sensor nodes are densely
scattered in an access regulated area in order to detect, identify and report
non authorized accesses to a base station for analysis. Two networks topologies
using different protocol stacks are investigated. Their performance evaluation
is presented in terms of reliability and latency.

ABSTRACT_BEGIN
  A new model is proposed giving the channel capability of a MB-IR-UWB system
versus the number of subband and the duty cycle. The architecture simulated
shows data rate ranging from 1.434 Gbits/s to 0.9 Gbits/s for 16 to 10 subbands
and duty cycle ranging from 20% to 12%.

ABSTRACT_BEGIN
  The IEEE 802.11 standard offers a cheap and promising solution for small
scale wireless networks. Due to the self configuring nature, WLANs do not
require large scale infrastructure deployment, and are scalable and easily
maintainable which incited its popularity in both literature and industry. In
real environment, these networks operate mostly under unsaturated condition. We
investigate performance of such a network with m-retry limit BEB based DCF. We
consider imperfect channel with provision for power capture. Our method employs
a Markov model and represents the most common performance measures in terms of
network parameters making the model and mathematical analysis useful in network
design and planning. We also explore the effects of packet error, network size,
initial contention window, and retry limit on overall performance of WLANs.

ABSTRACT_BEGIN
  The impact of the type of implementation is considered on the IR-UWB channel
capacity. This study is lead for analog and mostly digital implementation. Key
parameters and theirs impacts on the channel capacity are exposed in each case:
data converters for mostly digital implementations and pulse generators
capabilities for analog implementations. These two implementations are compared
from a data rate point of view. Their behaviors regarding an increase of the
operating frequency are also studied

ABSTRACT_BEGIN
  The increasing role of home automation in routine life and the rising demand
for sensor networks enhanced wireless personal area networks (WPANs)
development, pervasiveness of wireless & wired network, and research. Soon
arose the need of implementing the Internet Protocol in these devices in order
to WPAN standards, raising the way for questions on how to provide seamless
communication between wired and wireless technologies. After a quick overview
of the Low-rate WPAN standard (IEEE 802.15.4) and the Zigbee stack, this paper
focuses on understanding the implications when interconnecting low powered IEEE
802.15.4 devices and a wired IPv6 domain. Subsequently the focus will be on
existing approaches to connect LoWPAN devices to the internet and on how these
approaches try to solve these challenges, concluding with a critical analysis
of interoperability problems.

ABSTRACT_BEGIN
  This paper presents the realistic approach towards the quantitative analysis
and simulation of Energy Efficient Hierarchical Cluster (EEHC)-based routing
for wireless sensor networks. Here the efforts have been done to combine
analytical hardware model with the modified EEHC-based routing model. The
dependence of various performance metrics like: optimum number of clusters,
Energy Consumption, and Energy consumed per round etc. based on analytical
hardware sensor model and EEHC model has been presented.

ABSTRACT_BEGIN
  A Virtual Private Network (VPN) provides private network connections over a
publicly accessible shared network. The effective allocation of bandwidth for
VPNs assumes significance in the present scenario due to varied traffic. Each
VPN endpoint specifies bounds on the total amount of traffic that it is likely
to send or receive at any time. The network provider tailors the VPN so that
there is sufficient bandwidth for any traffic matrix that is consistent with
these bounds. The approach incorporates the use of Ad-hoc On demand Distance
Vector (AODV) protocol, with a view to accomplish an enhancement in the
performance of the mobile networks. The NS2 based simulation results are
evaluated in terms of its metrics for different bandwidth allocations, besides
analyzing its performance in the event of exigencies such as link failures. The
results highlight the suitability of the proposed strategy in the context of
real time applications.

ABSTRACT_BEGIN
  Mobile Ad hoc Networks are highly dynamic networks. Quality of Service (QoS)
routing in such networks is usually limited by the network breakage due to
either node mobility or energy depletion of the mobile nodes. Also, to fulfill
certain quality parameters, presence of multiple node-disjoint paths becomes
essential. Such paths aid in the optimal traffic distribution and reliability
in case of path breakages. Thus, to cater various challenges in QoS routing in
Mobile Add hoc Networks, a Node Disjoint Multipath Routing Considering Link and
Node Stability (NDMLNR) protocol has been proposed by the authors. The metric
used to select the paths takes into account the stability of the nodes and the
corresponding links. This paper studies various challenges in the QoS routing
and presents the characteristic evaluation of NDMLNR w.r.t various existing
protocols in this area.

ABSTRACT_BEGIN
  Paper has been withdrawn due to non-compliance with IJCSI terms and
conditions.

ABSTRACT_BEGIN
  Today, SIP is a protocol par Excellence in the field of communication over
Internet. But, the fact that it belongs to the application layer constitutes a
weakness vis-a-vis the NAT traversal. This weakness is due to the way in which
the server replies to the requests of clients on the one hand. On the other, it
is caused by the dynamic allocation of UDP ports for emission and reception of
packets RTP/RTCP. The TURN Protocol may face this weakness. However, its use
requires a certain number of exchanges between the clients and a TURN server
before establishing the multimedia sessions and this increase the latent time.
In this article, we propose to adapt TURN protocol for applications based on
SIP protocol such as telephony over Internet, conference video, etc. This
adaptation optimises the establishment of multimedia sessions by integrating a
manager of TCP connections and multimedia flow controller into SIP Proxy
server.

ABSTRACT_BEGIN
  RFID is not a new technology and has passed through many decades of use in
military, airline, library, security, healthcare, sports, animal farms and
other areas. Industries use RFID for various applications such as
personal/vehicle access control, departmental store security, equipment
tracking, baggage, fast food establishments, logistics, etc. The enhancement in
RFID technology has brought advantages that are related to resource
optimization, increased efficiency within business processes, and enhanced
customer care, overall improvements in business operations and healthcare. Our
research is part of a big project; its aim is to produce a model for mobile
technology implementation of hospital patients' movement process. However, the
focus of this paper is to explore the main RFID components, i.e. the tag,
antenna and reader. The results of the investigations conducted on the three
RFID components will be used to develop our research model.

ABSTRACT_BEGIN
  VANETs (Vehicular Ad hoc Networks) are highly mobile wireless ad hoc networks
and will play an important role in public safety communications and commercial
applications. Routing of data in VANETs is a challenging task due to rapidly
changing topology and high speed mobility of vehicles. Conventional routing
protocols in MANETs (Mobile Ad hoc Networks) are unable to fully address the
unique characteristics in vehicular networks. In this paper, we propose EBGR
(Edge Node Based Greedy Routing), a reliable greedy position based routing
approach to forward packets to the node present in the edge of the transmission
range of source/forwarding node as most suitable next hop, with consideration
of nodes moving in the direction of the destination. We propose Revival
Mobility model (RMM) to evaluate the performance of our routing technique. This
paper presents a detailed description of our approach and simulation results
show that packet delivery ratio is improved considerably compared to other
routing techniques of VANET.

ABSTRACT_BEGIN
  In this paper we build upon the recent observation that the 802.11 rate
region is log-convex and, for the first time, characterise max-min fair rate
allocations for a large class of 802.11 wireless mesh networks. By exploiting
features of the 802.11e/n MAC, in particular TXOP packet bursting, we are able
to use this characterisation to establish a straightforward, practically
implementable approach for achieving max-min throughput fairness. We
demonstrate that this approach can be readily extended to encompass time-based
fairness in multi-rate 802.11 mesh networks.

ABSTRACT_BEGIN
  The traditional TCP congestion control mechanism encounters a number of new
problems and suffers a poor performance when the IEEE 802.11 MAC protocol is
used in multihop ad hoc networks. Many of the problems result from medium
contention at the MAC layer. In this paper, I first illustrate that severe
medium contention and congestion are intimately coupled, and TCP s congestion
control algorithm becomes too coarse in its granularity, causing throughput
instability and excessively long delay. Further, we illustrate TCP s severe
unfairness problem due to the medium contention and the tradeoff between
aggregate throughput and fairness. Then, based on the novel use of channel
busyness ratio, a more accurate metric to characterize the network utilization
and congestion status, I propose a new wireless congestion control protocol
(WCCP) to efficiently and fairly support the transport service in multihop ad
hoc networks. In this protocol, each forwarding node along a traffic flow
exercises the internode and intranode fair resource allocation and determines
the MAC layer feedback accordingly. The endtoend feedback, which is ultimately
determined by the bottleneck node along the flow, is carried back to the source
to control its sending rate. Extensive simulations show that WCCP significantly
outperforms traditional TCP in terms of channel utilization, delay, and
fairness, and eliminates the starvation problem.

ABSTRACT_BEGIN
  Distributed contention based Medium Access Control (MAC) protocols are the
fundamental components for IEEE 802.11 based Wireless Local Area Networks
(WLANs). Contention windows (CW) change dynamically to adapt to the current
contention level, Upon each packet collision, a station doubles its CW to
reduce further collision of packets. IEEE 802.11 Distributed Coordination
Function (DCF) suffers from a common problem in erroneous channel. They cannot
distinguish noise lost packets from collision lost packets. In both situations
a station does not receive its ACK and doubles the CW to reduce further packet
collisions. This increases backoff overhead unnecessarily in addition to the
noise lost packets, reduces the throughput significantly. Furthermore, the
aggregate throughput of a practical WLAN strongly depends on the channel
conditions. In real radio environment, the received signal power at the access
point from a station is subjected to deterministic path loss, shadowing and
fast multipath fading. In this paper, we propose a new saturation throughput
analysis for IEEE 802.11 DCF considering erroneous channel and capture effects.
To alleviate the low performance of IEEE 802.11 DCF, we introduce a mechanism
that greatly outperforms under noisy environment with low network traffic and
compare their performances to the existing standards. We extend the
multidimensional Markov chain model initially proposed by Bianchi(3) to
characterize the behavior of DCF in order to account both real channel
conditions and capture effects, especially in a high interference radio
environment.

ABSTRACT_BEGIN
  WLAN localization has become an active research field recently. Due to the
wide WLAN deployment, WLAN localization provides ubiquitous coverage and adds
to the value of the wireless network by providing the location of its users
without using any additional hardware. However, WLAN localization systems
usually require constructing a radio map, which is a major barrier of WLAN
localization systems' deployment. The radio map stores information about the
signal strength from different signal strength streams at selected locations in
the site of interest. Typical construction of a radio map involves measurements
and calibrations making it a tedious and time-consuming operation. In this
paper, we present the AROMA system that automatically constructs accurate
active and passive radio maps for both device-based and device-free WLAN
localization systems. AROMA has three main goals: high accuracy, low
computational requirements, and minimum user overhead. To achieve high
accuracy, AROMA uses 3D ray tracing enhanced with the uniform theory of
diffraction (UTD) to model the electric field behavior and the human shadowing
effect. AROMA also automates a number of routine tasks, such as importing
building models and automatic sampling of the area of interest, to reduce the
user's overhead. Finally, AROMA uses a number of optimization techniques to
reduce the computational requirements. We present our system architecture and
describe the details of its different components that allow AROMA to achieve
its goals. We evaluate AROMA in two different testbeds. Our experiments show
that the predicted signal strength differs from the measurements by a maximum
average absolute error of 3.18 dBm achieving a maximum localization error of
2.44m for both the device-based and device-free cases.

ABSTRACT_BEGIN
  Wireless mobile grids are one of the emerging grid types, which help to pool
the resources of several willing and cooperative mobile devices to resolve a
computationally intensive task. The mobile grids exhibit stronger challenges
like mobility management of devices, providing transparent access to grid
resources, task management and handling of limited resources so that resources
are shared efficiently. Task execution on these devices should not be affected
by their mobility. The proposed work presents performance evaluation of
wireless mobile grid using normal walk mobility model. The normal walk model
represents daily motion of users and the direction of motion is mostly
symmetric in a real life environment, thus it is effective in location updating
of a mobile station and in turn helps task distribution among these available
mobile stations. Some of the performance parameters such as Task Execution
Time, task failure rate, communication overhead on Brokering Server and
Monitoring Cost are discussed.

ABSTRACT_BEGIN
  We propose an adaptive transmission technique for free space optical (FSO)
systems, operating in atmospheric turbulence and employing subcarrier phase
shift keying (S-PSK) intensity modulation. Exploiting the constant envelope
characteristics of S-PSK, the proposed technique offers efficient utilization
of the FSO channel capacity by adapting the modulation order of S-PSK,
according to the instantaneous state of turbulence induced fading and a
pre-defined bit error rate (BER) requirement. Novel expressions for the
spectral efficiency and average BER of the proposed adaptive FSO system are
presented and performance investigations under various turbulence conditions
and target BER requirements are carried out. Numerical results indicate that
significant spectral efficiency gains are offered without increasing the
transmitted average optical power or sacrificing BER requirements, in
moderate-to-strong turbulence conditions. Furthermore, the proposed variable
rate transmission technique is applied to multiple input multiple output (MIMO)
FSO systems, providing additional improvement in the achieved spectral
efficiency as the number of the transmit and/or receive apertures increases.

ABSTRACT_BEGIN
  WiMAX technology is based on the IEEE 802.16 specification of which IEEE
802.16-2004 and 802.16e amendment are Physical (PHY) layer specifications. IEEE
802.16-2004 currently supports several multiple-antenna options including
Space-Time Codes (STC), Multiple-Input Multiple-Output (MIMO) antenna systems
and Adaptive Antenna Systems (AAS). The most recent WiMAX standard (802.16e)
supports broadband applications to mobile terminals and laptops. Using Adaptive
Modulation and Coding (AMC) we analyze the performance of OFDM physical layer
in WiMAX based on the simulation results of Bit Error Rate (BER), and data
throughput. The performance analysis of OFDM PHY is done. In this paper, an
extension to the basic SISO mode, a number of 2 by 2 MIMO extensions are
analysed under different combinations of digital modulation (QPSK, 16QAM and
64QAM) and Convolutional Code (CC) with half, two-third and three quarter rated
codes. The intent of this paper is to provide an idea of the benefits of
multiple antenna systems over single antenna systems in WiMAX type deployments.

ABSTRACT_BEGIN
  Over the recent years a considerable amount of effort has been devoted
towards the performance evaluation and prediction of Mobile Networks.
Performance modeling and evaluation of mobile networks are very important in
view of their ever expending usage and the multiplicity of their component
parts together with the complexity of their functioning. The present paper
addresses current issues in traffic management and congestion control by
(signal to interference plus noise ratio) SINR prediction congestion control,
routing and optimization of cellular mobile networks.

ABSTRACT_BEGIN
  The capability to provide network service even under a significant network
system element disruption is the backbone for the survival of route optimize of
mobile network Technology in today s world. Keeping this view in mind, the
present paper highlights a new method based on memetic algorithm.

ABSTRACT_BEGIN
  Channel properties influence the development of wireless communication
systems. Unlike wired channels that are stationary and predictable, radio
channels are extremely random and dont offer easy analysis. A Radio Propagation
Model (RPM), also known as the Radio Wave Propagation Model (RWPM), is an
empirical mathematical formulation for the characterization of radio wave
propagation as a function of frequency. In mobile radio systems, path loss
models are necessary for proper planning, interference estimations, frequency
assignments and cell parameters which are the basic for network planning
process as well as Location Based Services (LBS) techniques. Propagation models
that predict the mean signal strength for an arbitrary transmitter receiver (T
R) separation distance which is useful in estimating the radio coverage area of
a transmitter are called large scale propagation models, since they
characterize signal strength over large TR separation distances. In this paper,
the large scale propagation performance of Okumura, Hata, and Lee models has
been compared varying Mobile Station (MS) antenna height, Transmitter Receiver
(TR) distance and Base Station (BS) antenna height, considering the system to
operate at 900 MHz. Through the MATLAB simulation it is turned out that the
Okumura model shows the better performance than that of the other large scale
propagation models.

ABSTRACT_BEGIN
  With the proliferation of mobile computing devices, the demand for continuous
network connectivity regardless of physical location has spurred interest in
the use of mobile ad hoc networks. Since Transmission Control Protocol (TCP) is
the standard network protocol for communication in the internet, any wireless
network with Internet service need to be compatible with TCP. TCP is tuned to
perform well in traditional wired networks, where packet losses occur mostly
because of congestion. However, TCP connections in Ad-hoc mobile networks are
plagued by problems such as high bit error rates, frequent route changes,
multipath routing and temporary network partitions. The throughput of TCP over
such connection is not satisfactory, because TCP misinterprets the packet loss
or delay as congestion and invokes congestion control and avoidance algorithm.
In this research, the performance of TCP in Adhoc mobile network with high Bit
Error rate (BER) and mobility is studied and investigated. Simulation model is
implemented and experiments are performed using the Network Simulatior 2 (NS2).

ABSTRACT_BEGIN
  In this paper, we have to concentrate on implementation of Weighted
Clustering Algorithm with the help of Genetic Algorithm (GA).Here we have
developed new algorithm for the implementation of GA-based approach with the
help of Weighted Clustering Algorithm (WCA) (4). ClusterHead chosen is a
important thing for clustering in adhoc networks. So, we have shown the
optimization technique for the minimization of ClusterHeads(CH) based on some
parameter such as degree difference, Battery power (Pv), degree of mobility,
and sum of the distances of a node in adhoc networks. ClusterHeads selection of
adhoc networks is an important thing for clustering. Here, we have discussed
the performance comparison between deterministic approach and GA based
approach. In this performance comparison, we have seen that GA does not always
give the good result compare to deterministic WCA algorithm. Here we have seen
connectivity (connectivity can be measured by the probability that a node is
reachable to any other node.) is better than the deterministic WCA algorithm
(4).

ABSTRACT_BEGIN
  Passive optical networks are increasingly used for access to the Internet and
it is important to understand the performance of future long-reach,
multi-channel variants. In this paper we discuss requirements on the dynamic
bandwidth allocation (DBA) algorithm used to manage the upstream resource in a
WDM EPON and propose a simple novel DBA algorithm that is considerably more
efficient than classical approaches. We demonstrate that the algorithm emulates
a multi-server polling system and derive capacity formulas that are valid for
general traffic processes. We evaluate delay performance by simulation
demonstrating the superiority of the proposed scheduler. The proposed scheduler
offers considerable flexibility and is particularly efficient in long-reach
access networks where propagation times are high.

ABSTRACT_BEGIN
  As passive optical networks (PON) are increasingly deployed to provide high
speed Internet access, it is important to understand their fundamental traffic
capacity limits. The paper discusses performance models applicable to
wavelength division multiplexing (WDM) EPONs and GPONs under the assumption
that users access the fibre via optical network units equipped with tunable
transmitters. The considered stochastic models are based on multiserver polling
systems for which explicit analytical results are not known. A large system
asymptotic, mean-field approximation, is used to derive closed form solutions
of these complex systems. Convergence of the mean field dynamics is proved in
the case of a simple network configuration. Simulation results show that, for a
realistic sized PON, the mean field approximation is accurate.

ABSTRACT_BEGIN
  Initially TCP was designed with the notion in mind that wired networks are
generally reliable and any segment loss in a transmission is due to congestion
in the network rather than an unreliable medium (The assumptions is that the
packet loss caused by damage is much less than 1 percent) . This notion doesnt
hold in wireless parts of the network. Wireless links are highly unreliable and
they lose segments all the time due to a number of factors. Very few papers are
available which uses TCP for MANET. In this paper, an attempt have been made to
justify the use of TCP variants (Tahoe and Reno) for loss of packet due to
random noise introduces in the MANET. For the present analysis the simulation
has been carried out for TCP variants (Tahoe and Reno) by introduces 0, 10, 20
and 30 percent noise. The comparison of TCP variants is made by running
simulation for 0, 10, 20 and 30 percent of data packet loss due to noise in the
transmission link and the effect of throughput and congestion window has been
examined. During the simulation we have observed that throughput has been
decreased when a drop of multiple segments happens, further we have observed in
the case of TCP variant (Reno) throughput is better at 1 percent (Figure 5)
which implies a network with short burst of error and low BER, causing only one
segment to be lost. When multiple segments are lost due to error prone nature
of link, Tahoe perform better than Reno (Figure 13), that gives a significant
saving of time (64.28 percent) in comparison with Reno (Table 4). Several
simulations have been run with ns 2 simulator in order to acquire a better
understanding of these TCP variants and the way they perform their function. We
conclude with a discussion of whether these TCP versions can be used in Mobile
Ad hoc Network.

ABSTRACT_BEGIN
  Networks on Chip is a recent solution paradigm adopted to increase the
performance of Multicore designs. The key idea is to interconnect various
computation modules (IP cores) in a network fashion and transport packets
simultaneously across them, thereby gaining performance. In addition to
improving performance by having multiple packets in flight, NoCs also present a
host of other advantages including scalability, power efficiency, and component
reuse through modular design. This work focuses on design and development of
high performance communication architectures for FPGAs using NoCs Once
completely developed, the above methodology could be used to augment the
current FPGA design flow for implementing multicore SoC applications. We design
and implement an NoC framework for FPGAs, MultiClock OnChip Network for
Reconfigurable Systems (MoCReS). We propose a novel microarchitecture for a
hybrid two layer router that supports both packetswitched communications,
across its local and directional ports, as well as, time multiplexed
circuitswitched communications among the multiple IP cores directly connected
to it. Results from place and route VHDL models of the advanced router
architecture show an average improvement of 20.4 percent in NoC bandwidth
(maximum of 24 percent compared to a traditional NoC). We parameterize the
hybrid router model over the number of ports, channel width and bRAM depth and
develop a library of network components (MoClib Library). For your paper to be
published in the conference proceedings, you must use this document as both an
instruction set and as a template into which you can type your own text. If
your paper does not conform to the required format, you will be asked to fix
it.

ABSTRACT_BEGIN
  The design and development of a complex system requires an adequate
methodology and efficient instrumental support in order to early detect and
correct anomalies in the functional and non-functional properties of the tested
protocols. Among the various tools used to provide experimental support for
such developments, network emulation relies on real-time production of
impairments on real traffic according to a communication model, either
realistically or not.
  This paper aims at simply presenting to newcomers in network emulation
(students, engineers, ...) basic principles and practices illustrated with a
few commonly used tools. The motivation behind is to fill a gap in terms of
introductory and pragmatic papers in this domain.
  The study particularly considers centralized approaches, allowing cheap and
easy implementation in the context of research labs or industrial developments.
In addition, an architectural model for emulation systems is proposed, defining
three complementary levels, namely hardware, impairment and model levels. With
the help of this architectural framework, various existing tools are situated
and described. Various approaches for modeling the emulation actions are
studied, such as impairment-based scenarios and virtual architectures,
real-time discrete simulation and trace-based systems. Those modeling
approaches are described and compared in terms of services and we study their
ability to respond to various designer needs to assess when emulation is
needed.

ABSTRACT_BEGIN
  Dynamic Spectrum Access systems exploit temporarily available spectrum
(`white spaces') and can spread transmissions over a number of non-contiguous
sub-channels. Such methods are highly beneficial in terms of spectrum
utilization. However, excessive fragmentation degrades performance and hence
off-sets the benefits. Thus, there is a need to study these processes so as to
determine how to ensure acceptable levels of fragmentation. Hence, we present
experimental and analytical results derived from a mathematical model. We model
a system operating at capacity serving requests for bandwidth by assigning a
collection of gaps (sub-channels) with no limitations on the fragment size. Our
main theoretical result shows that even if fragments can be arbitrarily small,
the system does not degrade with time. Namely, the average total number of
fragments remains bounded. Within the very difficult class of dynamic
fragmentation models (including models of storage fragmentation), this result
appears to be the first of its kind. Extensive experimental results describe
behavior, at times unexpected, of fragmentation under different algorithms. Our
model also applies to dynamic linked-list storage allocation, and provides a
novel analysis in that domain. We prove that, interestingly, the 50% rule of
the classical (non-fragmented) allocation model carries over to our model.
Overall, the paper provides insights into the potential behavior of practical
fragmentation algorithms.

ABSTRACT_BEGIN
  Information theoretic Broadcast Channels (BC) and Multiple Access Channels
(MAC) enable a single node to transmit data simultaneously to multiple nodes,
and multiple nodes to transmit data simultaneously to a single node
respectively. In this paper, we address the problem of link scheduling in
multihop wireless networks containing nodes with BC and MAC capabilities. We
first propose an interference model that extends protocol interference models,
originally designed for point to point channels, to include the possibility of
BC and MAC. Due to the high complexity of optimal link schedulers, we introduce
the Multiuser Greedy Maximum Weight algorithm for link scheduling in multihop
wireless networks containing BCs and MACs. Given a network graph, we develop
new local pooling conditions and show that the performance of our algorithm can
be fully characterized using the associated parameter, the multiuser local
pooling factor. We provide examples of some network graphs, on which we apply
local pooling conditions and derive the multiuser local pooling factor. We
prove optimality of our algorithm in tree networks and show that the
exploitation of BCs and MACs improve the throughput performance considerably in
multihop wireless networks.

ABSTRACT_BEGIN
  Multiple Input Multiple Output (MIMO) systems have recently emerged as a key
technology in wireless communication systems for increasing both data rates and
system performance. There are many schemes that can be applied to MIMO systems
such as space time block codes, space time trellis codes, and the Vertical Bell
Labs Space-Time Architecture (V-BLAST). This paper proposes a novel signal
detector scheme called MIMO detectors to enhance the performance in MIMO
channels. We study the general MIMO system, the general V-BLAST architecture
with Maximum Likelihood (ML), Zero- Forcing (ZF), Minimum Mean- Square Error
(MMSE), and Ordered Successive Interference Cancellation (SIC) detectors and
simulate this structure in Rayleigh fading channel. Also compares the
performances of MIMO system with different modulation techniques in Fading and
AWGN channels. Base on frame error rates and bit error rates, we compare the
performance and the computational complexity of these schemes with other
existence model.Simulations shown that V-BLAST implements a detection
technique, i.e. SIC receiver, based on ZF or MMSE combined with symbol
cancellation and optimal ordering to improve the performance with lower
complexity, although ML receiver appears to have the best SER performance-BLAST
achieves symbol error rates close to the ML scheme while retaining the
lowcomplexity nature of the V-BLAST.

ABSTRACT_BEGIN
  Denial of service attacks (DoS) can cause significant financial damages.
Flooding and Malicious packets are two kinds of DoS attacks. This paper
presents a new security approach which stops malicious packets and prevents
flooding in the critical systems. New concepts of packet stamp a
dynamic-multi-communication-point mechanism has been identified for this
proposed approach to make the prevention of flooding attacks easier and the
performing of malicious packet attacks harder. In addition, dynamic key
encryption technique has been adapted as a part of the proposed approach to
enhance its functionality.

ABSTRACT_BEGIN
  One of the biggest drawbacks of the wireless environment is the limited
bandwidth. However, the users sharing this limited bandwidth have been
increasing considerably.Space Division Multiple Access (SDMA) is a new
technology by which the capacity of existing mobile communication systems can
economically be increased. This paper has been presented how the capacity can
be enhanced by using SDMA with smart antennas in mobile communications system.
Based on Adaptive Antenna Array (AAA) technology the spatial dimension of the
existing system is exploited by means of forming independent radio beams in
each of the original channels. This paper analyses the comparison of average
Bit Error Rate (BER) of SDMA and CDMA technique and the different ways in which
SDMA can be introduced to increase the capacity of a cellular system. The
probability of error is found for a standard omni directional base station
antenna, and another set of curves is found for flat top beam having a
directivity of 5.1dB. It is assumed that k separate flat top beams can be
formed by base station and pointed each of the k users within the cell of
interest. Noticing that for an average probability of error greater than 0.1 in
a propagation path loss environment of n=4, the flat top beam will support 200
users, whereas the omni-directional antenna will support only 50 users. This
increase the number of user is roughly equal to the directivity offered by the
flat top beam system, and illustrates the promise SDMA offers for improving
capacity in wireless system. Here multipath fading is not considered.

ABSTRACT_BEGIN
  Commercial cellular networks, like the systems based on DS-CDMA, face many
types of interferences such as multi-user interference inside each sector in a
cell to interoperate interference. Independent Component Analysis (ICA) has
been used as an advanced preprocessing tool for blind suppression of
interfering signals in DS-CDMA communication systems. The role of ICA is to
provide an interference-mitigated signal to the conventional detection. This
paper evaluates the performance of some major ICA algorithms like Cardoso's
joint approximate diagonalization of eigen matrices (JADE), Hyvarinen's fixed
point algorithm and Comon's algorithm to solve the symbol estimation problem of
the multi users in a DSCDMA communication system. The main focus is on blind
separation of convolved CDMA mixture and the improvement of the downlink symbol
estimation. The results of numerical experiment are compared with those
obtained by the Single User Detection (SUD) receiver, ICA detector and combined
SUD-ICA detector.

ABSTRACT_BEGIN
  In this paper, Estimation of Distribution Algorithm (EDA) is used for Zone
Routing Protocol (ZRP) in Mobile Ad-hoc Network instead of Genetic Algorithm
(GA). It is an evolutionary approach, it is used when the network size grows
and the search space increases. When the destination is outside the zone, EDA
is applied to find the route with minimum cost and time. Finally, the
implementation of proposed method is compared with Genetic ZRP, i.e., GZRP and
the result demonstrates better performance for the proposed method. Since the
method provides a set of paths to the destination, it results in load balance
to the network. As both EDA and GA use random search method to reach the
optimal point, the searching cost reduced significantly, especially when the
number of data is large.

ABSTRACT_BEGIN
  MIMO (Multi Input Multi Output) wireless communication system is an
innovative solution to improve the bandwidth efficiency by exploiting
multipath-richness of the propagation environment. The degree of
multipath-richness of the channel will determine the capacity gain attainable
by MIMO deployment. Therefore, it is very important to have accurate knowledge
of the propagation environment/radio channel before MIMO implement. The radio
channel behavior can be estimated by channel measurement or channel sounding.
CDM (Code Division multiplexing) is one of the channel sounding techniques that
allow accurate measurement at the cost of hardware complexity. CDM based
channel sounder, requires code with excellent autocorrelation and
cross-correlation properties which generally difficult to achieve
simultaneously. Theoretical analysis and computer simulation result
demonstrated that, having excellent correlation propertied Loosely Synchronous
(LS) code sequence perform efficiently. Finally, the an efficient LS code
generator as a data source for transmitter implemented in Xilinx FPGA that can
be integrated into CDM based 2x2 MIMO complete channel sounder.

ABSTRACT_BEGIN
  We propose a scheme to reduce the overhead associated with channel state
information (CSI) feedback required for opportunistic scheduling in
multicarrier access networks. We study the case where CSI is partially
overheard by mobiles and one can suppress transmitting CSI reports for time
varying channel of inferior quality. As a means to assess channel quality and
exploit multiuser diversity we adopt maximum quantile (MQ) scheduling. We show
that the problem of minimizing the average feedback overhead can be formulated
as a Bayesian network problem. A greedy heuristic using probabilistic inference
is proposed to deal with the NP-hardness of the problem. Leveraging properties
of MQ scheduling we first show that networks having tree-like overhearing
graphs admit simple inference. We then present a class of more general network
structures for which exact inference is computationally tractable. Simulation
results are provided to demonstrate the improvements offered by the proposed
heuristic.

ABSTRACT_BEGIN
  In this paper, we analyze the performance of random load resampling and
migration strategies in parallel server systems. Clients initially attach to an
arbitrary server, but may switch server independently at random instants of
time in an attempt to improve their service rate. This approach to load
balancing contrasts with traditional approaches where clients make smart server
selections upon arrival (e.g., Join-the-Shortest-Queue policy and variants
thereof). Load resampling is particularly relevant in scenarios where clients
cannot predict the load of a server before being actually attached to it. An
important example is in wireless spectrum sharing where clients try to share a
set of frequency bands in a distributed manner.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) have gained worldwide attention in recent
years, particularly with the proliferation in Micro-Electro-Mechanical Systems
(MEMS) technology which has facilitated the development of smart sensors. The
paper discusses about classification of WSN and challenges of the Next
Generation WSN. One of the major challenges of Next Generation WSN is reduction
of power consumption. The two approaches are discussed: Ultra-Low-Power
Networks and Energy Harvesting. The paper also discusses about some major
applications as designing low cost secured Intelligent Buildings, In-Home
Health care and Agriculture.

ABSTRACT_BEGIN
  CDMA is a multiple access method in which the user's uses spread spectrum
techniques and occupy the entire spectrum whenever they transmit. In wireless
communication signal-to-noise ratio (SNR) is the very important parameter that
influences the system performance. Any mode of mobile transmission is not free
from channel impairment such as noise, interference and fading. This channel
impairment caused signal distortion and degradation in SNR.Also there are
differences between uplink (forward channel) and downlink (reverse
channel).Along with these differences, both the links use different codes for
chanellizing the individual users. This paper simulates the expressions for the
pdfs of the SNR for both uplink and downlink transmission assuming that the
system is operating at an average signal-to-noise ratio is 6dB per information
bit.

ABSTRACT_BEGIN
  The number of users using wireless Local Area Network is increasing
exponentially and their behavior is changing day after day. Nowadays, users of
wireless LAN are using huge amount of bandwidth because of the explosive growth
of some services and applications such as video sharing. This situation imposes
massive pressure on the wireless LAN performance especially in term of fairness
among wireless stations. The limited resources are not distributed fairly in
saturated conditions. The most important resource is the access point buffer
space. This importance is a result of access point being the bottleneck between
two different types of networks. These two types are wired network with
relatively huge bandwidth and wireless network with much smaller bandwidth.
Also the unfairness problem is keep getting worse because of the greedy nature
Transmission Control Protocol (TCP). In this paper, we conduct a comprehensive
study on wireless LAN dynamics and proposed a new mathematical model that
describes the performance and effects of its behavior. We validate the proposed
model by using the simulation technique. The proposed model was able to produce
very good approximation in most of the cases. It also gave us a great insight
into the effective variables in the wireless LAN behavior and what are the
dimensions of the unfairness problem.

ABSTRACT_BEGIN
  There is now an increased understanding of the need for realistic link layer
models in the wireless sensor networks. In this paper, we have used
mathematical techniques from communication theory to model and analyze low
power wireless links. Our work provides theoretical models for the link layer
showing how Packet Reception Rate vary with Signal to Noise Ratio and distance
for different modulation schemes and a comparison between MICA2 and TinyNode in
terms of PRR.

ABSTRACT_BEGIN
  We are interested in unicast traffic over wireless networks that employ
constructive inter-session network coding, including single-hop and multi-hop
schemes. In this setting, TCP flows do not fully exploit the network coding
opportunities due to their bursty behavior and due to the fact that TCP is
agnostic to the underlying network coding. In order to improve the performance
of TCP flows over coded wireless networks, we take the following steps. First,
we formulate the problem as network utility maximization and we present a
distributed solution. Second, mimicking the structure of the optimal solution,
we propose a "network-coding aware" queue management scheme (NCAQM) at
intermediate nodes; we make no changes to TCP or to the MAC protocol (802.11).
We demonstrate, via simulation, that NCAQM significantly improves TCP
performance compared to TCP over baseline schemes.

ABSTRACT_BEGIN
  One of the main issues in the design of sensor networks is energy efficient
communication of time-critical data. Energy wastage can be caused by failed
packet transmission attempts at each node due to channel dynamics and
interference. Therefore transmission control techniques that are unaware of the
channel dynamics can lead to suboptimal channel use patterns. In this paper we
propose a transmission controller that utilizes different "grades" of channel
side information to schedule packet transmissions in an optimal way, while
meeting a deadline constraint for all packets waiting in the transmission
queue. The wireless channel is modeled as a finite-state Markov channel. We are
specifically interested in the case where the transmitter has low-grade channel
side information that can be obtained based solely on the ACK/NAK sequence for
the previous transmissions. Our scheduler is readily implementable and it is
based on the dynamic programming solution to the finite-horizon transmission
control problem. We also calculate the information theoretic capacity of the
finite state Markov channel with feedback containing different grades of
channel side information including that, obtained through the ACK/NAK sequence.
We illustrate that our scheduler achieves a given throughput at a power level
that is fairly close to the fundamental limit achievable over the channel.

ABSTRACT_BEGIN
  In this paper correspondence between experimental data for packet delay and
two theoretical types of distribution is investigated. Statistical tests have
shown that only exponential distribution can be used for the description of
packet delays in global network. Precision experimental data to within
microseconds are gathered by means of the RIPE Test Box. Statistical
verification of hypothesis has shown that distribution parameters remain
constants during 500 second intervals at least. In paper cumulative
distribution function and generating function for packet delay in network are
in an explicit form written down, the algorithm of search of parameters of
distribution is resulted.

ABSTRACT_BEGIN
  The packet is the fundamental unit of transportation in modern communication
networks such as the Internet. Physical layer scheduling decisions are made at
the level of packets, and packet-level models with exogenous arrival processes
have long been employed to study network performance, as well as design
scheduling policies that more efficiently utilize network resources. On the
other hand, a user of the network is more concerned with end-to-end bandwidth,
which is allocated through congestion control policies such as TCP.
Utility-based flow-level models have played an important role in understanding
congestion control protocols. In summary, these two classes of models have
provided separate insights for flow-level and packet-level dynamics of a
network.

ABSTRACT_BEGIN
  In a network, arrival process is converted into departure process through
network elements. The departure process suffer propagation delay in the link,
processing delay at the network elements like router and data loss due to
buffer overflow or congestion. For providing guaranteed service resources need
to be reserved before conversation takes place. To reserve such resources
estimation of them are indispensable. The idea of service curve gives
beforehand deterministic value of these parameters. In this paper, we aim to
minimum and maximum buffer space required in the router, minimum link capacity
required to guarantee a pre-specified end-to-end delay for an ongoing session
in a wired-cum-wireless scenario by analyzing minimum and maximum service
curve. We assume that the network we are analyzing is an IP based mobile
network. The findings of the work are presented in the form of tables which can
be used for resource reservation to offer quality service to end-users.

ABSTRACT_BEGIN
  Mobile WiMAX (Worldwide Interoperability for Microwave Access) is being
touted as the most promising and potential broadband wireless technology. And
the popularity rate has been surging to newer heights as the knowledge-backed
service era unfolds steadily. Especially Mobile WiMAX is being projected as a
real and strategic boon for developing counties such as India due to its
wireless coverage acreage is phenomenally high. Mobile WiMAX has spurred
tremendous interest from operators seeking to deploy high-performance yet
cost-effective broadband wireless networks. The IEEE 802.16e standard based
Mobile WiMAX system will be investigated for the purpose of Quality of Service
provisioning. As a technical challenge, radio resource management will be
primarily considered and main is the costly spectrum and the increasingly more
demanding applications with ever growing number of subscribers. It is necessary
to provide Quality of Service (QoS) guaranteed with different characteristics.
As a possible solution the scheduling algorithms will be taken into main
consideration and the present well known algorithms will be described. In this
paper, we have highlighted the following critical issues for Mobile WiMAX
technologies. This paper specifically discussed about the below mentioned in
detail. - QoS Requirements For IEEE 802.16 Service Classes, Achieving efficient
radio resource management. - Deficit Round Robin (DRR) Scheduling algorithm. -
Modified Deficit Round Robin (MDRR) scheduling algorithm's attributes,
properties and architecture. System Model And Scenarios Using OPNET Modeler
Software. - Simulation Limitations And Constraints.

ABSTRACT_BEGIN
  The Principal Component Analysis (PCA) is a data dimensionality reduction
technique well-suited for processing data from sensor networks. It can be
applied to tasks like compression, event detection, and event recognition. This
technique is based on a linear transform where the sensor measurements are
projected on a set of principal components. When sensor measurements are
correlated, a small set of principal components can explain most of the
measurements variability. This allows to significantly decrease the amount of
radio communication and of energy consumption. In this paper, we show that the
power iteration method can be distributed in a sensor network in order to
compute an approximation of the principal components. The proposed
implementation relies on an aggregation service, which has recently been shown
to provide a suitable framework for distributing the computation of a linear
transform within a sensor network. We also extend this previous work by
providing a detailed analysis of the computational, memory, and communication
costs involved. A compression experiment involving real data validates the
algorithm and illustrates the tradeoffs between accuracy and communication
costs.

ABSTRACT_BEGIN
  This paper investigates the issue of connectivity of a wireless adhoc network
in the presence of channel impairments. We derive analytical expressions for
the node isolation probability in an adhoc network in the presence of
Nakagami-m fading with superimposed lognormal shadowing. The node isolation
probability is the probability that a randomly chosen node is not able to
communicate with none of the other nodes in the network. An extensive
investigation into the impact of path loss exponent, lognormal shadowing,
Nakagami fading severity index, node density, and diversity order on the node
isolation probability is conducted. The presented results are beneficial for
the practical design of ad hoc networks.

ABSTRACT_BEGIN
  In this paper we demonstrate that the Canine Pose Estimation (CPE) system can
provide a reliable estimate for some poses and when coupled with effective
wireless transmission over a mesh network. Pose estimates are time sensitive,
thus it is important that pose data arrives at its destination quickly.
Propagation delay and packet delivery ratio measuring algorithms were developed
and used to appraise Wireless Mesh Network (WMN) performance as a means of
carriage for this time-critical data. The experiments were conducted in the
rooms of a building where the radio characteristics closely resembled those of
a partially collapsed building-a typical US&R environment. This paper presents
the results of the experiments, which demonstrate that it is possible to
receive the canine pose estimation data in realtime although accuracy of the
results depend on the network size and the deployment environment.

ABSTRACT_BEGIN
  Position-based routing protocols take advantage of location information to
perform a stateless and efficient routing. To enable position-based routing, a
node must be able to discover the location of the messages' destination node.
This task is typically accomplished by a location service. Recently, several
location service protocols have been developed for ad hoc networks. In this
paper we propose a novel location service called PHLS: Predictive Hierarchical
Location Service. In PHLS, the entire network is partitioned into a hierarchy
of smaller and smaller regions. For each node, one node in each-level region of
the hierarchy is chosen as its local location server. When the network
initializes or when a node attaches the network, nodes contact their local
location server with their current location information (ie. position and
velocity). Then, they only need to update their location server when they move
away from their current region. Finally, nodes query their location servers and
get the exact or predicted location of destination nodes.

ABSTRACT_BEGIN
  In this paper, we propose a hybrid medium access control protocol (H-MAC) for
wireless sensor networks. It is based on the IEEE 802.11's power saving
mechanism (PSM) and slotted aloha, and utilizes multiple slots dynamically to
improve performance. Existing MAC protocols for sensor networks reduce energy
consumptions by introducing variation in an active/sleep mechanism. But they
may not provide energy efficiency in varying traffic conditions as well as they
did not address Quality of Service (QoS) issues. H-MAC, the propose MAC
protocol maintains energy efficiency as well as QoS issues like latency,
throughput, and channel utilization. Our numerical results show that H-MAC has
significant improvements in QoS parameters than the existing MAC protocols for
sensor networks while consuming comparable amount of energy.

ABSTRACT_BEGIN
  We present an evolutionary programming algorithm for solving the dynamic
routing and wavelength assignment (DRWA) problem in optical wavelength-division
multiplexing (WDM) networks under wavelength continuity constraint. We assume
an ideal physical channel and therefore neglect the blocking of connection
requests due to the physical impairments. The problem formulation includes
suitable constraints that enable the algorithm to balance the load among the
individuals and thus results in a lower blocking probability and lower mean
execution time than the existing bio-inspired algorithms available in the
literature for the DRWA problems. Three types of wavelength assignment
techniques, such as First fit, Random, and Round Robin wavelength assignment
techniques have been investigated here. The ability to guarantee both low
blocking probability without any wavelength converters and small delay makes
the improved algorithm very attractive for current optical switching networks.

ABSTRACT_BEGIN
  In the wireless environment, dissemination techniques may improve data access
for the users. In this paper, we show a description of dissemination
architecture that fits the overall telecommunication network. This architecture
is designed to provide efficient data access and power saving for the mobile
units. A concurrency control approach, MCD, is suggested for data consistency
and conflict checking. A performance study shows that the power consumption,
space overhead, and response time associated with MCD is far less than other
previous techniques.

ABSTRACT_BEGIN
  This paper is a quantitative analysis on packet switched network with a view
to generalize load balancing and determination of appropriate routing algorithm
in multipath environment. Several routing algorithms have been introduced for
routing of packets from source to destination. Some of them route packets
accurately with increased workload and some of them drastically cut down the
workload. A few of them can find out a minimum workload deviation for both UDP
and TCP packets. We simulated these approaches in a well defined simulator,
analyzed and evaluated their performance. After expanding our analysis with
varying weights and number of paths we found that the recently proposed routing
algorithm Mixed Weighted Fair Routing (MWFR) outperforms the existing routing
algorithms by reducing the routing and network overhead and saving the scarce
bandwidth as well as CPU consumption for packet switching networks.

ABSTRACT_BEGIN
  Multicast is the ability of a communication network to accept a single
message from an application and to deliver copies of the message to multiple
recipients at different location. With the development of Internet, Multicast
is widely applied in all kinds of multimedia real-time application: distributed
multimedia systems, collaborative computing, video-conferencing, distance
education, etc. In order to construct a delay-constrained multicast routing
tree, average distance heuristic (ADH) algorithm is analyzed firstly. Then a
delay-constrained algorithm called DCADH (delay-constrained average distance
heuristic) is presented. By using ADH a least cost multicast routing tree can
be constructed; if the path delay can't meet the delay upper bound, a shortest
delay path which is computed by Dijkstra algorithm will be merged into the
existing multicast routing tree to meet the delay upper bound. Simulation
experiments show that DCADH has a good performance in achieving a low-cost
multicast routing tree.

ABSTRACT_BEGIN
  This paper presents a new approach in the management of mobile ad hoc
networks. Our alternative, based on mobile agent technology, allows the design
of mobile centralized server in ad hoc network, where it is not obvious to
think about a centralized management, due to the absence of any administration
or fixed infrastructure in these networks. The aim of this centralized approach
is to provide permanent availability of services in ad hoc networks which are
characterized by a distributed management. In order to evaluate the performance
of the proposed approach, we apply it to solve the problem of mobile code
localization in ad hoc networks. A comparative study, based upon a simulation,
of centralized and distributed localization protocols in terms of messages
number exchanged and response time shows that the centralized approach in a
distributed form is more interesting than a totally centralized approach.

ABSTRACT_BEGIN
  Peer-to-Peer (P2P) networks provide a significant solution for file sharing
among peers connected to Internet. It is fast and completely decentralised
system with robustness. But due to absence of a server documents on a P2P
network are not rated which makes it difficult for a peer to obtain precise
information in result of a query. In past, some researchers tried to attach
ratings to the peers itself but it was complex and less effective. In this
paper, a novel P2P architecture is proposed which attaches ratings to the
uploaded document directly. These ratings then become as <Rating> element in
its XML advertisement which has several child elements for information
classification. The attached <Rating> element is extracted from the
advertisement in real time and the document is then sorted accordingly.
Therefore, the information can be easily sorted based on a request by a peer
according to the relevance of matter. The information regarding relevance is
obtained by the peer issuing the query. This research leads to a smart P2P
model, the Rated-Resource P2P network (R2P2P).

ABSTRACT_BEGIN
  We consider stability and network capacity in discrete time queueing systems.
Relationships between four common notions of stability are described.
Specifically, we consider rate stability, mean rate stability, steady state
stability, and strong stability. We then consider networks of queues with
random events and control actions that can be implemented over time to affect
arrivals and service at the queues. The control actions also generate a vector
of additional network attributes. We characterize the network capacity region,
being the closure of the set of all rate vectors that can be supported subject
to network stability and to additional time average attribute constraints. We
show that (under mild technical assumptions) the capacity region is the same
under all four stability definitions. Our capacity achievability proof uses the
drift-plus-penalty method of Lyapunov optimization, and provides full details
for the case when network states obey a decaying memory property, which holds
for finite state ergodic systems and more general systems.

ABSTRACT_BEGIN
  LA planning in cellular network is useful for minimizing location management
cost in GSM network. In fact, size of LA can be optimized to create a balance
between the LA update rate and expected paging rate within LA. To get optimal
result for LA planning in cellular network simulated annealing algorithm is
used. Simulated annealing give optimal results in acceptable run-time.

ABSTRACT_BEGIN
  In recent times the cost of mobile communication has dropped significantly
leading to a dramatic increase in mobile phone usage. The widespread usage has
led mobiles to emerge as a strong alternative for other applications one of
which is tracking. This has enabled law-enforcing agencies to detect
overspeeding vehicles and organizations to keep track its employees. The 3
major ways of tracking being employed presently are (a) via GPS [1] (b) signal
attenuation property of a packet [3] and (c) using GSM Network [2]. The initial
cost of GPS is very high resulting in low usage whereas (b) needs a very high
precision measuring device. The paper presents a GSM-based tracking technique
which eliminates the above mentioned overheads, implements it in NS2 and shows
the limitations of the real life simulation. An accuracy of 97% was achieved
during NS2 simulation which is comparable to the above mentioned alternate
methods of tracking.

ABSTRACT_BEGIN
  For high data rate ultra wideband communication system, performance
comparison of Rake, MMSE and Rake-MMSE receivers is attempted in this paper.
Further a detail study on Rake-MMSE time domain equalizers is carried out
taking into account all the important parameters such as the effect of the
number of Rake fingers and equalizer taps on the error rate performance. This
receiver combats inter-symbol interference by taking advantages of both the
Rake and equalizer structure. The bit error rate performances are investigated
using MATLAB simulation on IEEE 802.15.3a defined UWB channel models.
Simulation results show that the bit error rate probability of Rake-MMSE
receiver is much better than Rake receiver and MMSE equalizer. Study on
non-line of sight indoor channel models illustrates that bit error rate
performance of Rake-MMSE (both LE and DFE) improves for CM3 model with smaller
spread compared to CM4 channel model. It is indicated that for a MMSE equalizer
operating at low to medium SNR values, the number of Rake fingers is the
dominant factor to improve system performance, while at high SNR values the
number of equalizer taps plays a more significant role in reducing the error
rate.

ABSTRACT_BEGIN
  There are many challenges when designing and deploying wireless sensor
networks (WSNs). One of the key challenges is how to make full use of the
limited energy to prolong the lifetime of the network, because energy is a
valuable resource in WSNs. The status of energy consumption should be
continuously monitored after network deployment. In this paper, we propose
coverage and connectivity aware neural network based energy efficient routing
in WSN with the objective of maximizing the network lifetime. In the proposed
scheme, the problem is formulated as linear programming (LP) with coverage and
connectivity aware constraints. Cluster head selection is proposed using
adaptive learning in neural networks followed by coverage and connectivity
aware routing with data transmission. The proposed scheme is compared with
existing schemes with respect to the parameters such as number of alive nodes,
packet delivery fraction, and node residual energy. The simulation results show
that the proposed scheme can be used in wide area of applications in WSNs.

ABSTRACT_BEGIN
  Secured communication in ad hoc wireless networks is primarily important,
because the communication signals are openly available as they propagate
through air and are more susceptible to attacks ranging from passive
eavesdropping to active interfering. The lack of any central coordination and
shared wireless medium makes them more vulnerable to attacks than wired
networks. Nodes act both as hosts and routers and are interconnected by Multi-
hop communication path for forwarding and receiving packets to/from other
nodes. The objective of this paper is to propose a key exchange and encryption
mechanism that aims to use the MAC address as an additional parameter as the
message specific key[to encrypt]and forward data among the nodes. The nodes are
organized in spanning tree fashion, as they avoid forming cycles and exchange
of key occurs only with authenticated neighbors in ad hoc networks, where nodes
join or leave the network dynamically.

ABSTRACT_BEGIN
  Mobile Ad Hoc Network (MANET) is a collection of two or more devices or nodes
or terminals with wireless communications and networking capability that
communicate with each other without the aid of any centralized administrator
also the wireless nodes that can dynamically form a network to exchange
information without using any existing fixed network infrastructure. And it's
an autonomous system in which mobile hosts connected by wireless links are free
to be dynamically and some time act as routers at the same time, and we discuss
in this paper the distinct characteristics of traditional wired networks,
including network configuration may change at any time, there is no direction
or limit the movement and so on, and thus needed a new optional path Agreement
(Routing Protocol) to identify nodes for these actions communicate with each
other path, An ideal choice way the agreement should not only be able to find
the right path, and the Ad Hoc Network must be able to adapt to changing
network of this type at any time. and we talk in details in this paper all the
information of Mobile Ad Hoc Network which include the History of ad hoc,
wireless ad hoc, wireless mobile approaches and types of mobile ad Hoc
networks, and then we present more than 13 types of the routing Ad Hoc Networks
protocols have been proposed. In this paper, the more representative of routing
protocols, analysis of individual characteristics and advantages and
disadvantages to collate and compare, and present the all applications or the
Possible Service of Ad Hoc Networks.

ABSTRACT_BEGIN
  In 1991, Gnanajothi [4] proved that the path graph P_n with n vertex and n-1
edge is odd graceful, and the cycle graph C_m with m vertex and m edges is odd
graceful if and only if m even, she proved the cycle graph is not graceful if m
odd. In this paper, firstly, we studied the graph C_m $\cup$ P_m when m = 4,
6,8,10 and then we proved that the graph C_ $\cup$ P_n is odd graceful if m is
even. Finally, we described an algorithm to label the vertices and the edges of
the vertex set V(C_m $\cup$ P_n) and the edge set E(C_m $\cup$ P_n).

ABSTRACT_BEGIN
  Cryptography literally means "The art & science of secret writing & sending a
message between two parties in such a way that its contents cannot be
understood by someone other than the intended recipient". and Quantum word is
related with "Light". Thus, Quantum Cryptography is a way of descripting any
information in the form of quantum particles. There are no classical
cryptographic systems which are perfectly secure. In contrast to Classical
cryptography which depends upon Mathematics, Quantum Cryptography utilizes the
concepts of Quantum Physics which provides us the security against the
cleverest marauders of the present age. In the view of increasing need of
Network and Information Security, we do require methods to overcome the
Molecular Computing technologies (A future technology) and other techniques of
the various codebrakers. Both the parts i.e. Quantum Key distribution and
Information transference from Sender to Receiver are much efficient and secure.
It is based upon BB84 protocol. It can be of great use for Govt. agencies such
as Banks, Insurance, Brokerages firms, financial institutions, e-commerce and
most important is the Defense & security of any country. It is a Cryptographic
communication system in which the original users can detect unauthorized
eavesdropper and in addition it gives a guarantee of no eavesdropping. It
proves to be the ultra secure mode of communication b/w two intended parties.

ABSTRACT_BEGIN
  In wireless mesh networks such as WLAN (IEEE 802.11s) or WMAN (IEEE 802.11),
each node should help to relay packets of neighboring nodes toward gateway
using multi-hop routing mechanisms. Wireless mesh networks usually intensively
deploy mesh nodes to deal with the problem of dead spot communication. However,
the higher density of nodes deployed, the higher radio interference occurred.
This causes significant degradation of system performance. In this paper, we
first convert network problems into geometry problems in graph theory, and then
solve the interference problem by geometric algorithms. We first define line
intersection in a graph to reflect radio interference problem in a wireless
mesh network. We then use plan sweep algorithm to find intersection lines, if
any; employ Voronoi diagram algorithm to delimit the regions among nodes; use
Delaunay Triangulation algorithm to reconstruct the graph in order to minimize
the interference among nodes. Finally, we use standard deviation to prune off
those longer links (higher interference links) to have a further enhancement.
The proposed hybrid solution is proved to be able to significantly reduce
interference in a wireless mesh network in O(n log n) time complexity.

ABSTRACT_BEGIN
  Congestion is an important issue which researchers focus on in the
Transmission Control Protocol (TCP) network environment. To keep the stability
of the whole network, congestion control algorithms have been extensively
studied. Queue management method employed by the routers is one of the
important issues in the congestion control study. Active queue management (AQM)
has been proposed as a router-based mechanism for early detection of congestion
inside the network. In this paper we analyzed several active queue management
algorithms with respect to their abilities of maintaining high resource
utilization, identifying and restricting disproportionate bandwidth usage, and
their deployment complexity. We compare the performance of FRED, BLUE, SFB, and
CHOKe based on simulation results, using RED and Drop Tail as the evaluation
baseline. The characteristics of different algorithms are also discussed and
compared. Simulation is done by using Network Simulator(NS2) and the graphs are
drawn using X- graph.

ABSTRACT_BEGIN
  Disruption Tolerant Networks (DTN) have been a popular subject of recent
research and development. These networks are characterized by frequent, lengthy
outages and a lack of contemporaneous end-to-end paths. In this work we discuss
techniques for extending IP to operate more effectively in DTN scenarios. Our
scheme, Disruption Tolerant IP (DIP) uses existing IP packet headers, uses the
existing socket API for applications, is compatible with IPsec, and uses
familiar Policy-Based Routing techniques for network management.

ABSTRACT_BEGIN
  Radio Frequency IDentification (RFID) is a dedicated short range
communication technology. The term RFID is used to describe various
technologies that use radio waves to automatically identify people or objects.
RFID is a method of remotely storing and retrieving data using RFID tag. Radio
Frequency Identification (RFID) technology has been attracting considerable
attention with the expectation of improved supply chain visibility for consumer
goods, apparel, and pharmaceutical manufacturers, as well as retailers and
government procurement agencies. RFID technology is used today in many
applications, including security and access control, transportation and supply
chain tracking. Supply Chain Management (SCM) is now at the centre stage of
Manufacturing and service organizations. According to the strategies in
markets, supply chains and logistics are naturally being modelled as
distributed systems. The economic importance has motivated both private
companies and academic researchers to pursue the use of operations research and
management service tools to improve the efficiency of Transportation. Referring
to such scenario, in this work RFID Technique adopted with hybrid algorithm to
optimize supply chain distribution network.

ABSTRACT_BEGIN
  Hidden nodes in a wireless network refer to nodes that are out of range of
other nodes or a collection of nodes. We will discuss a few problems introduced
by the RTS/CTS mechanism of collision avoidance and focus on the virtual
jamming problem, which allows a malicious node to effectively jam a large
fragment of a wireless network at minimum expense of power. We have also
discussed WiCCP (Wireless Central Coordinated Protocol) which is a protocol
booster that also provides good solution to hidden nodes.

ABSTRACT_BEGIN
  This paper deals with providing Quality of Service (QoS) over IP based
networks. We are going to give a brief survey about this topic, and present our
work at this area. There are many solutions of the problem, but the
standardization of the methods is not finished yet. At the moment there are two
kinds of approaches of the reservation problem. The distributed method handles
the network nodes independently, and get the nodes making their own admittance
decisions along the reservation path (i.e. Border Gateway Reservation Protocol
BGRP. The centralized way -we discuss in details-, which collects the network
nodes into domains, and handles them using a network manager. Generally there
are two significant parts of the network management: intra domain, and
inter-domain. This article focuses on making reservations over several domains,
which is the part of the inter-domain functions.

ABSTRACT_BEGIN
  Performance of routing protocols in mobile ad-hoc networks is greatly
affected by the dynamic nature of nodes, route failures, wireless channels with
variable bandwidth and scalability issues. A mobility model imitates the real
world movement of mobile nodes and is central component to simulation based
studies. In this paper we consider mobility nodes which mimic the vehicular
motion of nodes like Manhattan mobility model and City Section mobility model.
We also propose a new Group Vehicular mobility model that takes the best
features of group mobility models like Reference Point Group mobility model and
applies it to vehicular models. We analyze the performance of our model known
as Group Vehicular mobility model (GVMM) and other vehicular mobility models
with various metrics. This analysis provides us with an insight about the
impact of mobility models on the performance of routing protocols for ad-hoc
networks. The routing protocols are simulated and measured for performance and
finally we arrive at the correlation about the impact of mobility models on
routing protocols, which are central to the design of mobile adhoc networks.

ABSTRACT_BEGIN
  This paper presents the design and development of a context-aware
notification system for university students using RFID technology. This system
is leveraging on the student's matrix card as the RFID tag (sensor), RFID
reader and server as the processors and screen monitor at the various locations
in the campus as the actuator of the output. This system aims to deliver urgent
notifications to the intended students immediately at their respective
locations. In addition, the system is also able to display personalized
information based on the students' preferences and current location when
accessing the system. The background of the study, the design approaches for
this system and the preliminary evaluation of the prototype are presented in
this paper. The evaluation results have indicated that the the proposed system
is useful and easy to use.

ABSTRACT_BEGIN
  This report deals with security in wireless sensor networks (WSNs),
especially in network layer. Multiple secure routing protocols have been
proposed in the literature. However, they often use the cryptography to secure
routing functionalities. The cryptography alone is not enough to defend against
multiple attacks due to the node compromise. Therefore, we need more
algorithmic solutions. In this report, we focus on the behavior of routing
protocols to determine which properties make them more resilient to attacks.
Our aim is to find some answers to the following questions. Are there any
existing protocols, not designed initially for security, but which already
contain some inherently resilient properties against attacks under which some
portion of the network nodes is compromised? If yes, which specific behaviors
are making these protocols more resilient? We propose in this report an
overview of security strategies for WSNs in general, including existing attacks
and defensive measures. In this report we focus at the network layer in
particular, and an analysis of the behavior of four particular routing
protocols is provided to determine their inherent resiliency to insider
attacks. The protocols considered are: Dynamic Source Routing (DSR),
Gradient-Based Routing (GBR), Greedy Forwarding (GF) and Random Walk Routing
(RWR).

ABSTRACT_BEGIN
  Diversity is a powerful means to increase the transmission performance of
wireless communications. For the case of fountain codes relaying, it has been
shown previously that introducing diversity is also beneficial since it
counteracts transmission losses on the channel. Instead of simply hop-by-hop
forwarding information, each sensor node diversifies the information flow using
XOR combinations of stored packets. This approach has been shown to be
efficient for random linear fountain codes. However, random linear codes
exhibit high decoding complexity. In this paper, we propose diversity increased
relaying strategies for the more realistic Luby Transform code in order to
maintain high transmission performance with low decoding computational
complexity in a linear network. Results are provided herein for a linear
network assuming uniform imperfect channel states.

ABSTRACT_BEGIN
  Traditional end-to-end congestion control mechanisms assume data transferring
happens between each pair user. In contrast, in a P2P network, many peers may
locally keep a copy of a specific data object. If the path between a pair of
peers is congested, the requesting peer who wants to download data will switch
to another peer in its neighbor peer list to fetch the data instead of
decreasing the download rate from the current peer. Thus, it is critical to
study the performance in multi-point-to-multi-point (M2M) transport protocol in
a P2P network. In this paper, we build a mathematical model for identifying the
key parameters for the M2M transport protocol and also the relationships among
these parameters. Finally, we conduct simulation experiments to validate our
model.

ABSTRACT_BEGIN
  Storage networking technology has enjoyed strong growth in recent years, but
security concerns and threats facing networked data have grown equally fast.
Today, there are many potential threats that are targeted at storage networks,
including data modification, destruction and theft, DoS attacks, malware,
hardware theft and unauthorized access, among others. In order for a Storage
Area Network (SAN) to be secure, each of these threats must be individually
addressed. In this paper, we present a comparative study by implementing
different security methods in IP Storage network.

ABSTRACT_BEGIN
  Constantly growing demands of high productivity and security of computer
systems and computer networks call the interest of specialists in the
environment of construction of optimum topologies of computer mediums. In
earliest phases of design, the study of the topological influence of the
processes that happen in computer systems and computer networks allows to
obtain useful information which possesses a significant value in the subsequent
design. It has always been tried to represent the different computer network
topologies using appropriate graph models. Graphs have huge contributions
towards the performance improvement factor of a network. Some major
contributors are de-Bruijn, Hypercube, Mesh and Pascal. They had been studied a
lot and different new features were always a part of research outcome. As per
the definition of interconnection network it is equivalent that a suitable
graph can represent the physical and logical layout very efficiently. In this
present study Pascal graph is researched again and a new characteristics has
been discovered. From the perspective of network topologies Pascal graph and
its properties were first studied more than two decades back. Since then, a
numerous graph models have emerged with potentials to be used as network
topologies. This new property is guaranteed to make an everlasting mark towards
the reliability of this graph to be used as a substantial contributor as a
computer network topology. This shows its credentials over so many other
topologies. This study reviews the characteristics of the Pascal graph and the
new property is established using appropriate algorithm and the results.

ABSTRACT_BEGIN
  VANETs (Vehicular Ad hoc Networks) are highly mobile wireless ad hoc networks
and will play an important role in public safety communications and commercial
applications. Routing of data in VANETs is a challenging task due to rapidly
changing topology and high speed mobility of vehicles. Position based routing
protocols are becoming popular due to advancement and availability of GPS
devices. One of the critical issues of VANETs are frequent path disruptions
caused by high speed mobility of vehicle that leads to broken links which
results in low throughput and high overhead . This paper argues the use of
information on vehicles' movement information (e.g., position, direction, speed
of vehicles) to predict a possible link-breakage event prior to its occurrence.
So in this paper we propose a Reliable Directional Greedy routing (RDGR), a
reliable position based routing approach which obtains position, speed and
direction of its neighboring nodes from GPS. This approach incorporates
potential score based strategy, which calculates link stability between
neighbor nodes in distributed fashion for reliable forwarding of data packet.

ABSTRACT_BEGIN
  Mobile data services are penetrating mobile markets rapidly. The mobile
industry relies heavily on data service to replace the traditional voice
services with the evolution of the wireless technology and market. A reliable
packet service network is critical to the mobile operators to maintain their
core competence in data service market. Furthermore, mobile operators need to
develop effective operational models to manage the varying mix of voice, data
and video traffic on a single network. Application of statistical models could
prove to be an effective approach. This paper first introduces the architecture
of Universal Mobile Telecommunications System (UMTS) packet switched (PS)
network and then applies multivariate statistical analysis to Key Performance
Indicators (KPI) monitored from network entities in UMTS PS network to guide
the long term capacity planning for the network. The approach proposed in this
paper could be helpful to mobile operators in operating and maintaining their
3G packet switched networks for the long run.

ABSTRACT_BEGIN
  The wide-band code division multiple access (WCDMA) based 3G and beyond
cellular mobile wireless networks are expected to provide a diverse range of
multimedia services to mobile users with guaranteed quality of service (QoS).
To serve diverse quality of service requirements of these networks it
necessitates new radio resource management strategies for effective utilization
of network resources with coding schemes. Call admission control (CAC) is a
significant component in wireless networks to guarantee quality of service
requirements and also to enhance the network resilience. In this paper capacity
enhancement for WCDMA network with convolutional coding scheme is discussed and
compared with block code and without coding scheme to achieve a better balance
between resource utilization and quality of service provisioning. The model of
this network is valid for the real-time (RT) and non-real-time (NRT) services
having different data rate. Simulation results demonstrate the effectiveness of
the network using convolutional code in terms of capacity enhancement and QoS
of the voice and video services.

ABSTRACT_BEGIN
  This paper presents a comprehensive analytical study of two competitive
cognitive operators' spectrum leasing and pricing strategies, taking into
account operators' heterogeneity in leasing costs and users' heterogeneity in
transmission power and channel conditions. We model the interactions between
operators and users as a three-stage dynamic game, where operators make
simultaneous spectrum leasing and pricing decisions in Stages I and II, and
users make purchase decisions in Stage III. Using backward induction, we are
able to completely characterize the game's equilibria. We show that both
operators make the equilibrium leasing and pricing decisions based on simple
threshold policies. Moreover, two operators always choose the same equilibrium
price despite their difference in leasing costs. Each user receives the same
signal-to-noise-ratio (SNR) at the equilibrium, and the obtained payoff is
linear in its transmission power and channel gain. We also compare the duopoly
equilibrium with the coordinated case where two operators cooperate to maximize
their total profit. We show that the maximum loss of total profit due to
operators' competition is no larger than 25%. The users, however, always
benefit from operators' competition in terms of their payoffs. We show that
most of these insights are robust in the general SNR regime.

ABSTRACT_BEGIN
  The development of mobile devices (CPU, memory, and storage) and the
introduction of mobile networks (Ad-Hoc, Wi-Fi, WiMAX, and 3.5G) have opened
new opportunities for next generation of mobile services. It becomes more
convenience and desirable for mobile internet users to be connected everywhere.
However, ubiquitous mobile access connectivity faces interoperation issues
between wireless network providers and wireless network technologies. Although
mobile users would like to get as many services as possible while they travel,
there is a lack of technology to identify visited users in current foreign
network authentication systems. This challenge lies in the fact that a foreign
network provider does not initially have the authentication credentials of a
mobile user. Existing approaches use roaming agreement to exchange
authentication information between home network and foreign network. This paper
proposes a roaming agreement-less approach designed based on our ubiquitous
mobile access model. Our approach consist of two tokens, Passport
(identification token) and Visa (authorisation token) to provide the mobile
user with a flexible authentication method to access foreign network services.
The security analysis indicates that our proposal is more suitable for
ubiquitous mobile communication especially in roaming agreement-less
environment.

ABSTRACT_BEGIN
  In this paper, we study the impact of network latency on the time required to
download a file distributed using BitTorrent. This study is essential to
understand if testbeds can be used for experimental evaluation of BitTorrent.
We observe that the network latency has a marginal impact on the time required
to download a file; hence, BitTorrent experiments can performed on testbeds.

ABSTRACT_BEGIN
  Mobile operators currently prefer optimizing their radio networks via
re-homing or cutting over the cell sites in 2G or 3G networks. The core
network, as the parental part of radio network, is inevitably impacted by the
re-homing in radio domain. This paper introduces the cell site re-homing in
radio network and analyzes its impact on the performance of GSM/UMTS core
network. The possible re-homing models are created and analyzed for core
networks. The paper concludes that appropriate re-homing in radio domain, using
correct algorithms, not only optimizes the radio network but also helps improve
the QoS of the core network and saves the carriers' OPEX and CAPEX on their
core networks.

ABSTRACT_BEGIN
  During the last three decades the Internet has experienced fascinating
evolution, both exponential growth in traffic and rapid expansion in topology.
The size of the Internet becomes enormous, yet the network is very `small' in
the sense that it is extremely efficient to route data packets across the
global Internet. This paper provides a brief review on three fundamental
properties of the Internet topology at the autonomous systems (AS) level.
Firstly the Internet has a power-law degree distribution, which means the
majority of nodes on the Internet AS graph have small numbers of links, whereas
a few nodes have very large numbers of links. Secondly the Internet exhibits a
property called disassortative mixing, which means poorly-connected nodes tend
to link with well-connected nodes, and vice versa. Thirdly the best-connected
nodes, or the rich nodes, are tightly interconnected with each other forming a
rich-club. We explain that it is these structural properties that make the
global Internet so 'small'.

ABSTRACT_BEGIN
  Denial of Service (DoS) attacks frequently happen on the Internet, paralyzing
Internet services and causing millions of dollars of financial loss. This work
presents NetFence, a scalable DoS-resistant network architecture. NetFence uses
a novel mechanism, secure congestion policing feedback, to enable robust
congestion policing inside the network. Bottleneck routers update the feedback
in packet headers to signal congestion, and access routers use it to police
senders' traffic. Targeted DoS victims can use the secure congestion policing
feedback as capability tokens to suppress unwanted traffic. When compromised
senders and receivers organize into pairs to congest a network link, NetFence
provably guarantees a legitimate sender its fair share of network resources
without keeping per-host state at the congested link. We use a Linux
implementation, ns-2 simulations, and theoretical analysis to show that
NetFence is an effective and scalable DoS solution: it reduces the amount of
state maintained by a congested router from per-host to at most per-(Autonomous
System).

ABSTRACT_BEGIN
  In this paper, we consider the problem of modelling the average delay in an
IEEE 802.11 DCF wireless mesh network with a single root node under light
traffic. We derive expression for mean delay for a co-located wireless mesh
network, when packet generation is homogeneous Poisson process with rate
\lambda. We also show how our analysis can be extended for non-homogeneous
Poisson packet generation. We model mean delay by decoupling queues into
independent M/M/1 queues. Extensive simulations are conducted to verify the
analytical results.

ABSTRACT_BEGIN
  As many sensor network applications require deployment in remote and
hard-to-reach areas, it is critical to ensure that such networks are capable of
operating unattended for long durations. Consequently, the concept of using
nodes with energy replenishment capabilities has been gaining popularity.
However, new techniques and protocols must be developed to maximize the
performance of sensor networks with energy replenishment. Here, we analyze
limits of the performance of sensor nodes with limited energy, being
replenished at a variable rate. We provide a simple localized energy management
scheme that achieves a performance close to that with an unlimited energy
source, and at the same time keeps the probability of complete battery
discharge low. Based on the insights developed, we address the problem of
energy management for energy-replenishing nodes with finite battery and finite
data buffer capacities. To this end, we give an energy management scheme that
achieves the optimal utility asymptotically while keeping both the battery
discharge and data loss probabilities low.

ABSTRACT_BEGIN
  Wireless sensor networks are collections of large number of sensor nodes. The
sensor nodes are featured with limited energy, computation and transmission
power. Each node in the network coordinates with every other node in forwarding
their packets to reach the destination. Since these nodes operate in a
physically insecure environment; they are vulnerable to different types of
attacks such as selective forwarding and sinkhole. These attacks can inject
malicious packets by compromising the node. Geographical routing protocols of
wireless sensor networks have been developed without considering the security
aspects against these attacks. In this paper, a secure routing protocol named
secured greedy perimeter stateless routing protocol (S-GPSR) is proposed for
mobile sensor networks by incorporating trust based mechanism in the existing
greedy perimeter stateless routing protocol (GPSR). Simulation results prove
that S-GPSR outperforms the GPSR by reducing the overhead and improving the
delivery ratio of the networks.

ABSTRACT_BEGIN
  Environmental monitoring is often performed through a wireless sensor
network, whose nodes are randomly deployed over the geographical region of
interest. Sensors sample a physical phenomenon (the so-called field) and send
their measurements to a {\em sink} node, which is in charge of reconstructing
the field from such irregular samples. In this work, we focus on scenarios of
practical interest where the sensor deployment is unfeasible in certain areas
of the geographical region, e.g., due to terrain asperities, and the delivery
of sensor measurements to the sink may fail due to fading or to transmission
collisions among sensors simultaneously accessing the wireless medium. Under
these conditions, we carry out an asymptotic analysis and evaluate the quality
of the estimation of a d-dimensional field when the sink uses linear filtering
as a reconstruction technique. Specifically, given the matrix representing the
sampling system, V, we derive both the moments and an expression of the
limiting spectral distribution of VV*, as the size of V goes to infinity and
its aspect ratio has a finite limit bounded away from zero. By using such
asymptotic results, we approximate the mean square error on the estimated field
through the eta-transform of VV*, and derive the sensor network performance
under the conditions described above.

ABSTRACT_BEGIN
  The sensor nodes in a Wireless Sensor Network are generally constrained with
limited power supply. Efficient power management is a must for any sensor
network to keep the sensor nodes in the network to be operational for a longer
period of time this increasing the lifetime of the sensor network. Hierarchy
based routing enables the sensor networks to be deployed in larger areas. In
this paper we present a hierarchical cluster based routing protocol which
improves the scalability as the data travels from one cluster level to another
covering a greater amount of distance and increases the lifetime of the
wireless sensor network by distributing the power dissipation load evenly among
all the sensor nodes within the network. Also the time delay in case of
critical data to be received by the Base Station has also been lowered.

ABSTRACT_BEGIN
  The performance analysis of long file TCP controlled transfers in a WLAN in
infrastructure mode is available in the present literature with one of the main
assumptions being equal window size for all TCP connections. In this paper, we
extend the analysis to TCP-controlled long file uploads and downloads with
different TCP windows. Our approach is based on simple Markov chain given in
the paper [1], [2] with arbitrary window sizes. We presented simulation results
to show the accuracy of the analytical model.

ABSTRACT_BEGIN
  The Next Generation Networks (NGN) aims to integrate for IP-based telecom
infrastructures and provide most advance & high speed emerging value added
services. NGN capable to provide higher innovative services, these services
will able to integrate communication and Web service into a single platform. IP
Multimedia Subsystem, a NGN leading technology, enables a variety of
NGN-compliant communications services to interoperate while being accessed
through different kinds of access networks, preferably broadband. IMS-NGN
services essential by both consumer and corporate users are by now used to
access services, even communications services through the web and web-based
communities and social networks, It is key for success of IMS-based services to
be provided with efficient web access, so users can benefit from those new
services by using web-based applications and user interfaces, not only NGN-IMS
User Equipments and SIP protocol. Many Service are under planning which
provided only under convergence of IMS & Web 2.0. Convergence between Web 2.0
and NGN-IMS creates and serves new invented innovative, entertainment and
information appealing as well as user centric services and applications. These
services merge features from WWW and Communication worlds. On the one hand,
interactivity, ubiquity, social orientation, user participation and content
generation, etc. are relevant characteristics coming from Web 2.0 services.
Parallel IMS enables services including multimedia telephony, media sharing
(video-audio), instant messaging with presence and context, online directory,
etc. all of them applicable to mobile, fixed or convergent telecom networks.
With this paper, this paper brings out the benefits of adopting web 2.0
technologies for telecom services. As the services are today mainly driven by
the user's needs, and proposed the concept of unique customizable service
interface.

ABSTRACT_BEGIN
  The Next Generation Networks (NGN) aims to integrate for IP-based telecom
infrastructures and provide most advance & high speed emerging value added
services. NGN capable to provide higher innovative services, these services
will able to integrate communication and Web service into a single platform. IP
Multimedia Subsystem, a NGN leading technology, enables a variety of
NGN-compliant communications services to interoperate while being accessed
through different kinds of access networks, preferably broadband. IMS-NGN
services essential by both consumer and corporate users are by now used to
access services, even communications services through the web and web-based
communities and social networks, It is key for success of IMS-based services to
be provided with efficient web access, so users can benefit from those new
services by using web-based applications and user interfaces, not only NGN-IMS
User Equipments and SIP protocol. Many Service are under planning which
provided only under convergence of IMS & Web 2.0. Convergence between Web 2.0
and NGN-IMS creates and serves new invented innovative, entertainment and
information appealing as well as user centric services and applications. These
services merge features from WWW and Communication worlds. On the one hand,
interactivity, ubiquity, social orientation, user participation and content
generation, etc. are relevant characteristics coming from Web 2.0 services.
Parallel IMS enables services including multimedia telephony, media sharing
(video-audio), instant messaging with presence and context, online directory,
etc. all of them applicable to mobile, fixed or convergent telecom networks.
With this paper, this paper brings out the benefits of adopting web 2.0
technologies for telecom services. As the services are today mainly driven by
the user's needs, and proposed the concept of unique customizable service
interface.

ABSTRACT_BEGIN
  The main focus of this article is to achieve prolonged network lifetime with
overall energy efficiency in wireless sensor networks through controlled
utilization of limited energy. Major percentage of energy in wireless sensor
network is consumed during routing from source to destination, retransmission
of data on packet loss. For improvement, cross layered algorithm is proposed
for routing and retransmission scheme. Simulation and results shows that this
approach can save the overall energy consumption

ABSTRACT_BEGIN
  This paper aims to propose a significant way of remote access and real time
monitoring of a particular geographic area by integrating wireless sensor
clouds with existing Telecom infrastructure and applications built around them
through a gateway. This utility is very potent for environment monitoring in
harsh and inaccessible places like mines, nuclear reactors, etc. We demonstrate
a scaled down version of multi-hop network of wireless sensor nodes and its
integration with existing telecom network infrastructure via a gateway. The
kind of results achieved like temperature monitoring etc. gives a glimpse of an
enormous step ahead in mine safety.

ABSTRACT_BEGIN
  With recent advances in wireless communication, networking, and low power
sensor technology, wireless sensor network (WSN) systems have begun to take
significant roles in various applications ranging from environmental sensing to
mobile healthcare sensing. While some WSN applications only require a lim- ited
amount of bandwidth, new emerging applications operate with a notice- ably
large amount of data transfers. One way to deal with such applications is to
maximize the available capacity by utilizing the use of multiple wireless
channels. This work proposes DynaChannAl, a distributed dynamic wireless
channel algorithm with the goal of effectively distributing nodes on multiple
wireless channels in WSN systems. Specifically, DynaChannAl targets applica-
tions where mobile nodes connect to a pre-existing wireless backbone and takes
the expected end-to-end queuing delay as its core metric. We use the link qual-
ity indicator (LQI) values provided by IEEE 802.15.4 radios white-list
potential links with good link quality and evaluate such links with the
aggregated packet transmission latency at each hop. Our approach is useful for
applications that require minimal end-to-end delay (i.e., healthcare
applications). DynaChannAl is a light weight and highly adoptable scheme that
can be easily incorporated with various pre-developed components and
pre-deployed applications. We eval- uate DynaChannAl in on a 45 node WSN
testbed. As the first study to consider end-to-end latency as the core metric
for channel allocation in WSN systems, the experimental results indicate that
DynaChannAl successfully distributes multi- ple (mobile) source nodes on
different wireless channels and enables the nodes to select wireless channel
and links that can minimize the end-to-end latency.

ABSTRACT_BEGIN
  Efficient resource discovery and availability improvement are very important
issues in unstructured P2P networks. In this paper, a bio-inspired resource
discovery scheme inspired by the principle of elephants migration is proposed.
A replication scheme based on Q-learning and erasure codes is also introduced.
Simulation results show that the proposed schemes significantly increases query
success rate and availability, and reduces the network traffic as the resources
are effectively distributed to well-performing nodes.

ABSTRACT_BEGIN
  Traffic and channel-data rate combined with the stream oriented methodology
can provide a scheme for offering optimized and guaranteed QoS. In this work a
stream oriented modeled scheme is proposed based on each node's self-scheduling
energy management. This scheme is taking into account the overall packet loss
in order to form the optimal effective -for the end-to-end connection-
throughput response. The scheme also -quantitatively- takes into account the
asymmetrical nature of wireless links and the caching activity that is used for
data revocation in the ad-hoc based connectivity scenario. Through the designed
middleware and the architectural layering and through experimental simulation,
the proposed energy-aware management scheme is thoroughly evaluated in order to
meet the parameters' values where the optimal throughput response for each
device/user is achieved.

ABSTRACT_BEGIN
  The paper has two objectives. The first is to study rigorously the transient
behavior of some P2P networks whenever information is replicated and
disseminated according to epidemic-like dynamics. The second is to use the
insight gained from the previous analysis in order to predict how efficient are
measures taken against peer-to-peer (P2P) networks. We first introduce a
stochastic model which extends a classical epidemic model and characterize the
P2P swarm behavior in presence of free riding peers. We then study a second
model in which a peer initiates a contact with another peer chosen randomly. In
both cases the network is shown to exhibit a phase transition: a small change
in the parameters causes a large change in the behavior of the network. We
show, in particular, how the phase transition affects measures that content
provider networks may take against P2P networks that distribute non-authorized
music or books, and what is the efficiency of counter-measures.

ABSTRACT_BEGIN
  Given the respective advantages of the two complimentary techniques for
peer-to-peer media streaming (namely tree-based push and mesh-based pull),
there is a strong trend of combining them into a hybrid streaming system.
Backed by recently proposed mechanisms to identify stable peers, such a hybrid
system usually consists of backbone trees formed by the stable peers and other
overlay structures in the second tier to accommodate the remaining peers. In
this paper, we embrace the hybrid push-pull structure for peer-to-peer media
streaming. Our protocol is dominated by a multi-tree push mechanism to minimize
the delay in the backbone and is complemented by other overlay structures to
cope with peer dynamics. What mainly distinguishes our multi-tree pushing from
the conventional ones is an unbalanced tree design guided by the so called
snow-ball streaming, which has a provable minimum delay and can be smoothly
"melded" with virtually any other existing overlay structures lying in the
second tier. We design algorithms to construct and maintain our SNowbAll
multi-tree Pushing (SNAP) overlay, and we also illustrate how to smoothly weld
the SNAP backbone with the second tier. Finally, we perform simulations in
ns-2; the results indicate that our approach outperforms a recently proposed
hybrid streaming system.

ABSTRACT_BEGIN
  One of the big challenges in ad hoc network design is packet routing. Studies
have shown that on-demand routing protocols perform better than table-driven
routing protocols. In order to avoid route discovery for each packet, on-demand
routing protocols cache routes previously learnt. A node in ad hoc network
learns routing information by overhearing or forwarding packets to other nodes
and keep learned routes in its route cache. However, node movement results
broken links and therefore increases risk of cache pollution. Ensuring cache
freshness in on-demand routing protocols, therefore, presents a serious
challenge. A lot of research has been done in route cache organization,
however, little effort has been done for route cache timeout policy to prevent
stale routes from being used. In this paper we propose a new cross-layer
framework to improve route cache performance in on-demand routing protocols.
The proposed framework presents novel use of Received Signal Strength Indicator
(RSSI) information to choose cache timeout of individual links in route cache.

ABSTRACT_BEGIN
  Loss tomography has received considerable attention in recent years and a
number of estimators have been proposed. Unfortunately, almost all of them are
devoted to the tree topology despite the general topology is more common in
practice. In addition, most of the works presented in the literature rely on
iterative approximation to search for the maximum of a likelihood function
formed from observations, which have been known neither scalable nor efficient.
In contrast to the tree topology, there is few paper dedicated to the general
topology because of the lack of understanding the impacts created by the probes
sent by different sources. We in this paper present the analytical results
obtained recently for the general topology that show the correlation created by
the probes sent by multiple sources to a node located in an intersection of
multiple trees. The correlation is expressed by a set of polynomials of the
pass rates of the paths connecting the sources to the node. In addition to the
expression, a closed form solution is proposed to obtain the MLE of the pass
rates of the paths connecting the sources to the node. Then, two strategies are
proposed to estimate the loss rate of a link for the general topology: one is
path-based and the other is link-based, depending on whether we need to obtain
the pass rate of a path first. The two strategies are compared in the context
of the general topology that shows each has its advantages and the link-based
one is more general. Apart from proving the estimates obtained are the MLEs, we
prove the estimator presented here has the optimal asymptotic property.

ABSTRACT_BEGIN
  Mobile ad-hoc network (MANET) is infrastructureless, self-organizable, multi
hop packet switched network. A number of routing protocols for MANETs have been
proposed in recent years. Dynamic Source Routing (DSR) protocol is one of the
most popular routing protocol for ad hoc networks. This paper presents a novel
method to enhance route maintenance part of DSR protocol. Our proposed route
maintenance significantly increases the efficiency of the protocol at the time
of route failures.

ABSTRACT_BEGIN
  Many mobile ad hoc network protocols use simple flooding, in order to adapt
to changes in time varying network topology. Most of the times, a network-wide
flood results in redundant packets and increases network congestion,
probability of packet collision, low utilization of available bandwidth, and
most important, higher power consumption. In this paper, we propose a new
cross-layer broadcast scheme to minimize broadcast traffic in mobile ad hoc
networks. Our scheme is based on use of received signal strength indicator,
RSSI, value to reduce the number of broadcast packets. The effectiveness of the
proposed technique is verified using simulations.

ABSTRACT_BEGIN
  Conserving power in mobile ad-hoc and sensor networks is a big challenge.
Most of the nodes in these networks, in general, are battery powered,
therefore, an efficient power saving protocol is required to extend the
lifetime of such networks. A lot of work has been done and several protocols
have been proposed to address this problem. Gossip based protocols, which are
based on the results of percolation theory, significantly reduce power
consumption with very little implementation overhead. However, not much work
has been done to make gossiping battery aware. In this paper we introduce a
simple gossip based battery aware sleep protocol. The protocol allows low
battery nodes to sleep more, therefore, improves overall network lifetime.

ABSTRACT_BEGIN
  In this report we propose a MultiObjective (MO) performance evaluation
framework for wireless ad hoc networks where criteria such as capacity,
robustness, energy and delay are optimized concurrently. Within such a
framework, we can determine both the Pareto-optimal performance bounds and the
networking parameters that provide these bounds. The originality of this
approach is that it accounts for the inherent broadcast properties of the
transmission and finely models the interference distribution. In the proposed
model, the network performance can be optimized when several flows (source-
destination transmissions) exist. One benefit of our approach is that the
complexity does not grow with the number of flows. The other major contribution
of this paper is the new analytical formulation of the performance metrics. It
relies on a matrix representation of the constraints imposed by the
interference- limited and broadcast wireless channel. Because of the similarity
of this matrix with a Markovian transition matrix, we can exploit classical
results from Markov chains theory to derive steady state performance metrics
relative to capacity, robustness, energy and delay. Another very interesting
feature of these new metrics is that the Pareto-optimal solutions related to
them provide a tight bound on capacity, robustness, energy and delay.

ABSTRACT_BEGIN
  In this paper an improved version of the graded precision localization
algorithm GRADELOC, called IGRADELOC is proposed. The performance of GRADELOC
is dependent on the regions formed by the overlapping radio ranges of the nodes
of the underlying sensor network. A different region pattern could
significantly alter the nature and precision of localization. In IGRADELOC, two
improvements are suggested. Firstly, modifications are proposed in the radio
range of the fixed-grid nodes, keeping in mind the actual radio range of
commonly available nodes, to allow for routing through them. Routing is not
addressed by GRADELOC, but is of prime importance to the deployment of any
adhoc network, especially sensor networks. A theoretical model expressing the
radio range in terms of the cell dimensions of the grid infrastructure is
proposed, to help in carrying out a deployment plan which achieves the
desirable precision of coarse-grained localization. Secondly, in GRADELOC it is
observed that fine-grained localization does not achieve significant
performance benefits over coarse-grained localization. In IGRADELOC, this
factor is addressed with the introduction of a parameter that could be used to
improve and fine-tune the precision of fine-grained localization.

ABSTRACT_BEGIN
  We study the problem of tracking an object moving through a network of
wireless sensors. In order to conserve energy, the sensors may be put into a
sleep mode with a timer that determines their sleep duration. It is assumed
that an asleep sensor cannot be communicated with or woken up, and hence the
sleep duration needs to be determined at the time the sensor goes to sleep
based on all the information available to the sensor. Having sleeping sensors
in the network could result in degraded tracking performance, therefore, there
is a tradeoff between energy usage and tracking performance. We design sleeping
policies that attempt to optimize this tradeoff and characterize their
performance. As an extension to our previous work in this area [1], we consider
generalized models for object movement, object sensing, and tracking cost. For
discrete state spaces and continuous Gaussian observations, we derive a lower
bound on the optimal energy-tracking tradeoff. It is shown that in the low
tracking error regime, the generated policies approach the derived lower bound.

ABSTRACT_BEGIN
  It is well known that links in CSMA wireless networks are prone to
starvation. Prior works focused almost exclusively on equilibrium starvation.
In this paper, we show that links in CSMA wireless networks are also
susceptible to temporal starvation. Specifically, although some links have good
equilibrium throughputs and do not suffer from equilibrium starvation, they can
still have no throughput for extended periods from time to time. Given its
impact on quality of service, it is important to understand and characterize
temporal starvation. To this end, we develop a "trap theory" to analyze
temporal throughput fluctuation. The trap theory serves two functions. First,
it allows us to derive new mathematical results that shed light on the
transient behavior of CSMA networks. For example, we show that the duration of
a trap, during which some links receive no throughput, is insensitive to the
distributions of the backoff countdown and transmission time (packet duration)
in the CSMA protocol. Second, we can develop analytical tools for computing the
"degrees of starvation" for CSMA networks to aid network design. For example,
given a CSMA network, we can determine whether it suffers from starvation, and
if so, which links will starve. Furthermore, the likelihood and durations of
temporal starvation can also be computed. We believe that the ability to
identify and characterize temporal starvation as established in this paper will
serve as an important first step toward the design of effective remedies for
it.

ABSTRACT_BEGIN
  We have explored our own innovative work about the design & development of
internal location-identification system for mobile devices based on integration
of RFID and wireless technology. The function of our system is based on
strategically located passive RFID tags placed on objects around building which
are identified using an RFID reader attached to a mobile device. The mobile
device reads the RFID tag and through the wireless network, sends the request
to the server. The server resolves the request and sends the desired
location-based information back to the mobile device. We had addressed that we
can go through the RFID technology for internal location identification
(indoor), which provides us better location accuracy because of no contact
between the tag and the reader, and the system requires no line of sight. In
this paper we had also focused on the issues of RFID technologies i.e.
Non-line-of-sight & High inventory speeds.

ABSTRACT_BEGIN
  In this paper, we consider the problem of modelling the average delay
experienced by a packet in a single cell IEEE 802.11 DCF wireless local area
network. The packet arrival process at each node i is assumed to be Poisson
with rate parameter \lambda_i. Since the nodes are sharing a single channel,
they have to contend with one another for a successful transmission. The mean
delay for a packet has been approximated by modelling the system as a 1-limited
Random Polling system with zero switchover time. We show that even for
non-homogeneous packet arrival processes, the mean delay of packets across the
queues are same and depends on the system utilization factor and the aggregate
throughput of the MAC. Extensive simulations are conducted to verify the
analytical results.

ABSTRACT_BEGIN
  A fundamental choice in femtocell deployments is the set of users which are
allowed to access each femtocell. Closed access restricts the set to
specifically registered users, while open access allows any mobile subscriber
to use any femtocell. Which one is preferable depends strongly on the distance
between the macrocell base station (MBS) and femtocell. The main results of the
paper are lemmas which provide expressions for the SINR distribution for
various zones within a cell as a function of this MBS-femto distance. The
average sum throughput (or any other SINR-based metric) of home users and
cellular users under open and closed access can be readily determined from
these expressions. We show that unlike in the uplink, the interests of home and
cellular users are in conflict, with home users preferring closed access and
cellular users preferring open access. The conflict is most pronounced for
femtocells near the cell edge, when there are many cellular users and fewer
femtocells. To mitigate this conflict, we propose a middle way which we term
shared access in which femtocells allocate an adjustable number of time-slots
between home and cellular users such that a specified minimum rate for each can
be achieved. The optimal such sharing fraction is derived. Analysis shows that
shared access achieves at least the overall throughput of open access while
also satisfying rate requirements, while closed access fails for cellular users
and open access fails for the home user.

ABSTRACT_BEGIN
  Nowadays, mobile phones are indispensable devices; it has become a trend now
that college and university students are owners of such devices and this
particular factor plays a very important role behind the coming up with the
proposed system. "PC 2 Phone event Announcer", is the name of the new proposed
system suggested to solve the existing communication problem between the
College staff and students. As the name suggests, it can be deduced that the
system will involve computers and phones, more specifically mobile phones.

ABSTRACT_BEGIN
  The following paper presents various methods and implementation techniques
used to harvest metadata efficiently from a Kademlia Distributed Hashtable
(DHT) as used in the BitTorrent P2P network to build an index of publicly
available files in the BitTorrent ecosystem. The indexer design makes various
tradeoffs between throughput and fairness towards other DHT nodes while also
being scaleable in a distributed environment.

ABSTRACT_BEGIN
  It has been recently brought into spotlight that through the exploitation of
network coding concepts at physical-layer, the interference property of the
wireless media can be proven to be a blessing in disguise. Nonetheless, most of
the previous studies on this subject have either held unrealistic assumptions
about the network properties, thus making them basically theoretical, or have
otherwise been limited to fairly simple network topologies. We, on the other
hand, believe to have devised a novel scheme, called Real Amplitude Scaling
(RAS), that relaxes the aforementioned restrictions, and works with a wider
range of network topologies and in circumstances that are closer to practice,
for instance in lack of symbol-level synchronization and in the presence of
noise, channel distortion and severe interference from other sources. The
simulation results confirmed the superior performance of the proposed method in
low SNRs, as well as the high SNR limits, where the effect of quantization
error in the digital techniques becomes comparable to the channel.

ABSTRACT_BEGIN
  Multi-cell processing, also called Coordinated Multiple Point (CoMP), is a
very promising distributed multi-antennas technique that uses neighbour cell's
antennas. This is expected to be part of next generation cellular networks
standards such as LTE-A. Small cell networks in dense urban environment are
mainly limited by interferences and CoMP can strongly take advantage of this
fact to improve cell-edge users' throughput. This paper provides an analytical
derivation of the capacity outage probability for CoMP experiencing fast
Rayleigh fading. Only the average received power (slow varying fading) has to
be known, and perfect Channel State Information (CSI) is not required. An
optimisation of the successfully received data-rate is then derived with
respect to the number of cooperating stations and the outage probability,
illustrated by numerical examples.

ABSTRACT_BEGIN
  We address the problem of opportunistic multiuser scheduling in downlink
networks with Markov-modeled outage channels. We consider the scenario in which
the scheduler does not have full knowledge of the channel state information,
but instead estimates the channel state information by exploiting the memory
inherent in the Markov channels along with ARQ-styled feedback from the
scheduled users. Opportunistic scheduling is optimized in two stages: (1)
Channel estimation and rate adaptation to maximize the expected immediate rate
of the scheduled user; (2) User scheduling, based on the optimized immediate
rate, to maximize the overall long term sum-throughput of the downlink. The
scheduling problem is a partially observable Markov decision process with the
classic 'exploitation vs exploration' trade-off that is difficult to quantify.
We therefore study the problem in the framework of Restless Multi-armed Bandit
Processes (RMBP) and perform a Whittle's indexability analysis. Whittle's
indexability is traditionally known to be hard to establish and the index
policy derived based on Whittle's indexability is known to have optimality
properties in various settings. We show that the problem of downlink scheduling
under imperfect channel state information is Whittle indexable and derive the
Whittle's index policy in closed form. Via extensive numerical experiments, we
show that the index policy has near-optimal performance.
  Our work reveals that, under incomplete channel state information, exploiting
channel memory for opportunistic scheduling can result in significant
performance gains and that almost all of these gains can be realized using an
easy-to-implement index policy.

ABSTRACT_BEGIN
  The MIMO wireless channel offers a rich ground for quality of service
analysis. In this work, we present a stochastic network calculus analysis of a
MIMO system, operating in spatial multiplexing mode, using moment generating
functions (MGF). We quantify the spatial multiplexing gain, achieved through
multiple antennas, for flow level quality of service (QoS) performance.
Specifically we use Gilbert-Elliot model to describe individual spatial paths
between the antenna pairs and model the whole channel by an N-State Markov
Chain, where N depends upon the degrees of freedom available in the MIMO
system. We derive probabilistic delay bounds for the system and show the impact
of increasing the number of antennas on the delay bounds under various
conditions, such as channel burstiness, signal strength and fading speed.
Further we present results for multi-hop scenarios under statistical
independence.

ABSTRACT_BEGIN
  By combining the features of CSMA and TDMA, fully decentralised WLAN MAC
schemes have recently been proposed that converge to collision-free schedules.
In this paper we describe a MAC with optimal long-run throughput that is almost
decentralised. We then design two \changed{schemes} that are practically
realisable, decentralised approximations of this optimal scheme and operate
with different amounts of sensing information. We achieve this by (1)
introducing learning algorithms that can substantially speed up convergence to
collision free operation; (2) developing a decentralised schedule length
adaptation scheme that provides long-run fair (uniform) access to the medium
while maintaining collision-free access for arbitrary numbers of stations.

ABSTRACT_BEGIN
  The U.S. Air Force currently is in the process of developing an Airborne
Network (AN) to provide support to its combat aircrafts on a mission. The
reliability needed for continuous operation of an AN is difficult to achieve
through completely infrastructure-less mobile ad hoc networks. In this paper we
first propose an architecture for an AN where airborne networking platforms
(ANPs - aircrafts, UAVs and satellites) form the backbone of the AN. In this
architecture, the ANPs can be viewed as mobile base stations and the combat
aircrafts on a mission as mobile clients. The combat aircrafts on a mission
move through a space called air corridor. The goal of the AN design is to form
a backbone network with the ANPs with two properties: (i) the backbone network
remains connected at all times, even though the topology of the network changes
with the movement of the ANPs, and (ii) the entire 3D space of the air corridor
is under radio coverage at all times by the continuously moving ANPs.
  In addition to proposing an architecture for an AN, the contributions of the
paper include, development of an algorithm that finds the velocity and
transmission range of the ANPs so that the dynamically changing backbone
network remains connected at all times, development of a routing algorithm that
ensures a connection between the source-destination node pair with the fewest
number of path switching, given the dimensions of the air corridor and the
radius of the coverage sphere associated with an ANP, development of an
algorithm that finds the fewest number of ANPs required to provide complete
coverage of the air corridor at all times, development of an algorithm that
provides connected-coverage to the air corridor at all times, and development
of a visualization tool that depicts the movement patterns of the ANPs and the
resulting dynamic graph and the coverage volume of the backbone network.

ABSTRACT_BEGIN
  This paper presents a TDMA based energy efficient cognitive radio
multichannel medium access control (MAC) protocol called ECR-MAC for wireless
Ad Hoc Networks. ECR-MAC requires only a single half-duplex radio transceiver
on each node that integrates the spectrum sensing at physical (PHY) layer and
the packet scheduling at MAC layer. In addition to explicit frequency
negotiation which is adopted by conventional multichannel MAC protocols,
ECR-MAC introduces lightweight explicit time negotiation. This two-dimensional
negotiation enables ECR-MAC to exploit the advantage of both multiple channels
and TDMA, and achieve aggressive power savings by allowing nodes that are not
involved in communication to go into doze mode. The IEEE 802.11 standard allows
for the use of multiple channels available at the PHY layer, but its MAC
protocol is designed only for a single channel. A single channel MAC protocol
does not work well in a multichannel environment, because of the multichannel
hidden terminal problem. The proposed energy efficient ECR-MAC protocol allows
SUs to identify and use the unused frequency spectrum in a way that constrains
the level of interference to the primary users (PUs). Extensive simulation
results show that our proposed ECR-MAC protocol successfully exploits multiple
channels and significantly improves network performance by using the licensed
spectrum band opportunistically and protects QoS provisioning over cognitive
radio ad hoc networks.

ABSTRACT_BEGIN
  This paper proposes a cross-layer based cognitive radio multichannel medium
access control (MAC) protocol with TDMA, which integrate the spectrum sensing
at physical (PHY) layer and the packet scheduling at MAC layer, for the ad hoc
wireless networks. The IEEE 802.11 standard allows for the use of multiple
channels available at the PHY layer, but its MAC protocol is designed only for
a single channel. A single channel MAC protocol does not work well in a
multichannel environment, because of the multichannel hidden terminal problem.
Our proposed protocol enables secondary users (SUs) to utilize multiple
channels by switching channels dynamically, thus increasing network throughput.
In our proposed protocol, each SU is equipped with only one spectrum agile
transceiver, but solves the multichannel hidden terminal problem using temporal
synchronization. The proposed cognitive radio MAC (CR-MAC) protocol allows SUs
to identify and use the unused frequency spectrum in a way that constrains the
level of interference to the primary users (PUs). Our scheme improves network
throughput significantly, especially when the network is highly congested. The
simulation results show that our proposed CR-MAC protocol successfully exploits
multiple channels and significantly improves network performance by using the
licensed spectrum band opportunistically and protects PUs from interference,
even in hidden terminal situations.

ABSTRACT_BEGIN
  Rapid progress in microelectromechanical system (MEMS) and radio frequency
(RF) design has enabled the development of low-power, inexpensive, and
network-enabled microsensors. These sensor nodes are capable of capturing
various physical information, such as temperature, pressure, motion of an
object, etc as well as mapping such physical characteristics of the environment
to quantitative measurements. A typical wireless sensor network (WSN) consists
of hundreds to thousands of such sensor nodes linked by a wireless medium. In
this paper, we present a comparative investigation of energy consumption for
few commercially available chipsets such as TR1001, CC1000 and CC1010 based on
different scheduling methods for two types of deployment strategies. We
conducted our experiment within the OMNeT++ simulator.

ABSTRACT_BEGIN
  In P2P systems, large volumes of data are declustered naturally across a
large number of peers. But it is very difficult to control the initial data
distribution because every user has the freedom to share any data with other
users. The system scalability can be improved by distributing the load across
multiple servers which is proposed by replication. The large scale content
distribution systems were improved broadly using the replication techniques.
The demanded contents can be brought closer to the clients by multiplying the
source of information geographically, which in turn reduce both the access
latency and the network traffic. In addition to this, due to the intrinsic
dynamism of the P2P environment, static data distribution cannot be expected to
guarantee good load balancing. If the hot peers become bottleneck, it leads to
increased user response time and significant performance degradation of the
system. Hence an effective load balancing mechanism is necessary in such cases
and it can be attained efficiently by intelligent data replication. In this
paper, we propose a cluster based replication architecture for load-balancing
in peer-to-peer content distribution systems. In addition to an intelligent
replica placement technique, it also consists of an effective load balancing
technique. In the intelligent replica placement technique, peers are grouped
into strong and weak clusters based on their weight vector which comprises
available capacity, CPU speed, access latency and memory size. In order to
achieve complete load balancing across the system, an intracluster and
inter-cluster load balancing algorithms are proposed. We are able to show that
our proposed architecture attains less latency and better throughput with
reduced bandwidth usage, through the simulation results.

ABSTRACT_BEGIN
  Achieving optimal transmission throughput in data networks in a multi-hop
wireless networks is fundamental but hard problem. The situation is aggravated
when nodes are mobile. Further, multi-rate system make the analysis of
throughput more complicated. In mobile scenario, link may break or be created
as nodes are moving within communication range. `Route Discovery' which is to
find the optimal route and transmission schedule is an important issue. Route
discovery entails some cost; so one would not like to initiate discovery too
often. On the other hand, not discovering reasonably often entails the risk of
being stuck with a suboptimal route and/or schedule, which hurts end-to-end
throughput. The implementation of the routing decision problem in one
dimensional mobile ad hoc network as Markov decision process problem is already
is discussed in the paper [1]. A heuristic based on threshold policy is
discussed in the same paper without giving a way to find the threshold. In this
paper, we suggested a rule for setting the threshold, given the parameters of
the system. We also point out that our results remain valid in a slightly
different mobility model; this model is a first step towards an `open' network
in which existing relay nodes can leave and/or new relay nodes can join the
network.

ABSTRACT_BEGIN
  In a multirate WLAN with a single access point (AP) and several stations
(STAs), we obtain analytical expressions for TCP-controlled long file transfer
throughputs allowing nonzero propagation delays between the file server and
STAs. We extend our earlier work in [3] to obtain AP and STA throughputs in a
multirate WLAN, and use these in a closed BCMP queueing network model to obtain
TCP throughputs. Simulation show that our approach is able to predict observed
throughputs with a high degree of accuracy.

ABSTRACT_BEGIN
  Routing algorithms for wireless sensor networks can be broadly divided into
two classes - proactive and reactive. Proactive routing is suitable for a
network with a fixed topology. On the other hand, reactive routing is more
suitable for a set of mobile nodes where routes are created on demand and there
is not much time to evaluate the worthiness of a route, the prime concern being
reachability due to constantly changing node positions. Sensor networks route
events of interest from source(s) to destination(s) where appropriate actions
could be taken. However, with mobile sensor nodes, it is not only important to
know the events but the location of the nodes generating the events. Most
sensor nodes are not equipped with expensive GPS or accurate RSSI computation
hardware to aid localization. Keeping these in view, we propose a modified
reactive routing algorithm, with added support for localization, to localize
mobile sensor nodes on the basis of information received from fixed sensor
nodes during mutual exchange of routing control packets. The accuracy of
localization depends on the ratio of the number of fixed nodes to the number of
mobile nodes and the topology of the fixed nodes. A typical application
scenario would be a mix of mobile nodes and fixed nodes, where fixed nodes know
their absolute location and the location of mobile nodes is derived from the
fixed nodes, in step with the reactive routing protocol in action. The modified
algorithm would be suitable for deployments where the approximate position of a
mobile node (i.e. the event location) is required but there is no external
support infrastructure available for localization.

ABSTRACT_BEGIN
  We analyze TCP-controlled bulk file transfers in a single station (STA) WLAN
with nonzero propagation delay between the file server and the WLAN. Our
approach is to model the flow of packets as a closed queueing network (BCMP
network) with 3 service centres, one each for the Access Point (AP) and the
STA, and the third for the propagation delay. The service rates of the first
two are obtained by analyzing the WLAN MAC. Simulations show a very close match
with the theory.

ABSTRACT_BEGIN
  In this paper, we consider the problem of modelling the average delay
experienced by an application packets of variable length in a single cell IEEE
802.11 DCF wireless local area network. The packet arrival process at each node
i is assumed to be a stationary and independent increment random process with
mean ai and second moment a(2) i . The packet lengths at node i are assumed to
be i.i.d random variables Pi with finite mean and second moment. A closed form
expression has been derived for the same. We assume the input arrival process
across queues to be uncorrelated Poison processes. As the nodes share a single
channel, they have to contend with one another for a successful transmission.
The mean delay for a packet has been approximated by modelling the system as a
1-limited Random Polling system with zero switchover times. Extensive
simulations are conducted to verify the analytical results.

ABSTRACT_BEGIN
  Studying animal movement and distribution is of critical importance to
addressing environmental challenges including invasive species, infectious
diseases, climate and land-use change. Motion sensitive camera traps offer a
visual sensor to record the presence of a broad range of species providing
location -specific information on movement and behavior. Modern digital camera
traps that record video present new analytical opportunities, but also new data
management challenges. This paper describes our experience with a terrestrial
animal monitoring system at Barro Colorado Island, Panama. Our camera network
captured the spatio-temporal dynamics of terrestrial bird and mammal activity
at the site - data relevant to immediate science questions, and long-term
conservation issues. We believe that the experience gained and lessons learned
during our year long deployment and testing of the camera traps as well as the
developed solutions are applicable to broader sensor network applications and
are valuable for the advancement of the sensor network research. We suggest
that the continued development of these hardware, software, and analytical
tools, in concert, offer an exciting sensor-network solution to monitoring of
animal populations which could realistically scale over larger areas and time
spans.

ABSTRACT_BEGIN
  The OSI model, developed by ISO in 1984, attempts to summarize complicated
network cases on layers. Moreover, network troubles are expressed by taking the
model into account. However, there has been no standardization for network
troubles up to now. Network troubles have only been expressed by the name of
the related layer. In this paper, it is pointed out that possible troubles on
the related layer vary and possible troubles on each layer are categorized for
functional network administration and they are standardized in an eligible way.
The proposed model for network trouble shooting was developed considering the
OSI model.

ABSTRACT_BEGIN
  A crucial issue for a mobile ad hoc network is the handling of a large number
of nodes. As more nodes join the mobile ad hoc network, contention and
congestion are more likely. The on demand routing protocols which broadcasts
control packets to discover routes to the destination nodes, generate a high
number of broadcast packets in a larger networks causing contention and
collision. We propose an efficient route discovery protocol, which reduces the
number of broadcast packet, using controlled flooding technique. The simulation
results show that the proposed probabilistic flooding decreases the number of
control packets floating in the network during route discovery phase, without
lowering the success ratio of path discoveries. Furthermore, the proposed
method adapts to the normal network conditions. The results show that up to 70%
of control packet traffic is saved in route discovery phase when the network is
denser.

ABSTRACT_BEGIN
  This paper proposes a fair and efficient QoS scheduling scheme for IEEE
802.16 BWA systems that satisfies both throughput and delay guarantee to
various real and non-real time applications. The proposed QoS scheduling scheme
is compared with an existing QoS scheduling scheme proposed in literature in
recent past. Simulation results show that the proposed scheduling scheme can
provide a tight QoS guarantee in terms of delay, delay violation rate and
throughput for all types of traffic as defined in the WiMAX standard, thereby
maintaining the fairness and helps to eliminate starvation of lower priority
class services. Bandwidth utilization of the system and fairness index of the
resources are also encountered to validate the QoS provided by our proposed
scheduling scheme.

ABSTRACT_BEGIN
  In this paper we address the problem of prolonging the lifetime of wireless
sensor networks (WSNs) deployed to monitor an area of interest. In this
scenario, a helpful approach is to reduce coverage redundancy and therefore the
energy expenditure due to coverage. We introduce the first algorithm which
reduces coverage redundancy by means of Sensor Activation and sensing Radius
Adaptation (SARA)in a general applicative scenario with two classes of devices:
sensors that can adapt their sensing range (adjustable sensors) and sensors
that cannot (fixed sensors). In particular, SARA activates only a subset of all
the available sensors and reduces the sensing range of the adjustable sensors
that have been activated. In doing so, SARA also takes possible heterogeneous
coverage capabilities of sensors belonging to the same class into account. It
specifically addresses device heterogeneity by modeling the coverage problem in
the Laguerre geometry through Voronoi-Laguerre diagrams.
  SARA executes quickly and is guaranteed to terminate. It provides a
configuration of the active set of sensors that lifetime and coverage
requirements of demanding WSN applications, not met by current solutions.
  By means of extensive simulations we show that SARA achieves a network
lifetime that is significantly superior to that obtained by previous algorithms
in all the considered scenarios.

ABSTRACT_BEGIN
  In this paper, we propose an architecture for Network of Information mobile
node (NetInf MN). It bears characteristics and features of basic NetInf node
architecture with features introduced in the LISP MN architecture. We also
introduce a virtual node layer for mobility management in the Network of
Information. Therefore, by adopting this architecture no major changes in the
contemporary network topologies is required. Thus, making our approach more
practical.

ABSTRACT_BEGIN
  This paper introduces the security and trust concepts in wireless sensor
networks and explains the difference between them, stating that even though
both terms are used interchangeably when defining a secure system, they are not
the same. The difference between reputation and trust is also explained,
highlighting that reputation partially affects trust. A survey of trust and
reputation systems in various domains is conducted, with more details given to
models in ad-hoc and sensor networks as they are closely related to each other
and to our research interests. The methodologies used to model trust and their
references are presented. The factors affecting trust updating are summarised
and some examples of the systems in which these factors have been implemented
are given. The survey states that, even though researchers have started to
explore the issue of trust in wireless sensor networks, they are still
examining the trust associated with routing messages between nodes (binary
events). However, wireless sensor networks are mainly deployed to monitor
events and report data, both continuous and discrete. This leads to the
development of new trust models addressing the continuous data issue and also
to combine the data trust and the communication trust to infer the total trust.

ABSTRACT_BEGIN
  This article has been withdrawn by arXiv administrators because it
plagiarises http://www2.ece.ohio-state.edu/~ekici/papers/crnroutingsurvey.pdf

ABSTRACT_BEGIN
  The Session Initiation Protocol (SIP) server overload management has
attracted interest since SIP is being widely deployed in the Next Generation
Networks (NGN) as a core signaling protocol. Yet all existing SIP overload
control work is focused on SIP-over-UDP, despite the fact that TCP is
increasingly seen as the more viable choice of SIP transport. This paper
answers the following questions: is the existing TCP flow control capable of
handling the SIP overload problem? If not, why and how can we make it work? We
provide a comprehensive explanation of the default SIP-over-TCP overload
behavior through server instrumentation. We also propose and implement novel
but simple overload control algorithms without any kernel or protocol level
modification. Experimental evaluation shows that with our mechanism the
overload performance improves from its original zero throughput to nearly full
capacity. Our work leads to the important general insight that the traditional
notion of TCP flow control alone is incapable of managing overload for
time-critical session-based applications, which would be applicable not only to
SIP, but also to a wide range of other common applications such as database
servers.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks (VANET) are expected to have great potential to
improve both traffic safety and comfort in the future. When many vehicles want
to access data through roadside unit, data scheduling become an important
issue. In this paper, we identify some challenges in roadside based data
access. To address these challenges we first review some existing scheduling
schemes. We then propose a priority scheduling and finally show that using this
idea can increase QOS compare to previous algorithms.

ABSTRACT_BEGIN
  Correlation in user connectivity patterns is generally considered a problem
for system designers, since it results in peaks of demand and also in the
scarcity of resources for peer-to-peer applications. The other side of the coin
is that these connectivity patterns are often predictable and that, to some
extent, they can be dealt with proactively.
  In this work, we build predictors aiming to determine the probability that
any given user will be online at any given time in the future. We evaluate the
quality of these predictors on various large traces from instant messaging and
file sharing applications.
  We also illustrate how availability prediction can be applied to enhance the
behavior of peer-to-peer applications: we show through simulation how data
availability is substantially increased in a distributed hash table simply by
adjusting data placement policies according to peer availability prediction and
without requiring any additional storage from any peer.

ABSTRACT_BEGIN
  Applications such as traffic engineering and network provisioning can greatly
benefit from knowing, in real time, what is the largest input rate at which it
is possible to transmit on a given path without causing congestion. We consider
a probabilistic formulation for available bandwidth where the user specifies
the probability of achieving an output rate almost as large as the input rate.
We are interested in estimating and tracking the network-wide probabilistic
available bandwidth (PAB) on multiple paths simultaneously with minimal
overhead on the network. We propose a novel framework based on chirps, Bayesian
inference, belief propagation and active sampling to estimate the PAB. We also
consider the time evolution of the PAB by forming a dynamic model and designing
a tracking algorithm based on particle filters. We implement our method in a
lightweight and practical tool that has been deployed on the PlanetLab network
to do online experiments. We show through these experiments and simulations
that our approach outperforms block-based algorithms in terms of input rate
cost and probability of successful transmission.

ABSTRACT_BEGIN
  In duty-cycled wireless sensor networks, the nodes switch between active and
dormant states, and each node may determine its active/dormant schedule
independently. This complicates the Minimum-Energy Multicasting (MEM) problem,
which has been primarily studied in always-active wireless ad-hoc networks. In
this paper, we study the duty-cycle-aware MEM problem in wireless sensor
networks, and we present a formulation of the Minimum-Energy Multicast Tree
Construction and Scheduling (MEMTCS) problem. We prove that the MEMTCS problem
is NP-hard, and it is unlikely to have an approximation algorithm with a
performance ratio of $(1-o(1))\ln\Delta$, where $\Delta$ is the maximum node
degree in a network. We propose a polynomial-time approximation algorithm for
the MEMTCS problem with a performance ratio of $\mathcal{O}(H(\Delta+1))$,
where $H(\cdot)$ is the harmonic number. We also provide a distributed
implementation of our algorithm. Finally, we perform extensive simulations and
the results demonstrate that our algorithm significantly outperform other known
algorithms in terms of both the total energy cost and the transmission
redundancy.

ABSTRACT_BEGIN
  Research in location determination for GSM phones has gained interest
recently as it enables a wide set of location based services. RSSI-based
techniques have been the preferred method for GSM localization on the handset
as RSSI information is available in all cell phones. Although the GSM standard
allows for a cell phone to receive signal strength information from up to seven
cell towers, many of today's cell phones are low-end phones, with limited API
support, that gives only information about the associated cell tower. In
addition, in many places in the world, the density of cell towers is very small
and therefore, the available cell tower information for localization is very
limited. This raises the challenge of accurately determining the cell phone
location with very limited information, mainly the RSSI of the associated cell
tower. In this paper we propose a Hidden Markov Model based solution that
leverages the signal strength history from only the associated cell tower to
achieve accurate GSM localization. We discuss the challenges of implementing
our system and present the details of our system and how it addresses the
challenges. To evaluate our proposed system, we implemented it on Androidbased
phones. Results for two different testbeds, representing urban and rural
environments, show that our system provides at least 156% enhancement in median
error in rural areas and at least 68% enhancement in median error in urban
areas compared to current RSSI-based GSM localization systems

ABSTRACT_BEGIN
  Reduction of unnecessary energy consumption is becoming a major concern in
wired networking, because of the potential economical benefits and of its
expected environmental impact. These issues, usually referred to as "green
networking", relate to embedding energy-awareness in the design, in the devices
and in the protocols of networks. In this work, we first formulate a more
precise definition of the "green" attribute. We furthermore identify a few
paradigms that are the key enablers of energy-aware networking research. We
then overview the current state of the art and provide a taxonomy of the
relevant work, with a special focus on wired networking. At a high level, we
identify four branches of green networking research that stem from different
observations on the root causes of energy waste, namely (i) Adaptive Link Rate,
(ii) Interface proxying, (iii) Energy-aware infrastructures and (iv)
Energy-aware applications. In this work, we do not only explore specific
proposals pertaining to each of the above branches, but also offer a
perspective for research.

ABSTRACT_BEGIN
  Rate adaptation plays a key role in determining the performance of wireless
LANs. In this paper, we introduce a semi-Markovian framework to analyze the
performance of two of the most popular rate adaptation algorithms used in
wireless LANs, namely Automatic Rate Fallback (ARF) and Adaptive Automatic Rate
Fallback (AARF). Given our modeling assumptions, the analysis is exact and
provides closed form expressions for the achievable throughput of ARF and AARF.
We illustrate the benefit of our analysis by numerically comparing the
throughput performance of ARF and AARF in two different channel regimes. The
results show that neither of these algorithms consistently outperforms the
other. We thus propose and analyze a new variant to AARF, called Persistent
AARF (or PAARF), and show that it achieves a good compromise between the two
algorithms, often performing close to the best algorithm in each of the studied
regimes. The numerical results also shed light into the impact of MAC overhead
on the performance of the three algorithms. In particular, they show that the
more conservative strategies AARF and PAARF scale better as the bit rate
increases.

ABSTRACT_BEGIN
  The natural or man-made disaster demands an efficient communication and
coordination among first responders to save life and other community resources.
Normally, the traditional communication infrastructures such as land line or
cellular networks are damaged and don't provide adequate communication services
to first responders for exchanging emergency related information. Wireless ad
hoc networks such as mobile ad hoc networks, wireless sensor networks and
wireless mesh networks are the promising alternatives in such type of
situations. The security requirements for emergency response communications
include privacy, data integrity, authentication, key management, access control
and availability. Various ad hoc communication frameworks have been proposed
for emergency response situations. The majority of the proposed frameworks
don't provide adequate security services for reliable and secure information
exchange. This paper presents a survey of the proposed emergency response
communication frameworks and the potential security services required by them
to provide reliable and secure information exchange during emergency
situations.

ABSTRACT_BEGIN
  Contemporary personal computing devices are increasingly required to be
portable and mobile enabling user's wireless access, to wired network
infrastructures and services. This approach to mobile computing and
communication is only appropriate in situations where a coherent infrastructure
is available. There are many situations where these requirements are not
fulfilled such as; developing nations, rural areas, natural disasters, and
military conflicts to name but a few. A practical solution is to use mobile
devices interconnected via a wireless medium to form a network, known as a
Mobile Ad-hoc Network (MANET), and provide the services normally found in wired
networks. Security in MANETs is an issue of paramount importance due to the
wireless nature of the communication links. Additionally due to the lack of
central administration security issues are different from conventional
networks. For the purposes of this article we have used the "WMN test-bed" to
enable secure routing in MANETs. The use of cryptography is an efficient proven
way of securing data in communications, but some cryptographic algorithms are
not as efficient as others and require more processing power, which is
detrimental to MANETs. In this article we have assessed different cryptographic
approaches to securing the OLSR (Optimised Link State Routing) protocol to
provide a basis for research. We conclude the paper with a series of
performance evaluation results regarding different cryptographic and hashing
schemes. Our findings clearly show that the most efficient combination of
algorithms used for authentication and encryption are SHA-1 and AES
respectively. Using this combination over their counterparts will lead to a
considerable reduction in processing time and delay on the network, creating an
efficient transaction moving towards satisfying resource constraints and
security requirements.

ABSTRACT_BEGIN
  Low-power and Lossy Networks (LLNs), like wireless networks based upon the
IEEE 802.15.4 standard, have strong energy constraints, and are moreover
subject to frequent transmission errors, not only due to congestion but also to
collisions and to radio channel conditions. This paper introduces an analytical
model to compute the total energy consumption in an LLN due to the TCP
protocol. The model allows us to highlight some tradeoffs as regards the choice
of the TCP maximum segment size, of the Forward Error Correction (FEC)
redundancy ratio, and of the number of link-layer retransmissions, in order to
minimize the total energy consumption.

ABSTRACT_BEGIN
  In this paper the problem of scheduling with power control in wireless
networks is studied: given a set of communication requests, one needs to assign
the powers of the network nodes, and schedule the transmissions so that they
can be done in a minimum time, taking into account the signal interference of
concurrently transmitting nodes. The signal interference is modeled by SINR
constraints. Approximation algorithms are given for this problem, which use the
mean power assignment. The problem of schduling with fixed mean power
assignment is also considered, and approximation guarantees are proven.

ABSTRACT_BEGIN
  BitTorrent has recently introduced LEDBAT, a novel application-layer
congestion control protocol for data exchange. The protocol design starts from
the assumption that network bottlenecks are at the access of the network, and
that thus user traffic competes creating self-inducing congestion. To relieve
from this phenomenon, LEDBAT is designed to quickly infer that self-induced
congestion is approaching (by detecting relative changes of the one-way delay
in the transmission path), and to react by reducing the sending rate prior that
congestion occurs. Prior work has however shown LEDBAT to be affected by a
latecomer advantage, where newly arriving connections can starve already
existing flows. In this work, we propose modifications to the congestion window
update mechanism of the LEDBAT protocol that aim at solving this issue,
guaranteeing thus intra-protocol fairness and efficiency. Closed-form
expressions for the stationary throughput and queue occupancy are provided via
a fluid model, whose accuracy is confirmed by means of ns2 packet level
simulations. Our results show that the proposed change can effective solve the
latecomer issue, without affecting the other original LEDBAT goals at the same
time.

ABSTRACT_BEGIN
  Mobile Ad Hoc Network (MANET) is a collection of nodes that can be rapidly
deployed as a multi-hop network without the aid of any centralized
administration. Misbehavior is challenged by bandwidth and energy efficient
medium access control and fair share of throughput. Node misbehavior plays an
important role in MANET. In this survey, few of the contention window
misbehavior is reviewed and compared. The contention window cheating either
minimizes the active communication of the network or reduces bandwidth
utilization of a particular node. The classification presented is in no case
unique but summarizes the chief characteristics of many published proposals for
contention window cheating. After getting insight into the different contention
window misbehavior, few of the enhancements that can be done to improve the
existing contention window are suggested. The purpose of this paper is to
facilitate the research efforts in combining the existing solutions to offer
more efficient methods to reduce contention window cheating mechanisms.

ABSTRACT_BEGIN
  We study the optimal usage-based pricing problem in a resource-constrained
network with one profit-maximizing service provider and multiple groups of
surplus-maximizing users. With the assumption that the service provider knows
the utility function of each user (thus complete information), we find that the
complete price differentiation scheme can achieve a large revenue gain (e.g.,
50%) compared to no price differentiation, when the total network resource is
comparably limited and the high willingness to pay users are minorities.
However, the complete price differentiation scheme may lead to a high
implementational complexity. To trade off the revenue against the
implementational complexity, we further study the partial price differentiation
scheme, and design a polynomial-time algorithm that can compute the optimal
partial differentiation prices. We also consider the incomplete information
case where the service provider does not know which group each user belongs to.
We show that it is still possible to realize price differentiation under this
scenario, and provide the sufficient and necessary condition under which an
incentive compatible differentiation scheme can achieve the same revenue as
under complete information.

ABSTRACT_BEGIN
  In this paper, we propose a new quality link metric, interference and
bandwidth adjusted ETX (IBETX) for wireless multi-hop networks. As MAC layer
affects the link performance and consequently the route quality, the metric
therefore, tackles the issue by achieving twofold MAC-awareness. Firstly,
interference is calculated using cross-layered approach by sending probes to
MAC layer. Secondly, the nominal bit rate information is provided to all nodes
in the same contention domain by considering the bandwidth sharing mechanism of
802.11. Like ETX, our metric also calculates link delivery ratios that directly
affect throughput and selects those routes that bypass dense regions in the
network. Simulation results by NS-2 show that IBETX gives 19% higher throughput
than ETX and 10% higher than Expected Throughput (ETP). Our metric also
succeeds to reduce average end-to-end delay up to 16% less than Expected Link
Performance (ELP) and 24% less than ETX.

ABSTRACT_BEGIN
  Peer-to-peer (P2P) locality has recently raised a lot of interest in the
community. Indeed, whereas P2P content distribution enables financial savings
for the content providers, it dramatically increases the traffic on inter-ISP
links. To solve this issue, the idea to keep a fraction of the P2P traffic
local to each ISP was introduced a few years ago. Since then, P2P solutions
exploiting locality have been introduced. However, several fundamental issues
on locality still need to be explored. In particular, how far can we push
locality, and what is, at the scale of the Internet, the reduction of traffic
that can be achieved with locality? In this paper, we perform extensive
experiments on a controlled environment with up to 10,000 BitTorrent clients to
evaluate the impact of high locality on inter-ISP links traffic and peers
download completion time. We introduce two simple mechanisms that make high
locality possible in challenging scenarios and we show that we save up to
several orders of magnitude inter-ISP traffic compared to traditional locality
without adversely impacting peers download completion time. In addition, we
crawled 214,443 torrents representing 6,113,224 unique peers spread among 9,605
ASes. We show that whereas the torrents we crawled generated 11.6 petabytes of
inter-ISP traffic, our locality policy implemented for all torrents could have
reduced the global inter-ISP traffic by up to 40%.

ABSTRACT_BEGIN
  With the increasing demands for new data and real-time services, wireless
networks should support calls with different traffic characteristics and
different Quality of Service (QoS)guarantees. In addition, various wireless
technologies and networks exist currently that can satisfy different needs and
requirements of mobile users. Since these different wireless networks act as
complementary to each other in terms of their capabilities and suitability for
different applications, integration of these networks will enable the mobile
users to be always connected to the best available access network depending on
their requirements. This integration of heterogeneous networks will, however,
lead to heterogeneities in access technologies and network protocols. To meet
the requirements of mobile users under this heterogeneous environment, a common
infrastructure to interconnect multiple access networks will be needed. In this
chapter, the design issues of a number of mobility management schemes have been
presented. Each of these schemes utilizes IP-based technologies to enable
efficient roaming in heterogeneous network. Efficient handoff mechanisms are
essential for ensuring seamless connectivity and uninterrupted service
delivery. A number of handoff schemes in a heterogeneous networking environment
are also presented in this chapter.

ABSTRACT_BEGIN
  In this paper we estimate lifetime bounds of a network of motes which
communicate with each other using IEEE 802.15.4 standard. Different frame
structures of IEEE 802.15.4 along with CSMA/CA medium access mechanism are
investigated to discover the overhead of channel acquisition, header and footer
of data frame, and transfer reliability during packet transmission. This
overhead makes the fixed component, and the data payload makes the incremental
component of a linear equation to estimate the power consumed during every
packet transmission. Finally we input this per-packet power consumption in a
mathematical model which estimates the lower and upper bounds of routings in
the network. We also implemented a series of measurements on CC2420 radio used
in a wide range of sensor motes to find the fixed and incremental components,
and finally the lifetime of a network composed of the motes using this radio.

ABSTRACT_BEGIN
  In this paper we study information flow paths in a data network, where
traffic generated by servers (or sources) takes a multi-hop path in order to
reach its clients (destinations). Each node in the middle of this multi-hop
path should route the incoming traffic and the traffic generated by itself to
the next hop in such a way that the traffic reaches its destination while
avoiding congestion in the links. For simplicity, we will assume the network
only carries single commodity traffic, i.e., all of the traffic should be
routed to a single destination.

ABSTRACT_BEGIN
  The availability of end-hosts and their assigned routable IP addresses has
impact on the ability to fight spammers and attackers, and on peer-to-peer
application performance. Previous works study the availability of hosts mostly
by using either active pinging or by studying access to a mail service, both
approaches suffer from inherent inaccuracies. We take a different approach by
measuring the IP addresses periodically reported by a uniquely identified group
of the hosts running the DIMES agent. This fresh approach provides a chance to
measure the true availability of end-hosts and the dynamics of their assigned
routable IP addresses. Using a two month study of 1804 hosts, we find that over
60% of the hosts have a fixed IP address and 90% median availability, while
some of the remaining hosts have more than 30 different IPs. For those that
have periodically changing IP addresses, we find that the median average period
per AS is roughly 24 hours, with a strong relation between the offline time and
the probability of altering IP address.

ABSTRACT_BEGIN
  The use of RFID tag which identifies a thing and an object will be expanded
with progress of ubiquitous society, and it is necessary to study how to
construct RFID network system as a social infrastructure like the Internet.
First, this paper proposes the virtualization method of RFID tag network system
to enable the same physical RFID network system to be used by multiple
different service systems. The system virtualization not only reduces the
system cost but also can dramatically reduce the installation space of physical
readers and the operation cost. It is proposed that all equipments in the RFID
network system except RFID tag could be shared with the conventional virtual
technologies. Then, this paper proposes the conditional tag ID processing and
the efficient tag ID transmission method which can greatly reduce the
processing time and processing load in RFID tag Infrastructure network The
conditional tag ID processing allows that tag ID is valid only at a certain
time zone of day or in a certain area. The efficient tag ID transmission method
uses the virtual network address of the service center as a part of the ID of
an RF tag, which allows the direct ID forwarding to the service center.

ABSTRACT_BEGIN
  Interference matrix (IM) has been widely used in frequency
planning/optimization of cellular systems because it describes the interaction
between any two cells. IM is generated from the source data gathered from the
cellular system, either mobile measurement reports (MMRs) or drive test (DT)
records. IM accuracy is not satisfactory since neither MMRs nor DT records
contain complete information on interference and traffic distribution. In this
paper, two IM generation algorithms based on source data fusion are proposed.
Data fusion in one algorithm is to reinforce MMRs data, using the
frequency-domain information of DT data from the same region. Data fusion in
another algorithm is to reshape DT data, using the traffic distribution
information extracted from MMRs from the same region. The fused data contains
more complete information so that more accurate IM can be obtained. Simulation
results have validated this conclusion.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) have become indispensable to the realization
of smart homes. The objective of this paper is to develop such a WSN that can
be used to construct smart home systems. The focus is on the design and
implementation of the wireless sensor node and the coordinator based on ZigBee
technology. A monitoring system is built by taking advantage of the GPRS
network. To support multi-hop communications, an improved routing algorithm
based on the Dijkstra algorithm is presented. Preliminary simulations have been
conducted to evaluate the performance of the algorithm.

ABSTRACT_BEGIN
  A high degree of reliability for critical data transmission is required in
body sensor networks (BSNs). However, BSNs are usually vulnerable to channel
impairments due to body fading effect and RF interference, which may
potentially cause data transmission to be unreliable. In this paper, an
adaptive and flexible fault-tolerant communication scheme for BSNs, namely
AFTCS, is proposed. AFTCS adopts a channel bandwidth reservation strategy to
provide reliable data transmission when channel impairments occur. In order to
fulfill the reliability requirements of critical sensors, fault-tolerant
priority and queue are employed to adaptively adjust the channel bandwidth
allocation. Simulation results show that AFTCS can alleviate the effect of
channel impairments, while yielding lower packet loss rate and latency for
critical sensors at runtime.

ABSTRACT_BEGIN
  Wireless sensor networks are often modeled in terms of a dense deployment of
smart sensor nodes in a two-dimensional region. Give a node deployment, the
\emph{critical geometric graph (CGG)} over these locations (i.e., the connected
\emph{geometric graph (GG)} with the smallest radius) is a useful structure
since it provides the most accurate proportionality between hop-count and
Euclidean distance. Hence, it can be used for GPS-free node localisation as
well as minimum distance packet forwarding. It is also known to be
asymptotically optimal for network transport capacity and power efficiency. In
this context, we propose DISCRIT, a distributed and asynchronous algorithm for
obtaining an approximation of the CGG on the node locations. The algorithm does
not require the knowledge of node locations or internode distances, nor does it
require pair-wise RSSI (Received Signal Strength Indication) measurements to be
made. Instead, the algorithm makes use of successful Hello receipt counts
(obtained during a Hello-protocol-based neighbour discovery process) as edge
weights, along with a simple distributed min-max computation algorithm. In this
paper, we first provide the theory for justifying the use of the above edge
weights. Then we provide extensive simulation results to demonstrate the
efficacy of DISCRIT in obtaining an approximation of the CGG. Finally, we show
how the CGG obtained from DISCRIT performs when used in certain network
self-organisation algorithms.

ABSTRACT_BEGIN
  It was shown recently that CSMA (Carrier Sense Multiple Access)-like
distributed algorithms can achieve the maximal throughput in wireless networks
(and task processing networks) under certain assumptions. One important, but
idealized assumption is that the sensing time is negligible, so that there is
no collision. In this paper, we study more practical CSMA-based scheduling
algorithms with collisions. First, we provide a Markov chain model and give an
explicit throughput formula which takes into account the cost of collisions and
overhead. The formula has a simple form since the Markov chain is "almost"
time-reversible. Second, we propose transmission-length control algorithms to
approach throughput optimality in this case. Sufficient conditions are given to
ensure the convergence and stability of the proposed algorithms. Finally, we
characterize the relationship between the CSMA parameters (such as the maximum
packet lengths) and the achievable capacity region.

ABSTRACT_BEGIN
  Dead Reckoning mechanisms are usually used to estimate the position of
simulated entity in virtual environment. However, this technique often ignores
available contextual information that may be influential to the state of an
entity, sacrificing remote predictive accuracy in favor of low computational
complexity. A novel extension of Dead Reckoning is suggested in this paper to
increase the network availability and fulfill the required Quality of Service
in large scale distributed simulation application. The proposed algorithm is
referred to as ANFIS Dead Reckoning, which stands for Adaptive Neuro-based
Fuzzy Inference System Dead Reckoning is based on a fuzzy inference system
which is trained by the learning algorithm derived from the neuronal networks
and fuzzy inference theory. The proposed mechanism takes its based on the
optimization approach to calculate the error threshold violation in networking
games. Our model shows it primary benefits especially in the decision making of
the behavior of simulated entities and preserving the consistence of the
simulation.

ABSTRACT_BEGIN
  Large-scale power blackouts caused by cascading failure are inflicting
enormous socioeconomic costs. We study the problem of cascading link failures
in power networks modelled by random geometric graphs from a percolation-based
viewpoint. To reflect the fact that links fail according to the amount of power
flow going through them, we introduce a model where links fail according to a
probability which depends on the number of neighboring links. We devise a
mapping which maps links in a random geometric graph to nodes in a
corresponding dual covering graph. This mapping enables us to obtain the
first-known analytical conditions on the existence and non-existence of a large
component of operational links after degree-dependent link failures. Next, we
present a simple but descriptive model for cascading link failure, and use the
degree-dependent link failure results to obtain the first-known analytical
conditions on the existence and non-existence of cascading link failures.

ABSTRACT_BEGIN
  Carrier Sense Multiple Access with Enhanced Collision Avoidance (CSMA/ECA) is
a distributed MAC protocol that allows collision-free access to the medium in
WLAN. The only difference between CSMA/ECA and the well-known CSMA/CA is that
the former uses a deterministic backoff after successful transmissions.
Collision-free operation is reached after a transient state during which some
collisions may occur. This article shows that the duration of the transient
state can be shortened by appropriately setting the contention parameters.
Standard absorbing Markov Chain theory can be used to describe the behaviour of
the system in the transient state and to predict the expected number of slots
to reach the collision-free operation.
  The article also introduces CSMA/E2CA, in which a deterministic backoff is
used two consecutive times after a successful transmission. CSMA/E2CA converges
quicker to collision-free operation and delivers higher performance than
CSMA/CA in harsh wireless scenarios with high frame error rates.
  To achieve collision-free operations when the number of contenders is large,
it may be necessary to dynamically adjust the contention parameter. The last
part of the article suggests an approach for such parameter adjustment which is
validated by simulation results.

ABSTRACT_BEGIN
  Traffic Engineering (TE) leverages information of network traffic to generate
a routing scheme optimizing the traffic distribution so as to advance network
performance. However, optimize the link weights for OSPF to the offered traffic
is an known NP-hard problem. In this paper, motivated by the fairness concept
of congestion control, we firstly propose a new generic objective function,
where various interests of providers can be extracted with different parameter
settings. And then, we model the optimal TE as the utility maximization of
multi-commodity flows with the generic objective function and theoretically
show that any given set of optimal routes corresponding to a particular
objective function can be converted to shortest paths with respect to a set of
positive link weights. This can be directly configured on OSPF-based protocols.
On these bases, we employ the Network Entropy Maximization(NEM) framework and
develop a new OSPF-based routing protocol, SPEF, to realize a flexible way to
split traffic over shortest paths in a distributed fashion. Actually, comparing
to OSPF, SPEF only needs one more weight for each link and provably achieves
optimal TE. Numerical experiments have been done to compare SPEF with the
current version of OSPF, showing the effectiveness of SPEF in terms of link
utilization and network load distribution.

ABSTRACT_BEGIN
  The high-level contribution of this paper is a simulation-based analysis to
evaluate the tradeoffs between lifetime and hop count of link-disjoint,
node-disjoint and zone-disjoint multi-path routes vis-\`a-vis single-path
minimum hop routes for mobile ad hoc networks. The link-disjoint, node-disjoint
and zone-disjoint algorithms proposed in this paper can be used to arrive at
benchmarks for the time between successive multi-path route discoveries, the
number of disjoint paths per multi-path set and the hop count per multi-path
set. We assume a multi-path set exists as long as at least one path in the set
exists. Simulation results indicate that the number of zone-disjoint paths per
multi-path set can be at most 2, which is far lower than the number of node and
link-disjoint paths available per multi-path set. Also, the time between
zone-disjoint multi-path discoveries would be far lower than the time between
node and link-disjoint multi-path route discoveries and can be at most 45% more
than the time between single minimum-hop path route discoveries. However, there
is no appreciable difference in the average hop counts per zone-disjoint,
node-disjoint and link-disjoint multi-path sets and it can be only at most 15%
more than the average minimum hop count determined using single-path routing.
We also observe that even though the number of link-disjoint paths per
multi-path set can be as large as 35-78% more than the number of node-disjoint
paths per multi-path set, the time between two successive link-disjoint
multi-path discoveries can be at most 15-25% more than the time between two
successive node-disjoint multi-path discoveries, without any significant
difference in the hop count per multi-path set.

ABSTRACT_BEGIN
  A sensor network can be described as a collection of sensor nodes which
co-ordinate with each other to perform some specific function. These sensor
nodes are mainly in large numbers and are densely deployed either inside the
phenomenon or very close to it. They can be used for various application areas
(e.g. health, military, home). Failures are inevitable in wireless sensor
networks due to inhospitable environment and unattended deployment. Therefore,
it is necessary that network failures are detected in advance and appropriate
measures are taken to sustain network operation. We previously proposed a
cellular approach for fault detection and recovery. In this paper we extend the
cellular approach and propose a new fault management mechanism to deal with
fault detection and recovery. We propose a hierarchical structure to properly
distribute fault management tasks among sensor nodes by introducing more
'self-managing' functions. The proposed failure detection and recovery
algorithm has been compared with some existing related work and proven to be
more energy efficient.

ABSTRACT_BEGIN
  A critical need in Mobile Wireless Sensor Network (MWSN) is to achieve energy
efficiency during routing as the sensor nodes have scarce energy resource. The
nodes' mobility in MWSN poses a challenge to design an energy efficient routing
protocol. Clustering helps to achieve energy efficiency by reducing the
organization complexity overhead of the network which is proportional to the
number of nodes in the network. This paper proposes a novel hybrid multipath
routing algorithm with an efficient clustering technique. A node is selected as
cluster head if it has high surplus energy, better transmission range and least
mobility. The Energy Aware (EA) selection mechanism and the Maximal Nodal
Surplus Energy estimation technique incorporated in this algorithm improves the
energy performance during routing. Simulation results can show that the
proposed clustering and routing algorithm can scale well in dynamic and energy
deficient mobile sensor network.

ABSTRACT_BEGIN
  We present a multi-channel P2P Video-on-Demand (VoD) system using
"plug-and-play" helpers. Helpers are heterogenous "micro-servers" with limited
storage, bandwidth and number of users they can serve simultaneously. Our
proposed system has the following salient features: (1) it minimizes the server
load; (2) it is distributed, and requires little or no maintenance overhead and
which can easily adapt to system dynamics; and (3) it is adaptable to varying
supply and demand patterns across multiple video channels irrespective of video
popularity. Our proposed solution jointly optimizes over helper-user topology,
video storage allocation and bandwidth allocation. The combinatorial nature of
the problem and the system demand for distributed algorithms makes the problem
uniquely challenging. By utilizing Lagrangian decomposition and Markov chain
approximation based arguments, we address this challenge by designing two
distributed algorithms running in tandem: a primal-dual storage and bandwidth
allocation algorithm and a "soft-worst-neighbor-choking" topology-building
algorithm. Our scheme provably converges to a near-optimal solution, and is
easy to implement in practice. Simulation results validate that the proposed
scheme achieves minimum sever load under highly heterogeneous combinations of
supply and demand patterns, and is robust to system dynamics of user/helper
churn, user/helper asynchrony, and random delays in the network.

ABSTRACT_BEGIN
  A comparison of three different Optical Burst Switching (OBS) architectures
is made, in terms of performance criteria, control and hardware complexity,
fairness, resource utilization, and burst loss probability. Regarding burst
losses, we distinguish the losses due to burst contentions from those due to
contentions of Burst Control Packets (BCP). The simulation results show that as
a counterpart of an its additional hardware complexity, the labelled OBS
(L-OBS) is an efficient OBS architecture compared to a Conventional OBS (C-OBS)
as well as in comparison with Offset Time-Emulated OBS (E-OBS).

ABSTRACT_BEGIN
  During the past few years, we have observed the emergence of new applications
that use multicast transmission. For a multicast routing algorithm to be
applicable in optical networks, it must route data only to group members,
optimize and maintain loop-free routes, and concentrate the routes on a subset
of network links. For an all-optical switch to play the role of a branching
router, it must be equipped with a light splitter. Light splitters are
expensive equipments and therefore it will be very expensive to implement
splitters on all optical switches. Optical light splitters are only implemented
on some optical switches. That limited availability of light splitters raises a
new problem when we want to implement multicast protocols in optical network
(because usual multicast protocols make the assumption that all nodes have
branching capabilities). Another issue is the knowledge of the locations of
light splitters in the optical network. Nodes in the network should be able to
identify the locations of light splitters scattered in the optical network so
it can construct multicast trees. These problems must be resolved by
implementing a multicast routing protocol that must take into consideration
that not all nodes can be branching node. As a result, a new signaling process
must be implemented so that light paths can be created, spanning from source to
the group members.

ABSTRACT_BEGIN
  A 60 GHz wireless Gigabit Ethernet (G.E.) communication system is developed
at IETR. As the 60 GHz radio link operates only in a single-room configuration,
an additional Radio over Fibre (RoF) link is used to ensure the communications
in all the rooms of a residential environment. The realized system covers 2 GHz
bandwidth. Due to the hardware constraints, a symbol rate at 875 Mbps is
attained using simple single carrier architecture. In the baseband (BB)
processing block, an original byte/frame synchronization process is designed to
provide a smaller value of the preamble missing detection and false alarm
probabilities. Bit error rate (BER) measurements have been realized in a large
gym for line-of-sight (LOS) conditions. A Tx-Rx distance greater than 30 meters
was attained with low BER using high gain antennas and forward error correction
RS (255, 239) coding.

ABSTRACT_BEGIN
  A 60 GHz wireless Gigabit Ethernet (G.E.) communication system capable of
near gigabit data rate has been developed at IETR. The realized system covers 2
GHz available bandwidth. This paper describes the design and realization of the
overall system including the baseband (BB), intermediate frequency (IF) and
radiofrequency (RF) blocks. A differential binary shift keying (DBPSK)
modulation and a differential demodulation are adopted at IF. In the BB
processing block, an original byte/frame synchronization technique is designed
to provide a small value of the preamble false alarm and missing probabilities.
For the system performances, two different real scenarios are investigated:
measurements carried out in a large gym and in hallways. Bit error rate (BER)
measurements have been performed in different configurations: with/without RS
(255, 239) coding, with frame synchronization using 32/64 bits preambles. As
shown by simulation, the 64 bits preamble provides sufficient robustness and
improves the system performance in term of BER. At a data rate of 875 Mbps, a
BER of 10-8 was measured at 30 m using high gain antennas for line of-sight
(LOS) conditions.

ABSTRACT_BEGIN
  Push message delivery, where a client maintains an ``always-on'' connection
with a server in order to be notified of a (asynchronous) message arrival in
real-time, is increasingly being used in Internet services. The key message in
this paper is that push message delivery on the World Wide Web is not scalable
for servers, intermediate network elements, and battery-operated mobile device
clients. We present a measurement analysis of a commercially deployed WWW push
email service to highlight some of these issues. Next, we suggest content-based
optimization to reduce the always-on connection requirement of push messaging.
Our idea is based on exploiting the periodic nature of human-to-human
messaging. We show how machine learning can accurately model the times of a day
or week when messages are least likely to arrive; and turn off always-on
connections these times. We apply our approach to a real email data set and our
experiments demonstrate that the number of hours of active always-on
connections can be cut by half while still achieving real-time message delivery
for up to 90% of all messages.

ABSTRACT_BEGIN
  In this research, we study the optimization challenges of MANET and
cross-layer technique to improve its performance. We propose an adaptive
retransmission limits algorithm for IEEE 802.11 MAC to reduce the false link
failures and predict the node mobility. We implemented cross layer interaction
between physical and MAC layers. The MAC layer utilizes the physical layer
information for differentiating false link failure from true link failure. The
MAC layer adaptively selects a retransmission limit (short and long) based on
the neighbour signal strength and sender node speed information from the
physical layer. The proposed approach tracks the signal strength of each node
in network and, while transmitting to a neighbour node, if it's received signal
strength is high and is received recently then Adaptive MAC persists in its
retransmission attempts. As there is high probability that neighbour node is
still in transmission range and may be not responding due to some problems
other then mobility. In this paper, we evaluate the performance of MANET and
show that how our Adaptive MAC greatly improves it. The simulation is done
using Network Simulator NS-2.

ABSTRACT_BEGIN
  Power line as an alternative for data transmission is being explored, and
also being used to a certain extent. But from the data transfer point of view,
power line, as a channel is highly dynamic and hence not quite suitable. To
convert the office or home wiring system to a Local Area Network (LAN),
adaptive changes are to be made to the existing protocols. In this paper, a
slotted transmission scheme is suggested, in which usable timeslots are found
out by physically sensing the media. Common usable timeslots for the
sender-receiver pair are used for communication. But these will not ensure safe
packet delivery since packets may be corrupted on the way during propagation
from sender to receiver. Therefore, we also suggest a proximity based
retransmission scheme where each machine in the LAN, buffers good packet and
machines close to the receiver retransmit on receiving a NACK.

ABSTRACT_BEGIN
  An admission control scheme should play the role of a coordinator for flows
in a data communication network, to provide the guarantees as the medium is
shared. The nodes of a wired network can monitor the medium to know the
available bandwidth at any point of time. But, in wireless ad hoc networks, a
node must consume the bandwidth of neighboring nodes, during a communication.
Hence, the consumption of bandwidth by a flow and the availability of resources
to any wireless node strictly depend upon the neighboring nodes within its
transmission range. We present a scalable and efficient admission control
scheme, Multi-hop Bandwidth Management Protocol (MBMP), to support the QoS
requirements in multi-hop ad hoc networks. We simulate several options to
design MBMP and compare the performances of these options through mathematical
analysis and simulation results, and compare its effectiveness with the
existing admission control schemes through extensive simulations.

ABSTRACT_BEGIN
  Virtualization technology facilitates a dynamic, demand-driven allocation and
migration of servers. This paper studies how the flexibility offered by network
virtualization can be used to improve Quality-of-Service parameters such as
latency, while taking into account allocation costs. A generic use case is
considered where both the overall demand issued for a certain service (for
example, an SAP application in the cloud, or a gaming application) as well as
the origins of the requests change over time (e.g., due to time zone effects or
due to user mobility), and we present online and optimal offline strategies to
compute the number and location of the servers implementing this service. These
algorithms also allow us to study the fundamental benefits of dynamic resource
allocation compared to static systems. Our simulation results confirm our
expectations that the gain of flexible server allocation is particularly high
in scenarios with moderate dynamics.

ABSTRACT_BEGIN
  This paper examines the use of Wireless Sensor Networks interfaced with light
fittings to allow for daylight substitution techniques to reduce energy usage
in existing buildings. This creates a wire free system for existing buildings
with minimal disruption and cost.

ABSTRACT_BEGIN
  Transmit power control in wireless networks has long been recognized as an
effective mechanism to mitigate co-channel interference. Due to the highly
non-convex nature, optimal power control is known to be difficult to achieve if
a system utility is to be maximized. To date, there does not yet exist a
distributed power control algorithm that maximizes any form of system utility,
despite the importance of distributed implementation for the wireless
infrastructureless networks such as ad hoc and sensor networks. This paper
fills this gap by developing a Gibbs Sampling based Asynchronous distributed
power control algorithm (referred to as GLAD). The proposed algorithm quickly
converges to the global optimal solution regardless of the concavity,
continuity, differentiability and monotonicity of the utility function. Same as
other existing distributed power control algorithms, GLAD requires extensive
message passing among all users in the network, which leads to high signaling
overhead and high processing complexity. To address this issue, this paper
further proposes a variant of the GLAD algorithm, referred to as I-GLAD, where
the prefix "I" stands for infrequent message passing. The convergence of I-GLAD
can be proved regardless of the reduction in the message passing rate. To
further reduce the processing complexity at each transmitter, we develop an
enhanced version of I-GLAD, referred to as NI-GLAD, where only the control
messages from the neighboring links are processed. Our simulation results show
that I-GLAD approximately converges to the global optimal solution regardless
of the type of the system utility function. Meanwhile, the optimality of the
solution obtained by NI-GLAD depends on the selection of the neighborhood size.

ABSTRACT_BEGIN
  The Dynamic Source Routing protocol (DSR) is a simple and efficient routing
protocol designed specifically for use in multi-hop wireless ad hoc networks of
mobile nodes. Preemptive DSR(PDSR) is the modified version of DSR. The main
objective of this paper is to analyze and compare the performance of Preemptive
DSR and Temporarily Ordered Routing Algorithm(TORA).It discusses the effect of
variation in number of nodes and average speed on protocol performance.
Simulation results (provided by the instructor) are analyzed to get an insight
into the operation of TORA and PDSR in small/large sized networks with
slow/fast moving nodes. Results show that PDSR outperforms TORA in terms of the
number of MANET control packets used to maintain/erase routes. Also, it is
concluded that TORA is a better choice than PDSR for fast moving highly
connected set of nodes. It is also observed that DSR provides better data
throughput than TORA and that routes can be created faster in PDSR than in
TORA. This paper tries to explain the reasons behind the nature of the results.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) are highly distributed networks consisting of
a large number of tiny, low-cost, light-weight wireless nodes deployed to
monitor an environment or a system. Each node in a WSN consists of three
subsystems: the sensor subsystem which senses the environment, the processing
subsystem which performs local computations on the sensed data, and the
communication subsystem which is responsible for message exchange with
neighboring sensor nodes. While an individual sensor node has limited sensing
region, processing power, and energy, networking a large number of sensor nodes
give rise to a robust, reliable, and accurate sensor network covering a wide
region. Thus, routing in WSNs is a very important issue. This paper presents a
query-based routing protocol for a WSN that provides different levels of
Quality of Service (QoS): energy-efficiency, reliability, low latency and
fault-tolerance-under different application scenarios. The algorithm has low
computational complexity but can dynamically guarantee different QoS support
depending on the requirement of the applications. The novelty of the proposed
algorithm is its ability to provide multiple QoS support without
reconfiguration and redeployment of the sensor nodes. The algorithm is
implemented in network simulator ns-2 and its performance has been evaluated.
The results show that the algorithm is more efficient than some of the
currently existing routing algorithms for WSNs.

ABSTRACT_BEGIN
  The proliferation of peer-to-peer (P2P) file sharing protocols is due to
their efficient and scalable methods for data dissemination to numerous users.
But many of these networks have no provisions to provide users with long term
access to files after the initial interest has diminished, nor are they able to
guarantee protection for users from malicious clients that wish to implicate
them in incriminating activities. As such, users may turn to supplementary
measures for storing and transferring data in P2P systems. We present a new
file sharing paradigm, called a Graffiti Network, which allows peers to harness
the potentially unlimited storage of the Internet as a third-party
intermediary. Our key contributions in this paper are (1) an overview of a
distributed system based on this new threat model and (2) a measurement of its
viability through a one-year deployment study using a popular web-publishing
platform. The results of this experiment motivate a discussion about the
challenges of mitigating this type of file sharing in a hostile network
environment and how web site operators can protect their resources.

ABSTRACT_BEGIN
  In this paper we study the dynamic aspects of the coverage of a mobile sensor
network resulting from continuous movement of sensors. As sensors move around,
initially uncovered locations are likely to be covered at a later time. A
larger area is covered as time continues, and intruders that might never be
detected in a stationary sensor network can now be detected by moving sensors.
However, this improvement in coverage is achieved at the cost that a location
is covered only part of the time, alternating between covered and not covered.
We characterize area coverage at specific time instants and during time
intervals, as well as the time durations that a location is covered and
uncovered. We further characterize the time it takes to detect a randomly
located intruder. For mobile intruders, we take a game theoretic approach and
derive optimal mobility strategies for both sensors and intruders. Our results
show that sensor mobility brings about unique dynamic coverage properties not
present in a stationary sensor network, and that mobility can be exploited to
compensate for the lack of sensors to improve coverage.

ABSTRACT_BEGIN
  We consider the sizing of network buffers in 802.11 based networks. Wireless
networks face a number of fundamental issues that do not arise in wired
networks. We demonstrate that the use of fixed size buffers in 802.11 networks
inevitably leads to either undesirable channel under-utilization or unnecessary
high delays. We present two novel dynamic buffer sizing algorithms that achieve
high throughput while maintaining low delay across a wide range of network
conditions. Experimental measurements demonstrate the utility of the proposed
algorithms in a production WLAN and a lab testbed.

ABSTRACT_BEGIN
  When data productions and consumptions are heavily unbalanced and when the
origins of data queries are spatially and temporally distributed, the so called
in-network data storage paradigm supersedes the conventional data collection
paradigm in wireless sensor networks (WSNs). In this paper, we first introduce
geometric quorum systems (along with their metrics) to incarnate the idea of
in-network data storage. These quorum systems are "geometric" because curves
(rather than discrete node sets) are used to form quorums. We then propose
GeoQuorum as a new quorum system, for which the quorum forming curves are
parameterized. Though our proposal stems from the existing work on using curves
to guide data replication and retrieval in dense WSNs, we significantly expand
this design methodology, by endowing GeoQuorum with a great flexibility to
fine-tune itself towards different application requirements. In particular, the
tunability allows GeoQuorum to substantially improve the load balancing
performance and to remain competitive in energy efficiency. Both our analysis
and simulations confirm the performance enhancement brought by GeoQuorum.

ABSTRACT_BEGIN
  Objective-The main purpose of this paper is to construct a data accuracy
model for the maximal set of sensor nodes that sense a point event and forms a
cluster with fully connected network between them. We determine the minimal set
of sensor nodes that are sufficient to give approximately the same data
accuracy achieve by the maximal set of sensor nodes. Design
approach/Procedure-L set of sensor nodes are randomly deployed over a region Z.
Since a point event S has occurred in the region Z, M maximal set of sensor
nodes wake up and start sensing the point event. The set of M sensor nodes
forms a cluster with fully connected network and remaining set of sensor nodes
continue to be in sleep mode. One sensor node is elected randomly as a cluster
head (CH) node which can estimate the data accuracy for the cluster before data
aggregation and finally send the data to the sink node. Findings - Since we
simulate the data accuracy for the cluster (M set of sensor nodes) at CH node,
there exist P minimal set of sensor nodes which give approximately the same
data accuracy level achieve by M set of sensor nodes .Moreover we find that as
the distance from the point event to the number of sensor nodes increases, the
data accuracy also get decreases. Design Limitation -This model is only
applicable to estimate data accuracy for the point event where the sensed data
are assumed to be spatially correlated with approximately same variations.
Practical implementation-Detect point event e.g. fire in forest.
Inventive/Novel idea - This is the first time that a data accuracy model is
performed for the cluster before data aggregation at the CH node which can
reduce data redundancy and communication overhead.

ABSTRACT_BEGIN
  Objective-The main purpose of this paper is to construct a distributed
clustering algorithm such that each distributed cluster can perform the data
accuracy at their respective cluster head node before data aggregation and
transmit the data to the sink node. Design approach/Procedure - We investigate
that the data are spatially correlated among the sensor nodes which form the
clusters in the spatial domain. Due to high correlation of data, these clusters
of sensor nodes are overlapped in the spatial domain. To overcome this problem,
we construct a distributed clustering algorithm with non-overlapping irregular
clusters in the spatial domain. Then each distributed cluster can perform data
accuracy at the cluster head node and finally send the data to the sink node.
Findings- Simulation result shows the associate sensor nodes of each
distributed cluster and clarifies their data accuracy profile in the spatial
domain. We demonstrate the simulation results for a single cluster to verify
that their exist an optimal cluster which give approximately the same data
accuracy level achieve by the single cluster. Moreover we find that as the
distance from the tracing point to the number of sensor node increases the data
accuracy decreases. Design Limitations - This model is only applicable to
estimate data accuracy for distributed clusters where the sensed data are
assumed to be spatially correlated with approximately same variations.
Practical implementation - Measure the moisture content in the distributed
agricultural field. Inventive/Novel idea- This is the first time that a data
accuracy model is performed for the distributed clusters before data
aggregation at the cluster head node which can reduce data redundancy and
communication overhead.

ABSTRACT_BEGIN
  The increasing popularity of web-based applications has led to several
critical services being provided over the Internet. This has made it imperative
to monitor the network traffic so as to prevent malicious attackers from
depleting the resources of the network and denying services to legitimate
users. This paper has presented a mechanism for protecting a web-server against
a distributed denial of service (DDoS) attack. Incoming traffic to the server
is continuously monitored and any abnormal rise in the inbound traffic is
immediately detected. The detection algorithm is based on a statistical
analysis of the inbound traffic on the server and a robust hypothesis testing
framework. While the detection process is on, the sessions from the legitimate
sources are not disrupted and the load on the server is restored to the normal
level by blocking the traffic from the attacking sources. To cater to different
scenarios, the detection algorithm has various modules with varying level of
computational and memory overheads for their execution. While the approximate
modules are fast in detection and involve less overhead, they have lower
detection accuracy. The accurate modules involve complex detection logic and
hence involve more overhead for their execution, but they have very high
detection accuracy. Simulations carried out on the proposed mechanism have
produced results that demonstrate effectiveness of the scheme.

ABSTRACT_BEGIN
  Access networks, in particular, Digital Subscriber Line (DSL) equipment, are
a significant source of energy consumption for wireline operators. Replacing
large monolithic DSLAMs with smaller remote DSLAM units closer to customers can
reduce the energy consumption as well as increase the reach of the access
network. This paper attempts to formalize the design and optimization of the
"last mile" wireline access network with energy as one of the costs to be
minimized. In particular, the placement of remote DSLAM units needs to be
optimized. We propose solutions for two scenarios. For the scenario where an
existing all-copper network from the central office to the customers is to be
transformed into a fiber-copper network with remote DSLAM units, we present
optimal polynomial-time solutions. In the green-field scenario, both the access
network layout and the placement of remote DSLAM units must be determined. We
show that this problem is NP-complete. We present an optimal ILP formulation
and also design an efficient heuristic-based approach to build a
power-and-cost-optimized access network. Our heuristic-based approach yields
results that are very close to optimal. We show how the power consumption of
the access network can be reduced by carefully laying the access network and
introducing remote DSLAM units.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) are rapidly emerging as an important new area
in wireless and mobile computing research. Applications of WSNs are numerous
and growing, and range from indoor deployment scenarios in the home and office
to outdoor deployment scenarios in adversary's territory in a tactical
battleground (Akyildiz et al., 2002). For military environment, dispersal of
WSNs into an adversary's territory enables the detection and tracking of enemy
soldiers and vehicles. For home/office environments, indoor sensor networks
offer the ability to monitor the health of the elderly and to detect intruders
via a wireless home security system. In each of these scenarios, lives and
livelihoods may depend on the timeliness and correctness of the sensor data
obtained from dispersed sensor nodes. As a result, such WSNs must be secured to
prevent an intruder from obstructing the delivery of correct sensor data and
from forging sensor data. To address the latter problem, end-to-end data
integrity checksums and post-processing of senor data can be used to identify
forged sensor data (Estrin et al., 1999; Hu et al., 2003a; Ye et al., 2004).
The focus of this chapter is on routing security in WSNs. Most of the currently
existing routing protocols for WSNs make an optimization on the limited
capabilities of the nodes and the application-specific nature of the network,
but do not any the security aspects of the protocols. Although these protocols
have not been designed with security as a goal, it is extremely important to
analyze their security properties. When the defender has the liabilities of
insecure wireless communication, limited node capabilities, and possible
insider threats, and the adversaries can use powerful laptops with high energy
and long range communication to attack the network, designing a secure routing
protocol for WSNs is obviously a non-trivial task.

ABSTRACT_BEGIN
  Location distinction is defined as determining whether or not the position of
a device has changed. We introduce methods and metrics for performing location
distinction in multiple-input multiple-output (MIMO) wireless networks. Using
MIMO channel measurements from two different testbeds, we evaluate the
performance of temporal signature-based location distinction with varying
system parameters, and show that it can be applied to MIMO channels with
favorable results. In particular, a 2x2 MIMO channel with a bandwidth of 80 MHz
allows a 64-fold reduction in miss rate over the SISO channel for a fixed false
alarm rate, achieving as small as 4 x 10^-4 probability of false alarm for a
2.4 x 10^-4 probability of missed detection. The very high reliability of MIMO
location distinction enables location distinction systems to detect the change
in position of a transmitter even when using a single receiver.

ABSTRACT_BEGIN
  Our work is motivated by geographical forwarding of sporadic alarm packets to
a base station in a wireless sensor network (WSN), where the nodes are
sleep-wake cycling periodically and asynchronously. When a node (referred to as
the source) gets a packet to forward, either by detecting an event or from an
upstream node, it has to wait for its neighbors in a forwarding set (referred
to as relays) to wake-up. Each of the relays is associated with a random reward
(e.g., the progress made towards the sink) that is iid. To begin with, the
source is uncertain about the number of relays, their wake-up times and the
reward values, but knows their distributions. At each relay wake-up instant,
when a relay reveals its reward value, the source's problem is to forward the
packet or to wait for further relays to wake-up. In this setting, we seek to
minimize the expected waiting time at the source subject to a lower bound on
the average reward. In terms of the operations research literature, our work
can be considered as a variant of the asset selling problem. We formulate the
relay selection problem as a partially observable Markov decision process
(POMDP), where the unknown state is the number of relays. We begin by
considering the case where the source knows the number of relays. For the
general case, where the source only knows a pmf on the number of relays, it has
to maintain a posterior pmf on the number of relays and forward the packet iff
the pmf is in an optimum stopping set. We show that the optimum stopping set is
convex and obtain inner and outer bounds to this set. The computational
complexity of the above policies motivates us to formulate an alternative
simplified model, the optimal policy for which is a simple threshold rule. We
provide simulation results to compare the performance of the various one-hop
and end-to-end forwarding policies.

ABSTRACT_BEGIN
  In operational networks, nodes are connected via multiple links for load
sharing and redundancy. This is done to make sure that a failure of a link does
not disconnect or isolate some parts of the network. However, link failures
have an effect on routing, as the routers find alternate paths for the traffic
originally flowing through the link which has failed. This effect is severe in
case of failure of a critical link in the network, such as backbone links or
the links carrying higher traffic loads. When routing is done using the Open
Shortest Path First (OSPF) routing protocol, the original weight selection for
the normal state topology may not be as efficient for the failure state. In
this paper, we investigate the single link failure issue with an objective to
find a weight setting which results in efficient routing in normal and failure
states. We engineer Tabu Search Iterative heuristic using two different
implementation strategies to solve the OSPF weight setting problem for link
failure scenarios. We evaluate these heuristics and show through experimental
results that both heuristics efficiently handle weight setting for the failure
state. A comparison of both strategies is also presented.

ABSTRACT_BEGIN
  At the same time as the emergence of multimedia in mobile Ad hoc networks,
research for the introduction of the quality of service (QoS) has received much
attention. However, when designing a QoS solution, the estimation of the
available resources still represents one of the main issues. This paper
suggests an approach to estimate available resources on a node. This approach
is based on the estimation of the busy ratio of the shared canal. We consider
in our estimation the several constraints related to the Ad hoc transmission
mode such as Interference phenomena. This approach is implemented on the AODV
routing protocol. We call AODVwithQOS our new routing protocol. We also
performed a performance evaluation by simulations using NS2 simulator. The
results confirm that AODVwithQoS provides QoS support in ad hoc wireless
networks with good performance and low overhead.

ABSTRACT_BEGIN
  Peer-to-Peer protocols currently form the most heavily used protocol class in
the Internet, with BitTorrent, the most popular protocol for content
distribution, as its flagship.
  A high number of studies and investigations have been undertaken to measure,
analyse and improve the inner workings of the BitTorrent protocol. Approaches
such as tracker message analysis, network probing and packet sniffing have been
deployed to understand and enhance BitTorrent's internal behaviour.
  In this paper we present a novel approach that aims to collect, process and
analyse large amounts of local peer information in BitTorrent swarms. We
classify the information as periodic status information able to be monitored in
real time and as verbose logging information to be used for subsequent
analysis. We have designed and implemented a retrieval, storage and
presentation infrastructure that enables easy analysis of BitTorrent protocol
internals. Our approach can be employed both as a comparison tool, as well as a
measurement system of how network characteristics and protocol implementation
influence the overall BitTorrent swarm performance.
  We base our approach on a framework that allows easy swarm creation and
control for different BitTorrent clients. With the help of a virtualized
infrastructure and a client-server software layer we are able to create,
command and manage large sized BitTorrent swarms. The framework allows a user
to run, schedule, start, stop clients within a swarm and collect information
regarding their behavior.

ABSTRACT_BEGIN
  In this paper, we propose an efficient mobility control algorithm for the
downlink multi-cell orthogonal frequency division multiplexing access (OFDMA)
system for co-channel interference reduction. It divides each cell into several
areas. The mobile nodes in each area find their own optimal position according
to their present location. Both the signal to interference plus noise ratio
(SINR) and the capacity for each node are increased by the proposed mobility
control algorithm. Simulation results say that, even the frequency reuse factor
(FRF) is equal to 1, the average capacity is improved after applying the
mobility control algorithm, compared to existing partial frequency reuse (PFR)
scheme.

ABSTRACT_BEGIN
  We propose an algorithm to locate the most critical nodes to network
robustness. Such critical nodes may be thought of as those most related to the
notion of network centrality. Our proposal relies only on a localized spectral
analysis of a limited subnetwork centered at each node in the network. We also
present a procedure allowing the navigation from any node towards a critical
node following only local information computed by the proposed algorithm.
Experimental results confirm the effectiveness of our proposal considering
networks of different scales and topological characteristics.

ABSTRACT_BEGIN
  Network virtualization is an important concept to overcome the ossification
of today's Internet as it facilitates innovation also in the network core and
as it promises a more efficient use of the given resources and infrastructure.
Virtual networks (VNets) provide an abstraction of the physical network:
multiple VNets may cohabit the same physical network, but can be based on
completely different protocol stacks (also beyond IP). One of the main
challenges in network virtualization is the efficient admission control and
embedding of VNets. The demand for virtual networks (e.g., for a video
conference) can be hard to predict, and once the request is accepted, the
specification / QoS guarantees must be ensured throughout the VNet's lifetime.
This requires an admission control algorithm which only selects high-benefit
VNets in times of scarce resources, and an embedding algorithm which realizes
the VNet in such a way that the likelihood that future requests can be embedded
as well is maximized.
  This article describes a generic algorithm for the online VNet embedding
problem which does not rely on any knowledge of the future VNet requests but
whose performance is competitive to an optimal offline algorithm that has
complete knowledge of the request sequence in advance: the so-called
competitive ratio is, loosely speaking, logarithmic in the sum of the
resources. Our algorithm is generic in the sense that it supports multiple
traffic models, multiple routing models, and even allows for nonuniform
benefits and durations of VNet requests.

ABSTRACT_BEGIN
  Privacy-preserving techniques for distributed computation have been proposed
recently as a promising framework in collaborative inter-domain network
monitoring. Several different approaches exist to solve such class of problems,
e.g., Homomorphic Encryption (HE) and Secure Multiparty Computation (SMC) based
on Shamir's Secret Sharing algorithm (SSS). Such techniques are complete from a
computation-theoretic perspective: given a set of private inputs, it is
possible to perform arbitrary computation tasks without revealing any of the
intermediate results. In fact, HE and SSS can operate also on secret inputs
and/or provide secret outputs. However, they are computationally expensive and
do not scale well in the number of players and/or in the rate of computation
tasks. In this paper we advocate the use of "elementary" (as opposite to
"complete") Secure Multiparty Computation (E-SMC) procedures for traffic
monitoring. E-SMC supports only simple computations with private input and
public output, i.e., it can not handle secret input nor secret (intermediate)
output. Such a simplification brings a dramatic reduction in complexity and
enables massive-scale implementation with acceptable delay and overhead.
Notwithstanding its simplicity, we claim that an E-SMC scheme is sufficient to
perform a great variety of computation tasks of practical relevance to
collaborative network monitoring, including, e.g., anonymous publishing and set
operations. This is achieved by combining a E-SMC scheme with data structures
like Bloom Filters and bitmap strings.

ABSTRACT_BEGIN
  We design a cross-layer approach to optimize the joint use of multi-packet
reception and network coding, in order to relieve congestion. We construct a
model for the behavior of the 802.11 MAC and apply it to several key canonical
topology components and their extensions to any number of nodes. The results
obtained from this model match the available experimental results, which are
for routing and opportunistic network coding, with fidelity. Using this model,
we show that fairness allocation by the MAC can seriously impact performance;
hence, we devise a new MAC that not only substantially improves throughput
relative to the current 802.11 MAC, but also provides fairness to flows of
information rather than to nodes. We show that the proper combination of
network coding, multi-packet reception, and our new MAC protocol achieves
super-additive throughput gains of up to 6.3 times that of routing alone with
the use of the standard 802.11 MAC. Finally, we extend the model to analyze the
asymptotic behavior of our new MAC as the number of nodes increases.

ABSTRACT_BEGIN
  In this paper, we tackle the problem of opportunistic spectrum access in
large-scale cognitive radio networks, where the unlicensed Secondary Users (SU)
access the frequency channels partially occupied by the licensed Primary Users
(PU). Each channel is characterized by an availability probability unknown to
the SUs. We apply evolutionary game theory to model the spectrum access problem
and develop distributed spectrum access policies based on imitation, a behavior
rule widely applied in human societies consisting of imitating successful
behavior. We first develop two imitation-based spectrum access policies based
on the basic Proportional Imitation (PI) rule and the more advanced Double
Imitation (DI) rule given that a SU can imitate any other SUs. We then adapt
the proposed policies to a more practical scenario where a SU can only imitate
the other SUs operating on the same channel. A systematic theoretical analysis
is presented for both scenarios on the induced imitation dynamics and the
convergence properties of the proposed policies to an imitation-stable
equilibrium, which is also the $\epsilon$-optimum of the system. Simple,
natural and incentive-compatible, the proposed imitation-based spectrum access
policies can be implemented distributedly based on solely local interactions
and thus is especially suited in decentralized adaptive learning environments
as cognitive radio networks.

ABSTRACT_BEGIN
  A Wireless Body Area Network (WBAN) is a collection of low-power and
lightweight wireless sensor nodes that are used to monitor the human body
functions and the surrounding environment. It supports a number of innovative
and interesting applications, including ubiquitous healthcare and Consumer
Electronics (CE) applications. Since WBAN nodes are used to collect sensitive
(life-critical) information and may operate in hostile environments, they
require strict security mechanisms to prevent malicious interaction with the
system. In this paper, we first highlight major security requirements and
Denial of Service (DoS) attacks in WBAN at Physical, Medium Access Control
(MAC), Network, and Transport layers. Then we discuss the IEEE 802.15.4
security framework and identify the security vulnerabilities and major attacks
in the context of WBAN. Different types of attacks on the Contention Access
Period (CAP) and Contention Free Period (CFP) parts of the superframe are
analyzed and discussed. It is observed that a smart attacker can successfully
corrupt an increasing number of GTS slots in the CFP period and can
considerably affect the Quality of Service (QoS) in WBAN (since most of the
data is carried in CFP period). As we increase the number of smart attackers
the corrupted GTS slots are eventually increased, which prevents the legitimate
nodes to utilize the bandwidth efficiently. This means that the direct
adaptation of IEEE 802.15.4 security framework for WBAN is not totally secure
for certain WBAN applications. New solutions are required to integrate high
level security in WBAN.

ABSTRACT_BEGIN
  The growth of the World Wide Web has emphasized the need for improvement in
user latency. One of the techniques that are used for improving user latency is
Caching and another is Web Prefetching. Approaches that bank solely on caching
offer limited performance improvement because it is difficult for caching to
handle the large number of increasingly diverse files. Studies have been
conducted on prefetching models based on decision trees, Markov chains, and
path analysis. However, the increased uses of dynamic pages, frequent changes
in site structure and user access patterns have limited the efficacy of these
static techniques. In this paper, we have proposed a methodology to cluster
related pages into different categories based on the access patterns.
Additionally we use page ranking to build up our prediction model at the
initial stages when users haven't already started sending requests. This way we
have tried to overcome the problems of maintaining huge databases which is
needed in case of log based techniques.

ABSTRACT_BEGIN
  A cognitive handoff is a multipurpose handoff that achieves many desirable
features simultaneously; e.g., seamlessness, autonomy, security, correctness,
adaptability, etc. But, the development of cognitive handoffs is a challenging
task that has not been properly addressed in the literature. In this paper, we
discuss the difficulties of developing cognitive handoffs and propose a new
model-driven methodology for their systematic development. The theoretical
framework of this methodology is the holistic approach, the functional
decomposition method, the model-based design paradigm, and the theory of design
as scientific problem-solving. We applied the proposed methodology and obtained
the following results: (i) a correspondence between handoff purposes and
quantitative environment information, (ii) a novel taxonomy of handoff mobility
scenarios, and (iii) an original state-based model representing the functional
behavior of the handoff process.

ABSTRACT_BEGIN
  Current handoffs are not designed to achieve multiple desirable features
simultaneously. This weakness has resulted in handoff schemes that are seamless
but not adaptive, or adaptive but not secure, or secure but not autonomous, or
autonomous but not correct, etc. To face this limitation, we initiated a
research project to develop a new kind of handoff system which attains multiple
purposes simultaneously by using context information from the external and
internal handoff environment. We envision a cognitive handoff as a
multipurpose, multi-criteria, environment-aware, and policy-based handoff that
trades-off multiple objectives to reach its intended goals. This paper presents
a conceptual (soft) model of cognitive handoffs using a holistic approach. We
applied the proposed model to identify cognitive handoff performance parameters
and tradeoffs between conflicting objectives. We argue that cognitive handoffs
are the archetype of handoffs for the future Internet.

ABSTRACT_BEGIN
  A challenging problem in multi-band multi-cell self-organized wireless
systems, such as multi-channel Wi-Fi networks, femto/pico cells in 3G/4G
cellular networks, and more recent wireless networks over TV white spaces, is
of distributed resource allocation. This involves four components: channel
selection, client association, channel access, and client scheduling. In this
paper, we present a unified framework for jointly addressing the four
components with the global system objective of maximizing the clients
throughput in a proportionally fair manner. Our formulation allows a natural
dissociation of the problem into two sub-parts. We show that the first part,
involving channel access and client scheduling, is convex and derive a
distributed adaptation procedure for achieving Pareto-optimal solution. For the
second part, involving channel selection and client association, we develop a
Gibbs-sampler based approach for local adaptation to achieve the global
objective, as well as derive fast greedy algorithms from it that achieve good
solutions.

ABSTRACT_BEGIN
  We consider the localization problem of multiple wideband sources in a
multi-path environment by coherently taking into account the attenuation
characteristics and the time delays in the reception of the signal. Our
proposed method leaves the space for unavailability of an accurate signal
attenuation model in the environment by considering the model as an unknown
function with reasonable prior assumptions about its functional space. Such
approach is capable of enhancing the localization performance compared to only
utilizing the signal attenuation information or the time delays. In this paper,
the localization problem is modeled as a cost function in terms of the source
locations, attenuation model parameters and the multi-path parameters. To
globally perform the minimization, we propose a hybrid algorithm combining the
differential evolution algorithm with the Levenberg-Marquardt algorithm.
Besides the proposed combination of optimization schemes, supporting the
technical details such as closed forms of cost function sensitivity matrices
are provided. Finally, the validity of the proposed method is examined in
several localization scenarios, taking into account the noise in the
environment, the multi-path phenomenon and considering the sensors not being
synchronized.

ABSTRACT_BEGIN
  Performance and reliability of content access in mobile networks is
conditioned by the number and location of content replicas deployed at the
network nodes. Location theory has been the traditional, centralized approach
to study content replication: computing the number and placement of replicas in
a static network can be cast as a facility location problem. The endeavor of
this work is to design a practical solution to the above joint optimization
problem that is suitable for mobile wireless environments. We thus seek a
replication algorithm that is lightweight, distributed, and reactive to network
dynamics. We devise a solution that lets nodes (i) share the burden of storing
and providing content, so as to achieve load balancing, and (ii) autonomously
decide whether to replicate or drop the information, so as to adapt the content
availability to dynamic demands and time-varying network topologies. We
evaluate our mechanism through simulation, by exploring a wide range of
settings, including different node mobility models, content characteristics and
system scales. Furthermore, we compare our mechanism to state-of-the-art
approaches to content delivery in static and mobile networks. Results show that
our mechanism, which uses local measurements only, is: (i) extremely precise in
approximating an optimal solution to content placement and replication; (ii)
robust against network mobility; (iii) flexible in accommodating various
content access patterns. Moreover, our scheme outperforms alternative
approaches to content dissemination both in terms of content access delay and
access congestion.

ABSTRACT_BEGIN
  We introduce a model of the Relentless Congestion Control proposed by Matt
Mathis. Relentless Congestion Control (RCC) is a modification of the AIMD
(Additive Increase Multiplicative Decrease) congestion control which consists
in decreasing the TCP congestion window by the number of lost segments instead
of halving it. Despite some on-going discussions at the ICCRG IRTF-group, this
congestion control has, to the best of our knowledge, never been modeled. In
this paper, we provide an analytical model of this novel congestion control and
propose an implementation of RCC for the commonly-used network simulator ns-2.
We also improve RCC with the addition of a loss retransmission detection scheme
(based on SACK+) to prevent RTO caused by a loss of a retransmission and called
this new version RCC+. The proposed models describe both the original RCC
algorithm and RCC+ improvement and would allow to better assess the impact of
this new congestion control scheme over the network traffic.

ABSTRACT_BEGIN
  This paper presents experimental checking of the model for measuring
available bandwidth in IPv6. The experiment was performed using a measuring
infrastructure RIPE test box, ensuring precision accuracy. The experimental
results showed that to increase the accuracy of available bandwidth, we need to
neutralize the effect of the variable part of the delay by increasing the
number of measurements. Finally, we made the computer simulation, which allowed
us to establish a dependence between the measurement error of the available
bandwidth and the number of measurements.

ABSTRACT_BEGIN
  The paper introduces an original MAC protocol for a passive optical
metropolitan area network using time-domain wavelength interleaved networking
(TWIN)% as proposed recently by Bell Labs . Optical channels are shared under
the distributed control of destinations using a packet-based polling algorithm.
This MAC is inspired more by EPON dynamic bandwidth allocation than the
slotted, GPON-like access control generally envisaged for TWIN. Management of
source-destination traffic streams is flow-aware with the size of allocated
time slices being proportional to the number of active flows. This emulates a
network-wide, distributed fair queuing scheduler, bringing the well-known
implicit service differentiation and robustness advantages of this mechanism to
the metro area network. The paper presents a comprehensive performance
evaluation based on analytical modelling supported by simulations. The proposed
MAC is shown to have excellent performance in terms of both traffic capacity
and packet latency.

ABSTRACT_BEGIN
  We study a simple general scenario of ad hoc networks based on IEEE 802.11
wireless communications, consisting in a chain of transmitters, each of them
being in the carrier sense area of its neighbors. Each transmitter always
attempts to send some data frames to one receiver in its transmission area,
forming a pair sender-receiver. This scenario includes the three pairs fairness
problem, and allows to study some fairness issues of the IEEE 802.11 medium
access mechanism. We show by simulation that interesting phenomena appear,
depending on the number n of pairs in the chain and of its parity. We also
point out a notable asymptotic behavior. We introduce a powerful modeling, by
simply considering the probability for a transmitter to send data while its
neighbors are waiting. This model leads to a non-linear system of equations,
which matches very well the simulations, and which allows to study both small
and very large chains. We then analyze the fairness issue in the chain
regarding some parameters, as well as the asymptotic behavior. By studying very
long chains, we notice good asymptotic fairness of the IEEE 802.11 medium
sharing mechanism. As an application, we show how to increase the fairness in a
chain of three pairs.

ABSTRACT_BEGIN
  Modeling and understanding BitTorrent (BT) dynamics is a recurrent research
topic mainly due to its high complexity and tremendous practical efficiency.
Over the years, different models have uncovered various phenomena exhibited by
the system, many of which have direct impact on its performance. In this paper
we identify and characterize a phenomenon that has not been previously
observed: homogeneous peers (with respect to their upload capacities)
experience heterogeneous download rates. The consequences of this phenomenon
have direct impact on peer and system performance, such as high variability of
download times, unfairness with respect to peer arrival order, bursty
departures and content synchronization. Detailed packet-level simulations and
prototype-based experiments on the Internet were performed to characterize this
phenomenon. We also develop a mathematical model that accurately predicts the
heterogeneous download rates of the homogeneous peers as a function of their
content. Although this phenomenon is more prevalent in unpopular swarms (very
few peers), these by far represent the most common type of swarm in BT.

ABSTRACT_BEGIN
  Wireless Body Area Networks (WBAN) has emerged as a key technology to provide
real-time health monitoring of a patient and diagnose many life threatening
diseases. WBAN operates in close vicinity to, on, or inside a human body and
supports a variety of medical and non-medical applications. IEEE 802 has
established a Task Group called IEEE 802.15.6 for the standardization of WBAN.
The purpose of the group is to establish a communication standard optimized for
low-power in-body/on-body nodes to serve a variety of medical and non-medical
applications. This paper explains the most important features of the new IEEE
802.15.6 standard. The standard defines a Medium Access Control (MAC) layer
supporting several Physical (PHY) layers. We briefly overview the PHY and MAC
layers specifications together with the bandwidth efficiency of IEEE 802.15.6
standard. We also discuss the security paradigm of the standard.

ABSTRACT_BEGIN
  Providing proper economic incentives is essential for the success of dynamic
spectrum sharing. Cooperative spectrum sharing is one effective way to achieve
this goal. In cooperative spectrum sharing, secondary users (SUs) relay
traffics for primary users (PUs), in exchange for dedicated transmission time
for the SUs' own communication needs. In this paper, we study the cooperative
spectrum sharing under incomplete information, where SUs' types (capturing
their heterogeneity in relay channel gains and evaluations of power
consumptions) are private information and not known by PUs. Inspired by the
contract theory, we model the network as a labor market. The single PU is the
employer who offers a contract to the SUs. The contract consists of a set of
contract items representing combinations of spectrum accessing time (i.e.,
reward) and relaying power (i.e., contribution). The SUs are employees, and
each of them selects the best contract item to maximize his payoff. We study
the optimal contract design for both weak and strong incomplete information
scenarios. First, we provide necessary and sufficient conditions for feasible
contracts in both scenarios. In the weak incomplete information scenario, we
further derive the optimal contract that achieves the same maximum PU's utility
as in the complete information benchmark. In the strong incomplete information
scenario, we propose a Decompose-and-Compare algorithm that achieves a
close-to-optimal contract. We future show that the PU's average utility loss
due to the suboptimal algorithm and the strong incomplete information are both
relatively small (less than 2% and 1:3%, respectively, in our numerical results
with two SU types).

ABSTRACT_BEGIN
  In this paper we establish the log-convexity of the rate region in 802.11
WLANs. This generalises previous results for Aloha networks and has immediate
implications for optimisation based approaches to the analysis and design of
802.11 wireless networks.

ABSTRACT_BEGIN
  Designing a wireless node that supports quality of service (QoS) in a mobile
ad hoc network is a challenging task. In this paper, we propose an architecture
of a wireless node that may be used to form a mobile ad hoc network that
supports QoS. We discuss the core functionalities required for such a node and
how those functionalities can be incorporated. A feature of our architecture is
that the node has the ability to utilize multiple paths, if available, for the
provision of QoS. However, in the absence of multiple paths it can utilize the
resources provided by a single path between the source and the destination. We
follow a modular approach where each module is expanded iteratively. We compare
the features of our architecture with the existing architectures proposed in
the literature. Our architecture has provisions of energy and mobility
management and it can be customized to design a system-on-chip (SoC).

ABSTRACT_BEGIN
  Previous studies have shown that the actual handoff schemes employed in the
IEEE 802.11 Wireless LANs (WLANs) do not meet the strict delay constraints
placed by many multimedia applications like Voice over IP. Both the active and
the passive supported scan modes in the standard handoff procedure have
important delay that affects the Quality of Service (QoS) required by the
real-time communications over 802.11 networks. In addition, the problem is
further compounded by the fact that limited coverage areas of Access Points
(APs) occupied in 802.11 infrastructure WLANs create frequent handoffs. We
propose a new optimized and fast handoff scheme that decrease both handoff
latency and occurrence by performing a seamless prevent scan process and an
effective next-AP selection. Through simulations and performance evaluation, we
show the effectiveness of the new adaptive handoff that reduces the process
latency and adds new context-based parameters. The Results illustrate a QoS
delay-respect required by applications and an optimized AP-choice that
eliminates handoff events that are not beneficial.

ABSTRACT_BEGIN
  In this paper, a novel fine timing algorithm has been tested and developed to
synchronize Ultra-Wideband (UWB) signals with pulse position modulation (PPM).
By applying this algorithm, we evaluate timing algorithms in both data-aided
(DA) and non-data-aided (NDA) modes. Based on correlation operations, our
algorithm remains operational in practical UWB settings. The proposed timing
scheme consists of two complementary floors or steps. The first floor consists
on a coarse synchronization which is founded on the recently proposed
acquisition scheme based on dirty templates (TDT). In the second floor, we
investigate a new fine synchronization algorithm which gives an improved
estimate of timing offset. Simulations confirm performance improvement of our
timing synchronization compared to the original TDT algorithm in terms of mean
square error.

ABSTRACT_BEGIN
  Random 1/2-disk routing in wireless ad-hoc networks is a localized geometric
routing scheme in which each node chooses the next relay randomly among the
nodes within its transmission range and in the general direction of the
destination. We introduce a notion of convergence for geometric routing schemes
that not only considers the feasibility of packet delivery through possibly
multi-hop relaying, but also requires the packet delivery to occur in a finite
number of hops. We derive sufficient conditions that ensure the asymptotic
\emph{convergence} of the random 1/2-disk routing scheme based on this
convergence notion, and by modeling the packet distance evolution to the
destination as a Markov process, we derive bounds on the expected number of
hops that each packet traverses to reach its destination.

ABSTRACT_BEGIN
  Intel Ethernet Flow Director is an advanced network interface card (NIC)
technology. It provides the benefits of parallel receive processing in
multiprocessing environments and can automatically steer incoming network data
to the same core on which its application process resides. However, our
analysis and experiments show that Flow Director cannot guarantee in-order
packet delivery in multiprocessing environments. Packet reordering causes
various negative impacts. E.g., TCP performs poorly with severe packet
reordering. In this paper, we use a simplified model to analyze why Flow
Director can cause packet reordering. Our experiments verify our analysis.

ABSTRACT_BEGIN
  Receive side scaling (RSS) is a network interface card (NIC) technology. It
provides the benefits of parallel receive processing in multiprocessing
environments. However, existing RSS-enabled NICs lack a critical data steering
mechanism that would automatically steer incoming network data to the same core
on which its application process resides. This absence causes inefficient cache
usage if an application is not running on the core on which RSS has scheduled
the received traffic to be processed. In Linux systems, it cannot even ensure
that packets in a TCP flow are processed by a single core, even if the
interrupts for the flow are pinned to a specific core. This results in degraded
performance. In this paper, we develop such a data steering mechanism in the
NIC for multicore or multiprocessor systems. This data steering mechanism is
mainly targeted at TCP, but it can be extended to other transport layer
protocols. We term a NIC with such a data steering mechanism "A Transport
Friendly NIC" (A-TFN). Experimental results have proven the effectiveness of
A-TFN in accelerating TCP/IP performance.

ABSTRACT_BEGIN
  The paper aims to design cross-layer optimal scheduling algorithms for
cooperative multi-hop Cognitive Radio Networks (CRNs), where secondary users
(SUs) assist primary user (PU)'s multi-hop transmissions and in return gain
authorization to access a share of the spectrum. We build two models for two
different types of PUs, corresponding to elastic and inelastic service classes.
For CRNs with elastic service, the PU maximizes its throughput while assigning
a time-share of the channel to SUs proportional to SUs' assistance. For the
inelastic case, the PU is guaranteed a minimum utility. The proposed algorithm
for elastic PU model can achieve arbitrarily close to the optimal PU
throughput, while the proposed algorithm for inelastic PU model can achieve
arbitrarily close to the optimal SU utility. Both algorithms provide
deterministic upper-bounds for PU queue backlogs. In addition, we show a
tradeoff between throughput/utility and PU's average end-to-end delay
upper-bounds for both algorithms. Furthermore, the algorithms work in both
backlogged as well as arbitrary arrival rate systems.

ABSTRACT_BEGIN
  Recently, wireless communication industries have begun to extend their
services to machine-type communication devices as well as to user equipments.
Such machine-type communication devices as meters and sensors need intermittent
uplink resources to report measured or sensed data to their serving data
collector. It is however hard to dedicate limited uplink resources to each of
them. Thus, efficient service of a tremendous number of devices with low
activities may consider simple random access as a solution. The data collectors
receiving the measured data from many sensors simultaneously can successfully
decode only signals with signal-to-interference-plus-noise-ratio (SINR) above a
certain value. The main design issues for this environment become how many data
collectors are needed, how much power sensor nodes transmit with, and how
wireless channels affect the performance. This paper provides answers to those
questions through a stochastic analysis based on a spatial point process and on
simulations.

ABSTRACT_BEGIN
  In this paper, we explore what \emph{network economics} is all about,
focusing on the interesting topics brought about by the Internet. Our intent is
make this a brief survey, useful as an outline for a course on this topic, with
an extended list of references. We try to make it as intuitive and readable as
possible. We also deliberately try to be critical at times, and hope our
interpretation of the topic will lead to interests for further discussions by
those doing research in the same field.

ABSTRACT_BEGIN
  We consider wireless mesh networks and the problem of scheduling the links of
a given set of routes under the assumption of a heavy-traffic pattern. We
assume some TDMA protocol provides a background of synchronized time slots and
seek to schedule the routes' links to maximize the number of packets that get
delivered to their destinations per time slot. Our approach is to construct an
undirected graph G and to heuristically obtain node multicolorings for G that
can be turned into efficient link schedules. In G each node represents a link
to be scheduled and the edges are set up to represent every possible
interference for any given set of interference assumptions. We present two
multicoloring-based heuristics and study their performance through extensive
simulations. One of the two heuristics is based on relaxing the notion of a
node multicoloring by dynamically exploiting the availability of communication
opportunities that would otherwise be wasted. We have found that, as a
consequence, its performance is significantly superior to the other's.

ABSTRACT_BEGIN
  The goal of this paper is to establish a general approach for analyzing
queueing models with repeated inhomogeneous vacations. The server goes on for a
vacation if the inactivity prolongs more than the vacation trigger duration.
Once the system enters in vacation mode, it may continue for several
consecutive vacations. At the end of a vacation, the server goes on another
vacation, possibly with a different probability distribution; if during the
previous vacation there have been no arrivals. However the system enters in
vacation mode only if the inactivity is persisted beyond defined trigger
duration. In order to get an insight on the influence of parameters on the
performance, we choose to study a simple M/G/1 queue (Poisson arrivals and
general independent service times) which has the advantage of being tractable
analytically. The theoretical model is applied to the problem of power saving
for mobile devices in which the sleep durations of a device correspond to the
vacations of the server. Various system performance metrics such as the frame
response time and the economy of energy are derived. A constrained optimization
problem is formulated to maximize the economy of energy achieved in power save
mode, with constraints as QoS conditions to be met. An illustration of the
proposed methods is shown with a WiMAX system scenario to obtain design
parameters for better performance. Our analysis allows us not only to optimize
the system parameters for a given traffic intensity but also to propose
parameters that provide the best performance under worst case conditions.

ABSTRACT_BEGIN
  There is a trend of applying machine learning algorithms to cognitive radio.
One fundamental open problem is to determine how and where these algorithms are
useful in a cognitive radio network. In radar and sensing signal processing,
the control of degrees of freedom (DOF)---or dimensionality---is the first
step, called pre-processing. In this paper, the combination of dimensionality
reduction with SVM is proposed apart from only applying SVM for classification
in cognitive radio. Measured Wi-Fi signals with high signal to noise ratio
(SNR) are employed to the experiments. The DOF of Wi-Fi signals is extracted by
dimensionality reduction techniques. Experimental results show that with
dimensionality reduction, the performance of classification is much better with
fewer features than that of without dimensionality reduction. The error rates
of classification with only one feature of the proposed algorithm can match the
error rates of 13 features of the original data. The proposed method will be
further tested in our cognitive radio network testbed.

ABSTRACT_BEGIN
  "Handover" is one of the techniques used to achieve the service continuity in
Fourth generation wireless networks (FGWNs). Seamless continuity is the main
goal in fourth generation Wireless networks (FGWNs), when a mobile terminal
(MT) is in overlapping area for service continuity Handover mechanism are
mainly used While moving in the heterogeneous wireless networks continual
connection is the main challenge. Vertical handover is used as a technique to
minimize the processing delay in heterogeneous wireless networks this paper,
Vertical handover decision schemes are compared and Technique of order
preference by similarity to ideal solution (TOPSIS) in a distributed manner.
TOPSIS is used to choose the best network from the available Visitor networks
(VTs) for the continuous connection by the mobile terminal. In our work we
mainly concentrated to the handover decision Phase and to reduce the processing
delay in the period of handover

ABSTRACT_BEGIN
  The type of business relationships between the Internet autonomous systems
(AS) determines the BGP inter-domain routing. Previous works on inferring AS
relationships relied on the connectivity information between ASes. In this
paper we infer AS relationships by analysing the routing polices of ASes
encoded in the BGP attributes Communities and the Locpref. We accumulate BGP
data from RouteViews, RIPE RIS and the public Route Servers in August 2010 and
February 2011. Based on the routing policies extracted from data of the two BGP
attributes, we obtain AS relationships for 39% links in our data, which include
all links among the Tier-1 ASes and most links between Tier-1 and Tier-2 ASes.
We also reveal a number of special AS relationships, namely the hybrid
relationship, the partial-transit relationship, the indirect peering
relationship and the backup links. These special relationships are relevant to
a better understanding of the Internet routing. Our work provides a profound
methodological progress for inferring the AS relationships.

ABSTRACT_BEGIN
  Intelligent products carrying their own information are more and more present
nowadays. In recent years, some authors argued the usage of such products for
the Supply Chain Management Industry. Indeed, a multitude of informational
vectors take place in such environments like fixed databases or manufactured
products on which we are able to embed significant proportion of data. By
considering distributed database systems, we can allocate specific data
fragments to the product useful to manage its own evolution. The paper aims to
analyze the Supply Chain performance according to different strategies of
information distribution. Thus, different distribution patterns between
informational vectors are studied. The purpose is to determine the key factors
which lead to improve information distribution performance in term of time
properties.

ABSTRACT_BEGIN
  Ecosystems monitoring is essential to properly understand their development
and the effects of events, both climatological and anthropological in nature.
The amount of data used in these assessments is increasing at very high rates.
This is due to increasing availability of sensing systems and the development
of new techniques to analyze sensor data. The Enviro-Net Project encompasses
several of such sensor system deployments across five countries in the
Americas. These deployments use a few different ground-based sensor systems,
installed at different heights monitoring the conditions in tropical dry
forests over long periods of time. This paper presents our experience in
deploying and maintaining these systems, retrieving and pre-processing the
data, and describes the Web portal developed to help with data management,
visualization and analysis.

ABSTRACT_BEGIN
  Fast Handovers for the MIPv6 (FMIPv6) has been proposed to reduce the
Handover latency, in the IETF. It could not find the acceptable reduction, so
led to more efforts to improve it and however the creation of multiple Handover
methods in the literature. A stable connection is very important in mobile
services so the mobility of device would not cause any interruption in network
services and thus mobility management plays a very important role. Mobile IPv6
has become a general solution for supporting mobility between different
networks on the internet which a flawless connection needs to be managed
properly. In order to select the appropriate method, in this paper, all the
proposed methods have been classified according to the identified performance
metrics. Call blocking probability, Handover blocking probability, Probability
of an unnecessary handover, Duration of interruption and delay, as the most
important Handover algorithm performance metrics are introduced. The AHP method
will be deployed to weight the metrics in a sample topology according to the
selected sound application. Then the TOPSIS method will be employed to find the
appropriate Handover algorithm.

ABSTRACT_BEGIN
  Today's wireless networks are characterized by fixed spectrum assignment
policy. The limited available spectrum and the inefficiency in the spectrum
usage necessitate a new communication paradigm to exploit the existing wireless
spectrum opportunistically. Cognitive radio is a paradigm for wireless
communication in which either a network or a wireless node changes its
transmission or reception parameters to communicate efficiently avoiding
interference with licensed or unlicensed users. In this work, a fuzzy logic
based system for spectrum management is proposed where the radio can share
unused spectrum depending on some parameters like distance, signal strength,
node velocity and availability of unused spectrum. The system is simulated and
is found to give satisfactory results.

ABSTRACT_BEGIN
  Recently, the world has witnessed the increasing occurrence of disasters,
some of natural origin and others caused by man. The intensity of the
phenomenon that cause such disasters, the frequency in which they occur, the
number of people affected and the material damage caused by them have been
growing substantially. Disasters are defined as natural, technological, and
human-initiated events that disrupt the normal functioning of the economy and
society on a large scale. Areas where disasters have occurred bring many
dangers to rescue teams and the communication network infrastructure is usually
destroyed. To manage these hazards, different wireless technologies can be
launched in the area of disaster. This paper discusses the innovative wireless
technologies for Disaster Management. Specifically, issues related to the
design of Hierarchical Hybrid Communication Network (arising in the
communication network for disaster relief) are discussed.

ABSTRACT_BEGIN
  Data mining is used to extract hidden information from large databases. In
Peer-to-Peer context, a challenging problem is how to find the appropriate Peer
to deal with a given query without overly consuming bandwidth. Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. An unstructured P2P system based on an organization of Peers
around Super-Peers that are connected to Super-Super-Peer according to their
semantic domains is considered. This paper integrates Decision Trees in P2P
architectures for predicting Query-Suitable Super-Peers representing a
community of Peers, where one among them is able to answer the given query. In
fact, by analyzing the queries' log file, a predictive model that avoids
flooding queries in the P2P networks constructed by predicting the appropriate
Super-Peer, and hence the Peer to answer the query. The proposed architecture
is based on a Decision Tree (Base-Knowledge - BK). The efficiency of these
architectures is discussed considering architecture without knowledge
(Baseline) using only the flooding queries method to answer queries. The
advantage of this knowledge based model is the robustness in Queries routing
mechanism and scalability in P2P Network.

ABSTRACT_BEGIN
  In Peer-to-Peer context, a challenging problem is how to find the appropriate
peer to deal with a given query without overly consuming bandwidth? Different
methods proposed routing strategies of queries taking into account the P2P
network at hand. This paper considers an unstructured P2P system based on an
organization of peers around Super-Peers that are connected to Super-Super-
Peer according to their semantic domains; By analyzing the queries log file, a
predictive model that avoids flooding queries in the P2P network is constructed
after predicting the appropriate Super-Peer, and hence the peer to answer the
query. A challenging problem in a schema-based Peer-to-Peer (P2P) system is how
to locate peers that are relevant to a given query. In this paper,
architecture, based on (Super-)Peers is proposed, focusing on query routing.
The approach to be implemented, groups together (Super-)Peers that have similar
interests for an efficient query routing method. In such groups, called
Super-Super-Peers (SSP), Super-Peers submit queries that are often processed by
members of this group. A SSP is a specific Super-Peer which contains knowledge
about: 1. its Super-Peers and 2. The other SSP. Knowledge is extracted by using
data mining techniques (e.g. Decision Tree algorithms) starting from queries of
peers that transit on the network. The advantage of this distributed knowledge
is that, it avoids making semantic mapping between heterogeneous data sources
owned by (Super-)Peers, each time the system decides to route query to other
(Super-) Peers. The set of SSP improves the robustness in queries routing
mechanism, and the scalability in P2P Network. Compared with a baseline
approach,the proposal architecture shows the effect of the data mining with
better performance in respect to response time and precision.

ABSTRACT_BEGIN
  In this paper, we analyze the numerical stability of the popular Longley-Rice
Irregular Terrain Model (ITM). This model is widely used to plan wireless
networks and in simulation-validated research and hence its stability is of
fundamental importance to the correctness of a large amount of work. We take a
systematic approach by first porting the reference ITM implementation to a
multiprecision framework and then generating loss predictions along many random
paths using real terrain data. We find that the ITM is not unstable for common
numerical precisions and practical prediction scenarios.

ABSTRACT_BEGIN
  In this paper, we present a detailed framework consisting of modeling of
routing overhead generated by three widely used proactive routing protocols;
Destination-Sequenced Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR). The questions like, how these protocols
differ from each other on the basis of implementing different routing
strategies, how neighbor estimation errors affect broadcast of route requests,
how reduction of broadcast overhead achieves bandwidth, how to cope with the
problem of mobility and density, etc, are attempted to respond. In all of the
above mentioned situations, routing overhead and delay generated by the chosen
protocols can exactly be calculated from our modeled equations. Finally, we
analyze the performance of selected routing protocols using our proposed
framework in NS-2 by considering different performance parameters; Route
REQuest (RREQ) packet generation, End-to-End Delay (E2ED) and Normalized
Routing Load (NRL) with respect to varying rates of mobility and density of
nodes in the underlying wireless network.

ABSTRACT_BEGIN
  This paper deals with HOW to analyze the requirements for setting up the WAN
Optimizer. The criteria's that needs to be taken into account, the steps
involved in the analysis of WAN optimization requirement. These entire analyses
will give a complete framework for setting up a WAN optimizer within an
organization and the organization will have a clear record on the analysis made
before setting up this WAN Optimizer.

ABSTRACT_BEGIN
  To cope with the increasing demand of wireless communication services
multi-carrier systems are being used. Radio resources are very limited and
efficient usages of these resources are inevitable to get optimum performance
of the system. Paging channel is a low-bandwidth channel and one of the most
important channels on which system performance depends significantly. Therefore
it is vulnerable to even moderate overloads. In this paper, an efficient paging
algorithm, Concurrent Search, is proposed for efficient use of paging channel
in Multi- carrier CDMA system instead of existing sequential searching
algorithm. It is shown by the simulation that the paging performance in
proposed algorithm is far better than the existing system.

ABSTRACT_BEGIN
  Sensor network has been recognized as the most significant technology for
next century. Despites of its potential application, wireless sensor network
encounters resource restriction such as low power, reduced bandwidth and
specially limited power sources. This work proposes an efficient technique for
the conservation of energy in a wireless sensor network (WSN) by forming an
effective cluster of the network nodes distributed over a wide range of
geographical area. The clustering scheme is developed around a specified class
of cellular automata (CA) referred to as the modified cyclic cellular automata
(mCCA). It sets a number of nodes in stand-by mode at an instance of time
without compromising the area of network coverage and thereby conserves the
battery power. The proposed scheme also determines an effective cluster size
where the inter-cluster and intra-cluster communication cost is minimum. The
simulation results establish that the cyclic cellular automata based clustering
for energy conservation in sensor networks (CCABC) is more reliable than the
existing schemes where clustering and CA based energy saving technique is used.

ABSTRACT_BEGIN
  The throughput benefits of random linear network codes have been studied
extensively for wirelined and wireless erasure networks. It is often assumed
that all nodes within a network perform coding operations. In
energy-constrained systems, however, coding subgraphs should be chosen to
control the number of coding nodes while maintaining throughput. In this paper,
we explore the strategic use of network coding in the wireless packet erasure
relay channel according to both throughput and energy metrics. In the relay
channel, a single source communicates to a single sink through the aid of a
half-duplex relay. The fluid flow model is used to describe the case where both
the source and the relay are coding, and Markov chain models are proposed to
describe packet evolution if only the source or only the relay is coding. In
addition to transmission energy, we take into account coding and reception
energies. We show that coding at the relay alone while operating in a rateless
fashion is neither throughput nor energy efficient. Given a set of system
parameters, our analysis determines the optimal amount of time the relay should
participate in the transmission, and where coding should be performed.

ABSTRACT_BEGIN
  There have been a bulk of analytic results about the performance of cellular
networks where base stations are regularly located on a hexagonal or square
lattice. This regular model cannot reflect the reality, and tends to
overestimate the network performance. Moreover, tractable analysis can be
performed only for a fixed location user (e.g., cell center or edge user). In
this paper, we use the stochastic geometry approach, where base stations can be
modeled as a homogeneous Poisson point process. We also consider the user
density, and derive the user outage probability that an arbitrary user is under
outage owing to low signal-to-interference-plus-noise ratio or high congestion
by multiple users. Using the result, we calculate the density of success
transmissions in the downlink cellular network. An interesting observation is
that the success transmission density increases with the base station density,
but the increasing rate diminishes. This means that the number of base stations
installed should be more than $n$-times to increase the network capacity by a
factor of $n$. Our results will provide a framework for performance analysis of
the wireless infrastructure with a high density of access points, which will
significantly reduce the burden of network-level simulations.

ABSTRACT_BEGIN
  Rapid progress made in the field of sensor technology, wireless
communication, and computer networks in recent past, led to the development of
wireless Ad-hoc sensor networks, consisting of small, low-cost sensors, which
can monitor wide and remote areas with precision and liveliness unseen to the
date without the intervention of a human operator. This work comes up with a
stochastic model for periodic sensor-deployment (in face of their limited
amount of battery-life) to maintain a minimal node-connectivity in wireless
sensor networks. The node deployment cannot be modeled by using results from
conventional continuous birth-death process, since new nodes are added to the
network in bursts, i.e. the birth process is not continuous in practical
situations. We analyze the periodic node deployment process using discrete
birth-continuous death process and obtain two important statistical measures of
the existing number of nodes in the network, namely the mean and variance. We
show that the above mentioned sequences of mean and variances always converge
to finite steady state values, thus ensuring the stability of the system. We
also develop a cost function for the process of periodic deployment of sensor
nodes and minimize it to find the optimal time ({\tau}) and optimum number of
re-deployment (q) for maintaining minimum connectivity in the network.

ABSTRACT_BEGIN
  Recent increase in energy prices has led researchers to find better ways for
capacity provisioning in data centers to reduce energy wastage due to the
variation in workload. This paper explores the opportunity for cost saving
utilizing the flexibility from the Service Level Agreements (SLAs) and proposes
a novel approach for capacity provisioning under bounded latency requirements
of the workload. We investigate how many servers to be kept active and how much
workload to be delayed for energy saving while meeting every deadline. We
present an offline LP formulation for capacity provisioning by dynamic deferral
and give two online algorithms to determine the capacity of the data center and
the assignment of workload to servers dynamically. We prove the feasibility of
the online algorithms and show that their worst case performance are bounded by
a constant factor with respect to the offline formulation. We validate our
algorithms on a MapReduce workload by provisioning capacity on a Hadoop cluster
and show that the algorithms actually perform much better in practice compared
to the naive `follow the workload' provisioning, resulting in 20-40%
cost-savings.

ABSTRACT_BEGIN
  Adhoc networks are characterized by connectivity through a collection of
wireless nodes and fast changing network topology. Wireless nodes are free to
move independent of each other which makes routing much difficult. This calls
for the need of an efficient dynamic routing protocol. Mesh-based multicast
routing technique establishes communications between mobile nodes of wireless
adhoc networks in a faster and efficient way. In this article the performance
of prominent on-demand routing protocols for mobile adhoc networks such as
ODMRP (On Demand Multicast Routing Protocol), AODV (Adhoc on Demand Distance
Vector) and FSR (Fisheye State Routing protocol) was studied. The parameters
viz., average throughput, packet delivery ration and end-to-end delay were
evaluated. From the simulation results and analysis, a suitable routing
protocol can be chosen for a specified network. The results show that the ODMRP
protocol performance is remarkably superior as compared with AODV and FSR
routing protocols. Keywords: MANET, Multicast Routing, ODMRP, AODV, FSR.

ABSTRACT_BEGIN
  Mobile Agents (MAs) represent a distributed computing technology that
promises to address the scalability problems of centralized network management.
A critical issue that will affect the wider adoption of MA paradigm in
management applications is the development of MA Platforms (MAPs) expressly
oriented to distributed management. However, most of available platforms impose
considerable burden on network and system resources and also lack of essential
functionality. In this paper, we discuss the design considerations and
implementation details of a complete MAP research prototype that sufficiently
addresses all the aforementioned issues. Our MAP has been implemented in Java
and tailored for network and systems management applications.

ABSTRACT_BEGIN
  In this paper, we show how to exploit real-time communication applications to
determine the IP address of a targeted user. We focus our study on Skype,
although other real-time communication applications may have similar privacy
issues. We first design a scheme that calls an identified targeted user
inconspicuously to find his IP address, which can be done even if he is behind
a NAT. By calling the user periodically, we can then observe the mobility of
the user. We show how to scale the scheme to observe the mobility patterns of
tens of thousands of users. We also consider the linkability threat, in which
the identified user is linked to his Internet usage. We illustrate this threat
by combining Skype and BitTorrent to show that it is possible to determine the
file-sharing usage of identified users. We devise a scheme based on the
identification field of the IP datagrams to verify with high accuracy whether
the identified user is participating in specific torrents. We conclude that any
Internet user can leverage Skype, and potentially other real-time communication
systems, to observe the mobility and file-sharing usage of tens of millions of
identified users.

ABSTRACT_BEGIN
  Mobile ad hoc networks (MANETs) are self-configuring wireless networks that
lack permanent infrastructure and are formed among mobile nodes on demand.
Rapid node mobility results in dramatic channel variation, or fading, that
degrades MANET performance. Employing channel state information (CSI) at the
transmitter can improve the throughput of routing and medium access control
(MAC) protocols for mobile ad hoc networks. Several routing algorithms in the
literature explicitly incorporate the fading signal strength into the routing
metric, thus selecting the routes with strong channel conditions. While these
studies show that adaptation to the time-variant channel gain is beneficial in
MANETs, they do not address the effect of the outdated fading CSI at the
transmitter. For realistic mobile node speeds, the channel gain is rapidly
varying, and becomes quickly outdated due the feedback delay. We analyze the
link throughput of joint rate adaptation and adaptive relay selection in the
presence of imperfect CSI. Moreover, for an 802.11 network that employs
geographic opportunistic routing with adaptive rate and relay selection, we
propose a novel method to reduce the effect of the feedback delay at the MAC
layer in the presence of Rayleigh fading. This method exploits channel
reciprocity and fading prediction and does not require significant modification
to the existing 802.11 frame structure. Extensive network simulations
demonstrate that the proposed approach significantly improves the throughput,
delay, and packet delivery ratio for high mobile velocities relative to
previously proposed approaches that employ outdated CSI at the transmitter.

ABSTRACT_BEGIN
  Seamless continuity is the main goal and challenge in fourth generation
Wireless networks (FGWNs), to achieve seamless connectivity "HANDOVER"
technique is used,Handover mechanism are mainly used when a mobile terminal(MT)
is in overlapping area for service continuity. In Heterogeneous wireless
networks main challenge is continual connection among the different networks
like WiFi, WiMax, WLAN, WPAN etc. In this paper, Vertical handover decision
schemes are compared, Simple Additive Weighting method (SAW) and Weighted
product model (WPM) are used to choose the best network from the available
Visitor networks(VTs) for the continuous connection by the mobile terminal. In
our work we mainly concentrated to the handover decision phase and to reduce
the processing delay in the period of handover. In this paper both SAW and WPM
methods are compared with the Qos parameters of the mobile terminal (MT) to
connect with the best network. Keywords: Handover, Vertical handover decision
schemes, Simple additive weighting, Weight product method.

ABSTRACT_BEGIN
  Designing energy-efficient all-to-all multicasting protocols is of of great
importance for multi-hop wireless networks such as wireless sensor networks and
wireless ad hoc networks. In an all-to-all multicast session, there exists a
set of wireless destination nodes, and each destination node needs to send some
data packets to all other destination nodes. We consider the problem of
building a shared multicast tree spanning the destination nodes such that the
total energy consumption of realizing an all-to-all multicast session using the
shared multicast tree is minimized. Since building such a multicast tree has
been proved to be NP-complete, we provide both centralized and distributed
approximation algorithms with provable approximation ratios for it. When the
transmission power of each wireless node is fixed, our centralized and
distributed algorithms have the approximation ratios of $4ln(\Delta+1)+7$ and
13, respectively, where $\Delta$ is the maximum node degree in the network.
When the transmission power of each wireless node is adjustable, both of our
centralized and distributed algorithms have the constant approximation ratio of
145.

ABSTRACT_BEGIN
  We study how long range directional beams can be used for self-organization
of a wireless network to exhibit small world properties. Using simulation
results for randomized beamforming as a guideline, we identify crucial design
issues for algorithm design. Subsequently, we propose an algorithm for
deterministic creation of small worlds. We define a new centrality measure that
estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. This
results in significant reduction in path length while maintaining connectivity.

ABSTRACT_BEGIN
  We investigate the problem of distributed sensors' failure detection in
networks with a small number of defective sensors, whose measurements differ
significantly from neighboring sensor measurements. Defective sensors are
represented by non-zero values in binary sparse signals. We build on the sparse
nature of the binary sensor failure signals and propose a new distributed
detection algorithm based on Group Testing (GT). The distributed GT algorithm
estimates the set of defective sensors from a small number of linearly
independent binary messages exchanged by the sensors. The distributed GT
algorithm uses a low complexity distance decoder that is robust to noisy
messages. We first consider networks with only one defective sensor and
determine the minimal number of linearly independent messages needed for
detection of the defective sensor with high probability. We then extend our
study to the detection of multiple defective sensors by modifying appropriately
the message exchange protocol and the decoding procedure. We show through
experimentation that, for small and medium sized networks, the number of
messages required for successful detection is actually smaller than the minimal
number computed in the analysis. Simulations demonstrate that the proposed
method outperforms methods based on random walk measurements collection in
terms of detection performance and convergence rate. Finally, the proposed
method is resilient to network dynamics due to the effective gossip-based
message dissemination protocol.

ABSTRACT_BEGIN
  In an autonomous wireless sensor network, self-organization of the nodes is
essential to achieve network wide characteristics. We believe that connectivity
in wireless autonomous networks can be increased and overall average path
length can be reduced by using beamforming and bio-inspired algorithms. Recent
works on the use of beamforming in wireless networks mostly assume the
knowledge of the network in aggregation to either heterogeneous or hybrid
deployment. We propose that without the global knowledge or the introduction of
any special feature, the average path length can be reduced with the help of
inspirations from the nature and simple interactions between neighboring nodes.
Our algorithm also reduces the number of disconnected components within the
network. Our results show that reduction in the average path length and the
number of disconnected components can be achieved using very simple local rules
and without the full network knowledge.

ABSTRACT_BEGIN
  One of the most important problems in wireless sensor network is to develop a
routing protocol that has energy efficiency. Since the power of the sensor
Nodes are limited, conserving energy and network life is a critical issue in
wireless sensor network. Clustering is one of the known methods widely used to
face these challenges. In this paper, a cluster based communication protocol
with considering the low energy consumption in wireless sensor networks, is
introduced which balances the energy load among sensor nodes. The nodes close
to each other have more overlap; they sense the same data from environment and
cause a waste of energy by generating repetitive data. In this paper, a cluster
based routing protocol is introduced, in the proposed protocol, in each round a
certain number of nodes are specified; the nodes which have at least one
neighboring node at a distance less than the threshold. Then, among them the
nodes with less energy and greater overlap with their neighbors have been
chosen to go to sleep mode, Also, the energy imbalance among sensor nodes is
reduced by integrating the distance of the nodes from the base station into
clustering policies.

ABSTRACT_BEGIN
  Advanced channel reservation is emerging as an important feature of ultra
high-speed networks requiring the transfer of large files. Applications include
scientific data transfers and database backup. In this paper, we present two
new, on-line algorithms for advanced reservation, called BatchAll and BatchLim,
that are guaranteed to achieve optimal throughput performance, based on
multi-commodity flow arguments. Both algorithms are shown to have
polynomial-time complexity and provable bounds on the maximum delay for
1+epsilon bandwidth augmented networks. The BatchLim algorithm returns the
completion time of a connection immediately as a request is placed, but at the
expense of a slightly looser competitive ratio than that of BatchAll. We also
present a simple approach that limits the number of parallel paths used by the
algorithms while provably bounding the maximum reduction factor in the
transmission throughput. We show that, although the number of different paths
can be exponentially large, the actual number of paths needed to approximate
the flow is quite small and proportional to the number of edges in the network.
Simulations for a number of topologies show that, in practice, 3 to 5 parallel
paths are sufficient to achieve close to optimal performance. The performance
of the competitive algorithms are also compared to a greedy benchmark, both
through analysis and simulation.

ABSTRACT_BEGIN
  In this paper we consider security-related and energy-efficiency issues in
multi-hop wireless networks. We start our work from the observation, known in
the literature, that shortest path routing creates congested areas in multi-hop
wireless networks. These areas are critical--they generate both security and
energy efficiency issues. We attack these problems and set out routing in outer
space, a new routing mechanism that transforms any shortest path routing
protocol (or approximated versions of it) into a new protocol that, in case of
uniform traffic, guarantees that every node of the network is responsible for
relaying the same number of messages, on expectation. We can show that a
network that uses routing in outer space does not have congested areas, does
not have the associated security-related issues, does not encourage selfish
positioning, and, in spite of using more energy globally, lives longer of the
same network using the original routing protocol.

ABSTRACT_BEGIN
  In this work we consider the problem of downlink resource allocation for
proportional fairness of long term received rates of data users and quality of
service for real time sessions in an OFDMA-based wireless system. The base
station allocates available power and bandwidth to individual users based on
long term average received rates, QoS based rate constraints and channel
conditions. We solve the underlying constrained optimization problem and
propose an algorithm that achieves the optimal allocation. Numerical evaluation
results show that the proposed algorithm provides better QoS to voice and video
sessions while providing more and fair rates to data users in comparison with
existing schemes.

ABSTRACT_BEGIN
  In this work we propose an efficient resource allocation algorithm for OFDMA
based wireless systems supporting heterogeneous traffic. The proposed algorithm
provides proportionally fairness to data users and short term rate guarantees
to real-time users. Based on the QoS requirements, buffer occupancy and channel
conditions, we propose a scheme for rate requirement determination for delay
constrained sessions. Then we formulate and solve the proportional fair rate
allocation problem subject to those rate requirements and power/bandwidth
constraints. Simulations results show that the proposed algorithm provides
significant improvement with respect to the benchmark algorithm.

ABSTRACT_BEGIN
  The Semantic Web drives towards the use of the Web for interacting with
logically interconnected data. Through knowledge models such as Resource
Description Framework (RDF), the Semantic Web provides a unifying
representation of richly structured data. Adding logic to the Web implies the
use of rules to make inferences, choose courses of action, and answer
questions. This logic must be powerful enough to describe complex properties of
objects but not so powerful that agents can be tricked by being asked to
consider a paradox. The Web has several characteristics that can lead to
problems when existing logics are used, in particular, the inconsistencies that
inevitably arise due to the openness of the Web, where anyone can assert
anything. N3Logic is a logic that allows rules to be expressed in a Web
environment. It extends RDF with syntax for nested graphs and quantified
variables and with predicates for implication and accessing resources on the
Web, and functions including cryptographic, string, math. The main goal of
N3Logic is to be a minimal extension to the RDF data model such that the same
language can be used for logic and data. In this paper, we describe N3Logic and
illustrate through examples why it is an appropriate logic for the Web.

ABSTRACT_BEGIN
  In this paper, the relationship between the network synchronizability and the
edge distribution of its associated graph is investigated. First, it is shown
that adding one edge to a cycle definitely decreases the network
sychronizability. Then, since sometimes the synchronizability can be enhanced
by changing the network structure, the question of whether the networks with
more edges are easier to synchronize is addressed. It is shown by examples that
the answer is negative. This reveals that generally there are redundant edges
in a network, which not only make no contributions to synchronization but
actually may reduce the synchronizability. Moreover, an example shows that the
node betweenness centrality is not always a good indicator for the network
synchronizability. Finally, some more examples are presented to illustrate how
the network synchronizability varies following the addition of edges, where all
the examples show that the network synchronizability globally increases but
locally fluctuates as the number of added edges increases.

ABSTRACT_BEGIN
  Epidemics-inspired techniques have received huge attention in recent years
from the distributed systems and networking communities. These algorithms and
protocols rely on probabilistic message replication and redundancy to ensure
reliable communication. Moreover, they have been successfully exploited to
support group communication in distributed systems, broadcasting, multicasting
and information dissemination in fixed and mobile networks. However, in most of
the existing work, the probability of infection is determined heuristically,
without relying on any analytical model. This often leads to unnecessarily high
transmission overheads.
  In this paper we show that models of epidemic spreading in complex networks
can be applied to the problem of tuning and controlling the dissemination of
information in wireless ad hoc networks composed of devices carried by
individuals, i.e., human-based networks. The novelty of our idea resides in the
evaluation and exploitation of the structure of the underlying human network
for the automatic tuning of the dissemination process in order to improve the
protocol performance. We evaluate the results using synthetic mobility models
and real human contacts traces.

ABSTRACT_BEGIN
  In ad hoc networks scalability is a critical requirement if these
technologies have to reach their full potential. Most of the proposed routing
protocols do not operate efficiently with networks of more than a few hundred
nodes. In this paper, we propose an augmented tree-based address space
structure and a hierarchical multi-path routing protocol, referred to as
Augmented Tree-based Routing (ATR), which utilizes such a structure in order to
solve the scalability problem and to gain good resilience against node
failure/mobility and link congestion/instability. Simulation results and
performance comparisons with existing protocols substantiate the effectiveness
of the ATR.

ABSTRACT_BEGIN
  We consider a network model where the nodes are grouped into a number of
clusters and propose a distributed dynamic frequency allocation algorithm that
achieves performance close to that of a centralized optimal algorithm. Each
cluster chooses its transmission frequency band based on its knowledge of the
interference that it experiences. The convergence of the proposed distributed
algorithm to a sub-optimal frequency allocation pattern is proved. For some
specific cases of spatial distributions of the clusters in the network,
asymptotic bounds on the performance of the algorithm are derived and
comparisons to the performance of optimal centralized solutions are made. These
analytic results and additional simulation studies verify performance close to
that of an optimum centralized frequency allocation algorithm. It is
demonstrated that the algorithm achieves about 90% of the Shannon capacities
corresponding to the optimum/near-optimum centralized frequency band
assignments. Furthermore, we consider the scenario where each cluster can be in
active or inactive mode according to a two-state Markov model. We derive
conditions to guarantee finite steady state variance for the output of the
algorithm using stochastic analysis. Further simulation studies confirm the
results of stochastic modeling and the performance of the algorithm in the
time-varying setup.

ABSTRACT_BEGIN
  The discovery of Autonomous Systems (ASes) interconnections and the inference
of their commercial Type-of-Relationships (ToR) has been extensively studied
during the last few years. The main motivation is to accurately calculate
AS-level paths and to provide better topological view of the Internet. An
inherent problem in current algorithms is their extensive use of heuristics.
Such heuristics incur unbounded errors which are spread over all inferred
relationships. We propose a near-deterministic algorithm for solving the ToR
inference problem. Our algorithm uses as input the Internet core, which is a
dense sub-graph of top-level ASes. We test several methods for creating such a
core and demonstrate the robustness of the algorithm to the core's size and
density, the inference period, and errors in the core.
  We evaluate our algorithm using AS-level paths collected from RouteViews BGP
paths and DIMES traceroute measurements. Our proposed algorithm
deterministically infers over 95% of the approximately 58,000 AS topology
links. The inference becomes stable when using a week worth of data and as
little as 20 ASes in the core. The algorithm infers 2-3 times more peer-to-peer
relationships in edges discovered only by DIMES than in RouteViews edges,
validating the DIMES promise to discover periphery AS edges.

ABSTRACT_BEGIN
  We consider a multi-channel opportunistic communication system where the
states of these channels evolve as independent and statistically identical
Markov chains (the Gilbert-Elliot channel model). A user chooses one channel to
sense and access in each slot and collects a reward determined by the state of
the chosen channel. The problem is to design a sensing policy for channel
selection to maximize the average reward, which can be formulated as a
multi-arm restless bandit process. In this paper, we study the structure,
optimality, and performance of the myopic sensing policy. We show that the
myopic sensing policy has a simple robust structure that reduces channel
selection to a round-robin procedure and obviates the need for knowing the
channel transition probabilities. The optimality of this simple policy is
established for the two-channel case and conjectured for the general case based
on numerical results. The performance of the myopic sensing policy is analyzed,
which, based on the optimality of myopic sensing, characterizes the maximum
throughput of a multi-channel opportunistic communication system and its
scaling behavior with respect to the number of channels. These results apply to
cognitive radio networks, opportunistic transmission in fading environments,
and resource-constrained jamming and anti-jamming.

ABSTRACT_BEGIN
  We consider point to point link scheduling in Spatial Time Division Multiple
Access (STDMA) wireless networks under the physical interference model. We
propose a novel link scheduling algorithm based on a line graph representation
of the network, by embedding the interferences between pairs of nodes into the
edge weights of the line graph. Our algorithm achieves lower schedule length
and lower run time complexity than existing algorithms.

ABSTRACT_BEGIN
  In the context of radio distributed networks, we present a generalized
approach for the Medium Access Control (MAC) with fixed congestion window. Our
protocol is quite simple to analyze and can be used in a lot of different
situations. We give mathematical evidence showing that our performance is
tight, in the sense that no protocol with fixed congestion window can do
better. We also place ourselves in the WiFi/WiMAX framework, and show
experimental results enlightening collision reduction of 14% to 21% compared to
the best known other methods. We show channel capacity improvement, and
fairness considerations.

ABSTRACT_BEGIN
  The fairness of IEEE 802.11 wireless networks (including Wireless LAN and
Ad-hoc networks) is hard to predict and control because of the randomness and
complexity of the MAC contentions and dynamics. Moreover, asymmetric channel
conditions such as those caused by capture and channel errors often lead to
severe unfairness among stations. In this paper we propose a novel distributed
scheduling algorithm that we call VLS, for ``{\em variable-length
scheduling}'', that provides weighted fairness to all stations despite the
imperfections of the MAC layer and physical channels. Distinct features of VLS
include the use of variable transmission lengths based on distributed
observations, compatibility with 802.11's contention window algorithm,
opportunistic scheduling to achieve high throughput in time-varying wireless
environments, and flexibility and ease of implementation. Also, VLS makes the
throughput of each station more smooth, which is appealing to real-time
applications such as video and voice. Although the paper mostly assumes 802.11
protocol, the idea generally applies to wireless networks based on CSMA
(Carrier Sensing Multiple Access).

ABSTRACT_BEGIN
  Prior work indicates that 802.11 is extremely inefficient for VoIP transport.
Only 12 and 60 VoIP sessions can be supported in an 802.11b and an 802.11g
WLAN, respectively. This paper shows that the bad news does not stop there.
When there are multiple WLANs in the vicinity of each other, the already-low
VoIP capacity can be further eroded in a significant manner. For example, in a
5-by-5, 25-cell multi-WLAN network, the VoIP capacities for 802.11b and 802.11g
are only 1.63 and 10.34 sessions per AP, respectively. This paper investigates
several solutions to improve the VoIP capacity. Based on a conflict graph
model, we propose a clique-analytical call-admission scheme, which increases
the VoIP capacity by 52% and 37% in 802.11b and 802.11g respectively. If all
the three orthogonal frequency channels available in 11b and 11g are used, the
capacity can be nearly tripled by the call-admission scheme. This paper also
proposes for the first time the use of coarse-grained time-division multiple
access (CoTDMA) in conjunction with the basic 802.11 CSMA to eliminate the
performance-degrading exposed-node and hidden-node problems. We find that
CoTDMA can further increase the VoIP capacity in the multi-WLAN scenario by an
additional 35%.

ABSTRACT_BEGIN
  Fast rate adaptation has been established as an effective way to improve the
PHY-layer raw date rate of wireless networks. However, within the current IEEE
802.11 legacy, MAC-layer throughput is dominated by users with the lowest data
rates, resulting in underutilization of bandwidth. In this paper, we propose
and analyze a novel distributed MAC strategy, referred to as Rate-aware DCF
(R-DCF), to leverage the potential of rate adaptation in IEEE 802.11 WLANs. The
key feature of R-DCF is that by introducing different mini slots according to
the instantaneous channel conditions, only contending stations with the highest
data rate can access the channel. In this way, the R-DCF protocol not only
exploits multi-user diversity in a fully distributed manner but also reduces
the loss of throughput due to collisions. Through analysis, we develop an
analytical model to derive the throughput of R-DCF in general multi-rate WLANs.
Using the analytical model we investigate the performance of R-DCF protocol in
various network settings with different rate adaptation strategies and channel
variations. Based on the analysis, we further derive the maximal throughput
achievable by R-DCF. For practical implementation, an offline adaptive backoff
method is developed to achieve a close-to-optimal performance at low runtime
complexity. The superiority of R-DCF is proven via extensive analyses and
simulations.

ABSTRACT_BEGIN
  This paper focuses on Hopf bifurcation control in a dual model of Internet
congestion control algorithms which is modeled as a delay differential equation
(DDE). By choosing communication delay as a bifurcation parameter, it has been
demonstrated that the system loses stability and a Hopf bifurcation occurs when
communication delay passes through a critical value. Therefore, a time-delayed
feedback control method is applied to the system for delaying the onset of
undesirable Hopf bifurcation. Theoretical analysis and numerical simulations
confirm that the delayed feedback controller is efficient in controlling Hopf
bifurcation in Internet congestion control system. Moreover, the direction of
the Hopf bifurcation and the stability of the bifurcating periodic solutions
are determinated by applying the center manifold theorem and the normal form
theory.

ABSTRACT_BEGIN
  This paper focuses on the delay induced Hopf bifurcation in a dual model of
Internet congestion control algorithms which can be modeled as a time-delay
system described by a one-order delay differential equation (DDE). By choosing
communication delay as the bifurcation parameter, we demonstrate that the
system loses its stability and a Hopf bifurcation occurs when communication
delay passes through a critical value. Moreover, the bifurcating periodic
solution of system is calculated by means of perturbation methods. Discussion
of stability of the periodic solutions involves the computation of Floquet
exponents by considering the corresponding Poincare -Lindstedt series
expansion. Finally, numerical simulations for verify the theoretical analysis
are provided.

ABSTRACT_BEGIN
  Wireless Sensor Network (WSN) is an emerging technology that shows great
promise for various futuristic applications both for mass public and military.
The sensing technology combined with processing power and wireless
communication makes it lucrative for being exploited in abundance in future.
The inclusion of wireless communication technology also incurs various types of
security threats. The intent of this paper is to investigate the security
related issues and challenges in wireless sensor networks. We identify the
security threats, review proposed security mechanisms for wireless sensor
networks. We also discuss the holistic view of security for ensuring layered
and robust security in wireless sensor networks.

ABSTRACT_BEGIN
  The miniaturization process of various sensing devices has become a reality
by enormous research and advancements accomplished in Micro Electro-Mechanical
Systems (MEMS) and Very Large Scale Integration (VLSI) lithography. Regardless
of such extensive efforts in optimizing the hardware, algorithm, and protocols
for networking, there still remains a lot of scope to explore how these
innovations can all be tied together to design Wireless Sensor Networks (WSN)
for smartening the surrounding environment for some practical purposes. In this
paper we explore the prospects of wireless sensor networks and propose a design
level framework for developing a smart environment using WSNs, which could be
beneficial for a developing country like Bangladesh. In connection to this, we
also discuss the major aspects of wireless sensor networks.

ABSTRACT_BEGIN
  The impact of extreme events across the globe is extraordinary which
continues to handicap the advancement of the struggling developing societies
and threatens most of the industrialized countries in the globe. Various fields
of Information and Communication Technology have widely been used for efficient
disaster management; but only to a limited extent though, there is a tremendous
potential for increasing efficiency and effectiveness in coping with disasters
with the utilization of emerging wireless network technologies. Early warning,
response to the particular situation and proper recovery are among the main
focuses of an efficient disaster management system today. Considering these
aspects, in this paper we propose a framework for developing an efficient
Disaster Management Communications and Information System (DMCIS) which is
basically benefited by the exploitation of the emerging wireless network
technologies combined with other networking and data processing technologies.

ABSTRACT_BEGIN
  This paper proposes an efficient approach of secure clustering in distributed
sensor networks. The clusters or groups in the network are formed based on
offline rank assignment and predistribution of secret keys. Our approach uses
the concept of weakly connected dominating set (WCDS) to reduce the number of
cluster-heads in the network. The formation of clusters in the network is
secured as the secret keys are distributed and used in an efficient way to
resist the inclusion of any hostile entity in the clusters. Along with the
description of our approach, we present an analysis and comparison of our
approach with other schemes. We also mention the limitations of our approach
considering the practical implementation of the sensor networks.

ABSTRACT_BEGIN
  This paper addresses the problem of reliable transmission of data through a
sensor network. We focus on networks rapidly deployed in harsh environments.
For these networks, important design requirements are fast data transmission
and rapid network setup, as well as minimized energy consumption for increased
network lifetime. We propose a novel broadcasting solution that accounts for
the interference impact and the congestion level of the channel, in order to
improve robustness, energy consumption and delay performance, compared to a
benchmark routing protocol, the GRAB algorithm. Three solutions are proposed:
P-GRAB, a probabilistic routing algorithm for interference mitigation, U-GRAB,
a utility-based algorithm that adjusts to real-time congestion and UP-GRAB, a
combination of P-GRAB and U-GRAB. It is shown that P-GRAB provides the best
performance for geometry-aware networks while the U-GRAB approach is the best
option for unreliable and unstable networks.

ABSTRACT_BEGIN
  This paper investigates the overload problem of a single congested router in
TCP (Transmission Control Protocol) networks. To cope with the congestion
phenomenon, we design a feedback control based on a multiple time-delays model
of the set TCP/AQM (Active Queue Management). Indeed, using robust control
tools, especially in the quadratic separation framework, the TCP/AQM model is
rewritten as an intercon- nected system and a structured state feedback is
constructed to stabilize the network variables. Finally, we illustrate the
proposed methodology with a numerical example and simulations using NS-2
simulator.

ABSTRACT_BEGIN
  Recent research has shown the link between congestion control in
communication networks and feedback control system. In this paper, the design
of an active queue management (AQM) which can be viewed as a controller, is
considered. Based on a state space representation of a linearized fluid flow
model of TCP, the AQM design is converted to a state feedback synthesis problem
for time delay systems. Finally, an example extracted from the literature and
simulations via a network simulator NS (under cross traffic conditions) support
our study.

ABSTRACT_BEGIN
  For the last few years, we assist to a growing interest of designing AQM
(Active Queue Management) using control theory. In this paper, we focus on the
synthesis of an AQM based on the Lyapunov theory for time delay systems. With
the help of a recently developed Lyapunov-Krasovskii functional and using a
state space representation of a linearized fluid model of TCP, two robust AQMs
stabilizing the TCP model are constructed. Notice that our results are
constructive and the synthesis problem is reduced to a convex optimization
scheme expressed in terms of linear matrix inequalities (LMIs). Finally, an
example extracted from the literature and simulations via {\it NS simulator}
support our study.

ABSTRACT_BEGIN
  Several studies have considered control theory tools for traffic control in
communication networks, as for example the congestion control issue in IP
(Internet Protocol) routers. In this paper, we propose to design a linear
observer for time-delay systems to address the traffic monitoring issue in
TCP/AQM (Transmission Control Protocol/Active Queue Management) networks. Due
to several propagation delays and the queueing delay, the set TCP/AQM is
modeled as a multiple delayed system of a particular form. Hence, appropriate
robust control tools as quadratic separation are adopted to construct a delay
dependent observer for TCP flows estimation. Note that, the developed mechanism
enables also the anomaly detection issue for a class of DoS (Denial of Service)
attacks. At last, simulations via the network simulator NS-2 and an emulation
experiment validate the proposed methodology.

ABSTRACT_BEGIN
  A new method of estimating some statistical characteristics of TCP flows in
the Internet is developed in this paper. For this purpose, a new set of random
variables (referred to as observables) is defined. When dealing with sampled
traffic, these observables can easily be computed from sampled data. By
adopting a convenient mouse/elephant dichotomy also dependent on traffic, it is
shown how these variables give a reliable statistical representation of the
number of packets transmitted by large flows during successive time intervals
with an appropriate duration. A mathematical framework is developed to estimate
the accuracy of the method. As an application, it is shown how one can estimate
the number of large TCP flows when only sampled traffic is available. The
algorithm proposed is tested against experimental data collected from different
types of IP networks.

ABSTRACT_BEGIN
  We present a novel geographical routing scheme for spontaneous wireless mesh
networks. Greedy geographical routing has many advantages, but suffers from
packet losses occurring at the border of voids. In this paper, we propose a
flexible greedy routing scheme that can be adapted to any variant of
geographical routing and works for any connectivity graph, not necessarily Unit
Disk Graphs. The idea is to reactively detect voids, backtrack packets, and
propagate information on blocked sectors to reduce packet loss. We also propose
an extrapolating algorithm to reduce the latency of void discovery and to limit
route stretch. Performance evaluation via simulation shows that our modified
greedy routing avoids most of packet losses.

ABSTRACT_BEGIN
  The stack in various forms has been widely used as an architectural template
for networking systems. Recently the stack has been subject to criticism for a
lack of flexibility. However, when it comes right down to it nobody has offered
a truly compelling alternative. Various cross-layer optimizations have been
proposed, but these optimizations are frequently hacks to achieve a particular
goal and offer no direct insight into why the existing network stack is
inadequate. We propose that a fundamental problem with the existing network
stack is that it attempts to layer functionality that is not well-suited to
layering. In this work we use a "bottom up" model of information computation,
storage, and transfer and the "top down" goals of networking systems to
formulate a modular decomposition of networking systems. Based on this modular
decomposition we propose a semantic layered structure for networking systems
that eliminates many awkward cross-layer interactions that arise in the
canonical layered stack.

ABSTRACT_BEGIN
  Energy consumption is the most challenging issue in routing protocol design
for mobile ad-hoc networks (MANETs), since mobile nodes are battery powered.
Furthermore, replacing or recharging batteries is often impossible in critical
environments such as in military or rescue missions. In a MANET, the energy
depletion of a node does not affect the node itself only, but the overall
network lifetime. In this paper, we present multipath and energy-aware on
demand source routing (MEA-DSR) protocol, which exploits route diversity and
information about batteries-energy levels for balancing energy consumption
between mobile nodes. Simulation results, have shown that MEA-DSR protocol is
more energy efficient than DSR in almost mobility scenarios.

ABSTRACT_BEGIN
  Mobile ad hoc networks (MANETs) consist of a collection of wireless mobile
nodes which dynamically exchange data without reliance on a fixed base station
or a wired backbone network, which makes routing a crucial issue for the design
of a ad hoc networks. In this paper we discussed a hybrid multipath routing
protocol named MP-OLSR. It is based on the link state algorithm and employs
periodic exchange of messages to maintain topology information of the networks.
In the mean time, it updates the routing table in an on-demand scheme and
forwards the packets in multiple paths which have been determined at the
source. If a link failure is detected, the algorithm recovers the route
automatically. Concerning the instability of the wireless networks, the
redundancy coding is used to improve the delivery ratio. The simulation in NS2
shows that the new protocol can effectively improve the performance of the
networks.

ABSTRACT_BEGIN
  In this paper we discussed the application and the implementation of
multipath routing and multiple description coding (MDC) extension of OLSR,
called MP-OLSR. It is based on the link state algorithm and employs periodic
exchange of messages to maintain topology information of the networks. In the
mean time, it updates the routing table in an on-demand scheme and forwards the
packets in multiple paths which have been determined at the source. If a link
failure is detected, the algorithm recovers the route automatically. Concerning
the instability of the wireless networks, the multiple description coding is
used to improve reliability of the network transmission, and several methods
are proposed to allocate the redundancy in different paths. The simulation in
NS2 shows that the new protocol can effectively improve the performance of the
networks. The implementation of MP-OLSR is also proposed in the end.

ABSTRACT_BEGIN
  To minimize the number of wavelengths required by a multicast session in
sparse light splitting wavelength division multiplexing (WDM) networks, a
light-hierarchy structure, which occupies the same wavelength on all links, is
proposed to span as many destinations as possible. Different from a light-tree,
a light-hierarchy accepts cycles, which are used to traverse crosswise a
4-degree (or above) multicast incapable (MI) node twice (or above) and switch
two light signals on the same wavelengths to two destinations in the same
multicast session. In this paper, firstly, a graph renewal and distance
priority light-tree algorithm (GRDP-LT) is introduced to improve the quality of
light-trees built for a multicast request. Then, it is extended to compute
light-hierarchies. Obtained numerical results demonstrate the GRDP-LT
light-trees can achieve a much lower links stress, better wavelength channel
cost, and smaller average end-to-end delay as well as diameter than the
currently most efficient algorithm. Furthermore, compared to light-trees, the
performance in terms of link stress and network throughput is greatly improved
again by employing the light-hierarchy, while consuming the same amount of
wavelength channel cost.

ABSTRACT_BEGIN
  Based on the false assumption that multicast incapable (MI) nodes could not
be traversed twice on the same wavelength, the light-tree structure was always
thought to be optimal for multicast routing in sparse splitting Wavelength
Division Multiplexing (WDM) networks. In fact, for establishing a multicast
session, an MI node could be crosswise visited more than once to switch a light
signal towards several destinations with only one wavelength through different
input and output pairs. This is called Cross Pair Switching (CPS). Thus, a new
multicast routing structure light-hierarchy is proposed for all-optical
multicast routing, which permits the cycles introduced by the CPS capability of
MI nodes. We proved that the optimal structure for minimizing the cost of
multicast routing is a set of light-hierarchies rather than the light-trees in
sparse splitting WDM networks. Integer linear programming (ILP) formulations
are developed to search the optimal light-hierarchies. Numerical results
verified that the light-hierarchy structure could save more cost than the
light-tree structure.

ABSTRACT_BEGIN
  In this articlewestudy themulticast routing problem in all-opticalWDMnetworks
under the spare light splitting constraint. To implement a multicast session,
several light-trees may have to be used due to the limited fanouts of network
nodes. Although many multicast routing algorithms have been proposed in order
to reduce the total number of wavelength channels used (total cost) for a
multicast session, the maximum number of wavelengths required in one fiber link
(link stress) and the end-to-end delay are two parameters which are not always
taken into consideration. It is known that the shortest path tree (SPT) results
in the optimal end-to-end delay, but it can not be employed directly for
multicast routing in sparse light splitting WDM networks. Hence, we propose a
novel wavelength routing algorithm which tries to avoid the multicast incapable
branching nodes (MIBs, branching nodes without splitting capability) in the
shortest-path-based multicast tree to diminish the link stress. Good parts of
the shortest-path-tree are retained by the algorithm to reduce the end-to-end
delay. The algorithm consists of tree steps: (1) aDijkstraPro algorithmwith
priority assignment and node adoption is introduced to produce a SPT with up to
38% fewer MIB nodes in the NSF topology and 46% fewerMIB nodes in the USA
Longhaul topology, (2) critical articulation and deepest branch heuristics are
used to process the MIB nodes, (3) a distance-based light-tree reconnection
algorithm is proposed to create the multicast light-trees. Extensive
simulations demonstrate the algorithm's efficiency in terms of link stress and
end-to-end delay.

ABSTRACT_BEGIN
  In sparse light splitting all-optical WDM networks, the more destinations a
light-tree can accommodate, the fewer light-trees andwavelengths amulticast
session will require. In this article, a Hypo-Steiner light-tree algorithm
(HSLT) is proposed to construct a HSLT light-tree to include as many
destinations as possible. The upper bound cost of the light-trees built by HSLT
is given as N(N -1)/2, where N is the number of nodes in the network. The
analytical model proves that, under the same condition, more destinations could
be held in a HSLT than a Member-Only (Zhang et al., J. Lightware Technol,
18(12), 1917-1927 2000.) light-tree. Extensive simulations not only validate
the proof but also show that the proposed heuristic outperforms the existing
multicast routing algorithms by a large margin in terms of link stress,
throughput, and efficiency ofwavelength usage.

ABSTRACT_BEGIN
  In this paper, we propose a new method of channel estimation for asynchronous
additive white Gaussian noise channels in satellite communications. This method
is based on signals correlation and multiuser interference cancellation which
adopts a successive structure. Propagation delays and signals amplitudes are
jointly estimated in order to be used for data detection at the receiver. As, a
multiuser detector, a single stage successive interference cancellation (SIC)
architecture is analyzed and integrated to the channel estimation technique and
the whole system is evaluated. The satellite access method adopted is the
direct sequence code division multiple access (DS CDMA) one. To evaluate the
channel estimation and the detection technique, we have simulated a satellite
uplink with an asynchronous multiuser access.

ABSTRACT_BEGIN
  We present an architecture of a hosting system consisting of a set of hosted
Web Services subject to QoS constraints, and a certain number of servers used
to run users demand. The traffic is session-based, while provider and users
agree on SLAs specifying the expected level of service performance such that
the service provider is liable to compensate his/her customers if the level of
performance is not satisfactory. The system is driven by a utility function
which tries to optimize the average earned revenue per unit time. The
middleware collects demand and performance statistics, and estimates traffic
parameters in order to make dynamic decisions concerning server allocation and
admission control. We empirically evaluate the effects of admission policies,
resource allocation and service differentiation schemes on the achieved
revenues, and we find that our system is robust enough to successfully deal
with session-based traffic under different conditions.

ABSTRACT_BEGIN
  Wide implementation of IEEE 802.11 based networks could lead to deployment of
localized wireless data communication environments with a limited number of
mobile hosts, called ad hoc networks. Implementation of a proper routing
methodology in ad hoc networks makes it efficient in terms of performance. A
wide spectrum of routing protocols has been contributed by several researchers.
Real time applications have been most popular among the applications, run by ad
hoc networks. Such applications strictly adhere to the Quality of Service (QoS)
requirements such as overall throughput, end-toend delay and power level.
Support of QoS requirements becomes more challenging due to dynamic nature of
MANETs, where mobility of nodes results in frequent change in topology. QoS
aware routing protocols can serve to the QoS support, which concentrate on
determining a path between source and destination with the QoS requirements of
the flow being satisfied. We propose a protocol, called Power and Delay aware
Temporally Ordered Routing Algorithm (PDTORA), based on Temporally Ordered
Routing Algorithm (TORA) Protocol, where verification of power and delay
requirements is carried out with a query packet at each node along the path
between source and destination. Simulations justify better performance of the
proposed new protocol in terms of network lifetime, end-to-end delay and packet
delivery ratio as compared to TORA.

ABSTRACT_BEGIN
  Spectrum sensing receives much attention recently in the cognitive radio (CR)
network research, i.e., secondary users (SUs) constantly monitor channel
condition to detect the presence of the primary users (PUs). In this paper, we
go beyond spectrum sensing and introduce the PU separation problem, which
concerns with the issues of distinguishing and characterizing PUs in the
context of collaborative spectrum sensing and monitor selection. The
observations of monitors are modeled as boolean OR mixtures of underlying
binary sources for PUs. We first justify the use of the binary OR mixture model
as opposed to the traditional linear mixture model through simulation studies.
Then we devise a novel binary inference algorithm for PU separation. Not only
PU-SU relationship are revealed, but PUs' transmission statistics and
activities at each time slot can also be inferred. Simulation results show that
without any prior knowledge regarding PUs' activities, the algorithm achieves
high inference accuracy even in the presence of noisy measurements.

ABSTRACT_BEGIN
  Electronic collaboration among devices in a geographically localized
environment is made possible with the implementation of IEEE 802.11 based
wireless ad hoc networks. Dynamic nature of mobile ad hoc networks(MANETs) may
lead to unpredictable intervention of attacks or fault occurrence, which
consequently may partition the network, degrade its performance, violate the
QoS requirements and most importantly, affect bandwidth allocation to mobile
nodes in the network. In this paper, we propose a new distributed cluster
scheme for MANETs, especially in harsh environments, based on the concept of
survivability to support QoS requirements and to protect bandwidth efficiently.
With the incorporation of clustering algorithms in survivability technology, we
employ a simple network configuration and expect to reduce occurrences of
faults in MANETs. At the same time, we address the scalability problem, which
represents a great challenge to network configuration. We do expect a
simplification of accessing bandwidth allocation with required QoS support for
different applications.

ABSTRACT_BEGIN
  The behavior of Wireless Sensor Networks (WSN) is nowadays widely analyzed.
One of the most important issues is related to their energy consumption, as
this has a major impact on the network lifetime. Another important application
requirement is to ensure data sensing synchronization, which leads to
additional energy consumption as a high number of messages is sent and received
at each node. Our proposal consists in implementing a combined synchronization
protocol based on the IEEE 1588 standard that was designed for wired networks
and the PBS (Pairwise Broadcast Synchronization) protocol that was designed for
sensor networks, as none of them is able to provide the needed synchronization
accuracy for our application on its own. The main goals of our new
synchronization protocol are: to ensure the accuracy of local clocks up to a
tenth of a microsecond and to provide an important energy saving. Our results
obtained using NS-2 (Network Simulator) show that the performance of our
solution (IEEE 1588-PBS) matches our application requirements with regard to
the synchronization, with a significant improvement in energy saving.

ABSTRACT_BEGIN
  A maximum entropy framework based on Tsallis entropy is proposed to depict
long tail behavior of queue lengths in broadband networks. Queue length
expression as measured in terms of number of packets involves Hurwitz-zeta
function. When the entropy parameter q in Tsallis entropy is less than unity,
the distribution of packets yields power law behavior. In the limit q tending
to unity, Tsallis entropy expression reduces to one due to Shannon and
well-known results of M/M/1 queuing system are recovered. Relationship between
Tsallis entropy parameter q and Hurst parameter H (measure of self-similarity)
is postulated. A numerical procedure based on Newton-Raphson method is outlined
to compute Lagrange's parameter b. Various relationships between traffic
intensity r and Lagrange's parameter b are examined using data generated from
mean number of packets from storage model due to Norros. It is found that best
fit corresponds to r being a linear combination of decaying exponential and
power exponent in b for different values of entropy parameter q. Explicit
expression for the probability that queue size exceeds a certain value is
derived and it is established that it asymptotically follows power law for q
less than one. The system utilization shows an interesting behavior when the
parameter r is varied. It attains lower values than that of M/M/1 system for
smaller values of r whereas situation reverses for higher values of r.

ABSTRACT_BEGIN
  Routing is the main part of wireless adhoc network conventionally there are
two approaches first one is Proactive and another one is Reactive. Both these
approaches have some substantial disadvantage and to overcome hybrid routing
protocols designed. ZRP (Zone Routing Protocol) is one of the hybrid routing
protocols, it takes advantage of proactive approach by providing reliability
within the scalable zone, and for beyond the scalable zone it looks for the
reactive approach. It (ZRP) uses the proactive and the reactive routing
according to the need of the application at that particular instance of time
depending upon the prevailing scenario. This work revolves around the
performance of ZRP against realistic parameters by varying various attributes
such as Zone Radius of ZRP in different node density. Results vary as we change
the node density on Qualnet 4.0 network simulator.

ABSTRACT_BEGIN
  The communications sector is undergoing significant changes, with the
emergence of a number of platforms available to provide a different range of
services. Some of these platforms are complementary to each other, while others
are competitive, or can provide a valid substitute for some of the services
provided. Up till now, the most important communications platform in most of
the developing countries has been the public switched telecommunication network
(PSTN) which provides access to all households and buildings. This universality
in providing access has also meant that the network has generally been
designated as one for universal service.This chapter focuses on the area where
the most significant changes are taking place in the communication sector. The
objective of this chapter is neither to give an overview of all communication
platforms, nor is it aimed to assess the relative extent to which different
platforms complement or compete with each other. The central theme of this
chapter is to examine the developments in what is commonly refereed to as next
generation access networks and next generation core networks and their role in
convergence.

ABSTRACT_BEGIN
  Current and emerging mobile devices are omni directional in wireless
communication. Such omni directionality not only limits device energy
efficiency but also poses a significant challenge toward the capacity of
wireless networks through inter-link interference. In this work, we seek to
make mobile clients directional with beamsteering. We first demonstrate that
beamsteering is already feasible to mobile devices such as Netbooks and eBook
readers in terms of form factor, power efficiency, and device mobility. We
further reveal that beamsteering mobile clients face a unique challenge to
balance client efficiency and network capacity. There is an optimal operating
point for a beamsteering mobile client in terms of the number of antennas and
transmit power that achieve the required capacity with lowest power. Finally,
we provide a distributed algorithm called BeamAdapt that allows each client to
closely approach its optimal point iteratively without central coordination. We
also offer a cellular system realization of BeamAdapt. Using Qualnet-based
simulation, we show that BeamAdapt with four antennas can reduce client power
consumption by 55% while maintaining a required network throughput for a
large-scale network, compared to the same network with omni directional mobile
clients.

ABSTRACT_BEGIN
  This work concerns physical layer collision recovery for cheap sensors with
allowed variations in frequency and delay of their communications. The work is
presented as a generic, communication theoretic framework and demonstrated
using UHF RFID tag technology. Previous work in this area has not provided
recovery for more than two tags, which is shown to be possible in this work.
Also presented is a novel mathematical model of the tag signal, incorporating
the allowed variations in frequency and delay. The main motivation is seen in
the observation that random variations in frequency and delay make the collided
signals of different tags separable. The collision recovery is done by
estimating the sensor specific variation in frequency and delay and using these
estimates in a successive interference cancellation algorithm and a maximum
likelihood sequence decoder, to iteratively reconstruct a sensor signal and
remove it from the received signal. Numerical simulations show that the
estimates and proposed algorithm are effective in recovering collisions. The
proposed algorithm is then incorporated into a numerical simulation of the
Qprotocol for UHF RFID tags and is shown to be effective in providing fast and
power efficient sensor arbitration.

ABSTRACT_BEGIN
  This paper attends to the problem of embedding flexibly specified CloudNets,
virtual networks connecting cloud resources (such as storage or computation).
We attend to a scenario where customers can request CloudNets at short notice,
and an infrastructure provider (or a potential itermediate broker or reseller)
first embeds the CloudNet fast (e.g., using a simple heuristic). Later,
however, long-lived CloudNets embeddings are optimized by migrating them to
more suitable locations, whose precise definition depends on a given objective
function. For instance, such migrations can be useful to reduce the peak
resource loads in the network by spreading CloudNets across the infrastructure,
to save energy by moving CloudNets together and switching off unused
components, or for maintenance purposes.
  We present a very generic algorithm to compute optimal embeddings of
CloudNets: It allows for different objective functions (such as load
minimization or energy conservation), supports cost-aware migration, and can
deal with all link types that arise in practice (e.g., full-duplex or even
wireless or wired broadcast links with multiple endpoints). Our evaluation
shows that such a rigorous optimization is even feasible in order to optimize a
moderate-size CloudNet of full flexibility (e.g., a router site, a small
physical infrastructure or virtual provider network).

ABSTRACT_BEGIN
  In this paper, we carry-out a study of the Quality of Service (QoS) mechanism
in IEEE802.11e Enhanced Distribution Coordination Function (EDCF) and how it is
achieved by providing traffics with different priorities. It can perform the
access to the radio channel or just simply it can considerably be declined
subsequently to a variation of network dynamicity. The results of the proposed
analysis show that the EDCF scheduler looses the ability of the traffic
differentiation and becomes insensitive to the QoS priority requirements.
Consequently, it goes away from the region of stability and EDCF doesn't offer
better performance than the conventional DCF scheme. Therefore, traffic
specifications are weakly applied only for the channel occupation time
distribution. During the handoff between the Base Stations (BS's), the response
time of the data rate application within the roaming process grows to the
initial throughput level. Performance metrics at the MAC layer, like
throughput, End-2-End delay, and packet loss have been evaluated.

ABSTRACT_BEGIN
  In this paper, we study Coded relay (Crelay) in multi-hop wireless networks.
Crelay exploits both partial packets and overhearing capabilities of the
wireless nodes, and uses Forward Error Correction code in packet forwarding.
When a node overhears a partial packet from an upstream node, it informs the
upstream node about the number of parity bytes needed to correct the errors,
such that the upstream node need only send a small amount of parity bytes
instead of the complete packet, hence improving the network efficiency. Our
main contributions include the following. First, we propose an efficient
network protocol that can exploit partial packets and overhearing. Second, we
study the routing problem in networks with Crelay and propose a greedy
algorithm for finding the paths. Third, we propose an error ratio estimator,
called AMPS, that can estimate the number of byte errors in a received frame
with good accuracy at a low overhead of only 8 bytes per frame, where the
estimator is needed for a node to find the number of needed parity bytes.
Fourth, we implement the proposed protocol and algorithm within the Click
modular router, and our experiments show that Crelay can significantly improve
the performance of wireless networks.

ABSTRACT_BEGIN
  In high rate DS-UWB systems with low spreading factor, the selective
multipath interference canceller with linear equalization (SMPIC-LE) is
developed to alleviate severe multipath interferences induced by the poor
orthogonality of spreading codes. The SMPIC iteratively mitigates the strongest
inter-path interference, inter-chip interference and inter-symbol interference,
while the former two are unresolvable in conventional RAKE-decision feedback
equalizer (DFE) receivers. The numerical results and complexity analysis
demonstrate that SMPIC-LE with proper parameters provides an attractive overall
advantage in performance and computational complexity compared with RAKE-DFE.
In addition, it approaches the matched filter bound well as the RAKE finger in
SMPIC increases.

ABSTRACT_BEGIN
  Being most popular and IETF standard metric, minimum hop count is
appropriately used by Ad hoc Networks, as new paths must rapidly be found in
the situations where quality paths could not be found in due time due to high
node mobility. There always has been a tradeoff between throughput and energy
consumption, but stationary topology of WMNs and high node density of WSN's
benefit the algorithms to consider quality-aware routing to choose the best
routes. In this paper, we analytically review ongoing research on wireless
routing metrics which are based on ETX (Expected Transmission Count) as it
performs better than minimum hop count under link availability. Performances
over ETX, target platforms and design requirements of these ETX based metrics
are high-lighted. Consequences of the criteria being adopted (in addition to
expected link layer transmissions & retransmissions) in the form of
incremental: (1) performance overheads and computational complexity causing
inefficient use of network resources and instability of the routing algorithm,
(2) throughput gains achieved with better utilization of wireless medium
resources have been elaborated.

ABSTRACT_BEGIN
  We consider a single station (STA) in the Power Save Mode (PSM) of an IEEE
802.11 infrastructure WLAN. This STA is assumed to be carrying uplink and
downlink traffic via the access point (AP). We assume that the transmission
queues of the AP and the STA are saturated, i.e., the AP and the STA always
have at least one packet to send. For this scenario, it is observed that uplink
and downlink throughputs achieved are different. The reason behind the
difference is the long term attempt rates of the STA and the AP due to the PSM
protocol. In this paper we first obtain the the long term attempt rates of the
STA and the AP and using these, we obtain the saturation throughputs of the AP
and the STA. We provide a validation of analytical results using the NS-2
simulator.

ABSTRACT_BEGIN
  Optimizing interconnection networks is a prime object in switching schemes.
In this work the authors present a novel approach for obtaining a required
channel arrangement in a multi-stage interconnection network, using a new
concept - a fundamental arrangement. The fundamental arrangement is an initial
N-1 stage switch arrangement that allows obtaining any required output channel
arrangement given an input arrangement, using N/2 binary switches at each
stage. The paper demonstrates how a fundamental arrangement can be achieved and
how, once this is done, any required arrangement may be obtained within 2(N-1)
steps.

ABSTRACT_BEGIN
  Cognitive Radio (CR) aims to increase the spectrum utilization by allowing
secondary users (SU) to access unused licensed spectrum bands. To maximize the
throughput given limited sensing capability, SUs need to strike a balance
between sensing the channels that are not heavily used by primary users (PU)
and avoiding collisions with other SUs. To randomize sensing decisions without
resorting to multiuser sensing policies, it is proposed to exploit the
spatially-variant fading channel conditions on different links by adapting the
reward to the channel state information (CSI). Moreover, the proposed
channel-adaptive policy favors links with high achievable transmission rate and
thus further improves the network throughput.

ABSTRACT_BEGIN
  Given a network infrastructure (e.g., data-center or on-chip-network) and a
distribution on the source-destination requests, the expected path (route)
length is an important measure for the performance, efficiency and power
consumption of the network. In this work we initiate a study on self-adjusting
networks: networks that use local-distributed mechanisms to adjust the position
of the nodes (e.g., virtual machines) in the network to best fit the route
requests distribution. Finding the optimal placement of nodes is defined as the
minimum expected path length (MEPL) problem. This is a generalization of the
minimum linear arrangement (MLA) problem where the network infrastructure is a
line and the computation is done centrally. In contrast to previous work, we
study the distributed version and give efficient and simple approximation
algorithms for interesting and practically relevant special cases of the
problem. In particular, we consider grid networks in which the distribution of
requests is a symmetric product distribution. In this setting, we show that a
simple greedy policy of position switching between neighboring nodes to locally
minimize an objective function, achieves good approximation ratios. We are able
to prove this result using the useful notions of expected rank of the
distribution and the expected distance to the center of the graph.

ABSTRACT_BEGIN
  the storage infrastructure is the foundation on which information relies and
therefore must support a company's business objectives and business model. In
this environment, simply deploying more and faster storage devices is not
enough; a new kind of infrastructure is needed, one that provides more enhanced
network availability, data accessibility, and system manageability than is
provided by today's infrastructure. The SAN meets this challenge. The SAN
liberates the storage device, so it is not on a particular server bus, and
attaches it directly to the network. In other words, storage is externalized
and functionally distributed across the organization. The SAN also enables the
centralizing of storage devices and the clustering of servers, which makes for
easier and less expensive administration. So the idea is to create an
intelligent SAN infrastructure that stretches to meet increased demands, allows
highly available and heterogeneous access to expanding information.

ABSTRACT_BEGIN
  We propose an all-optical networking solution for a wide area network (WAN)
based on the notion of multipoint-to-multipoint lightpaths that, for short, we
call "multipaths". A multipath concentrates the traffic of a group of source
nodes on a wavelength channel using an adapted MAC protocol and multicasts this
traffic to a group of destination nodes that extract their own data from the
confluent stream. The proposed network can be built using existing components
and appears less complex and more efficient in terms of energy consumption than
alternatives like OPS and OBS. The paper presents the multipath architecture
and compares its energy consumption to that of a classical router-based ISP
network. A flow-aware dynamic bandwidth allocation algorithm is proposed and
shown to have excellent performance in terms of throughput and delay.

ABSTRACT_BEGIN
  IP Multicast is one of the most absolute method for large bandwidth Internet
applications such as video conference, IPTV, E-Learning and Telemedicine etc.,
But due to security and management reason IP Multicast is not enabled in
Internet backbone routers. To achieve these challenges, lot of Application
Layer Multicast (ALM) has been proposed. All the existing protocols such as
NICE, ZIGZAG and OMNI are trying to reduce average delay by forming a Multicast
tree. But still that problem has not been addressed fully. We are proposing a
new protocol called NetRawALM, which will address the average delay,
Reliability between nodes, Scalability of conference, Heterogeneity and
resilient data distribution for real time multimedia applications by
constructing the Network based Resource aware Multicast tree algorithm. This is
very dynamic and decentralised. The proposed architecture is a LAN aware; it is
used to reduce Internet Traffic.

ABSTRACT_BEGIN
  In this paper, we propose QoS aware MAC protocol for Wire- less Sensor
Networks and its cross layer extension to network layer for providing QoS in
delay sensitive WSN scenarios. In WSNs, there can be two types of traffic one
is event driven traffic which requires immedi- ate attention and another is
periodic reporting. Event driven traffic is classified as Class I(delay
sensitive) traffic and periodic reporting is clas- sified as Class II(Best
Effort) Traffic. MAC layer adaptation can take place in terms of (i) Dynamic
contention window adjustment per class, (ii) Reducing the delay suffered by
difference in Sleep schedules(DSS) of communicating nodes by dynamically
adjusting Duty Cycle based on Utilization and DSS delay of class I traffic,
(iii) Different DIFS (DCF Inter Frame Spacing) per class, (iv) Adjusting all
the three schemes pro- posed above simultaneously. Cross layer extension is
also proposed, in which MAC layer uses network layer's next hop information for
better adaptation of duty cycle based on DSS delay. Routing protocols can uti-
lize MAC layer parameter DSS delay to select the routes which offer least DSS
delay latency, there by minimizing the overall end-to-end delay.

ABSTRACT_BEGIN
  Graph coloring is used in wireless networks to optimize network resources:
bandwidth and energy. Nodes access the medium according to their color. It is
the responsibility of the coloring algorithm to ensure that interfering nodes
do not have the same color. In this research report, we focus on wireless
sensor networks with grid topologies. How does a coloring algorithm take
advantage of the regularity of grid topology to provide an optimal periodic
coloring, that is a coloring with the minimum number of colors? We propose the
Vector-Based Coloring Method, denoted VCM, a new method that is able to provide
an optimal periodic coloring for any radio transmission range and for any h-hop
coloring, h>=1. This method consists in determining at which grid nodes a color
can be reproduced without creating interferences between these nodes while
minimizing the number of colors used. We compare the number of colors provided
by VCM with the number of colors obtained by a distributed coloring algorithm
with line and column priority assignments. We also provide bounds on the number
of colors of optimal general colorings of the infinite grid, and show that
periodic colorings (and thus VCM) are asymptotically optimal. Finally, we
discuss the applicability of this method to a real wireless network.

ABSTRACT_BEGIN
  This paper proposes and evaluates a new position-based Parallel Routing
Protocol (PRP) for simultaneously routing multiple data packets over disjoint
paths in a mobile ad-hoc network (MANET) for higher reliability and reduced
communication delays. PRP views the geographical region where the MANET is
located as a virtual 2-dimensional grid of cells. Cell-disjoint (parallel)
paths between grid cells are constructed and used for building pre-computed
routing tables. A single gateway node in each grid cell handles routing through
that grid cell reducing routing overheads. Each node maintains updated
information about its own location in the virtual grid using GPS. Nodes also
keep track of the location of other nodes using a new proposed cell-based
broadcasting algorithm. Nodes exchange energy level information with neighbors
allowing energy-aware selection of the gateway nodes. Performance evaluation
results have been derived showing the attractiveness of the proposed parallel
routing protocol from different respects including low communication delays,
high packet delivery ratios, high routing path stability, and low routing
overheads.

ABSTRACT_BEGIN
  Human motion in the vicinity of a wireless link causes variations in the link
received signal strength (RSS). Device-free localization (DFL) systems, such as
variance-based radio tomographic imaging (VRTI), use these RSS variations in a
static wireless network to detect, locate and track people in the area of the
network, even through walls. However, intrinsic motion, such as branches moving
in the wind and rotating or vibrating machinery, also causes RSS variations
which degrade the performance of a DFL system. In this paper, we propose and
evaluate two estimators to reduce the impact of the variations caused by
intrinsic motion. One estimator uses subspace decomposition, and the other
estimator uses a least squares formulation. Experimental results show that both
estimators reduce localization root mean squared error by about 40% compared to
VRTI. In addition, the Kalman filter tracking results from both estimators have
97% of errors less than 1.3 m, more than 60% improvement compared to tracking
results from VRTI.

ABSTRACT_BEGIN
  The main cause of wasted energy consumption in wireless sensor networks is
packet collision. The packet scheduling algorithm is therefore introduced to
solve this problem. Some packet scheduling algorithms can also influence and
delay the data transmitting in the real-time wireless sensor networks. This
paper presents the packet scheduling algorithm (PSA) in order to reduce the
packet congestion in MAC layer leading to reduce the overall of packet
collision in the system The PSA is compared with the simple CSMA/CA and other
approaches using network topology benchmarks in mathematical method. The
performances of our PSA are better than the standard (CSMA/CA). The PSA
produces better throughput than other algorithms. On other hand, the average
delay of PSA is higher than previous works. However, the PSA utilizes the
channel better than all algorithms.

ABSTRACT_BEGIN
  Industry experience indicates that the ability to incrementally expand data
centers is essential. However, existing high-bandwidth network designs have
rigid structure that interferes with incremental expansion. We present
Jellyfish, a high-capacity network interconnect, which, by adopting a random
graph topology, yields itself naturally to incremental expansion. Somewhat
surprisingly, Jellyfish is more cost-efficient than a fat-tree: A Jellyfish
interconnect built using the same equipment as a fat-tree, supports as many as
25% more servers at full capacity at the scale of a few thousand nodes, and
this advantage improves with scale. Jellyfish also allows great flexibility in
building networks with different degrees of oversubscription. However,
Jellyfish's unstructured design brings new challenges in routing, physical
layout, and wiring. We describe and evaluate approaches that resolve these
challenges effectively, indicating that Jellyfish could be deployed in today's
data centers.

ABSTRACT_BEGIN
  In today's world Wireless Ad-hoc sensor network, consists of small sensor
nodes having limited resources, has a great potential to solve problems in
various domain including disaster management. In this paper "QCS-protocol" is
modified which was introduced in our previous paper [1] and named as "Modified
QCS-protocol". This is the backbone of our Intelligent Energy Efficient Ad-hoc
Sensor Network. Two other protocols "Irregular Information Transfer" & "Final
Broadcast-Petrol Flow" protocol are also modified to enhance performance of the
new version of QCS protocol to run the system properly and to make the network
more energy efficient and perfect. The challenges in WASN are- limited node
power, Ad-hoc organization of network and reliability. Most of the existing
approaches addressed the problems separately, but not in a totality. This paper
shows how the network can have unlimited life and all time readiness with
overall stability to send information to the base station with minimum power
dissipation with the help of multimode "same type" sensor nodes and type
categorization of generated information. Moreover an effort is made to give
some light to the implementation issues and analyzed overall performance of the
network by MATLAB simulation.

ABSTRACT_BEGIN
  The Wireless Sensors Network (WSN) is an emergent technology resulting from
progress of various fields. Many applications of networks WSN are born. One of
the applications which have an operational effectiveness relates to the field
of health and allows a medical remote support. Miniature wireless sensors,
strategically placed on the human body, create a Wireless Body Sensor Network
(WBSN) which allows supervising various essential biological signals (rate of
heartbeat, pressure, etc). The sensitivity of medical information requires
mechanisms of safety. This performance constitutes a challenge for WBSN because
of their limitation in resources energy and data-processing. In this paper we
propose a new approach to symmetric cryptographic key establishment, based on
biometrics physiology. This approach takes into account WBSN constraints and
its topology.

ABSTRACT_BEGIN
  This paper is concerned with the issue of side payments between content
providers (CPs) and Internet service (access bandwidth) providers (ISPs) in an
Internet that is potentially not neutral. We herein generalize past results
modeling the ISP and CP interaction as a noncooperative game in two directions.
We consider different demand response models (price sensitivities) for
different provider types in order to explore when side payments are profitable
to the ISP. Also, we consider convex (non-linear) demand response to model
demand triggered by traffic which is sensitive to access bandwidth congestion,
particularly delay-sensitive interactive real-time applications. Finally, we
consider a model with two competing "eyeball" ISPs with transit pricing of net
traffic at their peering point to study the effects of caching remote content.

ABSTRACT_BEGIN
  Statistical network calculus is the probabilistic extension of network
calculus, which uses a simple envelope approach to describe arrival traffic and
service available for the arrival traffic in a node. One of the key features of
network calculus is the possibility to describe the service available in a
network using a network service envelope constructed from the service envelopes
of the individual nodes constituting the network. It have been shown that the
end-to-end worst case performance measures computed using the network service
envelope is bounded by $ {\cal O} (H) $, where $H$ is the number of nodes
traversed by a flow. There have been many attempts to achieve a similar linear
scaling for end-to-end probabilistic performance measures but with limited
success. In this paper, we present a simple general proof of computing
end-to-end probabilistic performance measures using network calculus that grow
linearly in the number of nodes ($H$).

ABSTRACT_BEGIN
  The signal to noise ratio (SNR) is one of the important measures for reducing
the noise.A technique that uses a linear prediction error filter (LPEF) and an
adaptive digital filter (ADF) to achieve noise reduction in a speech and image
degraded by additive background noise is proposed. Since a speech signal can be
represented as the stationary signal over a short interval of time, most of
speech signal can be predicted by the LPEF. This estimation is performed by the
ADF which is used as system identification. Noise reduction is achieved by
subtracting the reconstructed noise from the speech degraded by additive
background noise. Most of the MR image accelerating methods suffers from
degradation of acquired images, which is often correlated with the degree of
acceleration. However, Wideband MRI is a novel technique that transcends such
flaws.In this paper we proposed LPEF and ADF for reducing the noise in speech
and also we demonstrate that Wideband MRI is capable of obtaining images with
identical quality as conventional MR images in terms of SNR in wireless LAN.

ABSTRACT_BEGIN
  Noise is the major problem while working with wireless LAN. In this paper we
analyze the noise by using active receiving antenna and also propose the
detection mechanism based on RF energy duration. The standard back off
mechanism of 802.11 wireless LAN (WLAN) increases the contention window when a
transmission failure occurs in order to alleviate contentions in a WLAN. In
addition, many proposed schemes for 802.11 WLAN behave adaptively to
transmission failures. Transmission failures in WLANs occur mostly by two
causes: collision and channel noise. However, in 802.11 WLAN, a station cannot
know the cause of a transmission failure, thus the adaptive schemes assume the
ideal situation in which all transmission failures occur by only one of two
causes. For this reason, they may behave erroneously in a real world where
transmission failures occur by both causes. In this paper, we propose a novel
scheme to detect collision, which utilizes transmission time information and RF
energy duration on the channel. By detecting collisions, a station can
differentiate the causes of transmission failures and the adaptive schemes can
operate correctly by using the detection information.

ABSTRACT_BEGIN
  Standard congestion control cannot detect link failure losses which occur due
to mobility and power scarcity in multi-hop Ad-Hoc network (MANET). Moreover,
successive executions of Back-off algorithm deficiently grow Retransmission
Timeout (RTO) exponentially for new route. The importance of detecting and
responding link failure losses is to prevent sender from remaining idle
unnecessarily and manage number of packet retransmission overhead. In contrast
to Cross-layer approaches which require feedback information from lower layers,
this paper operates purely in Transport layer. This paper explores an
end-to-end threshold-based algorithm which enhances congestion control to
address link failure loss in MANET. It consists of two phases. First,
threshold-based loss classification algorithm distinguishes losses due to link
failure by estimating queue usage based on Relative One-way Trip Time (ROTT).
Second phase adjusts RTO for new route by comparing capabilities of new route
to the broken route using available information in Transport layer such as ROTT
and number of hops.

ABSTRACT_BEGIN
  It has been a challenging issue to provide digital quality multimedia data
stream to the remote user through the distributed system. The main aspects to
design the real distributed system, which reduce the cost of the network by
means of reduce packet loss and enhanced over all system performance. Since the
number of user increased rapidly in the network it posed heavy load to the
video servers. The requested clients, servers are all distributed in nature and
the data stream delivered to the user without error. In this work I have
presented the performance of the video on demand server by efficient traffic
control at real time with respect to incoming multirate traffic pattern . In
this work, I present how the overall system performance gradually decreases
when the client population sized in the clusters increase. This work indicated
the load balancing required for the on demand video distributed system to
provide efficient cost effective service to the local or remote clients.

ABSTRACT_BEGIN
  Security in any of the networks became an important issue in this paper we
have implemented a security mechanism on Medium Access Control layer by Assured
Neighbor based Security Protocol to provide authentication and confidentiality
of packets along with High speed transmission for Ad hoc networks. Here we have
divided the protocol into two different parts. The first part deals with
Routing layer information; in this part we have tried to implement a possible
strategy for detecting and isolating the malicious nodes. A trust counter for
each node is determined which can be actively increased and decreased depending
upon the trust value for the purpose of forwarding the packets from source node
to destination node with the help of intermediate nodes. A threshold level is
also predetermined to detect the malicious nodes. If the value of the node in
trust counter is less than the threshold value then the node is denoted
'malicious'. The second part of our protocol deals with the security in the
link layer. For this security reason we have used CTR (Counter) approach for
authentication and encryption. We have simulated all our strategies and schemes
in NS-2, the result of which gives a conclusion that our proposed protocol i.e.
Assured Neighbor based Security Protocol can perform high packet delivery
against various intruders and also packet delivery ratio against mobility with
low delays and low overheads.

ABSTRACT_BEGIN
  Context-aware applications have been gaining huge interest in the last few
years. With cell phones becoming ubiquitous computing devices, cell phone
localization has become an important research problem. In this paper, we
present CellSense, a prob- abilistic RSSI-based fingerprinting location
determi- nation system for GSM phones. We discuss the chal- lenges of
implementing a probabilistic fingerprinting localization technique in GSM
networks and present the details of the CellSense systemand how it addresses
these challenges. We then extend the proposed system using a hybrid technique
that combines probabilistic and deterministic estimation to achieve both high
ac- curacy and low computational overhead.Moreover, the accuracy of the hybrid
technique is robust to changes in its parameter values. To evaluate our
proposed system, we implemented CellSense on Android-based phones. Results from
two different testbeds, represent- ing urban and rural environments, for three
differ- ent cellular providers show that CellSense provides at least 108.57%
enhancement in accuracy in rural areas and at least 89.03% in urban areas
compared to the current state of the art RSSI-based GSM localization systems.
In additional, the proposed hybrid technique provides more than 6 times and 5.4
times reduction in computational requirements compared to the state of the art
RSSI-based GSM localization systems for the rural and urban testbeds
respectively.We also evaluate the effect of changing the different system
parameters on the accuracy-complexity tradeoff and how the cell towers density
and fingerprint density affect the system performance.

ABSTRACT_BEGIN
  A NoC is composed by IP cores (Intellectual Propriety) and switches connected
among themselves by communication channels. End-to-End Delay (EED)
communication is accomplished by the exchange of data among IP cores. Often,
the structure of particular messages is not adequate for the communication
purposes. This leads to the concept of packet switching. In the context of
NoCs, packets are composed by header, payload, and trailer. Packets are divided
into small pieces called Flits. It appears of importance, to meet the required
performance in NoC hardware resources. It should be specified in an earlier
step of the system design. The main attention should be given to the choice of
some network parameters such as the physical buffer size in the node. The EED
and packet loss are some of the critical QoS metrics. Some real-time and
multimedia applications bound up these parameters and require specific hardware
resources and particular management approaches in the NoC switch. A traffic
contract (SLA, Service Level Agreement) specifies the ability of a network or
protocol to give guaranteed performance, throughput or latency bounds based on
mutually agreed measures, usually by prioritizing traffic. A defined Quality of
Service (QoS) may be required for some types of network real time traffic or
multimedia applications. The main goal of this paper is, using the Network on
Chip modeling architecture, to define a QoS metric. We focus on the network
delay bound and packet losses. This approach is based on the Network Calculus
theory, a mathematical model to represent the data flows behavior between IPs
interconnected over NoC. We propose an approach of QoS-metric based on
QoS-parameter prioritization factors for multi applications-service using
calculus model.

ABSTRACT_BEGIN
  Till today we dreamt of imperceptible delay in a network. The computer
science research grows today faster than ever offering more and more services
(computational representational, graphical, intelligent implication etc) to its
user. But the problem lies in "greater the volume of services greater the
problem of delay". So tracing delay, or performance analysis focusing on time
required for computation, in a existing or newly configured network is
necessary to conclude the improvement. In this paper, we have done the job of
delay analysis in a multi-server system,. For this proposed work we have used
continuous -parameter Markov chains (Non -Birth -Death Process),for developing
the required models, and for developing the simulator we have used queuing
networking, different scheduling algorithms at the servers queue and process
scheduling . The work can be further extended to test the performance of
wireless domain.

ABSTRACT_BEGIN
  We develop a generalized optimization framework for graph-based
semi-supervised learning. The framework gives as particular cases the Standard
Laplacian, Normalized Laplacian and PageRank based methods. We have also
provided new probabilistic interpretation based on random walks and
characterized the limiting behaviour of the methods. The random walk based
interpretation allows us to explain di erences between the performances of
methods with di erent smoothing kernels. It appears that the PageRank based
method is robust with respect to the choice of the regularization parameter and
the labelled data. We illustrate our theoretical results with two realistic
datasets, characterizing di erent challenges: Les Miserables characters social
network and Wikipedia hyper-link graph. The graph-based semi-supervised
learning classi- es the Wikipedia articles with very good precision and perfect
recall employing only the information about the hyper-text links.

ABSTRACT_BEGIN
  We study the problem of wireless sensor network design by deploying a minimum
number of additional relay nodes (to minimize network cost) at a subset of
given potential relay locations, in order to convey the data from already
existing sensor nodes (hereafter called source nodes) to a Base Station, while
meeting a certain specified hop count bound (the hop count bound is chosen to
ensure a pre-determined probability of the data being delivered to the BS
within a given maximum delay). We study two variations of the problem.
  First we sudy the problem of guaranteed QoS connected network design, where
the objective is to have at least one path from each source to the BS with the
specified hop count bound. We show that the problem is NP-Hard. For a problem
in which the number of existing sensor nodes and potential relay locations is
n, we propose an O(n) approximation algorithm of polynomial time complexity.
Results show that the algorithm performs efficiently (in over 90% of the tested
scenarios, it gave solutions that were either optimal or were worse than
optimal by just one relay) in various randomly generated network scenarios.
  Next, we study the problem of survivable network design with guaranteed QoS,
i.e, the requirement is to have at least k > 1 node disjoint hop constrained
paths from each source to the BS. We show that the problem is NP-Hard. We also
show that the problem of finding a feasible solution to this optimization
problem is NP-Complete. We propose two polynomial time heuristics for this
problem, and compare their performance on various randomly generated network
scenarios.

ABSTRACT_BEGIN
  BitTorrent, one of the most widespread used P2P application for file-sharing,
recently got rid of TCP by introducing an application-level congestion control
protocol named uTP. The aim of this new protocol is to efficiently use the
available link capacity, while minimizing its interference with the rest of
user traffic (e.g., Web, VoIP and gaming) sharing the same access bottleneck.
In this paper we perform an experimental study of the impact of uTP on the
torrent completion time, the metric that better captures the user experience.
We run BitTorrent applications in a flash crowd scenario over a dedicated
cluster platform, under both homogeneous and heterogeneous swarm population.
Experiments show that an all-uTP swarms have shorter torrent download time with
respect to all-TCP swarms. Interestingly, at the same time, we observe that
even shorter completion times can be achieved under careful mixtures of TCP and
uTP traffic.

ABSTRACT_BEGIN
  Network calculus is an elegant theory which uses envelopes to determine the
worst-case performance bounds in a network. Statistical network calculus is the
probabilistic version of network calculus, which strives to retain the
simplicity of envelope approach from network calculus and use the arguments of
statistical multiplexing to determine probabilistic performance bounds in a
network. The tightness of the determined probabilistic bounds depends on the
efficiency of modelling stochastic properties of the arrival traffic and the
service available to the traffic at a network node. The notion of effective
bandwidth from large deviations theory is a well known statistical descriptor
of arrival traffic. Similarly, the notion of effective capacity summarizes the
time varying resource availability to the arrival traffic at a network node.
The main contribution of this paper is to establish an end-to-end stochastic
network calculus with the notions of effective bandwidth and effective capacity
which provides efficient end-to-end delay and backlog bounds that grows
linearly in the number of nodes ($H$) traversed by the arrival traffic, under
the assumption of independence.

ABSTRACT_BEGIN
  A characterization of systematic network coding over multi-hop wireless
networks is key towards understanding the trade-off between complexity and
delay performance of networks that preserve the systematic structure. This
paper studies the case of a relay channel, where the source's objective is to
deliver a given number of data packets to a receiver with the aid of a relay.
The source broadcasts to both the receiver and the relay using one frequency,
while the relay uses another frequency for transmissions to the receiver,
allowing for a full-duplex operation of the relay. We analyze the decoding
complexity and delay performance of two types of relays: one that preserves the
systematic structure of the code from the source; another that does not. A
systematic relay forwards uncoded packets upon reception, but transmits coded
packets to the receiver after receiving the first coded packet from the source.
On the other hand, a non-systematic relay always transmits linear combinations
of previously received packets. We compare the performance of these two
alternatives by analytically characterizing the expected transmission
completion time as well as the number of uncoded packets forwarded by the
relay. Our numerical results show that, for a poor channel between the source
and the receiver, preserving the systematic structure at the relay (i) allows a
significant increase in the number of uncoded packets received by the receiver,
thus reducing the decoding complexity, and (ii) preserves close to optimal
delay performance.

ABSTRACT_BEGIN
  We propose to demonstrate a mobile server assisted P2P system for on-demand
video streaming. Our proposed solution uses a combination of 3G and ad-hoc
Wi-Fi connections, to enable mobile devices to download content from a
centralised server in a way that minimises the 3G bandwidth use and cost. On
the customised GUI, we show the corresponding reduction in 3G bandwidth
achieved by increasing the number of participating mobile devices in the
combined P2P and ad-hoc Wi- Fi network, while demonstrating the good video
playout quality on each of the mobiles. We also demonstrate the implemented
trust mechanism which enables mobiles to only use trusted adhoc connections.
The system has been implemented on Android based smartphones.

ABSTRACT_BEGIN
  In this paper, we investigate the implementation of a Python code for a
Kalman Filter using the Numpy package. A Kalman Filtering is carried out in two
steps: Prediction and Update. Each step is investigated and coded as a function
with matrix input and output. These different functions are explained and an
example of a Kalman Filter application for the localization of mobile in
wireless networks is given.

ABSTRACT_BEGIN
  We consider Content Centric Network (CCN) interest forwarding problem as a
Multi-Armed Bandit (MAB) problem with delays. We investigate the transient
behaviour of the $\eps$-greedy, tuned $\eps$-greedy and Upper Confidence Bound
(UCB) interest forwarding policies. Surprisingly, for all the three policies
very short initial exploratory phase is needed. We demonstrate that the tuned
$\eps$-greedy algorithm is nearly as good as the UCB algorithm, the best
currently available algorithm. We prove the uniform logarithmic bound for the
tuned $\eps$-greedy algorithm. In addition to its immediate application to CCN
interest forwarding, the new theoretical results for MAB problem with delays
represent significant theoretical advances in machine learning discipline.

ABSTRACT_BEGIN
  In this paper we investigate the problem of gathering the data in wireless
sensor network using a single Mobile Element. In particular we consider the
case where the data are produced by measurements and they need to be delivered
to a predefined sink within a given time interval from the time the measurement
takes place. A mobile element travels the network in predefined paths, collect
the data from the nodes, and deliver them to the sink by a single long-distance
transmission. In this problem, the length of the mobile element path is bounded
by pre-determined length. This path will visit a subset of the nodes. These
selected nodes will work as caching points and will aggregate the other nodes'
data. The caching point nodes are selected with the aim of reducing the energy
expenditures due to multi-hop forwarding. We provide a heuristic-based solution
for this problem. We evaluate the performance of our algorithm by comparing it
to the best well-known algorithms from the literature.

ABSTRACT_BEGIN
  Multi-user spatial multiplexing combined with packet aggregation can
significantly increase the performance of Wireless Local Area Networks (WLANs).
In this letter, we present and evaluate a simple technique to perform packet
aggregation in IEEE 802.11ac MU-MIMO (Multi-user Multiple Input Multiple
Output) WLANs. Results show that in non-saturation conditions both the number
of active stations (STAs) and the queue size have a significant impact on the
system performance. If the number of stations is excessively high, the
heterogeneity of destinations in the packets contained in the queue makes it
difficult to take full advantage of packet aggregation. This effect can be
alleviated by increasing the queue size, which increases the chances to
schedule a large number of packets at each transmission, hence improving the
system throughput at the cost of a higher delay.

ABSTRACT_BEGIN
  Communication is the main motive in any Networks whether it is Wireless
Sensor Network, Ad-Hoc networks, Mobile Networks, Wired Networks, Local Area
Network, Metropolitan Area Network, Wireless Area Network etc, hence it must be
energy efficient. The main parameters for energy efficient communication are
maximizing network lifetime, saving energy at the different nodes, sending the
packets in minimum time delay, higher throughput etc. This paper focuses mainly
on the energy efficient communication with the help of Adjacency Matrix in the
Wireless Sensor Networks. The energy efficient scheduling can be done by
putting the idle node in to sleep node so energy at the idle node can be saved.
The proposed model in this paper first forms the adjacency matrix and
broadcasts the information about the total number of existing nodes with depths
to the other nodes in the same cluster from controller node. When every node
receives the node information about the other nodes for same cluster they
communicate based on the shortest depths and schedules the idle node in to
sleep mode for a specific time threshold so energy at the idle nodes can be
saved.

ABSTRACT_BEGIN
  VANET (Vehicular Ad-hoc Network) is a new technology which has taken enormous
attention in the recent years. Due to rapid topology changing and frequent
disconnection makes it difficult to design an efficient routing protocol for
routing data among vehicles, called V2V or vehicle to vehicle communication and
vehicle to road side infrastructure, called V2I. The existing routing protocols
for VANET are not efficient to meet every traffic scenarios. Thus design of an
efficient routing protocol has taken significant attention. So, it is very
necessary to identify the pros and cons of routing protocols which can be used
for further improvement or development of any new routing protocol. This paper
presents the pros and cons of VANET routing protocols for inter vehicle
communication.

ABSTRACT_BEGIN
  Vehicular adhoc network or VANET is special types of adhoc network consists
of moving cars referred to as nodes; provide a way to exchange any information
between cars without depending on fixed infrastructure. For efficient
communication between nodes various routing protocols and mobility models have
been proposed based on different scenarios. Due to rapid topology changing and
frequent disconnection makes it difficult to select suitable mobility model and
routing protocols. Hence performance evaluation and comparison between routing
protocols is required to understand any routing protocol as well as to develop
a new routing protocol. In this research paper, the performance of two
on-demand routing protocols AODV & DSR has been analyzed by means of packet
delivery ratio, loss packet ratio & average end-to-end delay with varying speed
limit and node density under TCP & CBR connection.

ABSTRACT_BEGIN
  Vehicular ad hoc network is formed by cars which are called nodes; allow them
to communicate with one another without using any fixed road side unit. It has
some unique characteristics which make it different from other ad hoc network
as well as difficult to define any exact mobility model and routing protocols
because of their high mobility and changing mobility pattern. Hence performance
of routing protocols can vary with the various parameters such as speed, pause
time, node density and traffic scenarios. In this research paper, the
performance of two on-demand routing protocols AODV & DSR has been analyzed by
means of packet delivery ratio, loss packet ratio & average end-to-end delay
with varying pause time and node density under TCP & CBR connection.

ABSTRACT_BEGIN
  Providing service continuity to the end users with best quality is a very
important issue in the next generation wireless communications. With the
evolution of the mobile devices towards a multimode architecture and the
coexistence of multitude of radio access technologies (RAT's), the users are
able to benefit simultaneously from these RAT's. However, the major issue in
heterogeneous wireless communications is how to choose the most suitable access
network for mobile's user which can be used as long as possible for
communication. To achieve this issue, this paper proposes an intelligent
network selection strategy which combines two multi attribute decision making
(MADM) methods such as analytic network process (ANP) and the technique for
order preference by similarity to an ideal solution (TOPSIS) method. The ANP
method is used to find the differentiate weights of available networks by
considering each criterion and the TOPSIS method is applied to rank the
alternatives. Our new strategy for network selection can dealing with the
limitations of MADM methods which are the ranking abnormality and the ping-ponf
effect.

ABSTRACT_BEGIN
  The increasing demand for real-time applications has made the Quality of
Service (Qos) support for wireless sensor networks (WSN) a fairly new research
framework. In this paper, we propose an extended model of the Beacon enabled
IEEE 802.15.4 including the Guaranteed Time Slot GTS allocation mechanism in
the aim to analyze and evaluate network performances. Series of extensive
simulations were performed to analyze the impact of the Beacon Order BO and the
Superframe Order SO on the network performance based on commonly known metrics.
In particular, we examine data packet delivery performance and the throughput
for different duty cycle rates. Also, we analyze the impact of the number of
nodes on collision probability. Thus, for high number of nodes, collision
becomes higher and the reachability begins to decline slightly. We discuss and
compare simulation results conducted under various parameter settings to the
IEEE 802.11 network.

ABSTRACT_BEGIN
  The bursting aggregation assembly in edge nodes is one of the key
technologies in OBS (Optical Burst Switching) network, which has a direct
impact on flow characteristics and packet loss rate. An optical burst assembly
technique supporting QoS is presented through this paper, which can
automatically adjust the threshold along with the increasing and decreasing
volume of business, reduce the operational burst, and generate corresponding
BDP (Burst Data Packet) and BCP (Burst Control Packet). In addition to the
burst aggregation technique a packet recovery technique by restoration method
is also described. The data packet loss due to the physical optical link
failure is not currently included in the QoS descriptions. This link failure is
also a severe problem which reduces the data throughput of the transmitter
node. A mechanism for data recovery from this link failure is vital for
guaranteeing the QoS demanded by each user. So this paper will also discusses a
specific protocol for reducing the packet loss by utilizing the features of
both optical circuit switching (OCS) and Optical Burst switching (OBS)
techniques.

ABSTRACT_BEGIN
  In this paper, a new technique for cross layer design, based on present Eb/N0
(bit energy per noise density) ratio of the connections and target values of
the Quality of Service (QoS) information parameters from MAC layer, is proposed
to dynamically select the Modulation and Coding Scheme (MCS) at the PHY layer
for WiMAX Broadband Wireless Access (BWA) networks. The QoS information
parameter includes New Connection Blocking Probability (NCBP), Hand off
Connection Dropping Probability (HCDP) and Connection Outage Probability (COP).
In addition, a Signal to Interference plus Noise Ratio (SINR) based Call
Admission Control (CAC) algorithm and Queue based Scheduling algorithm are
integrated for the cross layer design. An analytical model using the Continuous
Time Markov Chain (CTMC) is developed for performance evaluation of the
algorithms under various MCS. The effect of Eb/No is observed for QoS
information parameters in order to determine its optimum range. Simulation
results show that the integrated CAC and packet Scheduling model maximizes the
bandwidth utilization and fair allocation of the system resources for all types
of MCS and guarantees the QoS to the connections.

ABSTRACT_BEGIN
  In opportunistic networks the existence of a simultaneous path is not assumed
to transmit a message between a sender and a receiver. Information about the
context in which the users communicate is a key piece of knowledge to design
efficient routing protocols in opportunistic networks. But this kind of
information is not always available. When users are very isolated, context
information cannot be distributed, and cannot be used for taking efficient
routing decisions. In such cases, context oblivious based schemes are only way
to enable communication between users. As soon as users become more social,
context data spreads in the network, and context based routing becomes an
efficient solution. In this paper we design an integrated routing protocol that
is able to use context data as soon as it becomes available and falls back to
dissemination based routing when context information is not available. Then, we
provide a comparison between Epidemic and PROPHET, these are representative of
context oblivious and context aware routing protocols. Our results show that
integrated routing protocol is able to provide better result in term of message
delivery probability and message delay in both cases when context information
about users is available or not.

ABSTRACT_BEGIN
  There have been various navigation and tracking systems being developed with
the help of technologies like GPS, GSM, Bluetooth, IR, Wi-Fi and Radar. Outdoor
positioning systems have been deployed quite successfully using GPS but
positioning systems for indoor environments still do not have widespread
deployment due to various reasons. Most of these use only a single technology
for positioning but using more than one in cooperation with each other is
always advantageous for obtaining greater accuracy. Particularly, the ones
which use Bluetooth are better since they would enhance the scalability of such
a system because of the fact that this technology is in use by the common
people so it would always be easy to track them. Moreover it would also reduce
the hardware installation cost to some extent. The system that has been
introduced here uses Bluetooth primarily for positioning and tracking in
combination with Wi-Fi access points. The reason that makes the commercial
application of such a system easier and cheaper is that most of the localized
areas today like college campus, offices are being provided with internet
connectivity using these access points.

ABSTRACT_BEGIN
  Modern day's vehicles require advanced communication system on board to
enable passengers benefit the most from available services. IEEE 802.11p is the
new extension of IEEE 802.11 standards; especially proposed for the high
vehicular environment. The WAVE documentation represents enhancements to the
Media Access Control (MAC) and Physical (PHY) layer of IEEE 802.11 standards to
work efficiently in high vehicular environment. In this research work, the main
emphasis is on the new IEEE 802.11p enhancement of MAC and PHY layers. More
specifically, the target of this research is to setup a simulation environment
which will allow us to investigate the use of real time voice application,
using IEEE 802.11p (WAVE) enhance setting, in a single hop and multi-hop
environment where nodes are not directly connected. Also, the evaluation of
transmission between moving nodes are tested by simply sending and receiving
FTP file between them with varying speed of the moving nodes.

ABSTRACT_BEGIN
  Mobile Ad-hoc Networks (MANETS) is a collection of wireless nodes without any
infrastructure support. The nodes in MANET can act as either router or source
and the control of the network is distributed among nodes. The nodes in MANETS
are highly mobile and it maintains dynamic interconnection between those mobile
nodes. MANTEs have been considered as isolated stand-alone network. This can
turn the dream of networking "at any time and at any where" into reality. The
main purpose of this paper is to study the issues in route discovery process in
AODV protocol for MANET. Flooding of route request message imposes major
concern in route establishment. This paper suggests a new approach to reduce
the routing overhead during the route discovery phase. By considering the
previous behaviour of the network, the new protocol reduces the unwanted
searches during route establishment process

ABSTRACT_BEGIN
  A Connected Dominating Set (CDS) based virtual backbone plays an important
role in wireless ad hoc networks for efficient routing and broadcasting. Each
node in the network can select some of its 1-hop neighbors as Multi Point Relay
(MPR) to cover all its 2-hop neighbors. A MPR based CDS is a promising approach
for broadcasting. A node in the CDS consumes more energy and the energy
depletes quickly than non dominating nodes. Although previous CDS construction
algorithms achieve good results in terms of the size of CDS, a minimum size CDS
does not necessarily guarantee an optimal network performance from an energy
efficient point of view. In this paper, we propose a distributed algorithm for
energy efficient stable MPR based CDS construction to extend the lifetime of ad
hoc wireless networks by considering energy and velocity of nodes. We have also
implemented route discovery protocol to make use of the CDS nodes to relay
route request messages. The simulation results show that our algorithm
increases the lifetime up to 25% than previous works and 60% reduction in the
route request messages during route discovery process.

ABSTRACT_BEGIN
  We propose and analyze a new on the fly strategy that discovers, repairs and
maintains routes in hierarchical and distributed fashion called Janitor Based
Routing (JBR). The main motivation behind our JBR protocol is to decrease
flooding and routing overhead and increase efficiencies in packet movement. An
analytical model for the proposed JBR is presented and detailed simulation is
used to observe the performance of JBR. This route discovery and maintenance
protocol clearly achieved improvement in terms of reduction of flooding,
routing overhead, and, hence, provides enhanced reliability.

ABSTRACT_BEGIN
  In this paper, we propose LMEEC, a cluster-based routing protocol with low
energy consumption for wireless sensor networks. Our protocol is based on a
strategy which aims to provide a more reasonable exploitation of the selected
nodes (cluster-heads) energy. Simulation results show the effectiveness of
LMEEC in decreasing the energy consumption, and in prolonging the network
lifetime, compared to LEACH.

ABSTRACT_BEGIN
  In this paper, we design distributed spectrum access mechanisms with both
complete and incomplete network information. We propose an evolutionary
spectrum access mechanism with complete network information, and show that the
mechanism achieves an equilibrium that is globally evolutionarily stable. With
incomplete network information, we propose a distributed learning mechanism,
where each user utilizes local observations to estimate the expected throughput
and learns to adjust its spectrum access strategy adaptively over time. We show
that the learning mechanism converges to the same evolutionary equilibrium on
the time average. Numerical results show that the proposed mechanisms are
robust to the perturbations of users' channel selections.

ABSTRACT_BEGIN
  A reliable network infrastructure must be able to sustain traffic flows, even
when a failure occurs and changes the network topology. During the occurrence
of a failure, routing protocols, like OSPF, take from hundreds of milliseconds
to various seconds in order to converge. During this convergence period,
packets might traverse a longer path or even a loop. An even worse transient
behaviour is that packets are dropped even though destinations are reachable.
In this context, this paper describes a proactive fast rerouting approach,
named Fast Emergency Paths Schema (FEP-S), to overcome problems originating
from transient link failures in OSPF routing. Extensive experiments were done
using several network topologies with different dimensionality degrees. Results
show that the recovery paths, obtained by FEPS, are shorter than those from
other rerouting approaches and can improve the network reliability by reducing
the packet loss rate during the routing protocols convergence caused by a
failure.

ABSTRACT_BEGIN
  Wireless Sensor networks are dense networks of small, low-cost sensors, which
collect and disseminate environmental data and thus facilitate monitoring and
controlling of physical environment from remote locations with better accuracy.
The major challenge is to achieve energy efficiency during the communication
among the nodes. This paper aims at proposing a solution to schedule the node's
activities to reduce the energy consumption. We propose the construction of a
decentralized lifetime maximizing tree within clusters. We aim at minimizing
the distance of transmission with minimization of energy consumption. The
sensor network is distributed into clusters based on the close proximity of the
nodes. Data transfer among the nodes is done with a hybrid technique of both
TDMA/ FDMA which leads to efficient utilization of bandwidth and maximizing
throughput.

ABSTRACT_BEGIN
  In this paper, we study an interference relay network with a satellite as
relay. We propose a cooperative strategy based on physical layer network coding
and superposition modulation decoding for uni-directional communications among
users. The performance of our solution in terms of throughput is evaluated
through capacity analysis and simulations that include practical constraints
such as the lack of synchronization in time and frequency. We demonstrate
throughputs significantly larger than the classical time sharing case.

ABSTRACT_BEGIN
  In recent years, the wireless sensor network (WSN) is playing a key role in
sensing, collecting and disseminating information in various applications. An
important feature associated with WSN is to develop an efficient data
distribution and routing scheme to ensure better quality of service (QoS) that
reduces the power consumption and the end-to-end data delivery time. In this
work, we propose an adaptive framework to transmit data packets from a source
to the sink in WSN across multiples paths with strategically distributed data
packets so as to minimize the power consumption as well as the end-to-end data
delivery time.

ABSTRACT_BEGIN
  The wireless sensor network is a collection of energy-constrained nodes.
Their objective is to sense, collect and process information for some ad-hoc
purpose. Typically the nodes are deployed in geographically inaccessible
regions. Thus the most challenging task is to design a network with minimal
power consumption. As the nodes have to collect and process data very fast,
minimizing data delivery time is another objective. In addition to this, when
multiple sources transmit data simultaneously, the network load gradually
increases and it may lead to congestion. In this paper we have proposed an
adaptive framework in which multiple sources transmit data simultaneously with
minimal end-to-end data delivery time and minimal energy consumption besides
ensuring that congestion remains at an optimum low so that minimal number of
data packets are dropped. This paper presents an adaptive framework to achieve
the above-mentioned objectives. This framework has been used over Mac 802.11
and extensive simulations have been carried out in NS2 to prove the
effectiveness of the framework over traditional Mac as well as few other
existing protocols.

ABSTRACT_BEGIN
  Wireless sensor network (WSN) is a collection of nodes which can communicate
with each other without any prior infrastructure along with the ability to
collect data autonomously and effectively after being deployed in an ad-hoc
fashion to monitor a given area. One major problem encountered in data
gathering wireless systems is to obtain an optimal balance among the number of
nodes deployed, energy efficiency and lifetime as energy of nodes cannot be
replenished. In this paper we propose first a scheme to estimate the number of
nodes to be deployed in a WSN for a predetermined lifetime so that total energy
utilization and complete connectivity are ensured under all circumstances. This
scheme also guarantees that during each data gathering cycle, every node
dissipates the requisite amount of energy, which thus minimizes the number of
nodes required to achieve the desired network lifetime. Second, this paper has
proposed a framework to conduct data gathering in WSN. Extensive simulations
have been carried out in ns2 to establish the effectiveness of this framework.

ABSTRACT_BEGIN
  Location information of sensor nodes has become an essential part of many
applications in Wireless Sensor Networks (WSN). The importance of location
estimation and object tracking has made them the target of many security
attacks. Various methods have tried to provide location information with high
accuracy, while lots of them have neglected the fact that WSNs may be deployed
in hostile environments. In this paper, we address the problem of securely
tracking a Mobile Node (MN) which has been noticed very little previously. A
novel secure tracking algorithm is proposed based on Extended Kalman Filter
(EKF) that is capable of tracking a Mobile Node (MN) with high resolution in
the presence of compromised or colluding malicious beacon nodes. It filters out
and identifies the malicious beacon data in the process of tracking. The
proposed method considerably outperforms the previously proposed secure
algorithms in terms of either detection rate or MSE. The experimental data
based on different settings for the network has shown promising results.

ABSTRACT_BEGIN
  Delay-tolerant networking (DTN) is a term invented to describe and encompass
all types of long-delay, disconnected, disrupted or intermittently-connected
networks, where mobility and outages or scheduled contacts may be experienced.
'DTN' is also used to refer to the Bundle Protocol, which has been proposed as
the one unifying solution for disparate DTN networking scenarios, after
originally being designed solely for use in deep space for the 'Interplanetary
Internet.' We evaluated the Bundle Protocol by testing it in space and on the
ground. We have found architectural weaknesses in the Bundle Protocol that may
prevent engineering deployment of this protocol in realistic delay-tolerant
networking scenarios, and have proposed approaches to address these weaknesses.

ABSTRACT_BEGIN
  Although GPS has been considered a ubiquitous outdoor localization
technology, we are still far from a similar technology for indoor environments.
While a number of technologies have been proposed for indoor localization, they
are isolated efforts that are way from a true ubiquitous localization system. A
ubiquitous indoor positioning system is envisioned to be deployed on a large
scale worldwide, with minimum overhead, to work with heterogeneous devices, and
to allow users to roam seamlessly from indoor to outdoor environments. Such a
system will enable a wide set of applications including worldwide seamless
direction finding between indoor locations, enhancing first responders' safety
by providing anywhere localization and floor plans, and providing a richer
environment for location-aware social networking applications.
  We describe an architecture for the ubiquitous indoor positioning system
(IPS) and the challenges that have to be addressed to materialize it. We then
focus on the feasibility of automating the construction of a worldwide indoor
floor plan and fingerprint database which, as we believe, is one of the main
challenges that limit the existence of a ubiquitous IPS system. Our proof of
concept uses a crowd-sourcing approach that leverages the embedded sensors in
today's cell phones as a worldwide distributed floor plan generation tool. This
includes constructing the floor plans and determining the areas of interest
(corridors, offices, meeting rooms, elevators, etc). The cloud computing
concepts are also adopted for the processing and storage of the huge amount of
data generated and requested by the system's users. Our results show the
ability of the system to construct an accurate floor plan and identify the
areas of interest with more than 90% accuracy. We also identify different
research directions for addressing the challenges of realizing a true
ubiquitous IPS system.

ABSTRACT_BEGIN
  Despite the central role of mobility in wireless networks, analytical study
on its impact on network performance is notoriously difficult. This paper aims
to address this gap by proposing a random waypoint (RWP) mobility model defined
on the entire plane and applying it to analyze two key cellular network
parameters: handover rate and sojourn time. We first analyze the stochastic
properties of the proposed model and compare it to two other models: the
classical RWP mobility model and a synthetic truncated Levy walk model which is
constructed from real mobility trajectories. The comparison shows that the
proposed RWP mobility model is more appropriate for the mobility simulation in
emerging cellular networks, which have ever-smaller cells. Then we apply the
proposed model to cellular networks under both deterministic (hexagonal) and
random (Poisson) base station (BS) models. We present analytic expressions for
both handover rate and sojourn time, which have the expected property that the
handover rate is proportional to the square root of BS density. Compared to an
actual BS distribution, we find that the Poisson-Voronoi model is about as
accurate in terms of mobility evaluation as hexagonal model, though being more
pessimistic in that it predicts a higher handover rate and lower sojourn time.

ABSTRACT_BEGIN
  High Peak to Average Power Ratio (PAPR) of the transmitted signal is a
critical problem in multicarrier modulation systems (MCM) such as Orthogonal
Frequency Division Multiplexing (OFDM), and Multi-Carrier Code Division
Multiple Access (MC CDMA) systems, due to large number of subcarriers. High
PAPR leads to reduced resolution, and battery life. It also deteriorates system
performance. This paper focuses on review of different PAPR reduction
techniques with attendant technical issues as well as criteria for selection of
PAPR reduction technique. To reduce PAPR the constraints are low power
consumption, and low Bit Error Rate (BER). Spectral bandwidth is improved by
better spectral characteristics, and low complexity/cost.

ABSTRACT_BEGIN
  In this paper, we investigate the transmission range assignment for N
wireless nodes located on a line (a linear wireless network) for broadcasting
data from one specific node to all the nodes in the network with minimum
energy. Our goal is to find a solution that has low complexity and yet performs
close to optimal. We propose an algorithm for finding the optimal assignment
(which results in the minimum energy consumption) with complexity O(N^2). An
approximation algorithm with complexity O(N) is also proposed. It is shown
that, for networks with uniformly distributed nodes, the linear-time
approximate solution obtained by this algorithm on average performs practically
identical to the optimal assignment. Both the optimal and the suboptimal
algorithms require the full knowledge of the network topology and are thus
centralized. We also propose a distributed algorithm of negligible complexity,
i.e., with complexity O(1), which only requires the knowledge of the adjacent
neighbors at each wireless node. Our simulations demonstrate that the
distributed solution on average performs almost as good as the optimal one for
networks with uniformly distributed nodes.

ABSTRACT_BEGIN
  This paper presents a partial implementation of the ES-IS Routing Information
Exchange Protocol packet processing in Linux Kernel 2.6, which is for use in
conjunction with the Connectionless Network Protocol (CLNP) in Aeronautical
Telecommunication Network (ATN). First, we show the data structures involved in
the protocol operation. Second, we describe the map of the packet processing
whose design has been developed in the research. Third, we explain how the
protocol is implemented as a loadable kernel module. Finally, we conclude the
implementation result based on performed tests.

ABSTRACT_BEGIN
  In this paper, we present an implementation of CLNP ground-to-ground packet
processing for ATN in Linux kernel version 2.6. We present the big picture of
CLNP packet processing, the details of input, routing, and output processing
functions, and the implementation of each function based on ISO 8473-1. The
functions implemented in this work are PDU header decomposition, header format
analysis, header error detection, error reporting, reassembly, source routing,
congestion notification, forwarding, composition, segmentation, and transmit to
device functions. Each function is initially implemented and tested as a
separated loadable kernel module. These modules are successfully loaded into
Linux kernel 2.6.

ABSTRACT_BEGIN
  We use information theoretic achievable rate formulas for the multi-relay
channel to study the problem of optimal placement of relay nodes along the
straight line joining a source node and a sink node. The achievable rate
formulas that we use are for full-duplex radios at the relays and decode-
and-forward relaying. For the single relay case, and individual power
constraints at the source node and the relay node, we provide explicit formulas
for the optimal relay location and the optimal power allocation to the
source-relay channel, for the exponential and the power-law path-loss channel
models. For the multiple relay case, we consider exponential path-loss and a
total power constraint over the source and the relays, and derive an
optimization problem, the solution of which provides the optimal relay
locations. Numerical results suggest that at low attenuation the relays are
mostly clustered close to the source in order to be able to cooperate among
themselves, whereas at high attenuation they are uniformly placed and work as
repeaters.
  The structure of the optimal power allocation for a given placement of the
nodes, then motivates us to formulate the problem of impromptu ("as-you-go")
placement of relays along a line of exponentially distributed length, with
exponential path- loss, so as to minimize a cost function that is additive over
hops. The hop cost trades off a capacity limiting term, motivated from the
optimal power allocation solution, against the cost of adding a relay node. We
formulate the problem as a total cost Markov decision process, for which we
prove results for the value function, and provide insights into the placement
policy via numerical exploration.

ABSTRACT_BEGIN
  Femtocells represent a promising alternative solution for high quality
wireless access in indoor scenarios where conventional cellular system coverage
can be poor. Femtocell access points (FAP) are normally randomly deployed by
the end user, so only post deployment network planning is possible.
Furthermore, this uncoordinated deployment creates the potential for severe
interference to co-located femtocells, especially in dense deployments. This
paper presents a new femtocell network architecture using a generalized virtual
cluster femtocell (GVCF) paradigm, which groups together FAP, which are
allocated to the same femtocell gateway (FGW), into logical clusters. This
guarantees severely interfering and overlapping femtocells are assigned to
different clusters, and since each cluster operates on a different band of
frequencies, the corresponding virtual cluster controller only has to manage
its own FAP members, so the overall system complexity is low. The performance
of the GVCF algorithm is analysed from both a resource availability and cluster
number perspective, and a novel strategy is proposed for dynamically adapting
these to network environment changes, while upholding quality-of-service
requirements. Simulation results conclusively corroborate the superior
performance of the GVCF model in interference mitigation, particularly in high
density FAP scenarios.

ABSTRACT_BEGIN
  Wireless sensor networks have been increasingly used for real-time
surveillance over large areas. In such applications, it is important to support
end-to-end delay constraints for packet deliveries even when the corresponding
flows require multi-hop transmissions. In addition to delay constraints, each
flow of real-time surveillance may require some guarantees on throughput of
packets that meet the delay constraints. Further, as wireless sensor networks
are usually deployed in challenging environments, it is important to
specifically consider the effects of unreliable wireless transmissions.
  In this paper, we study the problem of providing end-to-end delay guarantees
for multi-hop wireless networks. We propose a model that jointly considers the
end-to-end delay constraints and throughput requirements of flows, the need for
multi-hop transmissions, and the unreliable nature of wireless transmissions.
We develop a framework for designing feasibility-optimal policies. We then
demonstrate the utility of this framework by considering two types of systems:
one where sensors are equipped with full-duplex radios, and the other where
sensors are equipped with half-duplex radios. When sensors are equipped with
full-duplex radios, we propose an online distributed scheduling policy and
proves the policy is feasibility-optimal. We also provide a heuristic for
systems where sensors are equipped with half-duplex radios. We show that this
heuristic is still feasibility-optimal for some topologies.

ABSTRACT_BEGIN
  Stochastic Processing Networks (SPNs) can be used to model communication
networks, manufacturing systems, service systems, etc. We consider a real-time
SPN where tasks generate jobs with strict deadlines according to their traffic
patterns. Each job requires the concurrent usage of some resources to be
processed. The processing time of a job may be stochastic, and may not be known
until the job completes. Finally, each task may require that some portion of
its tasks to be completed on time.
  In this paper, we study the problem of verifying whether it is feasible to
fulfill the requirements of tasks, and of designing scheduling policies that
actually fulfill the requirements. We first address these problems for systems
where there is only one resource. Such systems are analog to ones studied in a
previous work, and, similar to the previous work, we can develop sharp
conditions for feasibility and scheduling policy that is feasibility-optimal.
We then study systems with two resources where there are jobs that require both
resources to be processed. We show that there is a reduction method that turns
systems with two resources into equivalent single-resource systems. Based on
this method, we can also derive sharp feasibility conditions and
feasibility-optimal scheduling policies for systems with two resources.

ABSTRACT_BEGIN
  Cognitive radio nodes have been proposed as means to improve the spectrum
utilization. It reuses the spectrum of a primary service provider under the
condition that the primary service provider services are not harmfully
interrupted. A cognitive radio can sense its operating environment's conditions
and it is able to reconfigure itself and to communicate with other counterparts
based on the status of the environment and also the requirements of the user to
meet the optimal communication conditions and to keep quality of service (QoS)
as high as possible. The efficiency of spectrum sharing can be improved by
minimizing the interference. The Utility function that captures the cooperative
behavior to minimize the interference and the satisfaction to improve the
throughput is investigated. The dynamic spectrum sharing algorithm can maintain
the quality of service (QoS) of each network while the effective spectrum
utilisation is improved under a fluctuation traffic environment when the
available spectrum is limited.

ABSTRACT_BEGIN
  Multi cast communication is a key technology for wireless mesh networks.
Multicast provides efficient data distribution among a group of nodes,
Generally sensor networks and MANETs uses multicast algorithms which are
designed to be energy efficient and to achieve optimal route discovery among
mobile nodes whereas wireless mesh networks needs to maximize throughput. Here
we propose two multicast algorithms: The Level Channel Assignment (LCA)
algorithm and the Multi-Channel Multicast (MCM) algorithm to improve the
throughput for multichannel sand multi interface mesh networks. The algorithm
builds efficient multicast trees by minimizing the number of relay nodes and
total hop count distance of the trees. Shortest path computation is a classical
combinatorial optimization problem. Neural networks have been used for
processing path optimization problem. Pulse Coupled Neural Networks (PCNNS)
suffer from high computational cast for very long paths we propose a new PCNN
modal called dual source PCNN (DSPCNN) which can improve the computational
efficiency two auto waves are produced by DSPCNN one comes from source neuron
and other from goal neuron when the auto waves from these two sources meet the
DSPCNN stops and then the shortest path is found by backtracking the two auto
waves.

ABSTRACT_BEGIN
  In this paper, we consider medium access control of local area networks
(LANs) under limited-information conditions as befits a distributed system.
Rather than assuming "by rule" conformance to a protocol designed to regulate
packet-flow rates (e.g., CSMA windowing), we begin with a non-cooperative game
framework and build a dynamic altruism term into the net utility. The effects
of altruism are analyzed at Nash equilibrium for both the ALOHA and CSMA
frameworks in the quasistationary (fictitious play) regime. We consider either
power or throughput based costs of networking, and the cases of identical or
heterogeneous (independent) users/players. In a numerical study we consider
diverse players, and we see that the effects of altruism for similar players
can be beneficial in the presence of significant congestion, but excessive
altruism may lead to underuse of the channel when demand is low.

ABSTRACT_BEGIN
  Community Wireless Mesh Networks (WMN) is a paradigm in wireless
communication of 21st centuary as means of providing high speed braodband
access. Un-cooperative nodes, both selfish and malicious proves to be a
significant threat in Community WMN that require a solution independent of
routing protocols being used. We propose to implement Modified PIFA (MPIFA), an
Improved version of Protocol Independent Fairness Algorithm (PIFA) proposed by
Younghwan Yoo, Sanghyun and P. Agrawal [6] with ability to cater specific
requirements in Community WMN. MPIFA has malicious nodes detection rate
improvement of 50% when nodes demonstrate low probabilistic malicious behavior
of 10% to circumvent the security measures in place. Improvements were also
made to reduce false malicious node detections to 4% when node-to-node link
failures occur in Community WMN.

ABSTRACT_BEGIN
  This paper elaborates about the potential risk of systemic instabilities in
future networks and proposes a methodology to mitigate it. The starting concept
is modeling the network as a complex environment (e.g. ecosystem) of resources
and associated functional controllers in a continuous and dynamic game of
cooperation - competition. Methodology foresees defining and associating
utility functions to these controllers and elaborating a global utility
function (as a function of the controllers' utility functions) for the overall
network. It is conjectured that the optimization of the global utility function
ensures network stability and security evaluations. Paper concludes arguing
that self-governance (with limited human intervention) is possible provided
that proper local, global control rules are coded into these utility functions
optimization processes.

ABSTRACT_BEGIN
  We consider the problem of managing a bounded size First-In-First-Out (FIFO)
queue buffer, where each incoming unit-sized packet requires several rounds of
processing before it can be transmitted out. Our objective is to maximize the
total number of successfully transmitted packets. We consider both push-out
(when the policy is permitted to drop already admitted packets) and
non-push-out cases. In particular, we provide analytical guarantees for the
throughput performance of our algorithms. We further conduct a comprehensive
simulation study which experimentally validates the predicted theoretical
behaviour.

ABSTRACT_BEGIN
  Path delays in IP networks are important metrics, required by network
operators for assessment, planning, and fault diagnosis. Monitoring delays of
all source-destination pairs in a large network is however challenging and
wasteful of resources. The present paper advocates a spatio-temporal Kalman
filtering approach to construct network-wide delay maps using measurements on
only a few paths. The proposed network cartography framework allows efficient
tracking and prediction of delays by relying on both topological as well as
historical data. Optimal paths for delay measurement are selected in an online
fashion by leveraging the notion of submodularity. The resulting predictor is
optimal in the class of linear predictors, and outperforms competing
alternatives on real-world datasets.

ABSTRACT_BEGIN
  Most studies on path-vector routing stability have been conducted empirically
by means of ad-hoc analysis of BGP data traces. None of them consider prior
specification of an analytic method including the use of stability measurement
metrics for the systematic analysis of BGP traces and associated
meta-processing for determining the local state of the routing system. In this
paper, we define a set of metrics that characterize the local stability
properties of path-vector routing such as BGP (Border Gateway Protocol). By
means of these stability metrics, we propose a method to analyze the effects of
BGP policy- and protocol-induced instability on local routers.

ABSTRACT_BEGIN
  Most studies on path-vector routing stability have been conducted empirically
by means of ad-hoc analysis of BGP data traces. None of them consider prior
specification of an analytic method including the use of stability measurement
metrics for the systematic analysis of BGP traces and associated
meta-processing for determining the local state of the routing system. In this
paper, we define a set of metrics that characterize the local stability
properties of path-vector routing such as BGP (Border Gateway Protocol). By
means of these stability metrics, we propose a method to analyze the effects of
BGP policy- and protocol-induced instability on local routers.

ABSTRACT_BEGIN
  Node cooperation during packet forwarding operations is critically important
for fair resource utilization in Community Wireless Mesh Networks (CoWMNs). In
a CoWMN, node cooperation is achieved by using fairness protocols specifically
designed to detect and isolate malicious nodes, discourage unfair behavior, and
encourage node participation in forwarding packets. In general, these protocols
can be split into two groups: Incentive-based ones, which are managed
centrally, and use credit allocation schemes. In contrast, reputation-based
protocols that are decentralized, and rely on information exchange among
neighboring nodes. Centrally managed protocols inevitably suffer from
scalability problems. The decentralized, reputation-based protocols lacks in
detection capability, suffer from false detections and error propagation
compared to the centralized, incentive-based protocols. In this study, we
present a new fairness protocol management scheme, called Hybrid FPMS that
captures the superior detection capability of incentive-based fairness
protocols without the scalability problems inherently expected from a
centralized management scheme as a network's size and density grows. Simulation
results show that Hybrid FPMS is more efficient than the current centralized
approach and significantly reduces the network delays and overhead.

ABSTRACT_BEGIN
  Wireless communication is the fastest growing area of the communication
industry. To keep swiftness with the indefinite increase in customers' demands
and expectations, and the market competition among companies for the services
offered,there is need for higher data rate along with reliable communication at
low cost so that the applications can reach all. Until now, many technical
challenges remain in designing robust and fast wireless systems that deliver
the performance necessary to support emerging applications, due to the fact
that wireless channels are frequency selective, power-limited, susceptible to
noise and interference. Demand for high data rate and increasing applications
offered by a wireless device calls for an effective method. Due to limit on the
available bandwidth, there is a need for exploiting the available bandwidth in
a way so that we get maximum advantage. Multiple-Input Multiple-Output system
does exactly this thing by multiplying the data rate without any expansion in
the bandwidth. This system utilizes the spatial diversity property of the multi
channel system. The reliable transmission requires symbols to be effectively
recovered at the receiving end. V-BLAST detection technique is employed for
this purpose. This paper depicted the advantages of using multiple antennas by
exploiting signal diversity offered by multipath effect and the system offers
high spectral efficiency.

ABSTRACT_BEGIN
  Today, Internet involves many actors who are making revenues on it
(operators, companies, service providers,...). It is therefore important to be
able to make fair decisions in this large-scale and highly competitive
economical ecosystem. One of the main issues is to prevent actors from
manipulating the natural outcome of the decision process. For that purpose,
game theory is a natural framework. In that context, voting systems represent
an interesting alternative that, to our knowledge, has not yet been considered.
They allow competing entities to decide among different options. Strong
theoretical results showed that all voting systems are susceptible to be
manipulated by one single voter, except for some "degenerated" and
non-acceptable cases. However, very little is known about how much a voting
system is manipulable in practical scenarios. In this paper, we investigate
empirically the use of voting systems for choosing end-to-end paths in
multi-carrier networks, analyzing their manipulability and their economical
efficiency. We show that one particular system, called \Single Transferable
Vote (STV), is largely more resistant to manipulability than the natural system
which tries to get the economical optimum. Moreover, STV manages to select
paths close to the economical optimum, whether the participants try to cheat or
not.

ABSTRACT_BEGIN
  An important goal towards the design of Future Networks is to achieve the
best ratio of performance to energy consumption and at the same time assure
manageability. This paper presents a general problem formulation for
Energy-Aware Traffic Engineering and proposes a distributed, heuristic
Energy-Aware Traffic Engineering scheme (ETE) that provides load balancing and
energy-awareness in accordance with the operator's needs. Simulation results of
ETE compared to the optimal network performance confirm the capability of ETE
to meeting the needs of Future Networks.

ABSTRACT_BEGIN
  A general problem formulation for energy-efficient traffic engineering for
future core networks is presented. Moreover, a distributed heuristic algorithm
that provides jointly load balancing and energy efficiency is proposed,
approaching in this way the optimal network operation in terms of throughput
and energy consumption.

ABSTRACT_BEGIN
  In this paper we design and implement a resource management scheme based on
cooperative association, where the STAs can share useful information in order
to improve the performance of the association/handoff procedures. The
cooperative association mechanism is inspired by the rapidly designed
cooperative protocols in the field of wireless networks. Furthermore, we
introduce a load balancing mechanism that operates in a cross-layer manner
taking into account uplink and downlink channel conditions, routing performance
and congestion control. The iterative heuristic algorithms that we propose,
control the communication load of each mesh AP in a distributed manner. We
evaluate the performance of our mechanisms through OPNET simulations and
testbed experiments.

ABSTRACT_BEGIN
  The existing characteristics of the wireless networks nowadays, urgently
impose the exploitation of flexible networking solutions that will offer
increased efficiency in resource utilization and application Quality of Service
(QoS) provisioning and at the same time will reduce the energy consumption and
achieve green targets. In this respect, Operator-governed Opportunistic
Networks (ONs), which are dynamically created, temporary, coordinated
extensions of the infrastructure, are the basic constituents in the proposed
approach. In addition, Cognitive Management Systems (CMSs), which comprise
self-management and learning capabilities, can be exploited for ensuring fast
and reliable establishment of ONs, achieving efficiently the desired goals.
This paper presents the concept of ONs and their representative scenarios, as
well as an evaluation of indicative test cases as a proof of concept of the
aforementioned approach. Indicative simulation results are presented, which
yield the conditions in which the adoption of such a solution can lead to lower
costs and management decisions with a "greener" footprint.

ABSTRACT_BEGIN
  The high bandwidth demand of Internet applications has recently driven the
need of increasing the residential download speed. A practical solution to the
problem has been proposed aggregating the bandwidth of 802.11 Access Points
(APs) backhauls in range via 802.11 connections. Since 802.11 devices are
usually single-radio, the communication to multiple APs on different
radio-channels requires the introduction of a time-division multiple access
(TDMA) policy at the client station. Current investigation in this area
supposes that there is a sufficient number of TCP flows to saturate the
Asymmetric Digital Subscriber Line (ADSL) behind the APs. However, this may be
not guaranteed according to the user traffic pattern. As a consequence, a TDMA
policy introduces additional delays in the end-to-end transmissions that will
cause degradation of the TCP throughput and an under-utilization of the AP
backhauls. In this paper, we first perform an in-depth experimental analysis
with a customized 802.11 driver of how the usage of multi-AP TDMA affects the
observed Round-Trip-Time (RTT) of TCP flows. Then, we introduce a simple
analytical model that accurately predicts the TCP RTT when accessing the
wireless medium with a Multi-AP TDMA policy. Based on this model, we propose a
resource allocation algorithm that runs locally at the station and it greatly
reduces the observed TCP RTT with a very low computational cost. Our proposed
scheme can improve up to 1:5 times the aggregate throughput observed by the
station compared to state-of-the-art multi-AP TDMA allocations. We also show
that the throughput performance of the algorithm is very close to the
theoretical upper-bound in key simulation scenarios.

ABSTRACT_BEGIN
  In past 802.11 systems there is a single Radio Frequency (RF) chain on the
Wi-Fi device. Multiple antennas use the same hardware to process the radio
signal. So only one antenna can transmit or receive at a time as all radio
signals need to go through the single RF chain. In MIMO there can be a separate
RF chain for each antenna allowing multiple RF chains to coexist. MIMO
technology has attracted attention in wireless communications, because it
offers significant increases in data throughput and link range without
additional bandwidth or increased transmit power. It achieves this goal by
spreading the same total transmit power over the antennas to achieve an array
gain that improves the spectral efficiency (more bits per second per hertz of
bandwidth) or to achieve a diversity gain that improves the link reliability.
Multiple Input/Multiple Output (MIMO) is an area of intense development in the
wireless industry because it delivers profound gains in range, throughput and
reliability. As a result, manufacturers of wireless local area network (WLAN),
wireless metropolitan area network (WMAN), and mobile phone equipment are
embracing MIMO technology. In this paper we are interested to compare the MIMO
Antenna functions with traditional Antenna functions. And we take an example of
IRT for illustration.

ABSTRACT_BEGIN
  A Mobile ad hoc Network or MANET is a wireless network of mobile devices that
has the ability to self-configure and self-organise and is characterised by an
absence of centralised administration and network infrastructure. An
appreciable number of routing protocols used in a typical MANET have left the
critical aspect of security out of consideration by assuming that all of its
constituent nodes are trustworthy and non-malicious. In this paper, we discuss
some of the major threats that such networks are vulnerable to, because of
these inherently insecure protocols. The focus is specifically on the
source-initiated and on-demand routing protocols. Further, solutions and
modifications to these protocols that have been proposed over time, enabling
them to mitigate the aforementioned threats to some extent, are also analysed.

ABSTRACT_BEGIN
  Currently, TCP is the most popular and widely used network transmission
protocol. In actual fact, about 90% of connections on the internet use TCP to
communicate. Through several upgrades and improvements, TCP became well
optimized for the very reliable wired networks. As a result, TCP considers all
packet timeouts in wired networks as due to network congestion and not to bit
errors. However, with networking becoming more heterogeneous, providing wired
as well as wireless topologies, TCP suffers from performance degradation over
error-prone wireless links as it has no mechanism to differentiate error losses
from congestion losses. It therefore considers all packet losses as due to
congestion and consequently reduces the burst of packet, diminishing at the
same time the network throughput. This paper proposes a new TCP congestion
control scheme appropriate for wireless as well as wired networks and is
capable of distinguishing congestion losses from error losses. The proposed
scheme is based on using the reserved field of the TCP header to indicate
whether the established connection is over a wired or a wireless link.
Additionally, the proposed scheme leverages the SNR ratio to detect the
reliability of the link and decide whether to reduce packet burst or retransmit
a timed-out packet. Experiments conducted, revealed that the proposed scheme
proved to behave correctly in situations where timeouts were due to error and
not to congestion. Future work can improve upon the proposed scheme so much so
that it can leverage CRC and HEC errors so as to better determine the cause of
transmission timeouts in wireless networks.

ABSTRACT_BEGIN
  Radio frequency (RF) sensing networks are a class of wireless sensor networks
(WSNs) which use RF signals to accomplish tasks such as passive device-free
localization and tracking. The algorithms used for these tasks usually require
access to measurements of baseline received signal strength (RSS) on each link.
However, it is often impossible to collect this calibration data (measurements
collected during an offline calibration period when the region of interest is
empty of targets). We propose adapting background subtraction methods from the
field of computer vision to estimate baseline RSS values from measurements
taken while the system is online and obstructions may be present. This is done
by forming an analogy between the intensity of a background pixel in an image
and the baseline RSS value of a WSN link and then translating the concepts of
temporal similarity, spatial similarity and spatial ergodicity which underlie
specific background subtraction algorithms to WSNs. Using experimental data, we
show that these techniques are capable of estimating baseline RSS values with
enough accuracy that RF tomographic tracking can be carried out in a variety of
different environments without the need for a calibration period.

ABSTRACT_BEGIN
  In the literature, scaling laws for wireless mobile networks have been
characterized under various models of node mobility and several assumptions on
how communication occurs between nodes. To improve the realism in the analysis
of scaling laws, we propose a new analytical framework. The framework is the
first to consider a L\'{e}vy flight mobility pattern, which is known to closely
mimic human mobility patterns. Also, this is the first work that allows nodes
to communicate while being mobile. Under this framework, delays ($\bar{D}$) to
obtain various levels of per-node throughput $(\lambda)$ for L\'evy flight are
suggested as $\bar{D}(\lambda) = O(\sqrt{\min (n^{1+\alpha} \lambda, n^2)})$,
where L\'evy flight is a random walk of a power-law flight distribution with an
exponent $\alpha \in (0,2]$. The same framework presents a new tighter tradeoff
$\bar{D}(\lambda) = O(\sqrt{\max (1,n\lambda^3)})$ for \textit{i.i.d.}
mobility, whose delays are lower than existing results for the same levels of
per-node throughput.

ABSTRACT_BEGIN
  This work develops novel algorithms for high-performance networking in the
presence of obstacles based on a method for communicating via ultrasonic rays
reflected at the obstacles. The rays are curves determined by the variable
speed of sound and initial conditions and we develop ultrasonic ray models
based on a system of differential equations. We present new parallel algorithms
and software for shape and trajectory reconstruction of moving obstacles and
show how the reconstructed reflection point of a ray at an obstacle is a
natural router for messages between the ray's transmitter and receiver and
discuss the advantages of the new architecture. We discuss how the new
algorithms and software improve the performance and properties of the network
architecture.

ABSTRACT_BEGIN
  One of the major issues in Wireless Body Area Sensor Networks (WBASNs) is
efficient localization. There are various techniques for indoor and outdoor
environments to locate a person. This study evaluating and compares performance
of optimization schemes in indoor environments for optimal placement of
wireless sensors, where patients can perform their daily activities. In indoor
environments, the performance comparison between Distance Vector-Hop algorithm,
Ring Overlapping Based on Comparison Received Signal Strength Indicator
(ROCRSSI), Particle filtering and Kalman filtering based location tracking
techniques, in terms of localization accuracy is estimated. Results show that
particle filtering outperforms all. GPS and several techniques based on
GSMlocation tracking schemes are proposed for outdoor environments. Hidden
Markov GSM based location tracking scheme efficiently performs among all, in
terms of location accuracy and computational overheads.

ABSTRACT_BEGIN
  In this paper, a framework for experimental parameters in which Packet
Delivery Ratio (PDR), effect of link duration over End-to-End Delay (E2ED) and
Normalized Routing Overhead (NRO) in terms of control packets is analyzed and
modeled for Mobile Ad-Hoc NETworks (MANETs) and Vehicular Ad-Hoc NETworks
(VANETs) with the assumption that nodes (vehicles) are sparsely moving in two
different road. Moreover, this paper contributes the performance comparison of
one Proactive Routing Protocol; Destination Sequenced Distance vector (DSDV)
and two reactive protocols; DYnamic Source Routing (DSR) and DYnamic MANET
On-Demand (DYMO). A novel contribution of this work is enhancements in default
versions of selected routing protocols. Three performance parameters; PDR, E2ED
and NRO with varying scalabilities are measured to analyze the performance of
selected routing protocols with their original and enhanced versions. From
extensive simulations, it is observed that DSR outperforms among all three
protocols at the cost of delay. NS-2 simulator is used for simulation with
TwoRayGround propagation model to evaluate analytical results.

ABSTRACT_BEGIN
  In this paper, we evaluate and compare the impact of link duration and path
stability of routing protocols; Destination Sequence Distance vector (DSDV),
Dynamic MANET On- Demand (DYMO) and Optimized Link State Routing (OLSR) at
different number of connections and node density. In order to improve the
efficiency of selected protocols; we enhance DYMO and OLSR. Simulation and
comparison of both default and enhanced routing protocols is carried out under
the performance parameters; Packet Delivery Ratio (PDR), Average End-to End
Delay (AE2ED) and Normalized Routing Overhead (NRO). From the results, we
observe that DYMO performs better than DSDV, MOD-OLSR and OLSR in terms of PDR,
AE2ED, link duration and path stability at the cost of high value of NRO.

ABSTRACT_BEGIN
  Network virtualization is a technology of running multiple heterogeneous
network architecture on a shared substrate network. One of the crucial
components in network virtualization is virtual network embedding, which
provides a way to allocate physical network resources (CPU and link bandwidth)
to virtual network requests. Despite significant research efforts on virtual
network embedding in wired and cellular networks, little attention has been
paid to that in wireless multi-hop networks, which is becoming more important
due to its rapid growth and the need to share these networks among different
business sectors and users. In this paper, we first study the root causes of
new challenges of virtual network embedding in wireless multi-hop networks, and
propose a new embedding algorithm that efficiently uses the resources of the
physical substrate network. We examine our algorithm's performance through
extensive simulations under various scenarios. Due to lack of competitive
algorithms, we compare the proposed algorithm to five other algorithms, mainly
borrowed from wired embedding or artificially made by us, partially with or
without the key algorithmic ideas to assess their impacts.

ABSTRACT_BEGIN
  Handoff performance of NEMO BS protocol with existent improvement proposals
is still not sufficient for real time and QoS-sensitive applications and
further optimizations are needed. When dealing with single homed NEMO, handoff
latency and packet loss become irreducible all optimizations included, so that
it is impossible to meet requirements of the above applications. Then, How to
combine the different Fast handoff approaches remains an open research issue
and needs more investigation. In this paper, we propose a new Infrastructure
independent handoff approach combining multihoming and intelligent
Make-Before-Break Handoff. Based on required Handoff time estimation, L2 and L3
handoffs are initiated using effective and timely MIH triggers, reducing so the
anticipation time and increasing the probability of prediction. We extend MIH
services to provide tunnel establishment and switching before link break. Thus,
the handoff is performed in background with no latency and no packet loss while
pingpong scenario is almost avoided. In addition, our proposal saves cost and
power consumption by optimizing the time of simultaneous use of multiple
interfaces. We provide also NS2 simulation experiments identifying suitable
parameter values used for estimation and validating the proposed model

ABSTRACT_BEGIN
  Recent advances in wireless communications, system on chip and low power
sensor nodes allow realization of Wireless Body Area Networks (WBANs).WBANs
comprise of tiny sensors, which collect information of a patient's vital signs
and provide a real time feedback. In addition,WBANs also support many
applications including ubiquitous healthcare, entertainment, gaming, military,
etc. Ubiquitous healthcare is required by elderly people to facilitate them
with instant monitoring anywhere they move around. In this paper, we provide a
survey on different architectures used in WBANs for ubiquitous healthcare
monitoring. Different standards and devices used in these architectures are
also discussed in this paper. Finally, path loss in WBANs and its impact on
communication is presented with the help of simulations performed for different
models of In-Body communication and different factors (such as, attenuation,
frequency, distance etc) influencing path loss in On-Body communications.

ABSTRACT_BEGIN
  Several studies demonstrate that there are critical differences between real
wireless networks and simulation models. This finding has permitted to extract
spatial and temporal properties for links and to provide efficient methods as
biased link sampling to guarantee efficient routing structure. Other works have
focused on computing metrics to improve routing, specially the reuse of the
measure of entropy. From there, rises the idea of formulating a new measure of
entropy that gives an overview of the spatiotemporal stability of a link. This
measure will rely on spatial and temporal properties of links and fed with the
efficiency of biased link sampling.

ABSTRACT_BEGIN
  This paper presents a survey of energy efficiency of Medium Access Control
(MAC) protocols for Wireless Body Area Sensor Networks (WBASNs). We highlight
the features of MAC protocols along with their advantages and limitations in
context of WBASNs. Comparison of Low Power Listening (LPL), Scheduled
Contention and Time Division Multiple Access (TDMA) is also elaborated. MAC
protocols with respect to different approaches and techniques which are used
for energy minimization, traffic control mechanisms for collision avoidance are
discussed.We also present a survey of path loss models for In-body, On-body and
Off-body communications in WBASNs and analytically discuss that path loss is
maximum in In-body communication because of low energy levels to take care of
tissues and organs located inside the body. Survey of Power model for WBANs of
CSMA/CA and beacon mode is also presented.

ABSTRACT_BEGIN
  Constant monitoring of patients without disturbing their daily activities can
be achieved through mobile networks. Sensor nodes distributed in a home
environment to provide home assistance gives concept of Wireless Wearable Body
Area Networks. Gathering useful information and its transmission to the
required destination may face several problems. In this paper we figure out
different issues and discuss their possible solutions in order to obtain an
optimized infrastructure for the care of elderly people. Different channel
models along with their characteristics, noise filtering in different
equalization techniques, energy consumption and effect of different impairments
have been discussed in our paper. The novelty of this work is that we
highlighted multiple issues along with their possible solutions that a BAN
infrastructure is still facing.

ABSTRACT_BEGIN
  Enhancement of network lifetime is a key design criterion for most of the
energy constrained networks as nodes are battery operated. In multi-hop
wireless network, proper utilization of battery power is very much necessary to
maintain network connectivity. If the battery power of a node drains quickly
then its connectivity in its neighborhood will be lost. So the study of network
lifetime is very much crucial as compared to other network parameters.
Considering this importance we made an attempt to study the behaviour of three
most common routing protocols in ad hoc network. Extensive simulations are done
on AODV, DSR and ZRP to determine the network lifetime at different node
mobility and at different network load. Simulation results suggest that AODV is
the most energy efficient protocol as compared to other

ABSTRACT_BEGIN
  The hierarchical macro/femto cell based BWA networks are observed to be quite
promising for mobile operators as it improves their network coverage and
capacity at the outskirt of the macro cell. However, this new technology
introduces increased number of macro/femto handoff and wastage of electrical
energy which in turn may affect the system performance. Users moving with high
velocity or undergoing real-time transmission suffers degraded performance due
to huge number of unnecessary macro/femto handoff. On the other hand, huge
amount of electrical energy is wasted when a femto BS is active in the network
but remains unutilized due to low network load. Our proposed energy efficient
handoff decision algorithm eliminates the unnecessary handoff while balancing
the load of the macro and femto cells at minimal energy consumption. The
performance of the proposed algorithm is analyzed using Continuous Time Markov
Chain (CTMC) Model. In addition, we have also contributed a method to determine
the balanced threshold level of the received signal strength (RSS) from macro
base station (BS). The balanced threshold level provides equal load
distribution of the mobile users to the macro and femto BSs. The balanced
threshold level is evaluated based on the distant location of the femto cells
for small scaled networks. Numerical analysis shows that threshold level above
the balanced threshold results in higher load distribution of the mobile users
to the femto BSs.

ABSTRACT_BEGIN
  An energy efficient routing protocol is the major concern in Wireless Sensor
Networks (WSNs). In this survey paper, we present energy efficient hierarchical
routing protocols, developed from conventional LEACH routing protocol. Main
focus of our study is how these extended protocols work in order to increase
the life time and how quality routing protocol are improved for WSNs.
Furthermore, this paper also highlights some of the issues faced by LEACH and
also explains how these issues are tackled by extended versions of LEACH. We
compare the features and performance issues of the selected hierarchal routing
protocols.

ABSTRACT_BEGIN
  In recent years the concept of the effective capacity that relates the
physical layer characteristics of a wireless channel to the data link layer has
gained a lot of attraction in wireless networking research community. The
effective capacity is based on G\"artner-Ellis' large deviation theorem and it
is used to provide the statistical QoS provisioning in the wireless networks.
Effective capacity also helps in the analysis of the resource allocation or
scheduling policies in various wireless systems such as Relay networks,
multi-user systems and multi-carrier systems subject to statistical QoS
requirements. The effective capacity in noise limited wireless network has
already been investigated in the recent works. Considering the interference
limited wireless channels, in this paper we propose an analytical approach
based on Laplace's method for the effective capacity of uncorrelated Rayleigh
fading channel in the presence of uncorrelated Rayleigh fading interference.
The accuracy of the analytical model for the effective capacity is validated by
numerical simulations. We also provide the evaluation of tail probability of
the delay and maximum sustainable rate. The validation results reveal that the
proposed mathematical approach to the effective capacity can open the path for
further researches in statistical QoS provisioning in interference limited
wireless networks.

ABSTRACT_BEGIN
  EGP and IGP are the key components of the present internet infrastructure.
Routers in a domain forward IP packet within and between domains. Each domain
uses an intra-domain routing protocol known as Interior Gateway Protocol (IGP)
like IS-IS, OSPF, RIP etc to populate the routing tables of its routers.
Routing information must also be exchanged between domains to ensure that a
host in one domain can reach another host in remote domain. This role is
performed by inter-domain routing protocol called Exterior Gateway Protocol
(EGP). Basically EGP used these days is Border Gateway Protocol (BGP). Basic
difference between the both is that BGP has smaller convergence as compared to
the IGP's. And IGP's on the other hand have lesser scalability as compared to
the BGP. So in this paper a proposal to create a new protocol is given which
can act as an IGP when we consider inter-domain transfer of traffic and acts as
BGP when we consider intra-domain transfer of traffic.

ABSTRACT_BEGIN
  In Wireless sensor networks data aggregation with hundreds and thousands of
sensor nodes is very complex task. Recently, mobile agents have been proposed
for efficient data dissemination in sensor networks. In the traditional
client/server based computing architecture, data is collected from multiple
sources and forwarded to destination for further processing. It requires high
bandwidth, whereas in the mobile agent is a task specific executable code
traverses to the relevant source for gathering data. It reduces communication
overhead, reduce cost, low bandwidth. Agents have capability to perform task
for multiple applications. It will send only useful information to destination
node. The problem is to group similar mobile agents into a number of clusters
such that each cluster has similarity in responding to a group of nodes. By
clustering intelligent mobile agents, it is possible to reduce the cost of time
for each individual agent, decrease the demand imposed on network for a set of
required tasks, decrease total number of visits. This paper, we present the
problem of Multiple Criteria Clustering of Mobile Agents (MCCMA) where the
decision is to cluster mobile agents such that a group of similar intelligent
mobile agents will visit a group of similar sensor nodes.

ABSTRACT_BEGIN
  The lack of interoperability between cellular access networks has long been a
challenging burden, which telecommunication engineers and researchers are
trying to overcome. In second generation networks for example, this problem
lies in the lack of standardization. 3rd G networks is limited to a few
operating modes using different radio transmission technologies that are not
interoperable. 4G technology even being successful in its various trials cannot
guarantee interoperability. The undertaken approach to overcome this issue
within heterogeneous networks begins by establishing a holistic understanding
of cellular communication, and proposing an Ontological approach that expresses
the domain's concepts, classes, and properties in a formal and unambiguous way.
It begins by analyzing the structure of three different cellular technologies,
and producing feature models. Lte-Advanced cellular network is the target of
this ongoing analysis. The final objective sought is to build Ontology capable
of providing a common view of cellular network technologies' domain.

ABSTRACT_BEGIN
  Multiuser Multi-Packet Transmission (MPT) from an Access Point (AP) equipped
with multiple antennas to multiple single-antenna nodes can be achieved by
exploiting the spatial dimension of the channel. In this paper we present a
queueing model to analytically study such systems from the link-layer
perspective, in presence of random packet arrivals, heterogeneous channel
conditions and packet errors. The analysis relies on a blind estimation of the
number of different destinations among the packets waiting in the queue, which
allows for building a simple, but general model for MPT systems with per-node
First-In First-Out (FIFO) packet scheduling. Simulation results validate the
accuracy of the analytical model and provide further insights on the
cross-relations between the channel state, the number of antennas, and the
number of active users, as well as how they affect the system performance. The
simplicity and accuracy of the model makes it suitable for the evaluation of
Medium Access Control (MAC) protocols for Ad-Hoc or Wireless Local Area
Networks supporting multiuser MPT in non-saturation conditions, where the
queueing dynamics play an important role on the achieved performance, and
simple user selection algorithms are required.

ABSTRACT_BEGIN
  Wireless objects equipped with multiple antennas are able to simultaneously
transmit multiple packets by exploiting the channel's spatial dimensions. In
this paper, we study the benefits of such Multiple Packet Transmission (MPT)
approach, when it is used in combination with a Carrier Sense Multiple Access
with Collision Avoidance (CSMA/CA) protocol for fully interconnected networks,
addressing the interactions between the two mechanisms and showing the
performance gains that can be achieved. To this end, a very simple Media Access
Control (MAC) protocol that captures the fundamental properties and tradeoffs
of a CSMA/CA channel access protocol supporting MPT is introduced. Using this
protocol as a reference, a new analytical model is presented for the case of
non-saturated traffic sources with finite buffer space. Simulation results show
that the analytical model is able to accurately characterize the steady-state
behaviour of the reference protocol for different number of antennas and
different traffic loads, providing a useful tool for understanding the
performance gains achieved by MAC protocols supporting MPT.

ABSTRACT_BEGIN
  Energy efficient routing protocol for Wireless Sensor Networks (WSNs) is one
of the most challenging task for researcher. Hierarchical routing protocols
have been proved more energy efficient routing protocols, as compare to flat
and location based routing protocols. Heterogeneity of nodes with respect to
their energy level, has also added extra lifespan for sensor network. In this
paper, we propose a Centralized Energy Efficient Clustering (CEEC) routing
protocol. We design the CEEC for three level heterogeneous network. CEEC can
also be implemented in multi-level heterogeneity of networks. For initial
practical, we design and analyze CEEC for three level advance heterogeneous
network. In CEEC, whole network area is divided into three equal regions, in
which nodes with same energy are spread in same region.

ABSTRACT_BEGIN
  Extensive energy is consumed by Transceiver communication operation [1].
Existing research on MAC layer focuses to maximize battery-powered sensor
node's life. Bottleneck of MAC layer protocol design for WBAN is to achieve
high reliability and energy minimization. Majority of MAC protocols designed
for WBANs are based upon TDMA approach. However, a new protocol needs to be
defined to achieve high energy efficiency, fairness and avoid extra energy
consumption due to synchronization.

ABSTRACT_BEGIN
  This paper proposes a new protocol called Optimal DCF (O-DCF). Inspired by a
sequence of analytic results, O-DCF modifies the rule of adapting CSMA
parameters, such as backoff time and transmission length, based on a function
of the demand-supply differential of link capacity captured by the local queue
length. Unlike clean-slate design, O-DCF is fully compatible with 802.11
hardware, so that it can be easily implemented only with a simple device driver
update. Through extensive simulations and real experiments with a 16-node
wireless network testbed, we evaluate the performance of O-DCF and show that it
achieves near-optimality, and outperforms other competitive ones, such as
802.11 DCF, optimal CSMA, and DiffQ in a wide range of scenarios.

ABSTRACT_BEGIN
  A new two layer hierarchical routing protocol called Cluster Based
Hierarchical Routing Protocol (CBHRP) is proposed in this paper. It is an
extension of LEACH routing protocol. We introduce cluster head-set idea for
cluster-based routing where several clusters are formed with the deployed
sensors to collect information from target field. On rotation basis, a head-set
member receives data from the neighbor nodes and transmits the aggregated
results to the distance base station. This protocol reduces energy consumption
quite significantly and prolongs the life time of sensor network. It is found
that CBHRP performs better than other well accepted hierarchical routing
protocols like LEACH in term of energy consumption and time requirement.

ABSTRACT_BEGIN
  One of the main criteria in Vehicular Ad hoc Networks (VANETs) that has
attracted the researchers' consideration is congestion control. Accordingly,
many algorithms have been proposed to alleviate the congestion problem,
although it is hard to find an appropriate algorithm for applications and
safety messages among them. Safety messages encompass beacons and event-driven
messages. Delay and reliability are essential requirements for event-driven
messages. In crowded networks where beacon messages are broadcasted at a high
number of frequencies by many vehicles, the Control Channel (CCH), which used
for beacons sending, will be easily congested. On the other hand, to guarantee
the reliability and timely delivery of event-driven messages, having a
congestion free control channel is a necessity. Thus, consideration of this
study is given to find a solution for the congestion problem in VANETs by
taking a comprehensive look at the existent congestion control algorithms. In
addition, the taxonomy for congestion control algorithms in VANETs is presented
based on three classes, namely, proactive, reactive and hybrid. Finally, we
have found the criteria in which fulfill prerequisite of a good congestion
control algorithm.

ABSTRACT_BEGIN
  Device-free (DF) localization in WLANs has been introduced as a value-added
service that allows tracking indoor entities that do not carry any devices.
Previous work in DF WLAN localization focused on the tracking of a single
entity due to the intractability of the multi-entity tracking problem whose
complexity grows exponentially with the number of humans being tracked. In this
paper, we introduce Spot as an accurate and efficient system for multi-entity
DF detection and tracking. Spot is based on a probabilistic energy minimization
framework that combines a conditional random field with a Markov model to
capture the temporal and spatial relations between the entities' poses. A novel
cross-calibration technique is introduced to reduce the calibration overhead of
multiple entities to linear, regardless of the number of humans being tracked.
This also helps in increasing the system accuracy. We design the energy
minimization function with the goal of being efficiently solved in mind. We
show that the designed function can be mapped to a binary graph-cut problem
whose solution has a linear complexity on average and a third order polynomial
in the worst case. We further employ clustering on the estimated location
candidates to reduce outliers and obtain more accurate tracking. Experimental
evaluation in two typical testbeds, with a side-by-side comparison with the
state-of-the-art, shows that Spot can achieve a multi-entity tracking accuracy
of less than 1.1m. This corresponds to at least 36% enhancement in median
distance error over the state-of-the-art DF localization systems, which can
only track a single entity. In addition, Spot can estimate the number of
entities correctly to within one difference error. This highlights that Spot
achieves its goals of having an accurate and efficient software-only DF
tracking solution of multiple entities in indoor environments.

ABSTRACT_BEGIN
  One of the major task of sensor nodes in wireless sensor networks is to
transmit a subset of sensor readings to the sink node estimating a desired data
accuracy. Therefore in this paper, we propose an accuracy model using Steepest
Decent method called Adaptive Data Accuracy (ADA) model which doesn't require
any a priori information of input signal statistics to select an optimal set of
sensor nodes in the network. Moreover we develop another model using LMS filter
called Spatio-Temporal Data Prediction (STDP) model which captures the spatial
and temporal correlation of sensing data to reduce the communication overhead
under data reduction strategies. Finally using STDP model, we illustrate a
mechanism to trace the malicious nodes in the network under extreme physical
environment. Computer simulations illustrate the performance of ADA and STDP
models respectively.

ABSTRACT_BEGIN
  This paper investigates the fundamental building blocks of physical-layer
network coding (PNC). Most prior work on PNC focused on its application in a
simple two-way-relay channel (TWRC) consisting of three nodes only. Studies of
the application of PNC in general networks are relatively few. This paper is an
attempt to fill this gap. We put forth two ideas: 1) A general network can be
decomposed into small building blocks of PNC, referred to as the PNC atoms, for
scheduling of PNC transmissions. 2) We identify nine PNC atoms, with TWRC being
one of them. Three major results are as follows. First, using the decomposition
framework, the throughput performance of PNC is shown to be significantly
better than those of the traditional multi-hop scheme and the conventional
network coding scheme. For example, under heavy traffic volume, PNC can achieve
100% throughput gain relative to the traditional multi-hop scheme. Second, PNC
decomposition based on a variety of different PNC atoms can yield much better
performance than PNC decomposition based on the TWRC atom alone. Third, three
out of the nine atoms are most important to good performance. Specifically, the
decomposition based on these three atoms is good enough most of the time, and
it is not necessary to use the other six atoms.

ABSTRACT_BEGIN
  Given a set ${\cal V}$ of $n$ sensor node distributed on a 2-dimensional
plane and a source node $s \in {\cal V}$, the {\it interference problem} deals
with assigning transmission range to each $v \in {\cal V}$ such that the
members in ${\cal V}$ maintain connectivity predicate ${\cal P}$, and the
maximum/total interference is minimum. We propose algorithm for both {\it
minimizing maximum interference} and {\it minimizing total interference} of the
networks. For minimizing maximum interference we present optimum solution with
running time $O(({\cal P}_n + n^2) \log n)$ for connectivity predicate ${\cal
P}$ like strong connectivity, broadcast ($s$ is the source), $k$-edge(vertex)
connectivity, spanner, where $O({\cal P}_n)$ is the time complexity for
checking the connectivity predicate ${\cal P}$. The running time of the
previous best known solution was $O({\cal P}_n \times n^2)$ [Bil$\grave{o}$ and
Proietti, 2008].
  For the minimizing total interference we propose optimum algorithm for the
connectivity predicate broadcast. The running time of the propose algorithm is
O(n). For the same problem, the previous best known result was $2(1 + \ln
(n-1))$-factor approximation algorithm [Bil$\grave{o}$ and Proietti, 2008]. We
also propose a heuristic for minimizing total interference in the case of
strongly connected predicate and compare our result with the best result
available in the literature. Experimental results demonstrate that our
heuristic outperform existing result.

ABSTRACT_BEGIN
  Ad hoc network is a collection of wireless mobile nodes that dynamically form
a temporary network without the use of any existing network infrastructure or
centralized administration. A cognitive radio is a radio that can change its
transmitter parameters based on interaction with the environment in which it
operates. The basic idea of cognitive radio networks is that the unlicensed
devices (cognitive radio users or secondary users) need to vacate the spectrum
band once the licensed device (primary user) is detected. Cognitive capability
and reconfigurability are the key characteristics of cognitive radio. Routing
is an important issue in Mobile Cognitive Radio Ad Hoc Networks (MCRAHNs). In
this paper, a survey of routing protocols for mobile cognitive radio ad
networks is discussed.

ABSTRACT_BEGIN
  Second-generation (2G) digital cellular systems constitute the majority of
cellular communication deployed today. A variety of services of 2G systems has
increased significantly and this will continue to grow even further in the
emerging third-generation (3G) systems. Universal Mobile Telecommunication
System (UMTS) is a third-generation mobile communications system which uses the
Wide-Band Code Division Multiple Access (WCDMA) technique to support a wide
variety of services, like speech, video telephony, Internet browsing, etc.
These services require a wide range of Quality of Service (QoS) requirements.
QoS is an important issue as the number of multimedia services increases day by
day. Differentiated QoS methods allow the differentiation of users based on
their priority levels and channel conditions so that the network can allocate
the bandwidth for a particular request based on the QoS requirements. These
requirements are controlled by Radio Resource Management (RRM) mechanisms. In
this paper we have proposed two RRM algorithms which are modification to the
existing scheduling algorithms. One is Prioritized C/I scheduling, which takes
the priorities into consideration, and this algorithm serves the user with
highest priority. Other algorithm is Modified Inverse C/I scheduling, which
takes channel conditions into consideration and serves the users in degraded
conditions, thereby improving QoS. The performance evaluation of two algorithms
is done with EURANE extensions for NS-2. Simulation results shows the
improvement in QoS for the users who are at equidistance from Base Station (BS)
but requesting for different services by implementing Prioritized C/I
scheduling and also for the users who are in degraded conditions by
implementing Modified Inverse C/I scheduling when compared to Max C/I and
Inverse C/I scheduling algorithm respectively.

ABSTRACT_BEGIN
  Our work is motivated by the need for impromptu (or "as-you-go") deployment
of relay nodes (for establishing a packet communication path with a control
centre) by fire-men/commandos while operating in an unknown environment. We
consider a model, where a deployment operative steps along a random lattice
path whose evolution is Markov. At each step, the path can randomly either
continue in the same direction or take a turn "North" or "East," or come to an
end, at which point a data source (e.g., a temperature sensor) has to be placed
that will send packets to a control centre at the origin of the path. A
decision has to be made at each step whether or not to place a wireless relay
node. Assuming that the packet generation rate by the source is very low, and
simple link-by-link scheduling, we consider the problem of relay placement so
as to minimize the expectation of an end-to-end cost metric (a linear
combination of the sum of convex hop costs and the number of relays placed).
This impromptu relay placement problem is formulated as a total cost Markov
decision process. First, we derive the optimal policy in terms of an optimal
placement set and show that this set is characterized by a boundary beyond
which it is optimal to place. Next, based on a simpler alternative
one-step-look-ahead characterization of the optimal policy, we propose an
algorithm which is proved to converge to the optimal placement set in a finite
number of steps and which is faster than the traditional value iteration. We
show by simulations that the distance based heuristic, usually assumed in the
literature, is close to the optimal provided that the threshold distance is
carefully chosen.

ABSTRACT_BEGIN
  Multimedia streaming applications with stringent QoS requirements in 60GHz
mmWave wireless personal area networks (WPANs) demand high rate and low latency
data transfer as well as low service disruption. In this paper, we consider the
problem of robust relay placement in 60GHz WPANs. Relays forward traffic from
transmitter devices to receiver devices facilitating i) the primary
communication path for non-line-of-sight (NLOS) transceiver pairs, and ii)
secondary (backup) communication path for line-of-sight (LOS) transceiver
pairs. We formulate the robust minimum relay placement problem and the robust
maximum utility relay placement problem with the objective to minimize the
number of relays deployed and maximize the network utility, respectively.
Efficient algorithms are developed to solve both problems and have been shown
to incur less service disruption in presence of moving subjects that may block
the LOS paths in the environment.

ABSTRACT_BEGIN
  In Cognitive Radio Networks (CRNs), secondary users (SUs) are allowed to
opportunistically access the unused/under-utilized channels of primary users
(PUs). To utilize spectrum resources efficiently, an auction scheme is often
applied where an operator serves as an auctioneer and accepts spectrum requests
from SUs. Most existing works on spectrum auctions assume that the operator has
perfect knowledge of PU activities. In practice, however, it is more likely
that the operator only has statistical information of the PU traffic when it is
trading a spectrum hole, and it is acquiring more accurate information in real
time. In this paper, we distinguish PU channels that are under the control of
the operator, where accurate channel states are revealed in real-time, and
channels that the operator acquires from PUs out of its control, where a
sense-before-use paradigm has to be followed. Considering both spectrum
uncertainty and sensing inaccuracy, we study the social welfare maximization
problem for serving SUs with various levels of delay tolerance. We first model
the problem as a finite horizon Markov decision process when the operator knows
all spectrum requests in advance, and propose an optimal dynamic programming
based algorithm. We then investigate the case when spectrum requests are
submitted online, and propose a greedy algorithm that is 1/2-competitive for
homogeneous channels and is comparable to the offline algorithm for more
general settings. We further show that the online algorithm together with a
payment scheme achieves incentive compatibility for the SUs while guaranteeing
a non-negative revenue for the operator.

ABSTRACT_BEGIN
  A fundamental problem in the delay and backlog analysis across multi-hop
paths in wireless networks is how to account for the random properties of the
wireless channel. Since the usual statistical models for radio signals in a
propagation environment do not lend themselves easily to a description of the
available service rate on a wireless link, the performance analysis of wireless
networks has resorted to higher-layer abstractions, e.g., using Markov chain
models. In this work, we propose a network calculus that can incorporate common
statistical models of fading channels and obtain statistical bounds on delay
and backlog across multiple nodes. We conduct the analysis in a transfer
domain, which we refer to as the `SNR domain', where the service process at a
link is characterized by the instantaneous signal-to-noise ratio at the
receiver. We discover that, in the transfer domain, the network model is
governed by a dioid algebra, which we refer to as (min,x)-algebra. Using this
algebra we derive the desired delay and backlog bounds. An application of the
analysis is demonstrated for a simple multi-hop network with Rayleigh fading
channels and for a network with cross traffic.

ABSTRACT_BEGIN
  Frequent topological changes due to high mobility is one of the main issues
in Vehicular Ad-hoc NETworks (VANETs). In this paper, we model transmission
probabilities of 802.11p for VANETs and effect of these probabilities on
average transmission time. To evaluate the effect of these probabilities of
VANETs in routing protocols, we select Dynamic Source Routing (DSR), Fish-eye
State Routing (FSR) and Optimized Link State Routing (OLSR). Framework of these
protocols with respect to their packet cost is also presented in this work. A
novel contribution of this work is enhancement of chosen protocols to obtain
efficient behavior. Extensive simulation work is done to prove and compare the
efficiency in terms of high throughput of enhanced versions with default
versions of protocols in NS-2. For this comparison, we choose three performance
metrics; throughput, End-to-End Delay (E2ED) and Normalized Routing Load (NRL)
in different mobilities and scalabilities. Finally, we deduce that enhanced DSR
(DSR-mod) outperforms other protocols by achieving 16% more packet delivery for
all scalabilities and 28% more throughput in selected mobilities than original
version of DSR (DSR-orig).

ABSTRACT_BEGIN
  Tremendous growth in the demand for wireless applications such as streaming
audio/videos, Skype and video games require high data rate irrespective of
user's location in the cellular network. However, the Quality of Service (QoS)
of users degrades at the cell boundary. Relay enhanced multi-hop cellular
network is one of the cost effective solution to improve the performance of
cell edge users. Optimal deployment of Fixed Relay Nodes (FRNs) is essential to
satisfy the QoS requirement of edge users. We propose new schemes for channel
partitioning and FRN placement in cellular networks. Path-loss, Signal to
Interference and Noise Ratio (SINR) experienced by users, and effects of
shadowing have been considered. The analysis gives more emphasis on the
cell-edge users (worst case scenario). The results show that these schemes
achieve higher system performance in terms of spectral efficiency and also
increase the user data rate at the cell edge.

ABSTRACT_BEGIN
  Content-Centric Networking (CCN) is a concept being considered as a potential
future alternative to, or replacement for, today's Internet IP-style
packet-switched host-centric networking. One factor making CCN attractive is
its focus on content distribution, which dominates current Internet traffic and
which is arguably not well-served by IP. Named Data Networking (NDN) is a
prominent example of CCN. It is also one of several on-going research efforts
aiming to design and develop a full-blown candidate future Internet
architecture. Although NDN's primary motivation is content distribution, it is
envisioned to support other types of traffic, such as conferencing (audio,
video) as well as more historical applications, such as remote login. However,
it is unclear how suitable NDN is for applications that are not obviously
content-centric. In this paper, we explore NDN in the context of a class of
applications that involve low- latency bidirectional communication.
Specifically, we propose a few architectural amendments to NDN that provide
significantly better throughput and lower latency for this class of
applications. The proposed approach is validated via both simulations and
testbed experiments.

ABSTRACT_BEGIN
  While a natural fit for modeling and understanding mobile networks,
time-varying graphs remain poorly understood. Indeed, many of the usual
concepts of static graphs have no obvious counterpart in time-varying ones. In
this paper, we introduce the notion of temporal reachability graphs. A
(tau,delta)-reachability graph} is a time-varying directed graph derived from
an existing connectivity graph. An edge exists from one node to another in the
reachability graph at time t if there exists a journey (i.e., a spatiotemporal
path) in the connectivity graph from the first node to the second, leaving
after t, with a positive edge traversal time tau, and arriving within a maximum
delay delta. We make three contributions. First, we develop the theoretical
framework around temporal reachability graphs. Second, we harness our
theoretical findings to propose an algorithm for their efficient computation.
Finally, we demonstrate the analytic power of the temporal reachability graph
concept by applying it to synthetic and real-life datasets. On top of defining
clear upper bounds on communication capabilities, reachability graphs highlight
asymmetric communication opportunities and offloading potential.

ABSTRACT_BEGIN
  Connectivity and coverage are two crucial problems for wireless sensor
networks. Several studies have focused on proposing solutions for improving and
adjusting the initial deployment of a wireless sensor network to meet these two
criteria. In our work, we propose a new hierarchical architecture for sensor
networks that facilitates the gathering of redundancy information of the
topology. Several mobile robots must then relocate, in an optimized way,
redundant sensors to achieve optimal connectivity and coverage of the network.
Mobile robots have to cooperate and coordinate their movement. A performance
evaluation is conducted to study the trade-off between the number of required
robots and its impact on the rate of network connectivity and coverage.

ABSTRACT_BEGIN
  We study Batch Processor-Sharing (BPS) queuing model with hyper-exponential
service time distribution and Poisson batch arrival process. One of the main
goals to study BPS is the possibility of its application in size-based
scheduling, which is used in differentiation between Short and Long flows in
the Internet. In the case of hyper-exponential service time distribution we
find an analytical expression of the expected conditional response time for the
BPS queue. We show, that the expected conditional response time is a concave
function of the service time. We apply the received results to the Two Level
Processor-Sharing (TLPS) model with hyper-exponential service time distribution
and find the expression of the expected response time for the TLPS model. TLPS
scheduling discipline can be applied to size-based differentiation in TCP/IP
networks and Web server request handling.

ABSTRACT_BEGIN
  Netsukuku is a P2P network system designed to handle a large number of nodes
with minimal CPU and memory resources. It can be easily used to build a
worldwide distributed, anonymous and not controlled network, separated from the
Internet, without the support of any servers, ISPs or authority controls. In
this document, we give a generic and non technical description of the Netsukuku
network, emphasizing its main ideas and features.

ABSTRACT_BEGIN
  This document describes the QSPN, the routing discovery algorithm used by
Netsukuku. Through a deductive analysis the main proprieties of the QSPN are
shown. Moreover, a second version of the algorithm, is presented.

ABSTRACT_BEGIN
  In this document, we describe the fractal structure of the Netsukuku
topology. Moreover, we show how it is possible to use the QSPN v2 on the high
levels of the fractal.

ABSTRACT_BEGIN
  We present the Abnormal Netsukuku Domain Name Anarchy system. ANDNA is the
distributed, non hierarchical and decentralised system of hostname management
used in the Netsukuku network.

ABSTRACT_BEGIN
  Wireless networking is rapidly growing and becomes an inexpensive technology
which allows multiple users to simultaneously access the network and the
internet while roaming about the campus. In the present work, the software
development of a wireless LAN(WLAN) is highlighted. This WLAN utilizes direct
sequence spread spectrum (DSSS) technology at 902MHz RF carrier frequency in
its physical layer. Cost effective installation and antijaming property of
spread spectrum technology are the major advantages of this work.

ABSTRACT_BEGIN
  Scientists and Technologists involved in the development of radar and remote
sensing systems all over the world are now trying to involve themselves in
saving of manpower in the form of developing a new application of their ideas
in Intelligent Transport system(ITS). The world statistics shows that by
incorporating such wireless radar system in the car would decrease the world
road accident by 8-10% yearly. The wireless technology has to be chosen
properly which is capable of tackling the severe interferences present in the
open road. A combined digital technology like Spread spectrum along with
diversity reception will help a lot in this regard. Accordingly, the choice is
for FHSS based space diversity system which will utilize carrier frequency
around 5.8 GHz ISM band with available bandwidth of 80 MHz and no license. For
efficient design, the radio channel is characterized on which the design is
based. Out of two available modes e.g. Communication and Radar modes, the radar
mode is providing the conditional measurement of the range of the nearest car
after authentication of the received code, thus ensuring the reliability and
accuracy of measurement. To make the system operational in simultaneous mode,
we have started the Software Defined Radio approach for best speed and
flexibility.

ABSTRACT_BEGIN
  With the technological growth of broadband wireless technology like CDMA and
UWB, a lots of development efforts towards wireless communication system and
Imaging radar system are well justified. Efforts are also being imparted
towards a Convergence Technology.. the convergence between a communication and
radar technology which will result in ITS (Intelligent Transport System) and
other applications. This encourages present authors for this development. They
are trying to utilize or converge the communication technologies towards radar
and to achieve the Interference free and clutter free quality remote images of
targets using DS-UWB wireless technology.

ABSTRACT_BEGIN
  In the today's Internet and TCP/IP-networks, the queueing of packets is
commonly implemented using the protocol FIFO (First In First Out).
Unfortunately, FIFO performs poorly in the Adversarial Queueing Theory. Other
queueing strategies are researched in this model and better results are
performed by alternative queueing strategies, e.g. LIS (Longest In System).
This article introduces a new queueing protocol called interval-strategy that
is concerned with the reduction from dynamic to static routing. We discuss the
maximum system time for a packet and estimate with up-to-date results how this
can be achieved. We figure out the maximum amount of time where a packet can
spend in the network (i.e. worst case system time), and argue that the
universal instability of the presented interval-strategy can be reached through
these results. When a large group of queueing strategies is used for queueing,
we prove that the interval-strategy will be universally unstable. Finally, we
calculate the maximum time of the static routing to reach an universal stable
and polynomial - in detail linear - bounded interval-strategy. Afterwards we
close - in order to check this upper bound - with up-to-date results about the
delivery times in static routing.

ABSTRACT_BEGIN
  Two-tier networks, comprising a conventional cellular network overlaid with
shorter range hotspots (e.g. femtocells, distributed antennas, or wired
relays), offer an economically viable way to improve cellular system capacity.
The capacity-limiting factor in such networks is interference. The cross-tier
interference between macrocells and femtocells can suffocate the capacity due
to the near-far problem, so in practice hotspots should use a different
frequency channel than the potentially nearby high-power macrocell users.
Centralized or coordinated frequency planning, which is difficult and
inefficient even in conventional cellular networks, is all but impossible in a
two-tier network. This paper proposes and analyzes an optimum decentralized
spectrum allocation policy for two-tier networks that employ frequency division
multiple access (including OFDMA). The proposed allocation is optimal in terms
of Area Spectral Efficiency (ASE), and is subjected to a sensible Quality of
Service (QoS) requirement, which guarantees that both macrocell and femtocell
users attain at least a prescribed data rate. Results show the dependence of
this allocation on the QoS requirement, hotspot density and the co-channel
interference from the macrocell and surrounding femtocells. Design
interpretations of this result are provided.

ABSTRACT_BEGIN
  This paper studies the network throughput and transport delay of a multihop
wireless random access network based on a Markov renewal model of packet
transportation. We show that the distribution of the source-to-destination (SD)
distance plays a critical role in characterizing network performance. We
establish necessary and sufficient condition on the SD distance for scalable
network throughput, and address the optimal rate allocation issue with fairness
and the QoS requirements taken into consideration. In respect to the end-to-end
performance, the transport delay is explored in this paper along with network
throughput. We characterize the transport delay by relating it to nodal
queueing behavior and the SD-distance distribution; the former is a local
property while the latter is a global property. In addition, we apply the large
deviation theory to derive the tail distribution of transport delay. To put our
theory into practical network operation, several traffic scaling laws are
provided to demonstrate how network scalability can be achieved by localizing
the traffic pattern, and a leaky bucket scheme at the network access is
proposed for traffic shaping and flow control.

ABSTRACT_BEGIN
  Radio Frequency IDentification (RFID) systems are becoming more and more
popular in the field of ubiquitous computing, in particular for objects
identification. An RFID system is composed by one or more readers and a number
of tags. One of the main issues in an RFID network is the fast and reliable
identification of all tags in the reader range. The reader issues some queries,
and tags properly answer. Then, the reader must identify the tags from such
answers. This is crucial for most applications. Since the transmission medium
is shared, the typical problem to be faced is a MAC-like one, i.e. to avoid or
limit the number of tags transmission collisions. We propose a protocol which,
under some assumptions about transmission techniques, always achieves a 100%
perfomance. It is based on a proper recursive splitting of the concurrent tags
sets, until all tags have been identified. The other approaches present in
literature have performances of about 42% in the average at most. The
counterpart is a more sophisticated hardware to be deployed in the manufacture
of low cost tags.

ABSTRACT_BEGIN
  Searching in P2P networks is fundamental to all overlay networks.
  P2P networks based on Distributed Hash Tables (DHT) are optimized for single
key lookups, whereas unstructured networks offer more complex queries at the
cost of increased traffic and uncertain success rates. Our Distributed Tree
Construction (DTC) approach enables structured P2P networks to perform prefix
search, range queries, and multicast in an optimal way. It achieves this by
creating a spanning tree over the peers in the search area, using only
information available locally on each peer. Because DTC creates a spanning
tree, it can query all the peers in the search area with a minimal number of
messages. Furthermore, we show that the tree depth has the same upper bound as
a regular DHT lookup which in turn guarantees fast and responsive runtime
behavior. By placing objects with a region quadtree, we can perform a prefix
search or a range query in a freely selectable area of the DHT. Our DTC
algorithm is DHT-agnostic and works with most existing DHTs. We evaluate the
performance of DTC over several DHTs by comparing the performance to existing
application-level multicast solutions, we show that DTC sends 30-250% fewer
messages than common solutions.

ABSTRACT_BEGIN
  In this work we propose a network meta-architecture based on fundamental laws
of physics and a physical model of computation. This meta-architecture may be
used to frame discussions about novel network architectures as well as
cross-layer alterations to the canonical network stack.

ABSTRACT_BEGIN
  This paper proposes an adaptive variant of Random Early Detection (RED)
gateway queue management for packet-switched networks via a discrete state
analog of the non-stationary Master Equation i.e. Markov process. The
computation of average queue size, which appeared in the original RED
algorithm, is altered by introducing a probability $P(l,t)$, which defines the
probability of having $l$ number of packets in the queue at the given time $t$,
and depends upon the previous state of the queue. This brings the advantage of
eliminating a free parameter: queue weight, completely. Computation of
transition rates and probabilities are carried out on the fly, and determined
by the algorithm automatically. Simulations with unstructured packets
illustrate the method, the performance of the adaptive variant of RED
algorithm, and the comparison with the standard RED.

ABSTRACT_BEGIN
  Despite a large amount of effort devoted in the past years trying to limit
unsolicited mail, spam is still a major global concern. Content-analysis
techniques and blacklists, the most popular methods used to identify and block
spam, are beginning to lose their edge in the battle. We argue here that one
not only needs to look into the network-related characteristics of spam
traffic, as has been recently suggested, but also to look deeper into the
network core, in order to counter the increasing sophistication of spam-ing
methods. Yet, at the same time, local knowledge available at a given server can
often be irreplaceable in identifying specific spammers. To this end, in this
paper we show how the local intelligence of mail servers can be gathered and
correlated pas- sively at the ISP-level providing valuable network-wide
information. Specifically, we use first a large network flow trace from a
medium size, national ISP, to demonstrate that the pre-filtering decisions of
individual mail servers can be tracked and combined at the flow level. Then, we
argue that such aggregated knowledge not only can allow ISPs to develop and
evaluate powerful new methods for fighting spam, but also to monitor remotely
what their own servers are doing.

ABSTRACT_BEGIN
  In this short paper, we propose a new multi-fractal flow model, aiming to
provide a possible explanation for the crossover phenomena that appear in the
estimation of Hurst exponent for network traffic. It is shown that crossover
occurs if the network flow consists of several components with different Hurst
components. Our results indicate that this model might be useful in network
traffic modeling and simulation.

ABSTRACT_BEGIN
  In this research, we propose an architectural solution to implement the voice
over IP (VoIP) service in campus environment network. Voice over IP (VoIP)
technology has become a discussion issue for this time being. Today, the
deployment of this technology on an organization truly can give a great
financial benefit over traditional telephony. Therefore, this study is to
analyze the VoIP Codec selection and investigate the Mean Opinion Score (MOS)
performance areas evolved with the quality of service delivered by soft phone
and IP phone. This study focuses on quality of voice prediction such as i)
accuracy of MOS between automated system and human perception and ii) different
types of codec performance measurement via human perception using MOS
technique. In this study, network management system (NMS) is used to monitor
and capture the performance of VoIP in campus environment. In addition, the
most apparent of implementing soft phone and IP phone in campus environment is
to define the best codec selection that can be used in operational environment.
Based on the finding result, the MOS measurement through automated and manual
system is able to predict and evaluate VoIP performance. In addition, based on
manual MOS measurement, VoIP conversations over LAN contribute more reliability
and availability performance compare to WAN.

ABSTRACT_BEGIN
  The equilibrium distributions of a Markovian model describing the interaction
of several classes of permanent connections in a network are analyzed. It has
been introduced by Graham and Robert. For this model each of the connections
has a self-adaptive behavior in that its transmission rate along its route
depends on the level of congestion of the nodes on its route. It has been shown
that the invariant distributions are determined by the solutions of a fixed
point equation in a finite dimensional space. In this paper, several examples
of these fixed point equations are studied. The topologies investigated are
rings, trees and a linear network, with various sets of routes through the
nodes.

ABSTRACT_BEGIN
  Drawing from todays best-in-class solutions, we identify power-saving
strategies that have succeeded in the past and look forward to new ideas and
paradigms. We strongly believe that designing energy-efficient network
equipment can be compared to building sports cars, task-oriented, focused and
fast. However, unlike track-bound sports cars, ultra-fast and purpose-built
silicon yields better energy efficiency when compared to more generic family
sedan designs that mitigate go-to-market risks by being the masters of many
tasks. Thus, we demonstrate that the best opportunities for power savings come
via protocol simplification, best-of-breed technology, and silicon and software
optimization, to achieve the least amount of processing necessary to move
packets. We also look to the future of networking from a new angle, where
energy efficiency and environmental concerns are viewed as fundamental design
criteria and forces that need to be harnessed to continually create more
powerful networking equipment.

ABSTRACT_BEGIN
  Dense, unmanaged 802.11 deployments tempt saboteurs into launching jamming
attacks by injecting malicious interference. Nowadays, jammers can be portable
devices that transmit intermittently at low power in order to conserve energy.
In this paper, we first conduct extensive experiments on an indoor 802.11
network to assess the ability of two physical layer functions, rate adaptation
and power control, in mitigating jamming. In the presence of a jammer we find
that: (a) the use of popular rate adaptation algorithms can significantly
degrade network performance and, (b) appropriate tuning of the carrier sensing
threshold allows a transmitter to send packets even when being jammed and
enables a receiver capture the desired signal. Based on our findings, we build
ARES, an Anti-jamming REinforcement System, which tunes the parameters of rate
adaptation and power control to improve the performance in the presence of
jammers. ARES ensures that operations under benign conditions are unaffected.
To demonstrate the effectiveness and generality of ARES, we evaluate it in
three wireless testbeds: (a) an 802.11n WLAN with MIMO nodes, (b) an 802.11a/g
mesh network with mobile jammers and (c) an 802.11a WLAN. We observe that ARES
improves the network throughput across all testbeds by up to 150%.

ABSTRACT_BEGIN
  In this paper two complexity efficient soft sphere-decoder modifications are
proposed for computing the max-log LLR values in iterative MIMO systems, which
avoid the costly, typically needed, full enumeration and sorting (FES)
procedure during the tree traversal without compromising the max-log
performance. It is shown that despite the resulting increase in the number of
expanded nodes, they can be more computationally efficient than the typical
soft sphere decoders by avoiding the unnecessary complexity of FES.

ABSTRACT_BEGIN
  In the present paper, high transmission bit rate of a thermal arrayed
waveguide grating (AWG) which is composed of lithium niobate
(LiNbO3)/polymethyl metha acrylate (PMMA) hybrid materials on a silicon
substrate in Passive Optical Networks (PONs) has parametrically analyzed and
investigated over wide range of the affecting parameters. We have theoretically
investigated the temperature dependent wavelength shift of the arrayed
waveguide grating (AWG) depends on the refractive-indices of the materials and
the size of the waveguide. A thermalization of the AWG can be realized by
selecting proper values of the material and structural parameters of the
device. Moreover, we have analyzed the data transmission bit rate of a thermal
AWG in passsive optical networks (PONs) based on Maximum Time Division
Multiplexing (MTDM) technique.

ABSTRACT_BEGIN
  Rapid and innovative improvement in wireless communication technologies has
led to an increase in the demand for mobile internet transactions. However,
internet access from mobile devices is very expensive due to limited bandwidth
available on wireless links and high mobility rate of mobile hosts. When a user
executes a transaction with a web portal from a mobile device, the
disconnection necessitates failure of the transaction or redoing all the steps
after reconnection, to get back into consistent application state. Thus
considering challenges in wireless mobile networks, a new log management scheme
is proposed for recovery of mobile transactions.
  In this proposed approach, the model parameters that affect application state
recovery are analyzed. The proposed scheme is compared with the existing Lazy
and Pessimistic scheme and a trade off analysis between the cost invested to
manage log and the return of investment in terms of improved failure
recoverability is made. From the analysis, the best checkpoint interval period
that yields the best return of investment is identified.

ABSTRACT_BEGIN
  This paper studies the problem of utility maximization for clients with delay
based QoS requirements in wireless networks. We adopt a model used in a
previous work that characterizes the QoS requirements of clients by their delay
constraints, channel reliabilities, and delivery ratio requirements. In this
work, we assume that the utility of a client is a function of the delivery
ratio it obtains. We treat the delivery ratio for a client as a tunable
parameter by the access point (AP), instead of a given value as in the previous
work. We then study how the AP should assign delivery ratios to clients so that
the total utility of all clients is maximized.
  We apply the techniques introduced in two previous papers to decompose the
utility maximization problem into two simpler problems, a CLIENT problem and an
ACCESS-POINT problem. We show that this decomposition actually describes a
bidding game, where clients bid for the service time from the AP. We prove that
although all clients behave selfishly in this game, the resulting equilibrium
point of the game maximizes the total utility. In addition, we also establish
an efficient scheduling policy for the AP to reach the optimal point of the
ACCESS-POINT problem. We prove that the policy not only approaches the optimal
point but also achieves some forms of fairness among clients. Finally,
simulation results show that our proposed policy does achieve higher utility
than all other compared policies.

ABSTRACT_BEGIN
  We develop a general approach for designing scheduling policies for real-time
traffic over wireless channels. We extend prior work, which characterizes a
real-time flow by its traffic pattern, delay bound, timely-throughput
requirement, and channel reliability, to allow time-varying channels, allow
clients to have different deadlines, and allow for the optional employment of
rate adaptation. Thus, our model allow the treatment of more realistic fading
channels as well as scenarios with mobile nodes, and the usage of more general
transmission strategies.
  We derive a sufficient condition for a scheduling policy to be feasibility
optimal, and thereby establish a class of feasibility optimal policies. We
demonstrate the utility of the identified class by deriving a feasibility
optimal policy for the scenario with rate adaptation, time-varying channels,
and heterogeneous delay bounds. When rate adaptation is not available, we also
derive a feasibility optimal policy for time-varying channels. For the scenario
where rate adaptation is not available but clients have different delay bounds,
we describe a heuristic. Simulation results are also presented which indicate
the usefulness of the scheduling policies for more realistic and complex
scenarios.

ABSTRACT_BEGIN
  Passive network tomography uses end-to-end observations of network
communication to characterize the network, for instance to estimate the network
topology and to localize random or adversarial glitches. Under the setting of
linear network coding this work provides a comprehensive study of passive
network tomography in the presence of network (random or adversarial) glitches.
To be concrete, this work is developed along two directions: 1. Tomographic
upper and lower bounds (i.e., the most adverse conditions in each problem
setting under which network tomography is possible, and corresponding schemes
(computationally efficient, if possible) that achieve this performance) are
presented for random linear network coding (RLNC). We consider RLNC designed
with common randomness, i.e., the receiver knows the random code-books all
nodes. (To justify this, we show an upper bound for the problem of topology
estimation in networks using RLNC without common randomness.) In this setting
we present the first set of algorithms that characterize the network topology
exactly. Our algorithm for topology estimation with random network errors has
time complexity that is polynomial in network parameters. For the problem of
network error localization given the topology information, we present the first
computationally tractable algorithm to localize random errors, and prove it is
computationally intractable to localize adversarial errors. 2. New network
coding schemes are designed that improve the tomographic performance of RLNC
while maintaining the desirable low-complexity, throughput-optimal, distributed
linear network coding properties of RLNC. In particular, we design network
codes based on Reed-Solomon codes so that a maximal number of adversarial
errors can be localized in a computationally efficient manner even without the
information of network topology.

ABSTRACT_BEGIN
  In any communication network, the maximum number of link-disjoint paths
between any pair of communicating nodes, S and T, is limited by the S-T minimum
link-cut. Multipath routing protocols have been proposed in the literature to
make use of these S-T paths in enhancing the survivability of the S-T
information flow. This is usually accomplished by using a subset of these paths
to forward redundant data units or combinations (if network coding is allowed)
from S to T. Therefore, this enhancement in survivability reduces the useful
S-T information rate. In this paper we present a new way to enhance the
survivability of the S-T information flow without compromising the maximum
achievable S-T information rate. To do this, bottleneck links (in the min-cut)
should only forward useful information, and not redundant data units. We
introduce the idea of extra source or destination connectivity with respect to
a certain S-T max-flow, and then we study two problems: namely, pre-cut
protection and post-cut protection. Although our objective in both problems is
the same, where we aim to maximize the number of protected paths, our analysis
shows that the nature of these two problems are very different, and that the
pre-cut protection problem is much harder. Specifically, we prove the hardness
of the pre-cut protection problem, formulate it as an integer linear program,
and propose a heuristic approach to solve it. Simulations show that the
performance of the heuristic is acceptable even on relatively large networks.
In the post-cut problem we show that all the data units, forwarded by the
min-cut edges not incident to T, can be post-cut-protected.

ABSTRACT_BEGIN
  A few months ago, the BitTorrent developers announced that the transfer of
torrent data in the official client was about to switch to uTP, an
application-layer congestion-control protocol using UDP at the transport-layer.
This announcement immediately raised an unmotivated buzz about a new, imminent
congestion collapse of the whole Internet. Though this reaction was not built
on solid technical foundation, nevertheless a legitimate question remains:
i.e., whether this novel algorithm is a necessary building block for future
Internet applications, or whether it may result in an umpteenth addition to the
already well populated world of Internet congestion control algorithms.
  In this paper, we tackle precisely this issue. The novel protocol is now
under discussion at the IETF LEDBAT working group, and has been defined in a
draft document in March 2009, whose adoption decision will be taken at the
beginning of August 2009. Adhering to the IETF draft definition, we implement
the LEDBAT congestion control algorithm and investigate its performance by
means of packet-level simulations. Considering a simple bottleneck scenario
where LEDBAT competes against either TCP or other LEDBAT flows, we evaluate the
fairness of the resource share as well as its efficiency. Our preliminary
results show that indeed, there is an undoubted appeal behind the novel
application-layer congestion-control protocol. Yet, care must be taken in order
to ensure that some important points, such as intra-protocol fairness, are
fully clarified in the draft specification -- which we hope that this work can
contribute to.

ABSTRACT_BEGIN
  Wireless Multiuser receivers suffer from their relatively higher
computational complexity that prevents widespread use of this technique. In
addition, one of the main characteristics of multi-channel communications that
can severely degrade the performance is the inconsistent and low values of SNR
that result in high BER and poor channel capacity. It has been shown that the
computational complexity of a multiuser receiver can be reduced by using the
transformation matrix (TM) algorithm [4]. In this paper, we provide
quantification of SNR based on the computational complexity of TM algorithm. We
show that the reduction of complexity results high and consistent values of SNR
that can consequently be used to achieve a desirable BER performance. In
addition, our simulation results suggest that the high and consistent values of
SNR can be achieved for a desirable BER performance. The performance measure
adopted in this paper is the consistent values of SNR.

ABSTRACT_BEGIN
  Ethernet topology discovery has gained increasing interest in the recent
years. This trend is motivated mostly by increasing number of carrier Ethernet
networks as well as the size of these networks, and consequently the increasing
sales of these networks. To manage these networks efficiently, detailed and
accurate knowledge of their topology is needed. Knowledge of a network's
entities and the physical connections between them can be useful in various
prospective. Administrators can use topology information for network planning
and fault detecting. Topology information can also be used during protocol and
routing algorithm development, for performance prediction and as a basis for
accurate network simulations. From a network security perspective, threat
detection, network monitoring, network access control and forensic
investigations can benefit from accurate network topology information. In this
paper, we analyze market trends and investigate current tools available for
both research and commercial purposes.

ABSTRACT_BEGIN
  A good computer network is hard to disrupt. It is desired that the computer
communication network remains connected even when some of the links or nodes
fail. Since the communication links are expensive, one wants to achieve these
goals with fewer links. The computer communication network is fault tolerant if
it has alternative paths between vertices, the more disjoint paths, the better
is the survivability. This paper presents a method for generating k-connected
computer communication network with optimal number of links using bipartite
graph concept.

ABSTRACT_BEGIN
  In the present paper, the recent applications of optical parametric
amplifiers (OPAs) in hybrid wavelength division multiplexing (WDM)/time
division multiplexing (TDM) local area passive optical networks have been
modeled and parametrically investigated over wide range of the affecting
parameters. Moreover, we have analyzed the ability of the hybrid WDM/TDM
Passive optical networks to handle a triple play solution, offering voice,
video, and data services to the multiple users. Finally, we have investigated
the maximum time division multiplexing (MTDM) bit rates for optical network
units (ONUs) for maximum number of supported users with optical parametric
amplifier technique across the single mode fiber (SMF) or highly nonlinear
fiber (HNLF) cables to achieve both maximum network reach and quality of
service (QOS).

ABSTRACT_BEGIN
  TCP is designed for networks with assumption that major losses occur only due
to congestion of network traffic. On a wireless network TCP misinterprets the
transmission losses due to bit errors and handoffs as losses caused by
congestion, and triggers congestion control mechanisms. Because of its end to
end delivery model, congestion handling and avoidance mechanisms, TCP has been
widely accepted as Transport layer protocol for internetworks. Extension of
Internetworks over wireless links is inevitable with the spread of ubiquitous
computing and mobile communications. This paper presents study of different
mechanisms proposed to extend Transport Control Protocol and other alternate
solutions to enhance end to end performance over lossy wireless links. The
paper studies details of different design choices proposed and their technical
advantages and disadvantages. Finally, an analysis and proposal for best choice
of proposed schemes are made for wireless networks.

ABSTRACT_BEGIN
  This study aims to identify the advantages and disadvantages of several
mechanisms for service differentiation in mobile terminals of a wireless LAN to
establish a more better and more optimal. At the end of the analysis of
available approaches for the quality of service of the IEEE 802.11 standard,
the objective of this paper is to suggest a new method named DF-DCF
Differentiated Frame DCF. The performance of the suggested method in a Network
Simulator (NS) environment allowed its validation through a set of testing and
simulation scenarios. Simulation results have shown that the DF-DCF method is
better suited for mobile nodes in a wireless communication network.

ABSTRACT_BEGIN
  As we know, the member-only algorithm in provides the best links stress and
wavelength usage for the construction of multicast light-trees in WDM networks
with sparse splitting. However, the diameter of tree is too big and the average
delay is also too large, which are intolerant for QoS required multimedia
applications. In this paper, a distance priority based algorithm is proposed to
build light-trees for multicast routing, where the Candidate Destinations and
the Candidate Connectors are introduced. Simulations show the proposed
algorithm is able to greatly reduce the diameter and average delay of the
multicast tree (up to 51% and 50% respectively), while keep the same or get a
slightly better link stress as well as the wavelength usage than the famous
Member-Only algorithm.

ABSTRACT_BEGIN
  A widely used defense practice against malicious traffic on the Internet is
through blacklists: lists of prolific attack sources are compiled and shared.
The goal of blacklists is to predict and block future attack sources. Existing
blacklisting techniques have focused on the most prolific attack sources and,
more recently, on collaborative blacklisting. In this paper, we formulate the
problem of forecasting attack sources (also referred to as predictive
blacklisting) based on shared attack logs as an implicit recommendation system.
We compare the performance of existing approaches against the upper bound for
prediction, and we demonstrate that there is much room for improvement.
Inspired by the recent Netflix competition, we propose a multi-level prediction
model that is adjusted and tuned specifically for the attack forecasting
problem. Our model captures and combines various factors, namely:
attacker-victim history (using time-series) and attackers and/or victims
interactions (using neighborhood models). We evaluate our combined method on
one month of logs from Dshield.org and demonstrate that it improves
significantly the state-of-the-art.

ABSTRACT_BEGIN
  This paper proposes an adaptation of two network size estimation methods:
random tour and gossip-based aggregation to suit master/slave mobile ad hoc
networks. We show that it is feasible to accurately estimate the size of ad hoc
networks when topology changes due to mobility using both methods. The
algorithms were modified to account for the specific constraints of
master/slave ad hoc networks and the results show that the proposed
modifications perform better on these networks than the original protocols.
Each of the two algorithms presents strengths and weaknesses and these are
outlined in this paper.

ABSTRACT_BEGIN
  In this paper, we propose Multi-Rate Bandwidth Available in Real Time
(MR-BART) to estimate the end-to-end Available Bandwidth (AB) of a network
path. The proposed scheme is an extension of the Bandwidth Available in Real
Time (BART) which employs multi-rate (MR) probe packet sequences with Kalman
filtering. Comparing to BART, we show that the proposed method is more robust
and converges faster than that of BART and achieves a more AB accurate
estimation. Furthermore, we analyze the estimation error in MR-BART and obtain
analytical formula and empirical expression for the AB estimation error based
on the system parameters.

ABSTRACT_BEGIN
  Internet performance is tightly related to the properties of TCP and UDP
protocols, jointly responsible for the delivery of the great majority of
Internet traffic. It is well understood how these protocols behave under FIFO
queuing and what the network congestion effects. However, no comprehensive
analysis is available when flow-aware mechanisms such as per-flow scheduling
and dropping policies are deployed. Previous simulation and experimental
results leave a number of unanswered questions. In the paper, we tackle this
issue by modeling via a set of fluid non-linear ODEs the instantaneous
throughput and the buffer occupancy of N long-lived TCP sources under three
per-flow scheduling disciplines (Fair Queuing, Longest Queue First, Shortest
Queue First) and with longest queue drop buffer management. We study the system
evolution and analytically characterize the stationary regime: closed-form
expressions are derived for the stationary throughput/sending rate and buffer
occupancy which give thorough understanding of short/long-term fairness for TCP
traffic. Similarly, we provide the characterization of the loss rate
experienced by UDP flows in presence of TCP traffic.

ABSTRACT_BEGIN
  Clustering is a standard approach for achieving efficient and scalable
performance in wireless sensor networks. Traditionally, clustering algorithms
aim at generating a number of disjoint clusters that satisfy some criteria. In
this paper, we formulate a novel clustering problem that aims at generating
overlapping multi-hop clusters. Overlapping clusters are useful in many sensor
network applications, including inter-cluster routing, node localization, and
time synchronization protocols. We also propose a randomized, distributed
multi-hop clustering algorithm (KOCA) for solving the overlapping clustering
problem. KOCA aims at generating connected overlapping clusters that cover the
entire sensor network with a specific average overlapping degree. Through
analysis and simulation experiments we show how to select the different values
of the parameters to achieve the clustering process objectives. Moreover, the
results show that KOCA produces approximately equal-sized clusters, which
allows distributing the load evenly over different clusters. In addition, KOCA
is scalable; the clustering formation terminates in a constant time regardless
of the network size.

ABSTRACT_BEGIN
  In optical WDM networks, since each lightpath can carry a huge mount of
traffic, failures may seriously damage the end user applications. Hence fault
tolerance becomes an important issue on these networks. The light path which
carries traffic during normal operation is called as primary path. The traffic
is rerouted on a backup path in case of a failure. In this paper we propose to
design a reliable and fault tolerant routing algorithm for establishing primary
and backup paths. In order to establish the primary path, this algorithm uses
load balancing in which link cost metrics are estimated based on the current
load of the links. In backup path setup, the source calculates the blocking
probability through the received feedback from the destination by sending a
small fraction of probe packets along the existing paths. It then selects the
optimal light path with the lowest blocking probability. Based on the
simulation results, we show that the reliable and fault tolerant routing
algorithm reduces the blocking probability and latency while increasing the
throughput and channel utilization.

ABSTRACT_BEGIN
  BitTorrent suffers from one fundamental problem: the long-term availability
of content. This occurs on a massive-scale with 38% of torrents becoming
unavailable within the first month. In this paper we explore this problem by
performing two large-scale measurement studies including 46K torrents and 29M
users. The studies go significantly beyond any previous work by combining
per-node, per-torrent and system-wide observations to ascertain the causes,
characteristics and repercussions of file unavailability. The study confirms
the conclusion from previous works that seeders have a significant impact on
both performance and availability. However, we also present some crucial new
findings: (i) the presence of seeders is not the sole factor involved in file
availability, (ii) 23.5% of nodes that operate in seedless torrents can finish
their downloads, and (iii) BitTorrent availability is discontinuous, operating
in cycles of temporary unavailability. Due to our new findings, we consider it
is important to revisit the solution space; to this end, we perform large-scale
trace-based simulations to explore the potential of two abstract approaches.

ABSTRACT_BEGIN
  The challenges of optimizing end-to-end performance over diverse Internet
paths has driven widespread adoption of in-path optimizers, which can
destructively interfere with TCP's end-to-end semantics and with each other,
and are incompatible with end-to-end IPsec. We identify the architectural cause
of these conflicts and resolve them in Tng, an experimental next-generation
transport services architecture, by factoring congestion control from
end-to-end semantic functions. Through a technique we call "queue sharing", Tng
enables in-path devices to interpose on, split, and optimize congestion
controlled flows without affecting or seeing the end-to-end content riding
these flows. Simulations show that Tng's decoupling cleanly addresses several
common performance problems, such as communication over lossy wireless links
and reduction of buffering-induced latency on residential links. A working
prototype and several incremental deployment paths suggest Tng's practicality.

ABSTRACT_BEGIN
  Packet and flow scheduling algorithms for WiMAX has been a topic of interest
for a long time since the very inception of WiMAX networks. WiMAX offers
advantages particularly in terms of Quality of service it offers over a longer
range at the MAC level. In our work, we propose two credit based scheduling
schemes one in which completed flows distributes the left over credits equally
to all higher priority uncompleted flows(ODRREDC) and another in which
completed flows give away all the excess credits to the highest priority
uncompleted flow(ODRRSDC). Both the schemes are compatible with 802.16 MAC
protocol and can efficiently serve real time bursty traffic with reduced
latency and hence improved QOS for real time flows. We compare the two proposed
schemes for their latency, bandwidth utilization and throughput for real time
burst flows with the opportunity based Deficit Round Robin scheduling scheme.
While the ODRR scheduler focuses on reducing the credits for the flows with
errors, our approach also distributes these remaining credits together with the
credits from completed flows equally among the higher priority uncompleted
flows or totally to the highest priority uncompleted flow.

ABSTRACT_BEGIN
  The WIMAX technology based on air interface standard 802.16 wireless MAN is
configured in the same way as a traditional cellular network with base stations
using point to multipoint architecture to drive a service over a radius up to
several kilometers. The range and the Non Line of Sight (NLOS) ability of WIMAX
make the system very attractive for users, but there will be slightly higher
BER at low SNR. The aim of this paper is the comparative study of different
guard time intervals effect for improving BER at different SNR under digital
modulation (QPSK, 16QAM and 64QAM) techniques and different communication
channels AWGN and fading channels Stanford University Interim (SUI 1) of an
WIMAX system. The comparison between these effects with Reed-Solomon (RS)
encoder with Convolutional encoder (half) rated codes in FEC channel coding
will be investigated. The simulation results of estimated Bit Error Rate (BER)
displays that the implementation of interleaved RS code (255,239,8) with (half)
rated Convolutional code of 0.25 guard time intervals under QPSK modulation
technique over AWGN channel is highly effective to combat in the Wimax
communication system. To complete this performance analysis in Wimax based
systems, a segment of audio signal is used for analysis. The transmitted audio
message is found to have retrieved effectively under noisy situation.

ABSTRACT_BEGIN
  Multicasting is effective when its group members are sparse and the speed is
low. On the other hand, broadcasting is effective when the group members dense
and the speed are high. Since mobile ad hoc networks are highly dynamic in
nature, either of the above two strategies can be adopted at different
scenarios. In this paper, we propose an ant agent based adaptive, multicast
protocol that exploits group members desire to simplify multicast routing and
invoke broadcast operations in appropriate localized regimes. By reducing the
number of group members that participate in the construction of the multicast
structure and by providing robustness to mobility by performing broadcasts in
densely clustered local regions, the proposed protocol achieves packet delivery
statistics that are comparable to that with a pure multicast protocol but with
significantly lower overheads. By our simulation results, we show that our
proposed protocol achieves increased Packet Delivery Fraction (PDF) with
reduced overhead and routing load.

ABSTRACT_BEGIN
  Solving free riding and selecting a reliable service provider in P2P networks
has been separately investigated in last few years. Using trust has shown to be
one of the best ways of solving these problems. But using this approach to
simultaneously deal with both problems makes it impossible for newcomers to
join the network and the expansion of network is prevented. In this paper we
used the game theory to model the behavior of peers and developed a mechanism
in which free riding and providing bad service are dominated strategies for
peers. At the same time newcomers can participate and are encouraged to be
active in the network. The proposed model has been simulated and the results
showed that the trust value of free riders and bad service providers converge
to a finite value and trust of peers who provide good service is monotonically
increased despite the time they join the network.

ABSTRACT_BEGIN
  Congestion in network occurs due to exceed in aggregate demand as compared to
the accessible capacity of the resources. Network congestion will increase as
network speed increases and new effective congestion control methods are
needed, especially to handle bursty traffic of todays very high speed networks.
Since late 90s numerous schemes i.e. [1]...[10] etc. have been proposed. This
paper concentrates on comparative study of the different congestion control
schemes based on some key performance metrics. An effort has been made to judge
the performance of Maximum Entropy (ME) based solution for a steady state
GE/GE/1/N censored queues with partial buffer sharing scheme against these key
performance metrics.

ABSTRACT_BEGIN
  In hierarchical Mobile IPv6 networks, Mobility Anchor Point (MAP) may become
a single point of bottleneck as it handles more and more mobile nodes (MNs). A
number of schemes have been proposed to achieve load balancing among different
MAPs. However, signaling reduction is still imperfect because these schemes
also avoid the effect of the number of CNs. Also only the balancing of MN is
performed, but not the balancing of the actual traffic load, since CN of each
MN may be different. This paper proposes an efficient admission control
algorithm along with a replacement mechanism for HMIPv6 networks. The admission
control algorithm is based on the number of serving CNs and achieves actual
load balancing among MAPs. Moreover, a replacement mechanism is introduced to
decrease the new MN blocking probability and the handoff MN dropping
probability. By simulation results, we show that, the handoff delay and packet
loss are reduced in our scheme, when compared with the standard HMIPv6 based
handoff.

ABSTRACT_BEGIN
  This paper deals with the performance of Worldwide Interoperability for
Microwave Access (WiMAX), when we enhance its physical layer attributes with
help of different encoding techniques. For this evaluation Space Time Block
Codes (STBC) and Turbo codes are separately introduced into the architecture of
WiMAX that works on adaptive modulation technique.

ABSTRACT_BEGIN
  Integrating the sensing capabilities in Internet Protocol network will open
the opportunities to build a wide range of novel multimedia applications. The
problem when using sensors (e.g. temperature sensor, camera, audio, humidity,
etc.) connected to the network is to know dynamically at any time if they are
always connected or not, what type of data they can transmit and where they are
geographically located. This paper describes an application enabler: IMS Sensor
Search Engine Enabler (iSSEE), which allows IMS applications using sensors and
IMS based devices, to get information about the sensor availability, its
location and the type of the sensor. Using data collected by sensors and from
the web, mash-ups convergent applications use cases are proposed by combining
the contents from heterogeneous data.

ABSTRACT_BEGIN
  This paper proposes and investigates the concept of a safe carrier-sensing
range that can guarantee interference safe (also termed hidden-node-free)
transmissions in CSMA networks under the cumulative interference model.
Compared with the safe carrier-sensing range under the commonly assumed but
less realistic pairwise interference model, we show that the safe
carrier-sensing range required under the cumulative interference model is
larger by a constant multiplicative factor. The concept of a safe
carrier-sensing range, although amenable to elegant analytical results, is
inherently not compatible with the conventional power threshold carrier-sensing
mechanism (e.g., that used in IEEE 802.11). Specifically, the absolute power
sensed by a node in the conventional mechanism does not contain enough
information for it to derive its distances from other concurrent transmitter
nodes. We show that, fortunately, a carrier-sensing mechanism called
Incremental-Power Carrier-Sensing (IPCS) can realize the carrier-sensing range
concept in a simple way. Instead of monitoring the absolute detected power, the
IPCS mechanism monitors every increment in the detected power. This means that
IPCS can separate the detected power of every concurrent transmitter, and map
the power profile to the required distance information.

ABSTRACT_BEGIN
  A mobile ad-hoc network (MANET) is collection of intercommunicating mobile
hosts forming a spontaneous network without using established network
infrastructure. Unlike the cellular or infrastructure networks who have a wired
backbone connecting the base-station, the MANETs have neither fixed routers nor
fixed locations. Their performance largely depend upon the routing mechanism &
nature of mobility. Earlier research hints that the Destination Sequenced
Distance Vector (DSDV) routing protocol is one of the most efficient and
popular protocols, as far as general parameters have been concerned.[1,6] We
have experimentally evaluated, the performance metrics for network load, packet
delivery fraction and end-to-end delay with DSDV Protocol using NS2
Simulator.This paper presents, the performance of DSDV protocol for four
different mobility models namely: Random Waypoint, Reference Point Group
Mobility, Gauss Markov & Manhattan Mobility Model having varying network load &
speed. The experimental results suggest that DSDV protocol with RPGM mobility
model has optimized results for varying network load and speed.

ABSTRACT_BEGIN
  The large scale content distribution systems were improved broadly using the
replication techniques. The demanded contents can be brought closer to the
clients by multiplying the source of information geographically, which in turn
reduce both the access latency and the network traffic. The system scalability
can be improved by distributing the load across multiple servers which is
proposed by replication. If a copy of the requested object (e.g., a web page or
an image) is located in its closer proximity then the clients would feel low
access latency. Depending on the position of the replicas, the effectiveness of
replication tends to a large extent. A QoS based overlay network architecture
involving an intelligent replica placement algorithm is proposed in this paper.
Its main goal is to improve the network utilization and fault tolerance of the
P2P system. In addition to the replica placement, it also has a caching
technique, to reduce the search latency. We are able to show that our proposed
architecture attains less latency and better throughput with reduced bandwidth
usage, through the simulation results.

ABSTRACT_BEGIN
  Advantages of hypercube network and torus topology are used to derive an
embedded architecture for product network known as torus embedded hypercube
scalable interconnection network. This paper analyzes torus embedded hypercube
network pertinent to parallel architecture. The network metrics are used to
show how good embedded network can be designed for parallel computation.
Network parameter analysis and comparison of embedded network with basic
networks is presented.

ABSTRACT_BEGIN
  This paper studies the optimal investment and pricing decisions of a
cognitive mobile virtual network operator (C-MVNO) under spectrum supply
uncertainty. Compared with a traditional MVNO who often leases spectrum via
long-term contracts, a C-MVNO can acquire spectrum dynamically in short-term by
both sensing the empty "spectrum holes" of licensed bands and dynamically
leasing from the spectrum owner. As a result, a C-MVNO can make flexible
investment and pricing decisions to match the current demands of the secondary
unlicensed users. Compared to dynamic spectrum leasing, spectrum sensing is
typically cheaper, but the obtained useful spectrum amount is random due to
primary licensed users' stochastic traffic. The C-MVNO needs to determine the
optimal amounts of spectrum sensing and leasing by evaluating the trade off
between cost and uncertainty. The C-MVNO also needs to determine the optimal
price to sell the spectrum to the secondary unlicensed users, taking into
account wireless heterogeneity of users such as different maximum transmission
power levels and channel gains. We model and analyze the interactions between
the C-MVNO and secondary unlicensed users as a Stackelberg game. We show
several interesting properties of the network equilibrium, including threshold
structures of the optimal investment and pricing decisions, the independence of
the optimal price on users' wireless characteristics, and guaranteed fair and
predictable QoS among users. We prove that these properties hold for general
SNR regime and general continuous distributions of sensing uncertainty. We show
that spectrum sensing can significantly improve the C-MVNO's expected profit
and users' payoffs.

ABSTRACT_BEGIN
  The internet is now-a-days experiencing a stress due to some inherent
problems with the main interdomain routing protocol, boarder gateway protocol
(BGP), the amount of time it takes to converge, number of update message
exchanged followed by a failure to stabilize, the amount of time required to
get a valid alternate path following the failure, the way size of routing table
increasing, and security issues like integrity and privacy of routing tables
and routing updates exchanged among the routers, are of our primary concern. In
our proposed research work we plan to address aforementioned issues related to
internet routing specially in boarder gateway protocol to enable BGP to offer
expeditious unswerving routing to corroborate nascent internet. We plan to make
some changes in the design of boarder gateway protocol and may introduce
addition of extra features in BGP to help support above mentioned objective.

ABSTRACT_BEGIN
  The purposes of this paper have to discuss issues related to Network Traffic
Management. A relatively new category of network management is fast becoming a
necessity in converged business Networks. Mid-sized and large organizations are
finding they must control network traffic behavior to assure that their
strategic applications always get the resources they need to perform optimally.
Controlling network traffic requires limiting bandwidth to certain
applications, guaranteeing minimum bandwidth to others, and marking traffic
with high or low priorities. This exercise is called Network Traffic
Management.

ABSTRACT_BEGIN
  As the number and size of the Network increases, the deficiencies persist,
including network security problems. But there is no shortage of technologies
offered as universal remedy - EIGRP,BGP, OSPF, VoIP, IPv6, IPTV, MPLS, WiFi, to
name a few. There are multiple factors for the current situation. Now a day
during emergent and blossoming stages of network development is no longer
sufficient when the networks are mature and have become everyday tool for
social and business interactions. A new model of network is necessary to find
solutions for today's pressing problems, especially those related to network
security. In this paper out factors leading to current stagnation discusses
critical assumptions behind current networks, how many of them are no longer
valid and have become barriers for implementing real solutions. The paper
concludes by offering new directions for future needs and solving current
challenges.

ABSTRACT_BEGIN
  We analyze the multihop delay of ad hoc cognitive radio networks, where the
transmission delay of each hop consists of the propagation delay and the
waiting time for the availability of the communication channel (i.e., the
occurrence of a spectrum opportunity at this hop). Using theories and
techniques from continuum percolation and ergodicity, we establish the scaling
law of the minimum multihop delay with respect to the source-destination
distance in cognitive radio networks. When the propagation delay is negligible,
we show the starkly different scaling behavior of the minimum multihop delay in
instantaneously connected networks as compared to networks that are only
intermittently connected due to scarcity of spectrum opportunities.
Specifically, if the network is instantaneously connected, the minimum multihop
delay is asymptotically independent of the distance; if the network is only
intermittently connected, the minimum multihop delay scales linearly with the
distance. When the propagation delay is nonnegligible but small, we show that
although the scaling order is always linear, the scaling rate for an
instantaneously connected network can be orders of magnitude smaller than the
one for an intermittently connected network.

ABSTRACT_BEGIN
  Multi-channel wireless networks are increasingly being employed as
infrastructure networks, e.g. in metro areas. Nodes in these networks
frequently employ directional antennas to improve spatial throughput. In such
networks, given a source and destination, it is of interest to compute an
optimal path and channel assignment on every link in the path such that the
path bandwidth is the same as that of the link bandwidth and such a path
satisfies the constraint that no two consecutive links on the path are assigned
the same channel, referred to as "Channel Discontinuity Constraint" (CDC).
CDC-paths are also quite useful for TDMA system, where preferably every
consecutive links along a path are assigned different time slots.
  This paper contains several contributions. We first present an $O(N^{2})$
distributed algorithm for discovering the shortest CDC-path between given
source and destination. This improves the running time of the $O(N^{3})$
centralized algorithm of Ahuja et al. for finding the minimum-weight CDC-path.
Our second result is a generalized $t$-spanner for CDC-path; For any $\theta>0$
we show how to construct a sub-network containing only $O(\frac{N}{\theta})$
edges, such that that length of shortest CDC-paths between arbitrary sources
and destinations increases by only a factor of at most
$(1-2\sin{\tfrac{\theta}{2}})^{-2}$. We propose a novel algorithm to compute
the spanner in a distributed manner using only $O(n\log{n})$ messages. An
important conclusion of this scheme is in the case of directional antennas are
used. In this case, it is enough to consider only the two closest nodes in each
cone.

ABSTRACT_BEGIN
  The telephony over IP (ToIP) is becoming a new trend in technology widely
used nowadays in almost all business sectors. Its concepts rely on transiting
the telephone communications through the IP network. Today, this technology is
deployed increasingly what the cause of emergence of companies is offering this
service as Switzernet. For several highly demanded destinations, recently fake
vendors appeared in the market offering voice termination but providing only
false answer supervision. The answered signal is returned immediately and calls
are being charged without being connected. Different techniques are used to
keep the calling party on the line. One of these techniques is to play a record
of a ring back tone (while the call is already being charged). Another, more
sophisticated technique is to play a human voice randomly picked up from a set
of records containing contents similar to: hello, hello, I cannot hear you
Apart the fact that the fallaciously established calls are charged at rates of
real calls, such malicious routes seriously handicap the switching process. The
system does not detect a failure on signaling level and is unable to attempt
the call via backup routes, the call technically being already connected. Once
the call flow falls into such trap, the calls will continue being routed via
the fraudulent route until a manual intervention.

ABSTRACT_BEGIN
  Nodes of minimum connected dominating set (MCDS) form a virtual backbone in a
wireless adhoc network. In this paper, a modified approach is presented to
determine MCDS of an underlying graph of a Wireless Adhoc network. Simulation
results for a variety of graphs indicate that the approach is efficient in
determining the MCDS as compared to other existing techniques.

ABSTRACT_BEGIN
  A lot of work has been done on routing protocols for mobile ad hoc networks,
but still standardization of them requires some more issues less addressed by
the existing routing protocols. In this paper a new paradigm of maintaining
multiple connections in adhoc routing protocols has been highlighted which may
be crucial for efficient routing in mobile ad hoc networks. The problem of
multiple connections has been hardly worked on in adhoc networks. In this paper
the solution of route maintenance if nodes are maintaining multiple connections
has been proposed. This idea not only helps to solve the multiple connections
problem, but also take care of proper bandwidth distribution to different
connections as per different traffic types. Study has been incorporated on
existing AODV with changes. Simulation studies have been performed over packet
delivery ratio, throughput and message overheads. Results show that the
proposed solution for multiple connections is efficient and worth implementing
in existing as well as new protocols.

ABSTRACT_BEGIN
  Multi-hop random access networks have received much attention due to their
distributed nature which facilitates deploying many new applications over the
sensor and computer networks. Recently, utility maximization framework is
applied in order to optimize performance of such networks however delay is not
limited and proposed algorithms result in very large transmission delays. In
this paper, we will analyze delay in random access multi-hop networks and solve
the delay-constrained utility maximization problem. We define the network
utility as a combination of rate utility and energy cost functions and solve
the following two problems: 'optimal medium access control with link delay
constraint' and, 'optimal congestion and contention control with end-to-end
delay constraint'. The optimal tradeoff between delay, rate, and energy is
achieved for different values of delay constraint and the scaling factors
between rate and energy. Different distributed solutions will be proposed for
each problem and their performance will be compared in terms of convergence and
complexity.

ABSTRACT_BEGIN
  A new framework to perform routing at the Autonomous System level is proposed
in this paper. This mechanism, called Chain Routing, uses complete orders as
its main topological unit. Since complete orders are acyclic digraphs that
possess a known topology, it is possible to define an acyclic structure to
route packets between a group of Autonomous Systems. The adoption of complete
orders also allows easy identification and avoidance of persistent route
oscillations, eliminates the possibility of developing transient loops in
paths, and provides a structure that facilitates the implementation of traffic
engineering. Moreover, by combining Chain Routing with other mechanisms that
implement complete orders in time, we suggest that it is possible to design a
new routing protocol which could be more reliable and stable than BGP's current
implementation. Although Chain Routing will require an increase of the message
overhead and greater coordination between network administrators, the rewards
in stability and resilience should more than compensate for this effort.

ABSTRACT_BEGIN
  The main challenges of cognitive radio include spectrum sensing at the
physical (PHY) layer to detect the activity of primary users and spectrum
sharing at the medium access control (MAC) layer to coordinate access among
coexisting secondary users. In this paper, we consider a cognitive radio
network in which a primary user shares a channel with secondary users that
cannot distinguish the signals of the primary user from those of a secondary
user. We propose a class of distributed cognitive MAC protocols to achieve
efficient spectrum sharing among the secondary users while protecting the
primary user from potential interference by the secondary users. By using a MAC
protocol with one-slot memory, we can obtain high channel utilization by the
secondary users while limiting interference to the primary user at a low level.
The results of this paper suggest the possibility of utilizing MAC design in
cognitive radio networks to overcome limitations in spectrum sensing at the PHY
layer as well as to achieve spectrum sharing at the MAC layer.

ABSTRACT_BEGIN
  The connectivity of the Internet at the Autonomous System level is influenced
by the network operator policies implemented. These in turn impose a direction
to the announcement of address advertisements and, consequently, to the paths
that can be used to reach back such destinations. We propose to use directed
graphs to properly represent how destinations propagate through the Internet
and the number of arc-disjoint paths to quantify this network's path diversity.
Moreover, in order to understand the effects that policies have on the
connectivity of the Internet, numerical analyses of the resulting directed
graphs were conducted. Results demonstrate that, even after policies have been
applied, there is still path diversity which the Border Gateway Protocol cannot
currently exploit.

ABSTRACT_BEGIN
  The processing, computation and memory requirements posed by emerging mobile
broadband services require adaptive memory management and prefetching
techniques at the mobile terminals for satisfactory application performance and
sustained device battery lifetime. In this work we investigate a scenario where
tasks with varied computational requirements are fetched by a mobile device
from a central server over an error prone wireless link. We examine the buffer
dynamics at the mobile terminal and the central server under varying wireless
channel connectivity and device memory congestion states as variable sizes
tasks are executed on the terminal. Our goal is to minimize the latency
experienced by these tasks while judiciously utilizing the device buffering
capability. We use a dynamic programming framework to model the optimal
prefetching policy. We further propose a) a prefetching algorithm Fetch-or- Not
(FON), which uses quasi-static assumption on system state to make prefetching
decisions, and b) a prefetching policy RFON, which uses randomized
approximation to the optimal solution thus obviating the need for dynamic
online optimization and substantially reducing the computational complexity.
Through performance evaluation under slow and fast fading scenarios we show
that proposed algorithms come close to performance of the optimal scheme.

ABSTRACT_BEGIN
  Network intrusion detection systems play a critical role in protecting the
information infrastructure of an organization. Due to the sophistication and
complexity of techniques used for the analysis they are commonly based on
general-purpose workstations. Although cost-efficient, these general-purpose
systems are found to be inadequate as they are unable to perform efficiently at
high packet rates. The resulting packet loss degrades the system's overall
effectiveness, as the analyzing capability of the system is reduced. It has
been found that the performance of these sensors can be improved significantly
by filtering out unwanted packets. This paper presents the design of a
Programmable Ethernet Interface Card that is used to offload signature matching
from software and thereby improve the detection ratio and performance of the
system.

ABSTRACT_BEGIN
  We propose a network implementation with enhanced security at the physical
layer by means of time-hopping CDMA, supporting cryptographically secure
point-to-point and point-to-multipoint communication. In particular, we analyze
an active star topology optical network implementation capable of supporting
128 simultaneous users up to 20 km apart. The feasibility of the proposed
scheme is demonstrated through numerical simulation.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) can be a valuable decision-support tool for
farmers. This motivated our deployment of a WSN system to support rain-fed
agriculture in India. We defined promising use cases and resolved technical
challenges throughout a two-year deployment of our COMMON-Sense Net system,
which provided farmers with environment data. However, the direct use of this
technology in the field did not foster the expected participation of the
population. This made it difficult to develop the intended decision-support
system. Based on this experience, we take the following position in this paper:
currently, the deployment of WSN technology in developing regions is more
likely to be effective if it targets scientists and technical personnel as
users, rather than the farmers themselves. We base this claim on the lessons
learned from the COMMON-Sense system deployment and the results of an extensive
user experiment with agriculture scientists, which we describe in this paper.

ABSTRACT_BEGIN
  Vehicular Ad Hoc Networks (VANETs) are a peculiar subclass of mobile ad hoc
networks that raise a number of technical challenges, notably from the point of
view of their mobility models. In this paper, we provide a thorough analysis of
the connectivity of such networks by leveraging on well-known results of
percolation theory. By means of simulations, we study the influence of a number
of parameters, including vehicle density, proportion of equipped vehicles, and
radio communication range. We also study the influence of traffic lights and
roadside units. Our results provide insights on the behavior of connectivity.
We believe this paper to be a valuable framework to assess the feasibility and
performance of future applications relying on vehicular connectivity in urban
scenarios.

ABSTRACT_BEGIN
  Physical-layer Network Coding (PNC) makes use of the additive nature of the
electromagnetic (EM) waves to apply network coding arithmetic at the physical
layer. With PNC,the destructive effect of interference in wireless networks is
eliminated and the capacity of networks can be boosted significantly. This
paper addresses a key outstanding issue in PNC: synchronization among
transmitting nodes. We first investigate the impact of imperfect
synchronization (i.e., finite synchronization errors) in a 3-node network. It
is shown that with QPSK modulation, PNC still yields significantly higher
capacity than straightforward network coding when there are synchronization
errors. Significantly, this remains to be so even in the extreme case when
synchronization is not performed at all. Moving beyond a 3-node network, we
propose and investigate a synchronization scheme for PNC in a general chain
network. At last, numerical simulation verifies that PNC is robust to
synchronization errors. In particular, for the mutual information performance,
there is about 0.5dB loss without time synchronization and there is at most 2dB
loss without phase synchronization.

ABSTRACT_BEGIN
  Recent advances in Micro-Electro-Mechanical Systems (MEMS) technology,
integrated circuits, and wireless communication have allowed the realization of
Wireless Body Area Networks (WBANs). WBANs promise unobtrusive ambulatory
health monitoring for a long period of time and provide real-time updates of
the patient's status to the physician. They are widely used for ubiquitous
healthcare, entertainment, and military applications. This paper reviews the
key aspects of WBANs for numerous applications. We present a WBAN
infrastructure that provides solutions to on-demand, emergency, and normal
traffic. We further discuss in-body antenna design and low-power MAC protocol
for WBAN. In addition, we briefly outline some of the WBAN applications with
examples. Our discussion realizes a need for new power-efficient solutions
towards in-body and on-body sensor networks.

ABSTRACT_BEGIN
  In this paper, we analyse the joint impact of pathloss, shadowing and fast
fading on wireless networks. Taking into account the pathloss and the
shadowing, we first express the SINR distribution of a mobile located at a
given distance from its serving base-station (BS). The moments of this
distribution are easily computed, using the Fenton-Wilkinson method, and a
fluid model that considers the cellular network as a continuum of BS. Then
considering the joint impact of pathloss, shadowing and fast fading, we derive
an easily computable outage probability formula, for a mobile located at any
distance from its serving BS. We validate our approach by comparing all results
to Monte Carlo simulations performed in a traditional hexagonal network.
Indeed, we establish that the results given by the formula are close to the
ones given by Monte Carlo simulations. The proposed framework is a powerful
tool to study performances of cellular networks e.g. OFDMA systems (WiMAX,
LTE).

ABSTRACT_BEGIN
  Managing heterogeneous network systems is a difficult task because each of
these networks has its own curious management system. These networks usually
are constructed on independent management protocols which are not compatible
with each other. This results in the coexistence of many management systems
with different managing functions and services across enterprises.
Incompatibility of different management systems makes management of whole
system a very complex and often complicated job. Ideally, it is necessary to
implement centralized metalevel management across distributed heterogeneous
systems and their underlying supporting network systems where the information
flow and guidance is provided via a single console or single operating panels
which integrates all the management functions in spite of their individual
protocols and structures. This paper attempts to provide a novel network
management tool architecture which supports heterogeneous managements across
many different architectural platforms. Furthermore, an architectural approach
to integrate heterogeneous network is proposed. This architecture takes into
account both wireless fixed and mobile nodes.

ABSTRACT_BEGIN
  The Networks Mobility (NEMO) Protocol is a way of managing the mobility of an
entire network, and mobile internet protocol is the basic solution for Networks
Mobility. A hierarchical route optimization system for mobile network is
proposed to solve management of hierarchical route optimization problems. In
present paper, we study Hierarchical Route Optimization Scheme using Tree
Information Option (HROSTIO). The concept of optimization finding the extreme
of a function that maps candidate solution to scalar values of quality, is an
extremely general and useful idea. For solving this problem, we use a few
salient adaptations and we also extend HROSTIO perform routing between the
mobile networks.

ABSTRACT_BEGIN
  The Next Generation Wireless Networks (NGWN) will be heterogeneous in nature
where the different Radio Access Technologies (RATs) operate together .The
mobile terminals operating in this heterogeneous environment will have
different QoS requirements to be handled by the system. These QoS requirements
are determined by a set of QoS parameters. The radio resource management is one
of the key challenges in NGWN. Call admission control is one of the radio
resource management technique plays instrumental role in ensure the desired QoS
to the users working on different applications which have diversified QoS
requirements from the wireless networks . The call blocking probability is one
such QoS parameter for the wireless network. For better QoS it is desirable to
reduce the call blocking probability. In this customary scenario it is highly
desirable to obtain analytic Performance model. In this paper we propose a
higher order Markov chain based performance model for call admission control in
a heterogeneous wireless network environment. In the proposed algorithm we have
considered three classes of traffic having different QoS requirements and we
have considered the heterogeneous network environment which includes the RATs
that can effectively handle applications like voice calls, Web browsing and
file transfer applications which are with varied QoS parameters. The paper
presents the call blocking probabilities for all the three types of traffic
both for fixed and varied traffic scenario.

ABSTRACT_BEGIN
  The network communication scenario where one or more receivers request all
the information transmitted by different sources is considered. We introduce
distributed polynomial-time network codes in the presence of malicious nodes.
Our codes can achieve any point inside the rate region of multiple-source
multicast transmission scenarios both in the cases of coherent and non-coherent
network coding. For both cases the encoding and decoding algorithm runs in
poly(|E|)exp(s) time, where poly(|E|) is a polynomial function of the number of
edges |E| in the network and exp(s) is an exponential function of the number of
sources s. Our codes are fully distributed and different sources require no
knowledge of the data transmitted by their peers. Our codes are "end-to-end",
that is, all nodes apart from the sources and the receivers are oblivious to
the adversaries present in the network and simply implement random linear
network coding.

ABSTRACT_BEGIN
  As the Internet becomes increasingly heterogeneous, the issue of congestion
avoidance and control becomes ever more important. And the queue length,
end-to-end delays and link utilization is some of the important things in term
of congestion avoidance and control mechanisms. In this work we continue to
study the performances of the New-AIMD (Additive Increase Multiplicative
Decrease) mechanism as one of the core protocols for TCP congestion avoidance
and control algorithm, we want to evaluate the effect of using the AIMD
algorithm after developing it to find a new approach, as we called it the
New-AIMD algorithm to measure the Queue length, delay and bottleneck link
utilization, and use the NCTUns simulator to get the results after make the
modification for the mechanism. And we will use the Droptail mechanism as the
active queue management mechanism (AQM) in the bottleneck router. After
implementation of our new approach with different number of flows, we expect
the delay will less when we measure the delay dependent on the throughput for
all the system, and also we expect to get end-to-end delay less. And we will
measure the second type of delay a (queuing delay), as we shown in the figure 1
bellow. Also we will measure the bottleneck link utilization, and we expect to
get high utilization for bottleneck link with using this mechanism, and avoid
the collisions in the link.

ABSTRACT_BEGIN
  This paper reports our work on extending the Omnet INET Framework with a
directional radio model, putting a special emphasis on the implementation of
asymmetrical communications. We first analyze the original INET radio model,
focusing on its design and components. Then we discuss the modifications that
have been done to support directional communications. Our preliminary results
show that the new model is flexible enough to allow the user to provide any
antenna pattern shape, with only an additional reasonable computational cost.

ABSTRACT_BEGIN
  The technique of cooperative communication has recently gained momentum in
the research community; this technique utilizes the notion of relay, as an
intermediate node between the source and the destination, to enhance the
overall system performance. In this paper we ex-plored the benefits of adaptive
cooperation, in which the relay adapts its relaying process in response to
channel conditions and data priorities. We are particularly interested in
applying this concept to the cell border situation, in which two mobile nodes
acting as destinations com-municate with base stations (sources) through a
relay. The adaptive cooperation is proposed here since the transmission channel
conditions (Packet Error Rate for example) and data priori-ties are not the
same for both mobiles. We show that using the adaptive resource allocation
technique in combination with the cross layer design techniques, we can achieve
Real-Time data constraints with no additional overhead.

ABSTRACT_BEGIN
  Future Communication networks are tending towards a diverse wireless
networking world where the positioning information (PI) could be helpful in
different techniques like the dynamic resource allocation. On the other hand,
the PI could be widely used for cooperative techniques in the relay and/or
routing selection process. In this paper, we propose to use the PI in the
selection of the relays and then to apply an efficient double layer distributed
space time block code (DLSTBC) scheme between the different relays. Using the
amplify and forward (AF) technique, we show that the proposed code is very
efficient whatever the transmitted power is. Moreover, we show that the relay
selection process based on PI yields very powerful results when compared to the
random relay selection (RS) process

ABSTRACT_BEGIN
  In this paper we propose HYMAD, a Hybrid DTN-MANET routing protocol which
uses DTN between disjoint groups of nodes while using MANET routing within
these groups. HYMAD is fully decentralized and only makes use of topological
information exchanges between the nodes. We evaluate the scheme in simulation
by replaying real life traces which exhibit this highly dynamic connectivity.
The results show that HYMAD outperforms the multi-copy Spray-and-Wait DTN
routing protocol it extends, both in terms of delivery ratio and delay, for any
number of message copies. Our conclusion is that such a Hybrid DTN-MANET
approach offers a promising venue for the delivery of elastic data in mobile
ad-hoc networks as it retains the resilience of a pure DTN protocol while
significantly improving performance.

ABSTRACT_BEGIN
  We describe a new model for studying intermittently connected mobile
networks, based on Markovian random temporal graphs, that captures the
influence of message size, maximum tolerated delay and link stability on the
delivery ratio.

ABSTRACT_BEGIN
  Understanding transport capacity in intermittently connected mobile networks
(ICMN) is crucial since different applications have different interactivity and
bandwidth requirements. One practical issue is how to transform an
application's messages into packets suitable for transport over an ICMN. In
this paper, we propose a new Markovian model for random temporal graphs and
show, both analytically and by replaying a real life trace obtained in a
rollerblading tour, that the size of the messages sent over an ICMN has a
decisive impact on their delivery ratio. A given message could therefore be
broken down into smaller packets to increase reliability. However, we also show
that this gain in reliability only appears under tight constraints on the
maximum delay tolerated. Mobile application designers should therefore balance
message size against both application requirements and network topology
dynamics to improve performance.

ABSTRACT_BEGIN
  Withdrawn by arXiv administration. This article was plagiarised directly from
http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5061961 , which
appeared in the conference INFOCOM 2009.

ABSTRACT_BEGIN
  The growth in data traffic and the increased demand for quality of service
had generated a large demand for network systems to be more efficient. The
introduction of improved routing systems to meet the increasing demand and
varied protocols to accommodate various scales of challenges in network
efficiency had further complicated the operations. This means that a better
mode of intelligence has to be infused into networking for smoother operations
and better autonomic features. Cognitive networks are defined and analyzed in
this angle. They are identified to have the potential to deal with the future
user related quality and efficiency of service at optimized levels. The
cognitive elements of a system like perception, learning, planning, reasoning
and decision forming can enable the systems to be more aware of their
environment and offer better services. These approaches are expected to
transform the mode of operation of future networks.

ABSTRACT_BEGIN
  We address the difficult question of inferring plausible node mobility based
only on information from wireless contact traces. Working with mobility
information allows richer protocol simulations, particularly in dense networks,
but requires complex set-ups to measure, whereas contact information is easier
to measure but only allows for simplistic simulation models. In a contact trace
a lot of node movement information is irretrievably lost so the original
positions and velocities are in general out of reach. We propose a fast
heuristic algorithm, inspired by dynamic force-based graph drawing, capable of
inferring a plausible movement from any contact trace, and evaluate it on both
synthetic and real-life contact traces. Our results reveal that (i) the quality
of the inferred mobility is directly linked to the precision of the measured
contact trace, and (ii) the simple addition of appropriate anticipation forces
between nodes leads to an accurate inferred mobility.

ABSTRACT_BEGIN
  In this paper, we address the problem of distributing a large amount of bulk
data to a sparse vehicular network from roadside infostations, using efficient
vehicle-to-vehicle collaboration. Due to the highly dynamic nature of the
underlying vehicular network topology, we depart from architectures requiring
centralized coordination, reliable MAC scheduling, or global network state
knowledge, and instead adopt a distributed paradigm with simple protocols. In
other words, we investigate the problem of reliable dissemination from multiple
sources when each node in the network shares a limited amount of its resources
for cooperating with others. By using \emph{rateless} coding at the Road Side
Unit (RSU) and using vehicles as data carriers, we describe an efficient way to
achieve reliable dissemination to all nodes (even disconnected clusters in the
network). In the nutshell, we explore vehicles as mobile storage devices. We
then develop a method to keep the density of the rateless codes packets as a
function of distance from the RSU at the desired level set for the target
decoding distance. We investigate various tradeoffs involving buffer size,
maximum capacity, and the mobility parameter of the vehicles.

ABSTRACT_BEGIN
  We consider the problem of communicating information over a network secretly
and reliably in the presence of a hidden adversary who can eavesdrop and inject
malicious errors. We provide polynomial-time, rate-optimal distributed network
codes for this scenario, improving on the rates achievable in previous work.
Our main contribution shows that as long as the sum of the adversary's jamming
rate Zo and his eavesdropping rate Zi is less than the network capacity C,
(i.e., Zo+Zi<C), our codes can communicate (with vanishingly small error
probability) a single bit correctly and without leaking any information to the
adversary. We then use this to design codes that allow communication at the
optimal source rate of C-Zo-Zi, while keeping the communicated message secret
from the adversary. Interior nodes are oblivious to the presence of adversaries
and perform random linear network coding; only the source and destination need
to be tweaked. In proving our results we correct an error in prior work by a
subset of the authors in this work.

ABSTRACT_BEGIN
  The routing of packets are generally performed based on the destination
address and forward link channel available from the instantaneous Router
without sufficient cognizance of either the performance of the forward Router
or forward channel characteristics. The lack of awareness of forward channel
property can lead to packet loss or delayed delivery leading to
multipleretransmissions or routing to an underperforming pathway. This paper
describes an application of Cognitive Network to improve the network
performance by implementing a Hidden Markov Model (HMM) algorithm for learning
and predicting the performance of surrounding routers continuously while a
routing demand is initiated. The cognition segment/domain of every router can
gain knowledge about the quality of forward network. The information of the
current network conditions is shared between routers by the Forward Channel
Performance Index FCPI. This enables complete cognition of surroundings and
efficient delivery of messages in various paradigms of performance

ABSTRACT_BEGIN
  One of the challenges in the wireless sensor applications which are gaining
much attention is the real-time transmission of continuous data packets across
the network. Though advances in communication in sensor networks are providing
guaranteed quality data packet delivery they still have some drawbacks. One
such drawback is transmission of incessant data packets over high speed
networks. Here in this paper we have designed a concentric sensor network
having buffer just not at the sink but also in selected intermediate nodes to
minimize the packet loss caused due to congestion. This approach results in
haggle congestion and less packet loss in the designed network.

ABSTRACT_BEGIN
  Local multipoint distribution system (LMDS) uses cellular-like network
architecture of microwave radios placed at the client's location and at the
company's base station to deliver fixed services, mainly telephony, video and
Internet access. The use of time-division multiple access (TDMA) and FDMA
(frequency DMA) technology allows multiple customers within a 3-5 mile coverage
radius to share the same radio channel. Customers can receive data rates
between 64kbps to 155Mbps. LMDS was conceived as a broadband, fixed wireless,
point-to-multipoint technology for utilization in the last mile. Throughput
capacity and reliable distance of the link depends on common radio link
constraints and the modulation method used - either phase-shift keying or
amplitude modulation. In general deployment links of up to 5 miles (8 km) from
the base station are possible, but distance is typically limited to about 1.5
miles due to rain fading attenuation constraints. Point-to-point systems are
also capable of using the LMDS frequencies and can reach slightly farther
distances due to increased antenna gain.LMDS uses a scalable architecture
combined with industry standards to ensure service can be expanded as customer
demand increases.

ABSTRACT_BEGIN
  The Global Positioning System (GPS) is a U.S. space-based radionavigation
system that provides reliable positioning, navigation, and timing services to
civilian users on a continuous worldwide basis -- freely available to all. GPS
provides specially coded satellite signals that can be processed in a GPS
receiver, enabling the receiver to compute position, velocity and time.
Basically GPS works by using four GPS satellite signals to compute positions in
three dimensions (and the time offset) in the receiver clock. GPS provides
accurate location and time information for an unlimited number of people in all
weather, day and night, anywhere in the world. Anyone who needs to keep track
of where he or she is, to find his or her way to a specified location, or know
what direction and how fast he or she is going can utilize the benefits of the
global positioning system. Everyday activities such as banking, mobile phone
operations, and even the control of power grids, are facilitated by the
accurate timing provided by GPS.

ABSTRACT_BEGIN
  Broadband communications consists of the technologies and equipment required
to deliver packet-based digital voice, video, and data services to end users.
Broadband affords end users high-speed, always-on access to the Internet while
affording service providers the ability to offer value-added services to
increase revenues. Due to the growth of the Internet, there has been tremendous
buildout of high-speed, inter-city communications links that connect population
centers and Internet service providers (ISPs) points of presence (PoPs) around
the world. This build out of the backbone infrastructure or core network has
occurred primarily via optical transport technology. Broadband access
technologies are being deployed to address the bandwidth bottleneck for the
"last mile," the connection of homes and small businesses to this
infrastructure. One important aspect of broadband access to the home is that it
allows people to telecommute effectively by providing a similar environment as
when they are physically present in their office: simultaneous telephone and
computer access, high-speed Internet and intranet access for e-mail, file
sharing, and access to corporate servers.

ABSTRACT_BEGIN
  This is an Internet era. Most of the organizations try to establish their
development centers and branch offices across the World. Employees working from
their homes are also becoming very popular and organizations benefit
financially by utilizing less office space, and reducing total expenses
incurred by having office workers on site. To meet such requirements
organizations develop a need to communicate with these offices over highly
secure, confidential and reliable connections regardless of the location of the
office. Here the VPN plays a vital role in establishing a distributed business
model.

ABSTRACT_BEGIN
  This paper deals with the accomplishment of total area coverage of an
arbitrary region using sensors with a finite sensing radius of rs. For a given
region, we aim to obtain a deterministic placement of sensors which, apart from
ensuring that the entire region comes under the purview of at least a single
sensor, minimises the number of sensors utilised. We begin by considering
regions devoid of obstacles and thus having every location amenable for
placement. Herein, we formalise the popular notion that sensors at the centres
of the hexagons of a hexagonal tessellation provide the most optimal placement.
We then move on to regions which may comprise obstacles of arbitrary size at
arbitrary locations. We recognise two distinct classes of obstacles, namely
transparent and opaque obstacles, which are distinguished by their ability (or
the lack of it) to permit sensing radiation through them. In the real world,
transparent obstacles model lakes, ponds and swamps, while the opaque ones
stand for, inter alia, hills, trees and walls.We propose a polynomial-time
algorithm for achieving optimal placement in the aforesaid scenarios and we
prove its convergence.

ABSTRACT_BEGIN
  An important aspect of the Future Internet is the efficient utilization of
(wireless) network resources. In order for the - demanding in terms of QoS -
Future Internet services to be provided, the current trend is evolving towards
an "integrated" wireless network access model that enables users to enjoy
mobility, seamless access and high quality of service in an all-IP network on
an "Anytime, Anywhere" basis. The term "integrated" is used to denote that the
Future Internet wireless "last mile" is expected to comprise multiple
heterogeneous geographically coexisting wireless networks, each having
different capacity and coverage radius. The efficient management of the
wireless access network resources is crucial due to their scarcity that renders
wireless access a potential bottleneck for the provision of high quality
services. In this paper we propose an auction mechanism for allocating the
bandwidth of such a network so that efficiency is attained, i.e. social welfare
is maximized. In particular, we propose an incentive-compatible, efficient
auction-based mechanism of low computational complexity. We define a repeated
game to address user utilities and incentives issues. Subsequently, we extend
this mechanism so that it can also accommodate multicast sessions. We also
analyze the computational complexity and message overhead of the proposed
mechanism. We then show how user bids can be replaced from weights generated by
the network and transform the auction to a cooperative mechanism capable of
prioritizing certain classes of services and emulating DiffServ and time-of-day
pricing schemes. The theoretical analysis is complemented by simulations that
assess the proposed mechanisms properties and performance. We finally provide
some concluding remarks and directions for future research.

ABSTRACT_BEGIN
  Traditionally, the performance of ocr algorithms and systems is based on the
recognition of isolated characters. When a system classifies an individual
character, its output is typically a character label or a reject marker that
corresponds to an unrecognized character. By comparing output labels with the
correct labels, the number of correct recognition, substitution errors
misrecognized characters, and rejects unrecognized characters are determined.
Nowadays, although recognition of printed isolated characters is performed with
high accuracy, recognition of handwritten characters still remains an open
problem in the research arena. The ability to identify machine printed
characters in an automated or a semi automated manner has obvious applications
in numerous fields. Since creating an algorithm with a one hundred percent
correct recognition rate is quite probably impossible in our world of noise and
different font styles, it is important to design character recognition
algorithms with these failures in mind so that when mistakes are inevitably
made, they will at least be understandable and predictable to the person
working with the

ABSTRACT_BEGIN
  In this paper a scheme for handoff and connectivity, based on wireless sensor
nodetechniques is proposed. Scenes are created in Qualnet and simulated for a
simple case. Results are discussed.

ABSTRACT_BEGIN
  Bandwidth request-grant mechanisms are used in 802.16 networks to manage the
uplink bandwidth needs of subscriber stations (SSs). Requests may be sent by
SSs to the base station (BS) by means of several mechanisms defined in the
standard. Based on the incoming requests, the BS (which handles most of the
bandwidth scheduling in the system) schedules the transmission of uplink
traffic, by assigning transmission opportunities to the SSs in an
implementation-dependent manner. In this paper we present a study of some
bandwidth allocation issues, arising from the management of the perception of
subscriber stations' bandwidth needs at the base station. We illustrate how the
bandwidth perception varies depending on the policy used to handle requests and
grants. By means of ns-2 simulations, we evaluate the potential impact of such
policies on the system's aggregate throughput when the traffic is composed of
Best-Effort TCP flows.

ABSTRACT_BEGIN
  Opportunistic routing is a multi-hop routing scheme which allows for
selection of the best immediately available relay. In blind opportunistic
routing protocols, where transmitters blindly broadcast without knowledge of
the surrounding nodes, two fundamental design parameters are the node
transmission probability and the transmission spectral efficiency. In this
paper these parameters are selected to maximize end-to-end performance,
characterized by the product of transmitter density, hop distance and rate. Due
to the intractability of the problem as stated, an approximation function is
examined which proves reasonably accurate. Our results show how the above
design parameters should be selected based on inherent system parameters such
as the path loss exponent and the noise level.

ABSTRACT_BEGIN
  This paper gives an overview of radio interfaces devoted for high data rate
Wireless Sensor Networks. Four aerospace applications of WSN are presented to
underline the importance of achieving high data rate. Then, two modulation
schemes by which High Data Rate can be achieved are compared : Multi carrier
approaches, represented by the popular Orthogonal Frequency Division
Multiplexing (OFDM) and Single carrier methods, represented by Single Carrier
Frequency division Equalization and its application for multiple access Single
Carrier Frequency division multiple Access (SC-FDMA). SC-FDMA, with a very low
Peak Average Power Ratio (PAPR), is as strong alternative to the OFDM scheme
for highly power constraint application. The Chosen radio interface will be,
finally, tested by a model based design approach based on Simulink and FPGA
realization. SC-FDMA, with a very low Peak Average Power Ratio (PAPR), is as
strong alternative to the OFDM scheme for highly power constraint application.
The Chosen radio interface will be, finally, tested by a model based design
approach based on Simulink and FPGA realization.

ABSTRACT_BEGIN
  A wireless sensor network consists of light-weight, low power, small size
sensor nodes. Routing in wireless sensor networks is a demanding task. This
demand has led to a number of routing protocols which efficiently utilize the
limited resources available at the sensor nodes. Most of these protocols are
either based on single hop routing or multi hop routing and typically find the
minimum energy path without addressing other issues such as time delay in
delivering a packet, load balancing, and redundancy of data. Response time is
very critical in environment monitoring sensor networks where typically the
sensors are stationary and transmit data to a base station or a sink node. In
this paper a faster load balancing routing protocol based on location with a
hybrid approach is proposed.

ABSTRACT_BEGIN
  Two-ray ground reflection model has been widely used as the propagation model
to investigate the performance of an ad hoc network. But two-ray model is too
simple to represent a real world network. A more realistic model namely
shadowing propagation model has been used in this investigation. Under
shadowing propagation model, a mobile node may receive a packet at a signal
level that is below a required threshold level. This low signal level affects
the routing protocol as well as the medium access control protocol of a
network. An analytical model has been presented in this paper to investigate
the shadowing effects on the network performance. The analytical model has been
verified via simulation results. Simulation results show that the performance
of a network becomes very poor if shadowing propagation model is used in
compare to the simple two-ray model. Two solutions have also been proposed in
this paper to overcome the effects of shadowing. One solution is a physical
layer solution and the other one is a Medium Access Control (MAC) layer
solution. Simulation results show that these two solutions reduce the shadowing
effect and improve network performance.

ABSTRACT_BEGIN
  Network forensics deals with the capture, recording and analysis of network
events in order to discover evidential information about the source of security
attacks in a court of law. This paper discusses the different tools and
techniques available to conduct network forensics. Some of the tools discussed
include: eMailTrackerPro to identify the physical location of an email sender;
Web Historian to find the duration of each visit and the files uploaded and
downloaded from the visited website; packet sniffers like Etherea to capture
and analyze the data exchanged among the different computers in the network.
The second half of the paper presents a survey of different IP traceback
techniques like packet marking that help a forensic investigator to identify
the true sources of the attacking IP packets. We also discuss the use of
Honeypots and Honeynets that gather intelligence about the enemy and the tools
and tactics of network intruders.

ABSTRACT_BEGIN
  In multi-hop wireless systems, the need for cooperation among nodes to relay
each other's packets exposes them to a wide range of security attacks. A
particularly devastating attack is the wormhole attack, where a malicious node
records control traffic at one location and tunnels it to another compromised
node, possibly far away, which replays it locally. Routing security in ad hoc
networks is often equated with strong and feasible node authentication and
lightweight cryptography. Unfortunately, the wormhole attack can hardly be
defeated by crypto graphical measures, as wormhole attackers do not create
separate packets. They simply replay packets already existing on the network,
which pass the cryptographic checks. Existing works on wormhole detection have
often focused on detection using specialized hardware, such as directional
antennas, etc. In this paper, we present a cluster based counter-measure for
the wormhole attack, that alleviates these drawbacks and efficiently mitigates
the wormhole attack in MANET. Simulation results on MATLab exhibit the
effectiveness of the proposed algorithm in detecting wormhole attacks.

ABSTRACT_BEGIN
  As the explosive growth of the ISM band usage continues, there are many
scenarios where different systems operate in the same place at the same time.
One of growing concerns is the coexistence of heterogeneous wireless network
systems. For the successful deployment of mission-critical systems such as
wireless sensor networks, it is required to provide a solution for the
coexistence. In this paper, we propose a new scheme using inter packet delay
for the coexistence of IEEE 802.15.4 LRWPAN and IEEE 802.11b WLAN. To evaluate
the effectiveness of the proposed scheme, measurement and simulation study are
conducted using Qualnet 4.5 simulation software. The simulation results show
that the proposed scheme is effective in performance improvement for
coexistence network of IEEE 802.15.4 for various topologies.

ABSTRACT_BEGIN
  Many papers have been proposed in order to increase the wireless sensor
networks performance; This kind of network has limited resources, where the
energy in each sensor came from a small battery that sometime is hard to be
replaced or recharged. Transmission energy is the most concern part where the
higher energy consumption takes place. Clustered hierarchy has been proposed in
many papers; in most cases, it provides the network with better performance
than other protocols. In our paper, first we discuss some of techniques,
relates to this protocol, that have been proposed for energy efficiency; some
of them were proposed to provide the network with more security level. Our
proposal then suggests some modifications to some of these techniques to
provide the network with more energy saving that should lead to high
performance; also we apply our technique on an existing one that proposed to
increase the security level of cluster sensor networks.

ABSTRACT_BEGIN
  Wireless networks are a common place nowadays and almost all of the modern
devices support wireless communication in some form. These networks differ from
more traditional computing systems due to the ad-hoc and spontaneous nature of
interactions among devices. These systems are prone to security risks, such as
eavesdropping and require different techniques as compared to traditional
security mechanisms. Recently, secure device pairing in wireless environments
has got substantial attention from many researchers. As a result, a significant
set of techniques and protocols have been proposed to deal with this issue.
Some of these techniques consider devices equipped with infrared, laser,
ultrasound transceivers or 802.11 network interface cards; while others require
embedded accelerometers, cameras and/or LEDs, displays, microphones and/or
speakers. However, many of the proposed techniques or protocols have not been
implemented at all; while others are implemented and evaluated in a stand-alone
manner without being compared with other related work [1]. We believe that it
is because of the lack of specialized tools that provide a common platform to
test the pairing methods. As a consequence, we designed such a tool. In this
paper, we are presenting design and development of the Pairing Simulator (PSim)
that can be used to perform the analysis of device pairing methods.

ABSTRACT_BEGIN
  Mobile ip (mip) is an internet protocol that allows mobile nodes to have
continuous network connectivity to the internet without changing their ip
addresses while moving to other networks. The packets sent from correspondent
node (cn) to a mobile node (mn) go first through the mobile node's home agent
(ha), then the ha tunnels them to the mn's foreign network. One of the main
problems in the original mip is the triangle routing problem. Triangle routing
problem appears when the indirect path between cn and mn through the ha is
longer than the direct path. This paper proposes a new technique to improve the
performance of the original mip during the handoff. The proposed technique
reduces the delay, the packet loss and the registration time for all the
packets transferred between the cn and the mn. In this technique, tunneling
occurs at two levels above the ha in a hierarchical network. To show the
effectiveness of the proposed technique, it is compared with the original mip
and another technique for solving the same problem in which tunneling occurs at
one level above the ha. Simulation results presented in this paper are based on
the ns2 mobility software on linux platform. The simulations results show that
our proposed technique achieves better performance than the others, considering
the packet delay, the packet losses during handoffs and the registration time,
in different scenarios for the location of the mn with respect to the ha and
fas.

ABSTRACT_BEGIN
  This article has been withdrawn by arXiv admins because it contains
plagiarized content from International Conference on Computer Networks and
Security (ICCNS 2008, September 27-28, 2008): "Securing AODV for MANETs using
Message Digest with Secret Key", by Sunil J. Soni and Prashant B. Swadas.

ABSTRACT_BEGIN
  This paper presents a set of exploits an adversary can use to continuously
spy on most BitTorrent users of the Internet from a single machine and for a
long period of time. Using these exploits for a period of 103 days, we
collected 148 million IPs downloading 2 billion copies of contents. We identify
the IP address of the content providers for 70% of the BitTorrent contents we
spied on. We show that a few content providers inject most contents into
BitTorrent and that those content providers are located in foreign data
centers. We also show that an adversary can compromise the privacy of any peer
in BitTorrent and identify the big downloaders that we define as the peers who
subscribe to a large number of contents. This infringement on users' privacy
poses a significant impediment to the legal adoption of BitTorrent.

ABSTRACT_BEGIN
  Heterogeneous Networks is the integration of all existing networks under a
single environment with an understanding between the functional operations and
also includes the ability to make use of multiple broadband transport
technologies and to support generalized mobility. It is a challenging feature
for Heterogeneous networks to integrate several IP-based access technologies in
a seamless way. The focus of this paper is on the requirements of a mobility
management scheme for multimedia real-time communication services - Mobile
Video Conferencing. Nowadays, the range of available wireless access network
technologies includes cellular or wide-area wireless systems, such as cellular
networks (GSM/GPRS/UMTS) or Wi-Max, local area Network or personal area
wireless systems, comprising for example, WLAN (802.11 a/b/g) and Bluetooth. As
the mobile video conferencing is considered, the more advanced mobile terminals
are capable of having more than one interface active at the same time. In
addition, the heterogeneity of access technologies and also the seamless flow
of information will increase in the future, making the seamless integration of
the access network a key challenge for mobility management in a heterogeneous
network environment. Services must be provided to the user regardless of the
particular access technology and also the type of service provider or the
network used.

ABSTRACT_BEGIN
  The applications of Wireless Sensor Networks (WSN) contain a wide variety of
scenarios. In most of them, the network is composed of a significant number of
nodes deployed in an extensive area in which not all nodes are directly
connected. Then, the data exchange is supported by multihop communications.
Routing protocols are in charge of discovering and maintaining the routes in
the network. However, the correctness of a particular routing protocol mainly
depends on the capabilities of the nodes and on the application requirements.
This paper presents a dynamic discover routing method for communication between
sensor nodes and a base station in WSN. This method tolerates failures of
arbitrary individual nodes in the network (node failure) or a small part of the
network (area failure). Each node in the network does only local routing
preservation, needs to record only its neighbor nodes' information, and incurs
no extra routing overhead during failure free periods. It dynamically discovers
new routes when an intermediate node or a small part of the network in the path
from a sensor node to a base station fails. In our planned method, every node
decides its path based only on local information, such as its parent node and
neighbor nodes' routing information. So, it is possible to form a loop in the
routing path. We believe that the loop problem in sensor network routing is not
as serious as that in the Internet routing or traditional mobile ad-hoc
routing. We are trying to find all possible loops and eliminate the loops as
far as possible in WSN.

ABSTRACT_BEGIN
  Due to mobility in Ad-Hoc network the topology of the network may change
randomly, rapidly and unexpectedly, because of these aspects, the routes in the
network often disappear and new to arise. To avoid frequent route discovery and
route failure EAOMDV was proposed based on existing routing protocol AOMDV. The
EAOMDV (Enhanced Ad-Hoc on Demand Multipath Distance Vector) Routing protocol
was proposed to solve the "route failure" problem in AOMDV. EAOMDV protocol
reduces the route failure problem by preemptively predicting the link failure
by the signal power received by the receiver (pr). This proposed protocol
controls overhead, increases throughput and reduces the delay. The EAOMDV
protocol was implemented on NS-2 and evaluation results show that the EAOMDV
outperformed AOMDV.

ABSTRACT_BEGIN
  In this paper, we have given an idea of area specification and its
corresponding sensing of nodes in a dynamic network. We have applied the
concept of Monte Carlo methods in this respect. We have cited certain
statistical as well as artificial intelligence based techniques for realizing
the position of a node. We have also applied curve fitting concept for node
detection and relative verification.

ABSTRACT_BEGIN
  In Wireless Mesh Networks (WMN), a channel assignment has to balance the
objectives of maintaining connectivity and increasing the aggregate bandwidth.
The main aim of the channel assignment algorithm is to assign the channels to
the network interfaces, from the given expected load on each virtual link. From
the existing work done so far, we can examine that there is no combined
solution of multi-channel assignment with routing and congestion control. In
this paper, we propose a congestion control routing protocol along with
multi-channel assignment. We use a traffic aware metric in this protocol in
order to provide quality of service. The proposed protocol can improve the
throughput and channel utilization to very high extent because it provides
solution for multi-channel assignment and congestion control. The proposed
algorithm assigns the channels in a way that, congestion is avoided and
co-channel interference levels among links with same channel are reduced. By
our simulation results in NS2, we show that the proposed protocol attains high
throughput and channel utilization along with reduced latency.

ABSTRACT_BEGIN
  This paper intends to look deeper into finding an ideal mobile broadband
solution. Special stress has been put in the South Asian region through some
comparative analysis. Proving their competency in numerous aspects, WiMAX and
LTE already have already made a strong position in telecommunication industry.
Both WiMAX and LTE are 4G technologies designed to move data rather than voice
having IP networks based on OFDM technology. So, they aren't like typical
technological rivals as of GSM and CDMA. But still a gesture of hostility seems
to outburst long before the stable commercial launch of LTE. In this paper
various aspects of WiMAX and LTE for deployment have been analyzed. Again, we
tried to make every possible consideration with respect to south Asia i.e. how
mass people of this region may be benefited. As a result, it might be regarded
as a good source in case of making major BWA deployment decisions in this
region. Besides these, it also opens the path for further research and in depth
thinking in this issue.

ABSTRACT_BEGIN
  Transceiver impairments, including phase noise, residual frequency offset,
and imperfect channel estimation, significantly affect the performance of
Multiple-Input Multiple-Output (MIMO) system. However, these impairments are
not well addressed when analyzing the throughput performance of MIMO Ad Hoc
networks. In this paper, we present an analytical framework to evaluate the
throughput of MIMO OFDM system under the impairments of phase noise, residual
frequency offset, and imperfect channel estimation. Using this framework, we
evaluate the Maximum Sum Throughput (MST) in Ad Hoc networks by optimizing the
power and modulation schemes of each user. Simulations are conducted to
demonstrate not only the improvement in the MST from using multiple antennas,
but also the loss in the MST due to the transceiver impairments. The proposed
analytical framework is further applied for the distributed implementation of
MST in Ad Hoc networks, where the loss caused by impairments is also evaluated.

ABSTRACT_BEGIN
  Over the last decade, internet has seen an exponential increase in its
growth.With more and more people using it, efficient data delivery over the
internet has become a key issue. Peer-to-peer (P2P)/seed sharing based networks
have several desirable features for content distribution, such as low costs,
scalability, and fault tolerance. While the invention of each of such
specialized systems has improved the user experience, some fundamental
shortcomings of these systems have often been neglected. These shortcomings of
content distribution systems have become severe bottlenecks in scalability of
the internet.In order to combine the desired features of classical Content
Distribution Networks (CDNs) and P2P/seed sharing based networks, we propose a
hybrid CDN structure with a P2P/seed sharing based streaming protocol in the
access network . In this work, we focus on the problem of data redundancy (at
each node) and show how severely it impacts the network economics and the
experience of end-user and hence leads to low traffic load and redundancy

ABSTRACT_BEGIN
  Adding location to the available information enables a new category of
applications. With the constrained battery on cell phones, energy-efficient
localization becomes an important challenge. In this paper we introduce a
low-energy calibration-free localization scheme based on the available internal
sensors in many of today's phones. We start by energy profiling the different
sensors that can be used for localization. Based on that, we propose GAC: a
hybrid GPS/accelerometer/compass scheme that depends mainly on using the
low-energy accelerometer and compass sensors and uses the GPS infrequently for
synchronization. We implemented our system on Android-enabled cell phones and
evaluated it in both highways and intra-city driving environments. Our results
show that the proposed hybrid scheme has an exponential saving in energy, with
a linear loss in accuracy compared to the GPS accuracy. We also evaluate the
effect of the different parameters on the energy-accuracy tradeoff.

ABSTRACT_BEGIN
  Context-aware applications have been gaining huge interest in the last few
years. With cell phones becoming ubiquitous computing devices, cell phone
localization has become an important research problem. In this paper, we
present CellSense, a prob- abilistic RSSI-based fingerprinting location
determina- tion system for GSM phones.We discuss the challenges of implementing
a probabilistic fingerprinting local- ization technique in GSM networks and
present the details of the CellSense system and how it addresses the
challenges. To evaluate our proposed system, we implemented CellSense on
Android-based phones. Re- sults for two different testbeds, representing urban
and rural environments, show that CellSense provides at least 23.8% enhancement
in accuracy in rural areas and at least 86.4% in urban areas compared to other
RSSI-based GSMlocalization systems. This comes with a minimal increase in
computational requirements. We also evaluate the effect of changing the
different system parameters on the accuracy-complexity tradeoff.

ABSTRACT_BEGIN
  Composing Web services is a convenient means of dealing with complex
requests. However, the number of Web services on the Internet is increasing.
This explains the growing interest in composing Web services automatically.
Nevertheless, the Web services' semantics is necessary for any dynamic
composition process. In this article, we present an MDA approach to develop and
compose SAWSDL semantic Web services. To model Web services, we use a UML
profile which is independent of the description standards. The SAWSDL interface
files are generated by using transformation rules. To model the behavior of a
composite Web service and generate its executable BPEL file, we use the BPMN
notation in a platform of modeling and implementing business process. The main
contribution of this work is the easy and extensible solution to a model-driven
development of the semantic atomic and composite Web services.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) are commonly used in various ubiquitous and
pervasive applications. Due to limited power resources, the optimal dynamic
base station (BS) replacement could be Prolong the sensor network lifetime. In
this paper we'll present a dynamic optimum method for base station replacement
so that can save energy in sensors and increases network lifetime. Because
positioning problem is a NPhard problem [1], therefore we'll use genetic
algorithm to solve positioning problem. We've considered energy and distance
parameters for finding BS optimized position. In our represented algorithm base
station position is fixed just during each round and its positioning is done at
the start of next round then it'll be placed in optimized position. Evaluating
our proposed algorithm, we'll execute DBSR algorithm on LEACH & HEED Protocols.

ABSTRACT_BEGIN
  This paper has been withdrawn by the author due to a crucial sign error in
equation 1

ABSTRACT_BEGIN
  This paper investigates the performance of WPAN based on various topological
scenarios like: cluster, star and ring. The comparative results have been
reported for the performance metrics like: Throughput, Traffic sent, Traffic
received and Packets dropped. Cluster topology is best in comparison with star
and ring topologies as it has been shown that the throughput in case of cluster
topology (79.887 kbits / sec) as compared to star (31.815 kbits / sec) and ring
(1.179 kbits / sec).

ABSTRACT_BEGIN
  The Call admission control (CAC) is one of the Radio Resource Management
(RRM) techniques that plays influential role in ensuring the desired Quality of
Service (QoS) to the users and applications in next generation networks. This
paper proposes a fuzzy neural approach for making the call admission control
decision in multi class traffic based Next Generation Wireless Networks (NGWN).
The proposed Fuzzy Neural call admission control (FNCAC) scheme is an
integrated CAC module that combines the linguistic control capabilities of the
fuzzy logic controller and the learning capabilities of the neural networks.
The model is based on recurrent radial basis function networks which have
better learning and adaptability that can be used to develop intelligent system
to handle the incoming traffic in an heterogeneous network environment. The
simulation results are optimistic and indicates that the proposed FNCAC
algorithm performs better than the other two methods and the call blocking
probability is minimal when compared to other two methods.

ABSTRACT_BEGIN
  The seamless integration of low-power, miniaturised, invasive/non-invasive
lightweight sensor nodes have contributed to the development of a proactive and
unobtrusive Wireless Body Area Network (WBAN). A WBAN provides long-term health
monitoring of a patient without any constraint on his/her normal dailylife
activities. This monitoring requires low-power operation of
invasive/non-invasive sensor nodes. In other words, a power-efficient Medium
Access Control (MAC) protocol is required to satisfy the stringent WBAN
requirements including low-power consumption. In this paper, we first outline
the WBAN requirements that are important for the design of a low-power MAC
protocol. Then we study low-power MAC protocols proposed/investigated for WBAN
with emphasis on their strengths and weaknesses. We also review different
power-efficient mechanisms for WBAN. In addition, useful suggestions are given
to help the MAC designers to develop a low-power MAC protocol that will satisfy
the stringent WBAN requirements.

ABSTRACT_BEGIN
  This paper has been withdrawn by the author due to a crucial sign error in
equation 1

ABSTRACT_BEGIN
  Multiple-Input Multiple-Output (MIMO) based Medium Access Control (MAC)
protocols have received a good deal of attention as researchers look to enhance
overall performance of Ad Hoc networks by leveraging multi antenna enabled
nodes. To date such MAC protocols have been evaluated through comparative
simulation based studies that report on the number of concurrent links the
protocol can support. However, a bound on the maximum number of concurrent
links (MNCL) that a MIMO based MAC protocol should strive to achieve has
hitherto been unavailable. In this paper we present a theoretical formulation
for calculating the bound on the MNCL in a Mobile Ad Hoc Network (MANET) where
the nodes have multiple antenna capability, while guaranteeing a minimum
Quality of Service (QoS). In an attempt to make our findings as practical and
realistic as possible, the study incorporates models for the following PHY
layer and channel dependent elements: (a) path loss and fast fading effects, in
order to accurately model adjacent link interference; (b) a Minimum Mean
Squared Error (MMSE) based detector in the receiver which provides a balance
between completely nulling of neighboring interference and hardware complexity.
In calculating the bound on the MNCL our work also delivers the optimal power
control solution for the network as well as the optimal link selection. The
results are readily applicable to MIMO systems using Receive Diversity, Space
Time Block Coding (STBC), and Transmit Beamforming and show that with a 4
element antenna system, as much as 3x improvement in the total number of
concurrent links can be achieved relative to a SISO based network. The results
also show diminishing improvement as the number of antennas is increased beyond
4, and the maximum allowable transmit power is increased beyond 10 dBm (for the
simulated parameters).

ABSTRACT_BEGIN
  Many interactions between network users rely on trust, which is becoming
particularly important given the security breaches in the Internet today. These
problems are further exacerbated by the dynamics in wireless mobile networks.
In this paper we address the issue of trust advisory and establishment in
mobile networks, with application to ad hoc networks, including DTNs. We
utilize encounters in mobile societies in novel ways, noticing that mobility
provides opportunities to build proximity, location and similarity based trust.
Four new trust advisor filters are introduced - including encounter frequency,
duration, behavior vectors and behavior matrices - and evaluated over an
extensive set of real-world traces collected from a major university. Two sets
of statistical analyses are performed; the first examines the underlying
encounter relationships in mobile societies, and the second evaluates DTN
routing in mobile peer-to-peer networks using trust and selfishness models. We
find that for the analyzed trace, trust filters are stable in terms of growth
with time (3 filters have close to 90% overlap of users over a period of 9
weeks) and the results produced by different filters are noticeably different.
In our analysis for trust and selfishness model, our trust filters largely undo
the effect of selfishness on the unreachability in a network. Thus improving
the connectivity in a network with selfish nodes.
  We hope that our initial promising results open the door for further research
on proximity-based trust.

ABSTRACT_BEGIN
  School and College campuses face a perceived threat of violent crimes and
require a realistic plan against unpredictable emergencies and disasters.
Existing emergency systems (e.g., 911, campus-wide alerts) are quite useful,
but provide delayed response (often tens of minutes) and do not utilize
proximity or locality. There is a need to augment such systems with
proximity-based systems for more immediate response to attempt to prevent and
deter crime. In this paper we propose SHIELD, an on-campus emergency rescue and
alert management service. It is a fully distributed infrastructure-less
platform based on proximity-enabled trust and cooperation. It relies on
localized responses, sent using Bluetooth and/or WiFi on the fly to achieve
minimal response time and maximal availability thereby augmenting the
traditional notion of emergency services. Analysis of campus crime statistics
and WLAN traces surprisingly show a strong positive correlation (over 55%)
between on-campus crime statistics and spatio-temporal density distribution of
on-campus mobile users. This result provides a motivation to develop such
platform and points to the promise in reducing crime incidences. We also show
an implementation of a prototype application to be used in such scenarios.

ABSTRACT_BEGIN
  Study on human mobility is gaining increasing attention from the research
community with its multiple applications to use in mobile networks,
particularly for the purpose of message delivery in the Delay Tolerant
Networks. To better understand the potential of mobile nodes as message relays,
our study investigates the encounter pattern of mobile devices. Specifically,
we examine the extensive network traces that reflect mobility of communication
devices. We analyze the periodicity in encounter pattern by using power
spectral analysis. Strong periodicity was observed among rarely encountering
mobile nodes while the periodicity was weaker among frequently encountering
nodes. Further, we present a method to search regularly encountering pairs and
discuss the findings. To our knowledge, we are the first to analyze the
periodicity of encounter pattern with large network traces, which is a critical
basis for designing an efficient delivery scheme using mobile nodes.

ABSTRACT_BEGIN
  The Call admission control (CAC) is one of the Radio Resource Management
(RRM) techniques plays instrumental role in ensuring the desired Quality of
Service (QoS) to the users working on different applications which have
diversified nature of QoS requirements. This paper proposes a fuzzy neural
approach for call admission control in a multi class traffic based Next
Generation Wireless Networks (NGWN). The proposed Fuzzy Neural Call Admission
Control (FNCAC) scheme is an integrated CAC module that combines the linguistic
control capabilities of the fuzzy logic controller and the learning
capabilities of the neural networks .The model is based on Recurrent Radial
Basis Function Networks (RRBFN) which have better learning and adaptability
that can be used to develop the intelligent system to handle the incoming
traffic in the heterogeneous network environment. The proposed FNCAC can
achieve reduced call blocking probability keeping the resource utilisation at
an optimal level. In the proposed algorithm we have considered three classes of
traffic having different QoS requirements. We have considered the heterogeneous
network environment which can effectively handle this traffic. The traffic
classes taken for the study are Conversational traffic, Interactive traffic and
back ground traffic which are with varied QoS parameters. The paper also
presents the analytical model for the CAC .The paper compares the call blocking
probabilities for all the three types of traffic in both the models. The
simulation results indicate that compared to Fuzzy logic based CAC,
Conventional CAC, The simulation results are optimistic and indicates that the
proposed FNCAC algorithm performs better where the call blocking probability is
minimal when compared to other two methods.

ABSTRACT_BEGIN
  OPNET Modeler accelerates network R&D and improves product quality through
high-fidelity modeling and scalable simulation. It provides a virtual
environment for designing protocols and devices, and for testing and
demonstrating designs in realistic scenarios prior to production. OPNET Modeler
supports 802.15.4 standard and has been used to make a model of PAN. Iterations
have been performed by changing the Power of the transmitter and the throughput
will has been analyzed to arrive at optimal values.An energy-efficient wireless
home network based on IEEE 802.15.4, a novel architecture has been proposed. In
this architecture, all nodes are classified into stationary nodes and mobile
nodes according to the functionality of each node. Mobile nodes are usually
battery-powered, and therefore need low-power operation. In order to improve
power consumption of mobile nodes, effective handover sequence based on MAC
broadcast and transmission power control based on LQ (link quality) are
employed. Experimental results demonstrate that by using the proposed
architecture, communication time and power consumption of mobile nodes can be
reduced by 1.2 seconds and 42.8%, respectively.

ABSTRACT_BEGIN
  Optical MINs hold great promise and have advantages over their electronic
networks.they also hold their own challenges. More research has been done on
Electronic Multistage Interconnection Networks, (EMINs) but these days optical
communication is a good networking choice to meet the increasing demands of
high-performance computing communication applications for high bandwidth
applications. The electronic Multistage Interconnection Networks (EMINs) and
the Optical Multistage Interconnection Networks (OMINs) have many similarities,
but there are some fundamental differences between them such as the
optical-loss during switching and the crosstalk problem in the optical
switches. To reduce the negative effect of crosstalk, various approaches which
apply the concept of dilation in either the space or time domain have been
proposed. With the space domain approach, extra SEs are used to ensure that at
most one input and one output of every SE will be used at any given time. For
an Optical network without crosstalk, it is needed to divide the messages into
several groups, and then deliver the messages using one time slot (pass) for
each group, which is called the time division multiplexing. This Paper
discusses the permutation passability behavior of optical MINs. The bandwidth
of optical MINs with or without crosstalk has also been explained. The results
thus obtained shows that the performance of the networks improves by allowing
crosstalk to some extent.

ABSTRACT_BEGIN
  The main issue in the optical transmission is switching speed. The optical
packet switching faces many significant challenges in processing and buffering.
The generalized multilevel protocol switching seeks to eliminate the
asynchronous transfer mode and synchronous optical network layer, hence the
implementation of IP over WDM (wave length division multiplexing). Optical
burst switching attempts to minimize the need for processing and buffering by
aggregating flow of data packets in to burst. In this paper there is an
extensive overview on current technologies and techniques concerning optical
switching.

ABSTRACT_BEGIN
  The study of vehicular ad-hoc networks (VANETs) requires efficient and
accurate simulation tools. As the mobility of vehicles and driver behavior can
be affected by network messages, these tools must include a vehicle mobility
model integrated with a quality network simulator. We present the first
implementation of a well-known vehicle mobility model to ns-3, the next
generation of the popular ns-2 networking simulator. Vehicle mobility and
network communication are integrated through events. User-created event
handlers can send network messages or alter vehicle mobility each time a
network message is received and each time vehicle mobility is updated by the
model. To aid in creating simulations, we have implemented a straight highway
model that manages vehicle mobility, while allowing for various user
customizations. We show that the results of our implementation of the mobility
model matches that of the model's author and provide an example of using our
implementation in ns-3.

ABSTRACT_BEGIN
  Due to its large coverage area, low cost of deployment and high speed data
rates, WiMAX is a promising technology for providing wireless last-mile
connectivity. Physical and MAC layer of this technology refer to the IEEE
802.16e standard, which defines 5 different data delivery service classes that
can be used in order to satisfy Quality of Service (QoS) requirements of
different applications, such as VoIP, videoconference, FTP, Web, etc. The main
aim of the paper is to examine a case of QoS deployment over a cellular WiMAX
network. In particular, the paper compares the performance obtained using two
different QoS configurations differing from the delivery service class used to
transport VoIP traffic, i.e. UGS or ertPS. Results indicate that for
delay-sensitive traffic that fluctuates beyond its nominal rate, having the
possibility to give back some of its reserved bandwidth, ertPS has the
advantage to permit the transmission of BE traffic.

ABSTRACT_BEGIN
  All-Optical Network (AON) is a network where the user-network interface is
optical and the data does undergo optical to electrical conversion within the
network. AONs are attractive because they promise very high rates, flexible
switching and broad application support. There are two technologies for AON:
Wavelength Division Multiplexed (WDM) and Optical Time Division Multiplexed
(OTDM). OTDM transmission systems are becoming increasingly important as one of
the key technologies satisfying the growing demand for large capacity optical
networks. Although OTDM has several advantages in terms of operation system,
such as natural accommodation of higher bit rate payloads, it introduces many
security vulnerabilities, which do not exist in traditional networks. One of
the serious problems with OTDM is the fact that optical crosstalk is additive,
and thus the aggregate effect of crosstalk over a whole all-optical network
(AON) may be more nefarious than a single point of crosstalk. This is because
crosstalk can spread rapidly through the network, causing additional awkward
failures and triggering multiple undesirable alarms. This results in the
continuous monitoring and identification of the impairments becoming
challenging in the event of transmission failures. In this paper we propose a
novel approach for detecting and localizing crosstalk in OTDM transmission
systems that can participate in some tasks for fault management in optical
network.

ABSTRACT_BEGIN
  WiFi is widely implemented in campus wide including administrative, teaching
and student's accommodation. Wireless communications are associated with
interconnect devices which includes cellular networks, infrared, bluetooth and
WiFi enabled devices. It involves mobility and freedom of assessing information
anytime and anywhere. A study on WiFi networks in a campus environment is
presented in this paper. The aim of the research was to investigate the
connectivity problems to WiFi networks. The study includes WiFi performance
analysis as well as network auditing. Channel overlapping and saturation
condition were some of the problems encountered. Different types of software
were used for analyzing the results.

ABSTRACT_BEGIN
  The goal of an Intrusion Detection is inadequate to detect errors and unusual
activity on a network or on the hosts belonging to a local network by
monitoring network activity. Algorithms for building detection models are
broadly classified into two categories, Misuse Detection and Anomaly Detection.
The proposed approach should be taken into account, as the security system
violations caused by both incompliance with the security policy and attacks on
the system resulting in the need to describe models. However, it is based on
unified mathematical formalism which is provided for subsequent merger of the
models. The above formalism in this paper presents a state machine describing
the behavior of a system subject. The set of intrusion description models is
used by the evaluation module and determines the likelihood of undesired
actions the system is capable of detecting. The number of attacks which are not
described by models determining the completeness of detection by the IDS linked
to the ability of detecting security violations.

ABSTRACT_BEGIN
  Wireless sensors networks performance are strictly related to the medium
access mechanism. An effective one, require non-conventional paradigms for
protocol design due to several constraints. An adequate equilibrium between
communication improvement and data processing capabilities must be
accomplished. To achieve low power operation, several MAC protocols already
proposed for WSN. The aim of this paper is to survey and to analyze the most
energy efficient MAC protocol in order to categorize them and to compare their
performances. Furthermore we have implemented some of WSN MAC protocol under
OMNET++ with the purpose to evaluate their performances.

ABSTRACT_BEGIN
  The rapid development of telecommunication networks is driven by user demands
for new applications and advances in technologies. The explosive growth of the
internet traffic is due to its use for collecting the information,
communication, multimedia application, entertainment, etc. These applications
are imposing a tremendous demand for bandwidth capacity on telecommunication
network. The introduction of fiber optics had proved to meet the huge demand of
bandwidth. These requirement can be meet by all optical network which is
capable of transmitting enormous data at very high speed, around 50 Tera bits
per seconds (Tbps) A wavelength conversion technique is addressed in this paper
to reduced the blocking probability in wavelength routed networks. It is seen
that the blocking probability of traffic requests decreases as the wavelength
conversion factor increases. We explode the possibility for network with
different size with variation in wavelength per link. In this work the
evaluation of wavelength routed optical network with varying number of
wavelength converters, different traffic types are carried out and results are
shown that the blocking probability is minimum with 50% to 60% wavelength
convertible nodes. Wavelength convertible nodes more than 60% are not showing
much effect on reduction in blocking probability rather it results in increase
in overall cost of network.

ABSTRACT_BEGIN
  This paper proposes, a hardware implementation of Wireless Mesh Networks
(WMN) medium Access Controller (MAC) layer transmitter. In the literature a lot
of works are focused on WMN routing protocol as well as performance analysis
and software integration of WMN units, however few works deals with WMN
hardware implementation. In this field our contribution is to conceive and to
implements on FPGA a WMN MAC transmitter module. Our implementation, written in
hardware description language (HDL) is based on the IEEE 802.11 s standard. The
hardware implementation retains a good performance in speed.

ABSTRACT_BEGIN
  For the tree topology, previous studies show the maximum likelihood estimate
(MLE) of a link/path takes a polynomial form with a degree that is one less
than the number of descendants connected to the link/path. Since then, the main
concern is focused on searching for methods to solve the high degree polynomial
without using iterative approximation. An explicit estimator based on the Law
of Large Numbers has been proposed to speed up the estimation. However, the
estimate obtained from the estimator is not a MLE. When $n<\infty$, the
estimate may be noticeable different from the MLE. To overcome this, an
explicit MLE estimator is presented in this paper and a comparison between the
MLE estimator and the explicit estimator proposed previously is presented to
unveil the insight of the MLE estimator and point out the pitfall of the
previous one.

ABSTRACT_BEGIN
  An increasingly important requirement for many novel applications is sensing
the positions of people, equipment, etc. GPS technology has proven itself as a
successfull technology for positioning in outdoor environments but indoor no
technology has yet gained a similar wide-scale adoption. A promising indoor
positioning technique is radio-based location fingerprinting, having the major
advantage of exploiting already existing radio infrastructures, like IEEE
802.11, which avoids extra deployment costs and effort. The research goal of
this thesis is to address the limitations of current indoor location
fingerprinting systems. In particular the aim is to advance location
fingerprinting techniques for the challenges of handling heterogeneous clients,
scalability to many clients, and interference between communication and
positioning. The wireless clients used for location fingerprinting are
heterogeneous even when only considering clients for the same technology.
Heterogeneity is a challenge for location fingerprinting because it severely
decreases the precision of location fingerprinting. To support many clients
location fingerprinting has to address how to scale estimate calculation,
measurement distribution, and distribution of position estimates. This is a
challenge because of the number of calculations involved and the frequency of
measurements and position updates. Positioning using location fingerprinting
requires the measurement of, for instance, signal strength for nearby base
stations. However, many wireless communication technologies block communication
while collecting such measurements. This interference is a challenge because it
is not desirable that positioning disables communication. An additional goal is
to improve the conceptual foundation of location fingerprinting. A better
foundation will aid researchers to better survey and design location
fingerprinting systems.

ABSTRACT_BEGIN
  The most efficient receiver-driven multicast congestion control protocols use
dynamic channels. This means that each group has a cyclic rate variation with a
continuously decreasing phase. Despite promising results in terms of fairness,
using efficiently these dynamic groups could be a challenging task for
application programmers. This paper presents a sequencer which maps out
application data to dynamic groups in an optimal way. Multiple applications
such as file transfer or video streaming, can use this sequencer, thanks to a
simple API usable with any buffer containing the most important data first. To
evaluate this solution, we designed a file transfer software using a FEC
encoding. Results show the sequencer optimal behavior and the file transfer
efficiency, as a single download generates only little more overhead than TCP .
Moreover, download time is almost independent of the number of receivers, and
is already faster than TCP with 2 competing downloads.

ABSTRACT_BEGIN
  In recent years, high-speed wireless communication is in vogue. In wireless
communication systems, multipath fading, delay and interference occurres by
reflection or diffraction. In a high-speed wireless communication, it becomes a
necessary to separate desired signal from delay or interference signal. Thus to
overcome these problems Smart antenna systems have been developed. Basically
there are two types of smart antenna systems, one is Switched beam system and
another Adaptive array system.This paper presents the optimum design of a 4x4
plannar Butler matrix array as a key component of a switched beam smart antenna
system, operating at 5.2 GHz for WLAN with a dielectric substrate, FR4 of er
=4.9 and h=1.6mm. Conception details, simulation results and measurements are
also given for the components (microstrip antenna, hybrid couplers,
cross-coupler, phase shifter) used to implement the matrix. In this
dissertation, mathematical calculations for all the components using MATLAB is
done and then every individual component is designed using the commercial
software SONNET. Then these entire components have been combined on a single
substrate and simulated using SONNET.

ABSTRACT_BEGIN
  In this paper, a new transformation is generated from a three variable
Boolean function 3, which is used to produce a self-similar fractal pattern of
dimension 1.58. This very fractal pattern is used to reconstruct the whole
structural position of resources in wireless CDMA network. This reconstruction
minimizes the number of resources in the network and so naturally network
consumption costs are getting reduced. Now -a -days resource controlling and
cost minimization are still a severe problem in wireless CDMA network. To
overcome this problem fractal pattern produced in our research provides a
complete solution of structural position of resources in this Wireless CDMA
Network.

ABSTRACT_BEGIN
  We consider wireless communication networks where network users are subject
to critical events such as emergencies and crises. If a critical event occurs
to a user, the user needs to send critical traffic as early as possible.
However, most existing medium access control (MAC) protocols are not adequate
to meet the urgent need for data transmission by users with critical traffic.
In this paper, we devise a class of distributed MAC protocols that achieve
coordination using the finite-length memory of users containing their own
observations and traffic types. We formulate a protocol design problem and find
optimal protocols that solve the problem. We show that the proposed protocols
enable a user with critical traffic to transmit its critical traffic without
interruption from other users after a short delay while allowing users to share
the channel efficiently when there is no critical traffic. Moreover, the
proposed protocols require short memory and can be implemented without explicit
message passing.

ABSTRACT_BEGIN
  Enhancing the current services or deploying new services operating in RF
spectrum requires more licensed spectrum which may not be provided by the
regulatory bodies because of spectrum scarcity. On the contrary, recent studies
suggest that many portions of the licensed spectrum remains unused or underused
for significant period of time raising the issue of spectrum access without
license in an opportunistic manner. Among all the spectrum accessing
techniques, sensing based methods are considered optimal for their simplicity
and cost effectiveness. In this paper, we introduce a new cooperative spectrum
sensing technique which considers the spatial variation of secondary
(unlicensed) users and each user's contribution is weighted by a factor that
depends on received power and path loss. Compared to existing techniques, the
proposed one increases the sensing ability and spectrum utilization, and offers
greater robustness to noise uncertainty. Moreover, this cooperative technique
uses very simple energy detector as its building block thereby reduces the cost
and operational complexity.

ABSTRACT_BEGIN
  To solve the parameter sensitive issue of the traditional RED (random early
detection) algorithm, an adaptive buffer management algorithm called PAFD
(packet adaptive fair dropping) is proposed. This algorithm supports DiffServ
(differentiated services) model of QoS (quality of service). In this algorithm,
both of fairness and throughput are considered. The smooth buffer occupancy
rate function is adopted to adjust the parameters. By implementing buffer
management and packet scheduling on Intel IXP2400, the viability of QoS
mechanisms on NPs (network processors) is verified. The simulation shows that
the PAFD smoothes the flow curve, and achieves better balance between fairness
and network throughput. It also demonstrates that this algorithm meets the
requirements of fast data packet processing, and the hardware resource
utilization of NPs is higher.

ABSTRACT_BEGIN
  Mobility management and bandwidth management are two major research issues in
a cellular mobile network. Mobility management consists of two basic
components: location management and handoff management. To Provide QoS to the
users Handoff is a key element in wireless cellular networks. It is often
initiated either by crossing a cell boundary or by deterioration in the quality
of signal in the current channel. In this paper, a new admission control policy
for cellular mobile network is being proposed. Two important QoS parameter in
cellular networks are Call Dropping Probability (CDP) and Handoff Dropping
Probability (HDP). CDP represents the probability that a call is dropped due to
a handoff failure. HDP represents the probability of a handoff failure due to
insufficient available resources in the target cell. Most of the algorithms try
to limit the HDP to some target maximum but not CDP. In this paper, we show
that when HDP is controlled, the CDP is also controlled to a minimum extent
while maintaining lower blocking rates for new calls in the system.

ABSTRACT_BEGIN
  Voice over Internet protocol (VoIP) is one of the most important applications
for the IEEE 802.11 wireless local area networks (WLANs). For network planners
who are deploying VoIP over WLANs, one of the important issues is the VoIP
capacity. VoIP bandwidth consumption over a WAN is one of the most important
factors to consider when building a VoIP infrastructure. Failure to account for
VoIP bandwidth requirements will severely limit the reliability of a VoIP
system and place a huge burden on the WAN infrastructure. Less bandwidth
utilization is the key reasons for reduced number of channel accesses in VOIP.
But in the QoS point of view the free bandwidth of atleast 1-5% will improve
the voice quality. This proposal utilizes the maximum bandwidth by leaving 1-5%
free bandwidth. A Bandwidth Data rate Moderation (BDM) algorithm has been
proposed which correlates the data rate specified in IEEE802.11b with the free
bandwidth. At each time BDM will calculate the bandwidth utilization before
sending the packet to improve performance and voice quality of VoIP. The
bandwidth calculation in BDM can be done by using Erlang and VOIP bandwidth
calculator. Finally, ns2 experimental study shows the relationship between
bandwidth utilization, free bandwidth and data rate. The paper concludes that
marginal VoIP call rate has been increased by BDM algorithm.

ABSTRACT_BEGIN
  IEEE 802.16m amends the IEEE 802.16 Wireless MAN-OFDMA specification to
provide an advanced air interface for operation in licenced bands. It will meet
the cellular layer requirements of IMT-Advanced next generation mobile
networks. It will be designed to provide significantly improved performance
compared to other high rate broadband cellular network systems. For the next
generation mobile networks, it is important to consider increasing peak,
sustained data reates, corresponding spectral efficiencies, system capacity and
cell coverage as well as decreasing latency and providing QoS while carefully
considering overall system complexity. In this paper we provide an overview of
the state-of-the-art mobile WiMAX technology and its development. We focus our
discussion on Physical Layer, MAC Layer, Schedular,QoS provisioning and mobile
WiMAX specification.

ABSTRACT_BEGIN
  In this paper, we propose fairness-oriented packet scheduling (PS) schemes
with power-efficient control mechanism for future packet radio systems. In
general, the radio resource management functionality plays an important role in
new OFDMA based networks. The control of the network resource division among
the users is performed by packet scheduling functionality based on maximizing
cell coverage and capacity satisfying, and certain quality of service
requirements. Moreover, multiantenna transmit-receive schemes provide
additional flexibility to packet scheduler functionality. In order to mitigate
inter-cell and co-channel interference problems in OFDMA cellular networks soft
frequency reuse with different power masks patterns is used. Stemming from the
earlier enhanced proportional fair scheduler studies for single-input
multiple-output (SIMO) and multiple-input multipleoutput (MIMO) systems, we
extend the development of efficient packet scheduling algorithms by adding
transmit power considerations in the overall priority metrics calculations and
scheduling decisions. Furthermore, we evaluate the proposed scheduling schemes
by simulating practical orthogonal frequency division multiple access (OFDMA)
based packet radio system in terms of throughput, coverage and fairness
distribution among users. As a concrete example, under reduced overall transmit
power constraint and unequal power distribution for different sub-bands, we
demonstrate that by using the proposed power-aware multi-user scheduling
schemes, significant coverage and fairness improvements in the order of 70% and
20%, respectively, can be obtained, at the expense of average throughput loss
of only 15%.

ABSTRACT_BEGIN
  Sensor networks are currently an active research area mainly due to the
potential of their applications. In this paper we investigate the use of
Wireless Sensor Networks (WSN) for air pollution monitoring in Mauritius. With
the fast growing industrial activities on the island, the problem of air
pollution is becoming a major concern for the health of the population. We
proposed an innovative system named Wireless Sensor Network Air Pollution
Monitoring System (WAPMS) to monitor air pollution in Mauritius through the use
of wireless sensors deployed in huge numbers around the island. The proposed
system makes use of an Air Quality Index (AQI) which is presently not available
in Mauritius. In order to improve the efficiency of WAPMS, we have designed and
implemented a new data aggregation algorithm named Recursive Converging
Quartiles (RCQ). The algorithm is used to merge data to eliminate duplicates,
filter out invalid readings and summarise them into a simpler form which
significantly reduce the amount of data to be transmitted to the sink and thus
saving energy. For better power management we used a hierarchical routing
protocol in WAPMS and caused the motes to sleep during idle time.

ABSTRACT_BEGIN
  Energy is one of the most important and scarce resources in Wireless Sensor
Networks (WSN). WSN nodes work with the embedded operating system called
TinyOS, which addresses the constrains of the WSN nodes such as limited
processing power, memory, energy, etc and it uses the collection Tree Protocol
(CTP) to collect the data from the sensor nodes. It uses either the four-bit
link estimation or Link Estimation Exchange Protocol (LEEP) to predict the bi
directional quality of the wireless link between the nodes and the next hop
candidate is based on the estimated link quality. The residual energy of the
node is an important key factor, which plays a vital role in the lifetime of
the network and hence this has to taken as one of the metric in the parent
selection. In this work, we consider the remaining energy of the node as one of
the metric to decide the parent in addition to the link quality metrics. The
proposed protocol was compared with CTP protocol in terms of number of packets
forwarded by each node and packet reception ratio (PRR) of the network. This
work was simulated in TOSSIM simulator and the same was tested in Crossbow IRIS
radio test bed. The results show that our algorithm performs better than CTP in
terms of load distribution and hence the increased lifetime

ABSTRACT_BEGIN
  The nature of Mobile Ad hoc NETworks (MANETs) makes them suitable to be
utilized in the context of an extreme emergency for all involved rescue teams.
We use the term emergency MANETs (eMANETs) in order to describe next generation
IP-based networks, which are deployed in emergency cases such as forest fires
and terrorist attacks. The main goal within the realm of eMANETs is to provide
emergency workers with intelligent devices such as smart phones and PDAs. This
technology allows communication "islets" to be established between the members
of the same or different emergency teams (policemen, firemen, paramedics). In
this article, we discuss an adaptive and secure routing protocol developed for
the purposes of eMANETs. We evaluate the performance of the protocol by
comparing it with other widely used routing protocols for MANETs. We finally
show that the overhead introduced due to security considerations is affordable
to support secure ad-hoc communications among lightweight devices.

ABSTRACT_BEGIN
  The advent of ubiquitous computing and the proliferation of portable
computing devices have raised the importance of mobile ad-hoc network. A major
challenge lies in adapting multicast communication into such environments where
mobility and link failures are inevitable. The purpose of this paper is to
study impact of mobility models in performance of multicast routing protocols
in MANET. In this work, three widely used mobility models such as Random Way
Point, Reference Point Group and Manhattan mobility models and three popular
multicast routing protocols such as On-Demand Multicast Routing Protocol,
Multicast Ad hoc On-demand Distance Vector Routing protocol and Adaptive Demand
driven Multicast Routing protocol have been chosen and implemented in NS2.
Several experiments have been carried out to study the relative strengths,
weakness and applicability of multicast protocols to these mobility models.

ABSTRACT_BEGIN
  In this paper we develop an integrated model for request mechanism and data
transmission in multi-channel wireless local area networks. We calculated the
performance parameters for single and multi-channel wireless networks when the
channel is noisy. The proposed model is general it can be applied to different
wireless networks such as IEEE802.11x, IEEE802.16, CDMA operated networks and
Hiperlan\2.

ABSTRACT_BEGIN
  This paper addresses handover decision instability which impacts negatively
on both user perception and network performances. To this aim, a new technique
called The HandOver Decision STAbility Technique (HODSTAT) is proposed for
horizontal handover in Wireless Local Area Networks (WLAN) based on IEEE
802.11standard. HODSTAT is based on a hysteresis margin analysis that, combined
with a utilitybased function, evaluates the need for the handover and
determines if the handover is needed or avoided. Indeed, if a Mobile Terminal
(MT) only transiently hands over to a better network, the gain from using this
new network may be diminished by the handover overhead and short usage
duration. The approach that we adopt throughout this article aims at reducing
the minimum handover occurrence that leads to the interruption of network
connectivity (this is due to the nature of handover in WLAN which is a break
before make which causes additional delay and packet loss). To this end, MT
rather performs a handover only if the connectivity of the current network is
threatened or if the performance of a neighboring network is really better
comparing the current one with a hysteresis margin. This hysteresis should make
a tradeoff between handover occurrence and the necessity to change the current
network of attachment. Our extensive simulation results show that our proposed
algorithm outperforms other decision stability approaches for handover decision
algorithm.

ABSTRACT_BEGIN
  Recent studies have shown that the majority of today's internet traffic is
related to Peer to Peer (P2P) traffic. The study of bandwidth in P2P networks
is very important. Because it helps us in more efficient capacity planning and
QoS provisioning when we would like to design a large scale computer networks.
In this paper motivated by the behavior of peers (sources or seeds) that is
modeled by Ornstein Uhlenbeck (OU) process, we propose a model for bandwidth in
P2P networks. This model is represented with a stochastic integral. We also
model the bandwidth when we have multiple downloads or uploads. The
autocovariance structure of bandwidth in either case is studied and the
statistical parameters such as mean, variance and autocovariance are obtained.
We then study the queue length behavior of the bandwidth model. The methods for
generating synthetic bandwidth process and estimation of the bandwidth
parameters using maximum likehood estimation are presented.

ABSTRACT_BEGIN
  In the next generation wireless networks, the growing demand for new wireless
applications is accompanied with high expectations for better quality of
service (QoS) fulfillment especially for multimedia applications. Furthermore,
the coexistence of future unlicensed users with existing licensed users is
becoming a challenging task in the next generation communication systems to
overcome the underutilization of the spectrum. A QoS and interference aware
resource allocation is thus of special interest in order to respond to the
heterogeneous constraints of the next generation networks. In this work, we
address the issue of resource allocation under heterogeneous constraints for
unlicensed multiband ultra-wideband (UWB) systems in the context of Future Home
Networks, i.e. the wireless personal area network (WPAN). The problem is first
studied analytically using a heterogeneous constrained optimization problem
formulation. After studying the characteristics of the optimal solution, we
propose a low-complexity suboptimal algorithm based on a cross-layer approach
that combines information provided by the PHY and MAC layers. While the PHY
layer is responsible for providing the channel quality of the unlicensed UWB
users as well as their interference power that they cause on licensed users,
the MAC layer is responsible for classifying the unlicensed users using a
two-class based approach that guarantees for multimedia services a
high-priority level compared to other services. Combined in an efficient and
simple way, the PHY and MAC information present the key elements of the aimed
resource allocation. Simulation results demonstrate that the proposed scheme
provides a good tradeoff between the QoS satisfaction of the unlicensed
applications with hard QoS requirements and the limitation of the interference
affecting the licensed users.

ABSTRACT_BEGIN
  In the next generation network (NGN) environment specific consideration is on
bandwidth minimization, because this reduces the cost of network. In response
to the growing market demand for multimedia traffic transmission, NGN concept
has been produced. The next generation network provides multimedia services
over high speed networks, which supports DVD quality video on demand. Although
it has numerous advantages, more exploration of the large-scale deployment
video on demand is still needed. The focus of the research presented in this
paper is a class based admission control by the complete partitioning of the
video on demand server. In this paper we present analytically and by simulation
how the blockage probability of the server significantly affects the on demand
video request and the service. We also present how the blockage probability
affects the performance of the video on demand server.

ABSTRACT_BEGIN
  Orchestrating a live field trial of wireless mobile networking involves
significant cost and logistical issues relating to mobile platforms, support
personnel, network and experiment automation and support equipment. The
significant cost and logistics required to execute such a field trial can also
be limiting in terms of achieving meaningful test results that exercise a
practical number of mobile nodes over a significant set of test conditions
within a given time. There is no argument that field trials are an important
component of dynamic network testing. A field test of prototype will show
whether simulations were on right track or not, but that's a big leap to take;
going from the simulator directly to the real thing. In conceiving our work, we
envisioned a mobile network emulation system that is low cost, flexible and
controllable. This paper describes our wireless MANET test bed under
development which emulates an actual MANET. Here, we focuses that, this test
bed allows the users to automatically generate arbitrary logically network
topologies in order to perform real time operations on adhoc network at a
relatively low cost in a laboratory environment without having to physically
move the nodes in the adhoc network. Thus, we try to "compress" wireless
network so that it fits on a single table.

ABSTRACT_BEGIN
  Routers are one of the important entities in computer networks specially the
Internet. Forwarding IP packets is a valuable and vital function in Internet
routers. Routers extract destination IP address from packets and lookup those
addresses in their own routing table. This task is called IP lookup. Internet
address lookup is a challenging problem due to the increasing routing table
sizes. Ternary Content-Addressable Memories (TCAMs)are becoming very popular
for designing high-throughput address lookup?engines on routers: they are fast,
cost-effective and simple to manage. Despite the TCAMs speed, their high power
consumption is their major drawback. In this paper, Multilevel Enabling
Technique (MLET), a power efficient TCAM based hardware architecture has been
proposed. This scheme is employed after an Espresso-II minimization algorithm
to achieve lower power consumption. The performance evaluation of the proposed
approach shows that it can save considerable amount of routing table's power
consumption.

ABSTRACT_BEGIN
  Communication protocols and techniques are often evaluated using simulation
techniques. However, the use of formal modeling and analysis techniques for
verification and evaluation in particular for Wireless Sensor Networks (WSN)
becomes a necessity. In this paper we present a formal analysis of the backoff
procedure integrated in the medium access control protocol named ECo-MAC
designed for WSN. We describe this backoff procedure in terms of discrete time
Markov chains (DTMCs) and evaluated using the well known probabilistic model
checker PRISM. After checking the different invariants of the proposed model,
we study the effect of contention window length (in number of time contention
unit) on the acceptable number of simultaneous senders in a neighborhood of a
given receiver. The obtained quantitative results confirm those provided by the
simulation using OPNET tool and justify the validity of the adopted value for
the time contention unit TCU.

ABSTRACT_BEGIN
  With the advent of large-scale cloud computing infrastructure, network
extension and migration has emerged as a major challenge in the management of
modern enterprise networks. Many enterprises are considering extending or
relocating their network components, in whole or in part, to remote, private
and public data centers, in order to attain scalability, failure resilience,
and cost savings for their network applications. In this paper, we conduct a
first rigorous study on the extension and migration of an enterprise network
while preserving its performance and security requirements, such as layer
2/layer 3 reachability, and middle-box traversal through load balancer,
intrusion detection and ACLs. We formulate this increasingly important problem,
present preliminary designs, and conduct experiments to validate the
feasibility of our designs.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) generate massive amount of live data and
events sensed through dispersedly deployed tiny sensors. This generated data
needed to be disseminate to the sink with slight consumption of network
resources. One of the ways to efficiently transmit this bulk data is gossiping.
An important consideration in gossip-based dissemination protocols is to keep
routing table up to date. Considering the inherent resource constrained nature
of adhoc wireless sensor networks, we propose a gossip based protocol that
consumes little resources. Our proposed scheme aims to keep the routing table
size R as low as possible yet it ensures that the diameter is small too. We
learned the performance of our proposed protocol through simulations. Results
show that our proposed protocol attains major improvement in network
reachability and connectivity.

ABSTRACT_BEGIN
  For stationary wireless ad hoc networks, one of the key challenging issues in
routing and multicasting is to conserve as much energy as possible without
compromising path efficiency measured as end-to-end delay. In this paper, we
address the problem of path efficient and energy aware multicasting in static
wireless ad hoc networks. We propose a novel distributed scalable algorithm for
finding a virtual multicast backbone (VMB). Based on this VMB, we have further
developed a multicasting scheme that jointly improves path efficiency and
energy conservation. By exploiting inherent broadcast advantage of wireless
communication and employing a more realistic energy consumption model for
wireless communication which not only depends on radio propagation losses but
also on energy losses in transceiver circuitry, our simulation results show
that the proposed VMB-based multicasting scheme outperforms existing prominent
tree based energy conserving, path efficient multicasting schemes.

ABSTRACT_BEGIN
  Over the years, communication speed of networks has increased from a few Kbps
to several Mbps, as also the bandwidth demand, Communication Protocols, however
have not improved to that extent. With the advent of Wavelength Division
Multiplexing (WDM), it is now possible to "tune" protocols to current and
future demands. The purpose of this paper is to evolve a High Speed Network
architecture, which will cater to the needs of bandwidth-consuming
applications, such as voice, video and high definition image transmission.

ABSTRACT_BEGIN
  In some underwater sensor networks, sensor nodes may be deployed at various
depths of an ocean making those networks three-dimensional (3D). While most
terrestrial sensor networks can usually be modeled as two dimensional (2D)
networks, these underwater sensor networks must be modeled as 3D networks. This
leads to new research challenges in the area of network architecture and
topology. In this paper, we present two different network architectures for 3D
underwater sensor networks. The first one is a hierarchical architecture that
uses a relatively small number of robust backbone nodes to create the network
where a large number of inexpensive sensors communicate with their nearest
backbone nodes, and packets from a backbone node to the sink is routed through
other backbone nodes. This hierarchical approach allows creating a network of
smaller number of expensive backbone nodes while keeping the mobile sensors
simple and inexpensive. Along with network topology, we also study energy
efficiency and frequency reuse issues for such 3D networks. The second approach
is a nonhierarchical architecture which assumes that all nodes are identical
and randomly deployed. It partitions the whole 3D network space into identical
cells and keeps one node active in each cell such that sensing coverage and
connectivity are maintained while limiting the energy consumed. We also study
closeness to optimality of our proposed scheme.

ABSTRACT_BEGIN
  In the current Internet, there is no clean way for affected parties to react
to poor forwarding performance: when a domain violates its Service Level
Agreement (SLA) with a contractual partner, the partner must resort to ad-hoc
probing-based monitoring to determine the existence and extent of the
violation. Instead, we propose a new, systematic approach to the problem of
forwarding-performance verification. Our mechanism relies on voluntary
reporting, allowing each domain to disclose its loss and delay performance to
its neighbors; it does not disclose any information regarding the participating
domains' topology or routing policies beyond what is already publicly
available. Most importantly, it enables verifiable performance measurements,
i.e., domains cannot abuse it to significantly exaggerate their performance.
Finally, our mechanism is tunable, allowing each participating domain to
determine how many resources to devote to it independently (i.e., without any
inter-domain coordination), exposing a controllable trade-off between
performance-verification quality and resource consumption. Our mechanism comes
at the cost of deploying modest functionality at the participating domains'
border routers; we show that it requires reasonable processing and memory
resources within modern network capabilities.

ABSTRACT_BEGIN
  Routing the packets efficiently in mobile ad hoc network does not have end to
end paths. Multiple copies are forwarded from the source to the destination. To
deal with such networks, researches introduced flooding based routing schemes
which leads to high probability of delivery. But the flooding based routing
schemes suffered with contention and large delays. Here the proposed protocol
"Spray Select Focus", sprays a few message copies into the network, neighbors
receives a copy and by that relay nodes we are choosing the shortest route and
then route that copy towards the destination. Previous works assumption is that
there is no contention and dead ends. But we argue that contention and dead
ends must be considered for finding efficiency in routing. So we are including
a network which has contention and dead ends and we applied the proposed
protocol. We can say that this protocol works well for the contention based
network.

ABSTRACT_BEGIN
  Multicast plays an important role in implementing the group communications in
bandwidth scarce multihop mobile ad hoc networks. However, due to the dynamic
topology of MANETs it is very difficult to build optimal multicast trees and
maintaining group membership, making even more challenging to implement
scalable and robust multicast in Mobile Ad hoc Networks (MANET). A scalable and
energy efficient location aware multicast algorithm, called SEELAMP, for mobile
ad hoc networks is presented in the paper that is based on creation of shared
tree using the physical location of the nodes for the multicast sessions. It
constructs a shared bi-directional multicast tree for its routing operations
rather than a mesh, which helps in achieving more efficient multicast delivery.
The algorithm uses the concept of small overlapped zones around each node for
proactive topology maintenance with in the zone. Protocol depends on the
location information obtained using a distributed location service, which
effectively reduces the overheads for route searching and shared multicast tree
maintenance. In this paper a new technique of local connectivity management is
being proposed that attempts to improve the performance and reliability. It
employs a preventive route reconfiguration to avoid the latency in case of link
breakages and to prevent the network from splitting.

ABSTRACT_BEGIN
  Wireless Sensor Networks are basically used for gathering information needed
by smart environments but they are particularly useful in unattended situations
where terrain, climate and other environmental constraints may hinder in the
deployment of wired/conventional networks. Unlike traditional networks, these
sensor networks do not have a continuous power supply at their disposal. Rather
the individual sensors are battery operated and the lifetime of the individual
sensors and thus the overall network depend heavily on duty cycle of these
sensors. Analysis on WSNs shows that communication module is the main part
which consumes most of the sensor energy and that is why energy conservation is
the major optimization goal. Since routing protocols and MAC protocols directly
access the communication module therefore the design of protocols in these two
domains should take into account the energy conservation goal. In this paper,
we discuss different state-of-the-art protocols both in MAC and routing domains
that have been proposed for WSNs to achieve the overall goal of prolonging the
network lifetime. The routing protocols in WSNs are generally categorized into
three groups - data centric, hierarchical and location-based but we focus on
only the first two categories because location-based routing protocols
generally require a prior knowledge about sensors location which most of the
times is not available due to random deployment of the sensors. We then discuss
how schedule-based and contention-based MAC protocols can contribute to achieve
optimal utilization of the limited energy resource by avoiding or reducing the
chances of collisions and thus the need for retransmission.

ABSTRACT_BEGIN
  In recent years, WLAN technology has been gaining popularity around the world
with its sub standard 802.11b receiving major deployments in many indoor and
outdoor environments. In this article we investigate the performance of IEEE
802.11b infrastructure networks in the lossless and lossy environments by means
of a simulation study. Also, this study shows how the FIFO discipline of the
802.11b MAC affects on the global performance when at least one channel is
under the influence of the bursty errors. Furthermore, this paper proposes a
channel aware backoff algorithm for the Access Point (AP) to prioritize its
transmissions and to accelerate the transmissions in the poor radio channels to
enhance the performance of the real time applications. The final results of
this simulation study showed that the proposed algorithm is able to enhance the
throughput and the delay in lossy environment by an average of 49% and 83%
respectively.

ABSTRACT_BEGIN
  As the demand of, requesting the Internet without any disturbance by the
mobile users of any network is increasing the IETF started working on Network
Mobility (NEMO). Maintaining the session of all the nodes in mobile network
with its home network and external nodes can be provided by the basic Network
Mobility support protocol. It provides mobility at IP level to complete
networks, allowing a Mobile Network to change its point of attachment to the
Internet, while maintaining the ongoing sessions of the nodes of the network.
The Mobile Router (MR) manages the mobility even though the nodes don't know
the status of mobility. This article discusses few basic concepts and
limitations of NEMO protocol and proposes two ways to optimize the NEMO routing
technique for registered and unregistered Correspondent Nodes (CN) of the
Mobile Network Node (MNN).

ABSTRACT_BEGIN
  This paper has been withdrawn.

ABSTRACT_BEGIN
  Clustering in wireless sensor networks (WSNs) is an important technique to
ease topology management and routing. Clustering provides an effective method
for prolonging lifetime of a WSN. This paper proposes energy efficient
multi-level clustering schemes for wireless sensor networks. Wireless sensor
nodes are extremely energy constrained with a limited transmission range. Due
to large area of deployment, the network needs to have a multi-level clustering
protocol that will enable far-off nodes to communicate with the base station.
Simulation is used to analyze the proposed protocols and compare their
performance with existing protocol EEMC. Simulation results demonstrate that
our proposed protocols are effective in prolonging the network lifetime.

ABSTRACT_BEGIN
  Internet faces the problem of congestion due to its increased use. AQM
algorithm is a solution to the problem of congestion control in the Internet.
There are various existing algorithms that have evolved over the past few years
to solve the problem of congestion in IP networks. Congested link causes many
problems such as large delay, underutilization of the link and packet drops in
burst. There are various existing algorithms that have evolved over the past
few years to solve the problem of congestion in IP networks. In this paper,
study of these existing algorithms is done. This paper discusses algorithms
based on various congestion-metrics and classifies them based on certain
factors. This helps in identifying the algorithms that regulate the congestion
more effectively.

ABSTRACT_BEGIN
  Admission control schemes and scheduling algorithms are designed to offer QoS
services in 802.16/802.16e networks and a number of studies have investigated
these issues. But the channel condition and priority of traffic classes are
very rarely considered in the existing scheduling algorithms. Although a number
of energy saving mechanisms have been proposed for the IEEE 802.16e, to
minimize the power consumption of IEEE 802.16e mobile stations with multiple
real-time connections has not yet been investigated. Moreover, they mainly
consider non real- time connections in IEEE 802.16e networks. In this paper, we
propose to design an adaptive power efficient packet scheduling algorithm that
provides a minimum fair allocation of the channel bandwidth for each packet
flow and additionally minimizes the power consumption. In the adaptive
scheduling algorithm, packets are transmitted as per allotted slots from
different priority of traffic classes adaptively, depending on the channel
condition. Suppose if the buffer size of the high priority traffic queues with
bad channel condition exceeds a threshold, then the priority of those flows
will be increased by adjusting the sleep duty cycle of existing low priority
traffic, to prevent the starvation. By simulation results, we show that our
proposed scheduler achieves better channel utilization while minimizing the
delay and power consumption.

ABSTRACT_BEGIN
  Mobile Ad hoc NETworks (MANETs) are leaving the confines of research
laboratories, to find place in real-world deployments. Outside specialized
domains (military, vehicular, etc.), city-wide communitynetworks are emerging,
connecting regular Internet users with each other, and with the Internet, via
MANETs. Growing to encompass more than a handful of "trusted participants", the
question of preserving the MANET network connectivity, even when faced with
careless or malicious participants, arises, and must be addressed. A first step
towards protecting a MANET is to analyze the vulnerabilities of the routing
protocol, managing the connectivity. By understanding how the algorithms of the
routing protocol operate, and how these can be exploited by those with ill
intent, countermeasures can be developed, readying MANETs for wider deployment
and use. This paper takes an abstract look at the algorithms that constitute
the Optimized Link State Routing Protocol version 2 (OLSRv2), and identifies
for each protocol element the possible vulnerabilities and attacks -- in a
certain way, provides a "cookbook" for how to best attack an operational OLSRv2
network, or for how to proceed with developing protective countermeasures
against these attacks.

ABSTRACT_BEGIN
  Backpressure-based adaptive routing algorithms where each packet is routed
along a possibly different path have been extensively studied in the
literature. However, such algorithms typically result in poor delay performance
and involve high implementation complexity. In this paper, we develop a new
adaptive routing algorithm built upon the widely-studied back-pressure
algorithm. We decouple the routing and scheduling components of the algorithm
by designing a probabilistic routing table which is used to route packets to
per-destination queues. The scheduling decisions in the case of wireless
networks are made using counters called shadow queues. The results are also
extended to the case of networks which employ simple forms of network coding.
In that case, our algorithm provides a low-complexity solution to optimally
exploit the routing-coding tradeoff.

ABSTRACT_BEGIN
  Wireless sensor networks are conceived to monitor a certain application or
physical phenomena and are supposed to function for several years without any
human intervention for maintenance. Thus, the main issue in sensor networks is
often to extend the lifetime of the network by reducing energy consumption. On
the other hand, some applications have high priority traffic that needs to be
transferred within a bounded end-to-end delay while maintaining an energy
efficient behavior. We propose MaCARI, a time segmentation protocol that saves
energy, improves the overall performance of the network and enables quality of
service in terms of guaranteed access to the medium and end-to-end delays. This
time segmentation is achieved by synchronizing the activity of nodes using a
tree-based beacon propagation and allocating activity periods for each cluster
of nodes. The tree-based topology is inspired from the cluster-tree proposed by
the ZigBee standard. The efficiency of our protocol is proven analytically, by
simulation and through real testbed measurements.

ABSTRACT_BEGIN
  The IEEE 802.16 technology (WiMAX) is a promising technology for providing
last-mile connectivity by radio link due to its high speed data rates, low cost
of deployment, and large coverage area. However, the maximum number of channels
defined in the current system may cause a potential bottleneck and limit the
overall system capacity. The aim of this paper is to compare the impact on
system performance of different solutions used to mitigate the impairments due
to the radio channel. In particular, taking into account the WiMAX system
capacity as well as application delays, the paper presents the simulation
results obtained when a static QPSK 1/2 Modulation and Coding Scheme (MCS) is
adopted. Then, the study is aimed at evaluating the improvements introduced by
the adoption of an adaptive modulation and coding (AMC) and an AMC jointly with
Hybrid Automatic Repeat reQuest (HARQ). Results indicate that the best strategy
is to use an aggressive AMC table with the HARQ.

ABSTRACT_BEGIN
  Design and simulation of future mobile networks will center around human
interests and behavior. We propose a design paradigm for mobile networks driven
by realistic models of users' on-line behavior, based on mining of billions of
wireless-LAN records. We introduce a systematic method for large-scale
multi-dimensional coclustering of web activity for thousands of mobile users at
79 locations. We find surprisingly that users can be consistently modeled using
ten clusters with disjoint profiles. Access patterns from multiple locations
show differential user behavior. This is the first study to obtain such
detailed results for mobile Internet usage.

ABSTRACT_BEGIN
  In a virtualized infrastructure where physical resources are shared, a single
physical server failure will terminate several virtual servers and crippling
the virtual infrastructures which contained those virtual servers. In the worst
case, more failures may cascade from overloading the remaining servers. To
guarantee some level of reliability, each virtual infrastructure, at
instantiation, should be augmented with backup virtual nodes and links that
have sufficient capacities. This ensures that, when physical failures occur,
sufficient computing resources are available and the virtual network topology
is preserved. However, in doing so, the utilization of the physical
infrastructure may be greatly reduced. This can be circumvented if backup
resources are pooled and shared across multiple virtual infrastructures, and
intelligently embedded in the physical infrastructure. These techniques can
reduce the physical footprint of virtual backups while guaranteeing
reliability.

ABSTRACT_BEGIN
  In this work we introduce the principles of an algorithm that constructs and
maintains a spanning forest in a mobile telecommunication network-a MANET. The
algorithm is based on the random walk of a token and is entirely decentralized.
A probability analysis is performed when the network is static. Then we show
that performances can be slightly enhanced when adding a memory process in the
walk on the token.

ABSTRACT_BEGIN
  Random scale-free overlay topologies provide a number of properties like for
example high resilience against failures of random nodes, small (average)
diameter as well as good expansion and congestion characteristics that make
them interesting for the use in large-scale distributed systems. A number of
these properties have been shown to be influenced by the exponent \gamma of
their degree distribution P(k) ~ k^{-\gamma}. In this article, we present a
distributed rewiring scheme that is suitable to effectuate scale-free overlay
topologies with an adjustable exponent. The scheme uses a biased random walk
strategy to sample new endpoints of edges being rewired and relies on a simple
equilibrium model for scale-free networks. The bias of the random walk strategy
can be tuned to produce random scale-free networks with arbitrary degree
distribution exponents greater than two. We argue that the rewiring strategy
can be implemented in a distributed fashion based on a node's information about
its immediate neighbors. We present both analytical arguments as well as
results that have been obtained using an implementation of the proposed
protocol.

ABSTRACT_BEGIN
  The geographical location of Internet IP addresses has an importance both for
academic research and commercial applications. Thus, both commercial and
academic databases and tools are available for mapping IP addresses to
geographic locations. Evaluating the accuracy of these mapping services is
complex since obtaining diverse large scale ground truth is very hard. In this
work we evaluate mapping services using an algorithm that groups IP addresses
to PoPs, based on structure and delay. This way we are able to group close to
100,000 IP addresses world wide into groups that are known to share a
geo-location with high confidence. We provide insight into the strength and
weaknesses of IP geolocation databases, and discuss their accuracy and
encountered anomalies.

ABSTRACT_BEGIN
  Given a bidirected ring with capacities and a demand graph, we present an
approximation algorithm to the problem of finding the minimum $\alpha$ such
that there exists a feasible unsplittable routing of the demands after
multiplying each capacity by $\alpha$. We also give an approximation scheme to
the problem.

ABSTRACT_BEGIN
  Space Division Multiple Access (SDMA) based Medium Access Control (MAC)
protocols have been proposed to enable concurrent communications and improve
link throughput in Multi-Input Multi-Output (MIMO) Ad Hoc networks. For the
most part, the works appearing in the literature make idealized and simplifying
assumptions about the underlying physical layer as well as some aspects of the
link adaptation protocol. The result is that the performance predicted by such
works may not necessarily be a good predictor of actual performance in a fully
deployed system. In this paper we look to introduce elements into the SDMA MAC
concept that would allow us to better predict their performance under realistic
operating conditions. Using a generic SDMA-MAC we look at how the network sum
throughput changes with the introduction of the following: $(a)$ use of the
more practical MMSE algorithm instead of the zero-forcing or SVD based nulling
algorithms used for receive beamnulling; $(b)$ impact of channel estimation
errors; $(c)$ introduction of link adaptation mechanism specifically designed
for concurrent SDMA MACs; $(d)$ incorporation of TX beamforming along with RX
beamnulling. Following on the transmission window during which concurrent
transmissions are allowed by the MAC, we qualify the impact of each of these
four elements in isolation. At the conclusion, the performance of a system that
incorporates elements $a-d$ is presented and compared against the baseline
system, showing an improvement of up to 5x in the overall network sum
throughput.

ABSTRACT_BEGIN
  Multiple Input Multiple Output (MIMO) wireless communication link has been
theoretically proven to be reliable and capable of achieving high capacity.
However, these two advantageous characteristics tend to be addressed separately
in many major researches. Researches on various approaches to attain both
characteristics in a single MIMO system are still on-going and an established
approach is yet to be concluded. To address this problem, in this paper a
Vertical Bell Laboratories Layered Space-Time (V-BLAST) MIMO enhanced with
Rate-Compatible Convolutional (RCPC) codes with Zero Forcing (ZF) and Minimum
Mean Squared Error (MMSE)-based detection is proposed. The analytical BER of
the system is presented and numerically analyzed. The system performance is
analyzed in Nakagami-m fading channel, which provides accuracy and flexibility
in matching the signals statistics compared to other fading models. The
complexity which arises in the calculations of the RCPC codes parameters is
significantly reduced by using equivalent convolutional codes. Results show
that the use of high-rate code allows for bandwidth efficiency and at the same
time does not severely degrades the system performance. It is also shown that
the MMSE-based system outperforms the conventional ZF-based system especially
in the low Eb/N0 region and in severe fading conditions.

ABSTRACT_BEGIN
  This paper proposes new scheme for efficient rate allocation in conjunction
with reducing peak-to-average power ratio (PAPR) in orthogonal
frequency-division multiplexing (OFDM) systems. Modification of the set
partitioning in hierarchical trees (SPIHT) image coder is proposed to generate
four different groups of bit-stream relative to its significances. The
significant bits, the sign bits, the set bits and the refinement bits are
transmitted in four different groups. The proposed method for reducing the PAPR
utilizes twice the unequal error protection (UEP), using the Read-Solomon codes
(RS), in conjunction with bit-rate allocation and selective interleaving to
provide minimum PAPR. The output bit-stream from the source code (SPIHT) will
be started by the most significant types of bits (first group of bits). The
optimal unequal error protection (UEP) of the four groups is proposed based on
the channel destortion. The proposed structure provides significant improvement
in bit error rate (BER) performance. Performed computer simulations have shown
that the proposed scheme outperform the performance of most of the recent PAPR
reduction techniques in most cases. Moreover, the simulation results indicate
that the proposed scheme provides significantly better PSNR performance in
comparison to well-known robust coding schemes.

ABSTRACT_BEGIN
  This paper proposes a two-stage optical packet switch with second stage of
recirculate switch of FDL to reduce the number of the FDL used in the switch
for contention resolution. The contention resolution scheme with priority in
packet releasing from FDL is tested in the two-stage switch for performance
evaluation. Simulation result shows that zero packet loss rate achievable with
{\i}< 0.8 for 32x 32 two-stage switch.

ABSTRACT_BEGIN
  Motivated by MIMO broad-band fading channel model, in this section we deals
with the capacity behaviour of wireless MIMO and OFDM based spatial
multiplexing systems in broad-band fading environments for the case where the
channel is unknown at the transmitter and perfectly known at the receiver. This
influence the propagation and system parameters on ergodic capacity, we
furthermore demonstrate that, unlike the single-input single-output (SISO)
case, delay spread channels may provide advantage over flat-fading channels not
only in terms of outage capacity but also in terms of ergodic capacity.
Therefore, MIMO delay spread channels will in general provide both higher
diversity gain and higher multiplexing gain than MIMO flat-fading channels.

ABSTRACT_BEGIN
  The topic of this paper is the evaluation of QoS parameters in live Pre-Wimax
environments. The main contribution is the validation of an analytical
delay-jitter behavior model. These models can be used in optimization
algorithms in order to provide opportunistic and reliable all-IP networks. It
allows understanding the impact of the jitter constraints on the throughput and
packet loss in wireless systems. However, we show that the real-time QoS
requirements of real-time and interactive services can be avoided to a large
degree by controlling only the packet delay-jitter in a fixed and mobile
environment. The QoS metrics have been computed from live measurements in a
Pre-Wimax realistic environment (Toulouse/Blagnac Airport).

ABSTRACT_BEGIN
  The analysis and simulation of transmit and receive pulse shaping filter is
an important aspect of digital wireless communication since it has a direct
effect on error probabilities. Pulse shaping for wireless communication over
time as well as frequency selective channels is the need of hour for 3G and 4G
systems. The pulse shaping filter is a useful means to shape the signal
spectrum and avoid interferences. Basically digital filters are used to modify
the characteristics of signal in time and frequency domain and have been
recognized as primary digital signal processing operations.

ABSTRACT_BEGIN
  Voice communication over internet not be possible without a reliable data
network, this was first available when distributed network topologies were used
in conjunction with data packets. Early network used single centre node network
in which a single workstation (Server) is responsible for the communication.
This posed problems as if there was a fault with the centre node, (workstation)
nothing would work. This problem was solved by the distributed system in which
reliability increases by spreading the load between many nodes. The idea of
packet switching & distributed network were combined, this combination were
increased reliability, speed & responsible for voice communication over
internet, Voice-over-IP (VoIP)These data packets travel through a
packet-switched network such as the Internet and arrive at their destination
where they are decompressed using a compatible Codec (audio coder/decoder) and
converted back to analogue audio. This paper deals with the Simulink
architecture for VoIP network.

ABSTRACT_BEGIN
  Handoff has become an inevitable part of wireless cellular communication,
Soon users will carry small portable handheld devices which will incorporate
the computer, phone, camera, GPS, personal control module etc. This paper
proposes a new scheme to deal with seam less roaming and reduce failed
handoffs. The simulation is done using software called Qualnet meant for
wireless communication. The results clearly indicate the advantages of this new
scheme.

ABSTRACT_BEGIN
  This paper presents a Q-learning based scheme for managing the partial
coverage problem and the ill-effects of free riding in unstructured P2P
networks. Based on various parameter values collected during query routing,
reward for the actions are computed and these rewards are used for updating the
corresponding Q-values of peers. Thus, the routing is done through only nodes
which have shown high performance in the past. Simulation experiments are
conducted in several times and the results are plotted. Results show that the
proposed scheme effectively manages free riders, generates high hit ratio,
reduces network traffic and manages partial coverage problem.

ABSTRACT_BEGIN
  This paper presents a solution for reducing the ill effects of free-riders in
decentralised unstructured P2P networks. An autonomous replication scheme is
proposed to improve the availability and enhance system performance. Q-learning
is widely employed in different situations to improve the accuracy in decision
making by each peer. Based on the performance of neighbours of a peer, every
neighbour is awarded different levels of ranks. At the same time a
low-performing node is allowed to improve its rank in different ways.
Simulation results show that Q-learning based free riding control mechanism
effectively limits the services received by free-riders and also encourages the
low-performing neighbours to improve their position. The popular files are
autonomously replicated to nodes possessing required parameters. Due to this
improvement of quantity of popular files, free riders are given opportunity to
lift their position for active participation in the network for sharing files.
Q-feed effectively manages queries from free riders and reduces network traffic
significantly

ABSTRACT_BEGIN
  In this paper, we consider the problem of blocking malicious traffic on the
Internet, via source-based filtering. In particular, we consider filtering via
access control lists (ACLs): these are already available at the routers today
but are a scarce resource because they are stored in the expensive ternary
content addressable memory (TCAM). Aggregation (by filtering source prefixes
instead of individual IP addresses) helps reduce the number of filters, but
comes also at the cost of blocking legitimate traffic originating from the
filtered prefixes. We show how to optimally choose which source prefixes to
filter, for a variety of realistic attack scenarios and operators' policies. In
each scenario, we design optimal, yet computationally efficient, algorithms.
Using logs from Dshield.org, we evaluate the algorithms and demonstrate that
they bring significant benefit in practice.

ABSTRACT_BEGIN
  The area of mobile ad hoc networking has received considerable attention of
the research community in recent years. These networks have gained immense
popularity primarily due to their infrastructure-less mode of operation which
makes them a suitable candidate for deployment in emergency scenarios like
relief operation, battlefield etc., where either the pre-existing
infrastructure is totally damaged or it is not possible to establish a new
infrastructure quickly. However, MANETs are constrained due to the limited
transmission range of the mobile nodes which reduces the total coverage area.
Sometimes the infrastructure-less ad hoc network may be combined with a fixed
network to form a hybrid network which can cover a wider area with the
advantage of having less fixed infrastructure. In such a combined network, for
transferring data, we need base stations which act as gateways between the
wired and wireless domains. Due to the hybrid nature of these networks, routing
is considered a challenging task. Several routing protocols have been proposed
and tested under various traffic conditions. However, the simulations of such
routing protocols usually do not consider the hybrid network scenario. In this
work we have carried out a systematic performance study of the two prominent
routing protocols: Destination Sequenced Distance Vector Routing (DSDV) and
Dynamic Source Routing (DSR) protocols in the hybrid networking environment. We
have analyzed the performance differentials on the basis of three metrics -
packet delivery fraction, average end-to-end delay and normalized routing load
under varying pause time with different number of sources using NS2 based
simulation.

ABSTRACT_BEGIN
  In this paper, we study stability and latency of routing in wireless networks
where it is assumed that no collision will occur. Our approach is inspired by
the adversarial queuing theory, which is amended in order to model wireless
communication. More precisely, there is an adversary that specifies
transmission rates of wireless links and injects data in such a way that an
average number of data injected in a single round and routed through a single
wireless link is at most $r$, for a given $r\in (0,1)$. We also assume that the
additional "burst" of data injected during any time interval and scheduled via
a single link is bounded by a given parameter $b$.
  Under this scenario, we show that the nodes following so called {\em
work-conserving} scheduling policies, not necessarily the same, are guaranteed
stability (i.e., bounded queues) and reasonably small data latency (i.e.,
bounded time on data delivery), for injection rates $r<1/d$, where $d$ is the
maximum length of a routing path. Furthermore, we also show that such a bound
is asymptotically optimal on $d$.

ABSTRACT_BEGIN
  This paper experimentally evaluates the effects of applying autonomic
management to the scheduling of maintenance operations in a deployed Chord
network, for various membership churn and workload patterns. Two versions of an
autonomic management policy were compared with a static configuration. The
autonomic policies varied with respect to the aggressiveness with which they
responded to peer access error rates and to wasted maintenance operations. In
most experiments, significant improvements due to autonomic management were
observed in the performance of routing operations and the quantity of data
transmitted between network members. Of the autonomic policies, the more
aggressive version gave slightly better results.

ABSTRACT_BEGIN
  Throughput improvement of the Wireless LANs has been a constant area of
research. Most of the work in this area, focuses on designing throughput
optimal schemes for fully connected networks (no hidden nodes). But, we
demonstrate that the proposed schemes, though perform optimally in fully
connected network, achieve significantly lesser throughput even than that of
standard IEEE 802.11 in a network with hidden nodes. This motivates the need
for designing schemes that provide near optimal performance even when hidden
nodes are present. The primary reason for the failure of existing protocols in
the presence of hidden nodes is that these protocols are based on the model
developed by Bianchi. However this model does not hold when hidden nodes exist.
Moreover, analyzing networks with hidden nodes is still an open problem. Thus,
designing throughput optimal schemes in networks with hidden nodes is
particularly challenging. The novelty of our approach is that it is not based
on any underlying mathematical model, rather it directly tunes the control
variables so as to maximize the throughput. We demonstrate that this model
independent approach achieves maximum throughput in networks with hidden
terminals as well. Apart from this major contribution, we present stochastic
approximation based algorithms for achieving weighted fairness in a connected
networks. We also present a throughput optimal exponential backoff based random
access algorithm. We demonstrate that the exponential backoff based scheme may
outperform an optimal p-persistent scheme in networks with hidden terminals.
This demonstrates the merit of exponential backoff based random access schemes
which was deemed unnecessary by results shown by Bianchi.

ABSTRACT_BEGIN
  A new generation of "behavior-aware" delay tolerant networks is emerging in
what may define future mobile social networks. With the introduction of novel
behavior-aware protocols, services and architectures, there is a pressing need
to understand and realistically model mobile users behavioral characteristics,
their similarity and clustering. Such models are essential for the analysis,
performance evaluation, and simulation of future DTNs. This paper addresses
issues related to mobile user similarity, its definition, analysis and
modeling. To define similarity, we adopt a behavioral-profile based on users
location preferences using their on-line association matrix and its SVD, then
calculate the behavioral distance to capture user similarity. This measures the
difference of the major spatio-temporal behavioral trends and can be used to
cluster users into similarity groups or communities. We then analyze and
contrast similarity distributions of mobile user populations in two settings:
(i) based on real measurements from four major campuses with over ten thousand
users for a month, and (ii) based on existing mobility models, including random
direction and time-varying community models. Our results show a rich set of
similar communities in real mobile societies with distinct behavioral clusters
of users. This is true for all the traces studied, with the trend being
consistent over time. Surprisingly, however, we find that the existing mobility
models do not explicitly capture similarity and result in homogeneous users
that are all similar to each other. Thus the richness and diversity of user
behavioral patterns is not captured to any degree in the existing models. These
findings strongly suggest that similarity should be explicitly captured in
future mobility models, which motivates the need to re-visit mobility modeling
in the future.

ABSTRACT_BEGIN
  In the literature of Round-Robin scheduling scheme, each job is processed,
one after the another after giving a fix quantum. In case of First-come
first-served, each process is executed, if the previously arrived processed is
completed. Both these scheduling schemes are used in this paper as its special
cases. A Markov chain model is used to compare several scheduling schemes of
the class. An index measure is defined to compare the model based efficiency of
different scheduling schemes. One scheduling scheme which is the mixture of
FIFO and round robin is found efficient in terms of model based study. The
system simulation procedure is used to derive the conclusion of the content

ABSTRACT_BEGIN
  In this paper,a new design of wireless sensor network (WSN)node is discussed
which is based on components with ultra low power.We ha e de eloped a Low cost
and low power WSN Node using MSP430 and nRF24L01.The architectural circuit
details are presented.This architecture fulfils the requirements like low
cost,low power,compact size and self organization.Various tests are carried out
to test the performance of the nRF24L01 module.The packet loss,free Space loss
(FSL)and battery lifetime calculations are described.These test results will
help the researchers to build new applications using abo e node and to work
efficiently with nRF24L01.

ABSTRACT_BEGIN
  The associatie memory feature of the Hopfield type recurrent neural network
is used for the pattern storage and pattern authentication.This paper outlines
an optimization relaxation approach for signature verification based on the
Hopfield neural network (HNN)which is a recurrent network.The standard sample
signature of the customer is cross matched with the one supplied on the
Cheque.The difference percentage is obtained by calculating the different
pixels in both the images.The network topology is built so that each pixel in
the difference image is a neuron in the network.Each neuron is categorized by
its states,which in turn signifies that if the particular pixel is changed.The
network converges to unwavering condition based on the energy function which is
derived in experiments.The Hopfield's model allows each node to take on two
binary state values (changed/unchanged)for each pixel.The performance of the
proposed technique is evaluated by applying it in various binary and gray scale
images.This paper contributes in finding an automated scheme for verification
of authentic signature on bank Cheques.The derived energy function allows a
trade off between the influence of its neighborhood and its own criterion.This
device is able to recall as well as complete partially specified inputs.The
network is trained via a storage prescription that forces stable states to
correspond to (local)minima of a network "energy" function.

ABSTRACT_BEGIN
  In the faceless world of the Internet,online fraud is one of the greatest
reasons of loss for web merchants.Advanced solutions are needed to protect e
businesses from the constant problems of fraud.Many popular fraud detection
algorithms require supervised training,which needs human intervention to
prepare training cases.Since it is quite often for an online transaction
database to ha e Terabyte level storage,human investigation to identify
fraudulent transactions is very costly.This paper describes the automatic
design of user profiling method for the purpose of fraud detection.We use a FP
(Frequent Pattern) Tree rule learning algorithm to adaptively profile
legitimate customer behavior in a transaction database.Then the incoming
transactions are compared against the user profile to uncover the anomalies The
anomaly outputs are used as input to an accumulation system for combining
evidence to generate high confidence fraud alert value. Favorable experimental
results are presented.

ABSTRACT_BEGIN
  Reliable transport protocols such as TCP are tuned to perform well in
traditional networks where packet losses occur mostly because of congestion.
Many applications of wireless sensor networks are useful only when connected to
an external network. Previous research on transport layer protocols for sensor
networks has focused on designing protocols specifically targeted for sensor
networks. The deployment of TCP/IP in sensor networks would, however, enable
direct connection between the sensor network and external TCP/IP networks. In
this paper we focus on the performance of TCP in the context of wireless sensor
networks. TCP is known to exhibit poor performance in wireless environments,
both in terms of throughput and energy efficiency. To overcome these problems
we introduce a mechanism called TCP Segment Caching .We show by simulation that
TCP Segment Caching significantly improves TCP Performance so that TCP can be
useful e en in wireless sensor

ABSTRACT_BEGIN
  Computer system models provide detailed answer to system performance.In this
paper a two stage tandem network system with Blocking and Feedback is
considered and it performance has been analyzed by spectral expansion
method.The study state system with balance equations has been discussed.

ABSTRACT_BEGIN
  Campus Grid computing involves heterogeneous resources of an organization
working in collaboration to sol e the problems that cannot be addressed by a
single resource. However, basic problem for Campus Grid users is how to disco
er the best resources required for the particular type of a job. There are
various approaches using which Grid Discovery can be performed. This paper pro
ides the grid resource discovery solutions for Campus Grid using Globus Toolkit
which will enable us to customize the resource information according to the
requirements based on the jobs to be run on the Campus Grid and present it in
our own format. Here we propose building up our own service on top of globus
MDS in order to process the information provided by MDS and use it in our
Campus Grid Portal.

ABSTRACT_BEGIN
  computers into the real world, to serve humans where the ubiquitous network
is the underneath infrastructure. In order to provide ubiquitous services
(u-Service) which deliver useful information to service users without human
intervention, this paper implements a proactive information delivery system
using Bluetooth technology. Bluetooth is a lowpowered networking service that
supports several protocol profiles, most importantly file transfer.Combined
together, ubiquitous computing and Bluetooth ha e the potential to furnish
ubiquitous solutions (u-Solutions) that are efficient, employ simplified design
characteristics, and collaboratively perform functions they are otherwise not
capable. Thus, this paper first addresses the current Bluetooth technology.
Then, it suggests and develops the proactive information delivery system
utilizing Bluetooth and ubiquitous computing network concepts. The proactive
information delivery system can be used in many ubiquitous applications such as
ubiquitous commerce (u-Commerce) and ubiquitous education (u- Education)

ABSTRACT_BEGIN
  The Model / View / Controller design pattern divides an application
environment into three components to handle the user-interactions, computations
and output respectively. This separation greatly favors architectural
reusability. The pattern works well in the case of single-address space and not
proven to be efficient for web applications involving multiple address spaces.
Web applications force the designers to decide which of the components of the
pattern are to be partitioned between the server and client(s) before the
design phase commences. For any rapidly growing web application, it is very
difficult to incorporate future changes in policies related to partitioning.
One solution to this problem is to duplicate the Model and controller
components at both server and client(s). However, this may add further problems
like delayed data fetch, security and scalability issues. In order to overcome
this, a new architecture SPIM has been proposed that deals with the
partitioning problem in an alternative way. SPIM shows tremendous improvements
in performance when compared with a similar architecture.

ABSTRACT_BEGIN
  IEEE 802.16 standard supports two different topologies: point to multipoint
(PMP) and Mesh. In this paper, a QoS mechanism for point to multipoint of IEEE
802.16 and BS scheduler for PMP Mode is proposed. This paper also describes
quality of service over WiMAX networks. Average WiMAX delay, Average WiMAX load
and Average WiMAX throughput at base station is analyzed and compared by
applying different scheduler at Base station and at fixed nodes.

ABSTRACT_BEGIN
  As the technology rapidly grows, the trend is clear that the use of mobile
devices is gain an attention nowadays, thus designing a system by integrating
it with notification feature is becoming an important aspect especially in
tracking and monitoring system. Conventional security surveillance systems
require the constant attention from the user, to monitor the location
concurrently. In order to reduce the cost of computing power and advance
technology of mobile phone in widespread acceptance of the Internet as a viable
communication medium, this paper is aimed to design a low cost web-based system
as a platform to view the image captured. When the network camera detects any
movement from the intruders, it automatically captures the image and sends it
to the database of the web-based directly by the network through File Transfer
Protocol (FTP). The camera is attached through an Ethernet connection and power
source. Therefore, the camera can be viewed from either standard Web browser or
cell phone. Nowadays, when a security camera is installed, user is notified as
long as the camera is switched on since any slight movement requires the
attention of the supervisor. The utility of the system has proven
theoretically. This system will also notify the user by sending a notification
through Short Messages Services (SMS).

ABSTRACT_BEGIN
  Denial-of-service (DOS) attacks increasingly gained reputation over the past
few years. As the Internet becomes more ubiquitous, the threat of the
denial-of-service attacks becomes more realistic and important for individuals,
businesses, governmental organizations, and even countries. There is intensive
need to detect an attack in progress as soon as possible. The efficiency of
diagnosing the DOS attack using concepts of queuing theory and performance
parameter of the system has been investigated in the present work, as the
servers definitely have some mechanisms to store and process the requests.
Utilizing this concept of queuing theory, the collection of data patterns were
generated. With the performance parameter of the system, the analysis of the
data pattern had been made to diagnose the network anomaly. Performance
analysis and results show the accuracy of the proposed scheme in detecting
anomalies.

ABSTRACT_BEGIN
  The widespread emergence of the Internet as a platform for electronic data
distribution and the advent of structured information have revolutionized our
ability to deliver information to any corner of the world. Although Service
Oriented Architecture (SOA) is a paradigm for organizing and utilizing
distributed capabilities that may be under the control of different ownership
domains and implemented using various technology stacks and every organization
may not be geared up for this. To harness the various software / service
resources placed on various systems, we have proposed and implemented a model
that is able to establish discovery and sharing in load balanced P-grid
environment. The experimental results show that the proposed approach has
dramatically lowered the network traffic (nearly negligible), while achieving
load balancing in P2P grid systems. Our model is able to support discovery and
sharing of resources also.

ABSTRACT_BEGIN
  Last year, the official BitTorrent client switched to LEDBAT, a new
congestion control algorithm targeting a lower-than Best Effort transport
service. In this paper, we study this new protocol through packet-level
simulations, with a special focus on a performance comparison with other
lower-than Best Effort protocols such as TCP-LP and TCP-NICE: our aim is indeed
to quantify and relatively weight the level of Low-priority provided by such
protocols.
  Our results show that LEDBAT transport generally achieves the lowest possible
level of priority, with the default configurations of TCP-NICE and TCP-LP
representing increasing levels of aggressiveness. In addition, we perform a
careful sensitivity analysis of LEDBAT performance, by tuning its main
parameters in both inter-protocol (against TCP) and intra-protocol (against
LEDBAT itself) scenarios. In the inter-protocol case, although in case of
misconfiguration LEDBAT competes as aggressively as TCP, however we show that
it is not possible to achieve an arbitrary level of low-priority by merely
tuning its parameters. In the intra-protocol case, we show that coexistence of
legacy flows with slightly dissimilar settings, or experiencing different
network conditions, can result in significant unfairness.

ABSTRACT_BEGIN
  BitTorrent developers have recently introduced a new application layer
congestion control algorithm based on UDP framing at transport layer and
currently under definition at the IETF LEDBAT Working Group. LEDBAT is a
delay-based protocol which aims at offering a "lower than Best Effort" data
transfer service, with a lower priority with respect to elastic TCP and
interactive traffic (e.g., VoIP, game). However, in its current specification,
LEDBAT is affected by a late-comer advantage: indeed the last flow arriving at
the bottleneck is more aggressive due to a wrong estimation of the base delay
and finally takes over all resources. In this work, we study several solutions
to the late-comer problem by means of packet level simulations and simple
analysis: in the investigation process, we individuate the root cause for
LEDBAT unfairness and propose effective countermeasures.

ABSTRACT_BEGIN
  This paper presents a thorough survey of recent work addressing energy
efficient multicast routing protocols and secure multicast routing protocols in
Mobile Ad hoc Networks (MANETs). There are so many issues and solutions which
witness the need of energy management and security in ad hoc wireless networks.
The objective of a multicast routing protocol for MANETs is to support the
propagation of data from a sender to all the receivers of a multicast group
while trying to use the available bandwidth efficiently in the presence of
frequent topology changes. Multicasting can improve the efficiency of the
wireless link when sending multiple copies of messages by exploiting the
inherent broadcast property of wireless transmission. Secure multicast routing
plays a significant role in MANETs. However, offering energy efficient and
secure multicast routing is a difficult and challenging task. In recent years,
various multicast routing protocols have been proposed for MANETs. These
protocols have distinguishing features and use different mechanisms

ABSTRACT_BEGIN
  In sensor networks communication by broadcast methods involves many hazards,
especially collision. Several MAC layer protocols have been proposed to resolve
the problem of collision namely ARBP, where the best achieved success rate is
90%. We hereby propose a MAC protocol which achieves a greater success rate
(Success rate is defined as the percentage of delivered packets at the source
reaching the destination successfully) by reducing the number of collisions,
but by trading off the average propagation delay of transmission. Our proposed
protocols are also shown to be more energy efficient in terms of energy
dissipation per message delivery, compared to the currently existing protocol.

ABSTRACT_BEGIN
  Managing network complexity, accommodating greater numbers of subscribers,
improving coverage to support data services (e.g. email, video, and music
downloads), keeping up to speed with fast-changing technology, and driving
maximum value from existing networks - all while reducing CapEX and OpEX and
ensuring Quality of Service (QoS) for the network and Quality of Experience
(QoE) for the user. These are just some of the pressing business issues faced
by mobileservice providers, summarized by the demand to "achieve more, for
less." The ultimate goal of optimization techniques at the network and
application layer is to ensure End-user perceived QoS. The next generation
networks (NGN), a composite environment of proven telecommunications and
Internet-oriented mechanisms have become generally recognized as the
telecommunications environment of the future. However, the nature of the NGN
environment presents several complex issues regarding quality assurance that
have not existed in the legacy environments (e.g., multi-network, multi-vendor,
and multi-operator IP-based telecommunications environment, distributed
intelligence, third-party provisioning, fixed-wireless and mobile access,
etc.). In this Research Paper, a service aware policy-based approach to NGN
quality assurance is presented, taking into account both perceptual quality of
experience and technologydependant quality of service issues. The respective
procedures, entities, mechanisms, and profiles are discussed. The purpose of
the presented approach is in research, development, and discussion of pursuing
the end-to-end controllability of the quality of the multimedia NGN-based
communications in an environment that is best effort in its nature and promotes
end user's access agnosticism, service agility, and global mobility.

ABSTRACT_BEGIN
  Asterisk and Open IMS use SIP signal protocol to enable both of them can be
connected. To facilitate both relationships, Enum server- that is able to
translate the numbering address such as PSTN (E.164) to URI address (Uniform
Resource Identifier)- can be used. In this research, we interconnect Open IMS
and Asterisk server Enum server. We then analyze the server performance and PDD
(Post Dial Delay) values resulted by the system. As the result of the
experiment, we found that, for a call from Open IMS user to analog Asterisk
telephone (FXS) with a arrival call each servers is 30 call/sec, the maximum
PDD value is 493.656 ms. Open IMS is able to serve maximum 30 call/s with
computer processor 1.55 GHz, while the Asterisk with computer processor 3.0
GHz, may serve up to 55 call/sec. Enum on server with 1.15 GHz computer
processor have the capability of serving maximum of 8156 queries/sec.

ABSTRACT_BEGIN
  In this paper we are proposing a new concept in MAC layer protocol design for
Cognitive radio by combining information held by physical layer and MAC layer
with analytical engine based on knowledge based reasoning approach. In the
proposed system a cross layer information regarding signal to interference and
noise ratio (SINR) and received power are analyzed with help of knowledge based
reasoning system to determine minimum power to transmit and size of contention
window, to minimize backoff, collision, save power and drop packets. The
performance analysis of the proposed protocol indicates improvement in power
saving, lowering backoff and significant decrease in number of drop packets.
The simulation environment was implement using OMNET++ discrete simulation tool
with Mobilty framework and MiXiM simulation library.

ABSTRACT_BEGIN
  Nowadays, It has been shown that spectrum scarcity increased due to
tremendous growth of new players in wireless base system by the evolution of
the radio communication. Resent survey found that there are many areas of the
radio spectrum that are occupied by authorized user/primary user (PU), which
are not fully utilized. Cognitive radios (CR) prove to next generation wireless
communication system that proposed as a way to reuse this under-utilised
spectrum in an opportunistic and non-interfering basis. A CR is a self-directed
entity in a wireless communications environment that senses its environment,
tracks changes, and reacts upon its findings and frequently exchanges
information with the networks for secondary user (SU). However, CR facing
collision problem with tracks changes i.e. reallocating of other empty channels
for SU while PU arrives. In this paper, channels reallocation technique based
on DNA sequence alignment algorithm for CR networks has been proposed.

ABSTRACT_BEGIN
  During the past few years, advances in mobile communication theory have
enabled the development and deployment of different wireless technologies,
complementary to each other. Hence, their integration can realize a unified
wireless system that has the best features of the individual networks.
Next-Generation Wireless Systems (NGWS) integrate different wireless systems,
each of which is optimized for some specific services and coverage area to
provide ubiquitous communications to the mobile users. In this paper, we
propose to enhance the handoff performance of mobile IP in wireless IP networks
by reducing the false handoff probability in the NGWS handoff management
protocol. Based on the information of false handoff probability, we analyze its
effect on mobile speed and handoff signaling delay.

ABSTRACT_BEGIN
  Delay/Disruption-Tolerant Network (DTN) protocols typically address sparse
intermittently connected networks whereas Mobile Ad-hoc Network (MANET)
protocols address the fairly stable and fully connected ones. But many
intermediate situations may occur on mobility dynamics or radio link
instability. In such cases, where the network frequently splits into evolving
connected groups, none of the conventional routing paradigms (DTN or MANET) are
fully satisfactory. In this paper we propose HYMAD, a Hybrid DTN-MANET routing
protocol which uses DTN between disjoint groups of nodes while using MANET
routing within these groups. HYMAD is fully decentralized and only makes use of
topological information exchanges between the nodes. The strength of HYMAD lies
in its ability to adapt to the changing connectivity patterns of the network.
We evaluate the scheme in simulation by replaying synthetic and real life
mobility traces which exhibit a broad range of connectivity dynamics. The
results show that HYMAD introduces limited overhead and outperforms the
multi-copy Spray-and-Wait DTN routing protocol it extends, both in terms of
delivery ratio and delay. This hybrid DTN-MANET approach offers a promising
venue for the delivery of elastic data in mobile ad-hoc networks as it retains
the resilience of a \textit{pure} DTN protocol while significantly improving
performance.

ABSTRACT_BEGIN
  Energy consumption and delay incurred in packet delivery are the two
important metrics for measuring the performance of geographic routing protocols
for Wireless Adhoc and Sensor Networks (WASN). A protocol capable of ensuring
both lesser energy consumption and experiencing lesser delay in packet delivery
is thus suitable for networks which are delay sensitive and energy hungry at
the same time. Thus a smart packet forwarding technique addressing both the
issues is thus the one looked for by any geographic routing protocol. In the
present paper we have proposed a Fermat point based forwarding technique which
reduces the delay experienced during packet delivery as well as the energy
consumed for transmission and reception of data packets.

ABSTRACT_BEGIN
  In this paper we analytically propose an alternative approach to achieve
better fairness in scheduling mechanisms which could provide better quality of
service particularly for real time application. Our proposal oppose the
allocation of the bandwidth which adopted by all previous scheduling mechanism.
It rather adopt the opposition approach be proposing the notion of
Maxmin-charge which fairly distribute the congestion. Furthermore, analytical
proposition of novel mechanism named as Just Queueing is been demonstrated.

ABSTRACT_BEGIN
  Hahn and Wallsten wrote that network neutrality "usually means that broadband
service providers charge consumers only once for Internet access, do not favor
one content provider over another, and do not charge content providers for
sending information over broadband lines to end users." In this paper we study
the implications of non-neutral behaviors under a simple model of linear
demand-response to usage-based prices. We take into account advertising
revenues and consider both cooperative and non-cooperative scenarios. In
particular, we model the impact of side-payments between service and content
providers. We also consider the effect of service discrimination by access
providers, as well as an extension of our model to non-monopolistic content
providers.

ABSTRACT_BEGIN
  In this paper, we study the optimal secondary-link beamforming pattern that
balances between the SU's throughput and the interference it causes to PUs in
MIMO cognitive radio networks. In particular, we aim to maximize the throughput
of the SU, while keeping the interference temperature at the primary receivers
below a certain threshold.
  Unlike traditional MIMO systems, SUs may not have the luxury of knowing the
channel state information (CSI) on the links to PUs. This presents a key
challenge for a secondary transmitter to steer interference away from primary
receivers. In this paper, we consider three scenarios, namely when the
secondary transmitter has complete, partial, or no knowledge about the channels
to the primary receivers. In particular, when complete CSI is not available,
the interference-temperature constraints are to be satisfied with high
probability, thus resulting in chance constraints that are typically hard to
deal with. Our contribution is fourfold. First, by analyzing the distributional
characteristics of MIMO channels, we propose a unified homogeneous QCQP
formulation that can be applied to all three scenarios. The homogeneous QCQP
formulation, though non-convex, is amenable to semidefinite programming (SDP)
relaxation methods. Secondly, we show that the SDP relaxation admits no gap
when the number of primary links is no larger than two. Thirdly, we propose a
randomized polynomial-time algorithm for constructing a near-optimal solution
to the QCQP problem when there are more than two primary links. Finally, we
show that when the secondary transmitter has no CSI on the links to primary
receivers, the optimal solution to the QCQP problem can be found by a simple
matrix eigenvalue-eigenvector computation, which can be done much more
efficiently than solving the QCQP directly.

ABSTRACT_BEGIN
  Multi-packet reception (MPR) has been recognized as a powerful
capacity-enhancement technique for random-access wireless local area networks
(WLANs). As is common with all random access protocols, the wireless channel is
often under-utilized in MPR WLANs. In this paper, we propose a novel
multi-round contention random-access protocol to address this problem. This
work complements the existing random-access methods that are based on
single-round contention. In the proposed scheme, stations are given multiple
chances to contend for the channel until there are a sufficient number of
``winning" stations that can share the MPR channel for data packet
transmission. The key issue here is the identification of the optimal time to
stop the contention process and start data transmission. The solution
corresponds to finding a desired tradeoff between channel utilization and
contention overhead. In this paper, we conduct a rigorous analysis to
characterize the optimal strategy using the theory of optimal stopping. An
interesting result is that the optimal stopping strategy is a simple
threshold-based rule, which stops the contention process as soon as the total
number of winning stations exceeds a certain threshold. Compared with the
conventional single-round contention protocol, the multi-round contention
scheme significantly enhances channel utilization when the MPR capability of
the channel is small to medium. Meanwhile, the scheme automatically falls back
to single-round contention when the MPR capability is very large, in which case
the throughput penalty due to random access is already small even with
single-round contention.

ABSTRACT_BEGIN
  Interference at the radio receiver is a key source of degradation in quality
of service of wireless communication systems. This paper presents a unified
framework for OFDM/FBMC interference characterization and analysis in
asynchronous environment. Multi-user interference is caused by the timing
synchronization errors which lead to the destruction of the orthogonality
between subcarriers. In this paper, we develop a theoretical analysis of the
asynchronous interference considering the multi-path effects on the
interference signal. We further propose an accurate model for interference that
provides a useful computational tool in order to evaluate the performance of an
OFDM/FBMC system in a frequency selective fading environment. Finally,
simulation results confirmed the accuracy of the proposed model.

ABSTRACT_BEGIN
  In this work, we study the target detection and tracking problem in mobile
sensor networks, where the performance metrics of interest are probability of
detection and tracking coverage, when the target can be stationary or mobile
and its duration is finite. We propose a physical coverage-based mobility
model, where the mobile sensor nodes move such that the overlap between the
covered areas by different mobile nodes is small. It is shown that for
stationary target scenario the proposed mobility model can achieve a desired
detection probability with a significantly lower number of mobile nodes
especially when the detection requirements are highly stringent. Similarly,
when the target is mobile the coverage-based mobility model produces a
consistently higher detection probability compared to other models under
investigation.

ABSTRACT_BEGIN
  In this paper we show that in a multiclass Markovian network with unit rate
servers, the condition that the average load $\rho$ at every server is less
than unity is indeed sufficient for the stability or positive recurrence for
\emph{any} work conserving scheduling policy and \emph{class-independent}
routing. We use a variation of the positive recurrence criterion for
multidimensional discrete-time Markov chains over countable state spaces due to
Rosberg (JAP, Vol.~17, No.~3, 1980) and a monotonicity argument to establish
this assertion.

ABSTRACT_BEGIN
  Adaptive OFDMA has recently been recognized as a promising technique for
providing high spectral efficiency in future broadband wireless systems. The
research over the last decade on adaptive OFDMA systems has focused on adapting
the allocation of radio resources, such as subcarriers and power, to the
instantaneous channel conditions of all users. However, such "fast" adaptation
requires high computational complexity and excessive signaling overhead. This
hinders the deployment of adaptive OFDMA systems worldwide. This paper proposes
a slow adaptive OFDMA scheme, in which the subcarrier allocation is updated on
a much slower timescale than that of the fluctuation of instantaneous channel
conditions. Meanwhile, the data rate requirements of individual users are
accommodated on the fast timescale with high probability, thereby meeting the
requirements except occasional outage. Such an objective has a natural chance
constrained programming formulation, which is known to be intractable. To
circumvent this difficulty, we formulate safe tractable constraints for the
problem based on recent advances in chance constrained programming. We then
develop a polynomial-time algorithm for computing an optimal solution to the
reformulated problem. Our results show that the proposed slow adaptation scheme
drastically reduces both computational cost and control signaling overhead when
compared with the conventional fast adaptive OFDMA. Our work can be viewed as
an initial attempt to apply the chance constrained programming methodology to
wireless system designs. Given that most wireless systems can tolerate an
occasional dip in the quality of service, we hope that the proposed methodology
will find further applications in wireless communications.

ABSTRACT_BEGIN
  Thanks to its simplicity and cost efficiency, wireless local area network
(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless
services in hot spots and indoor environments. Traditional WLAN
medium-access-control (MAC) protocols assume that only one station can transmit
at a time: simultaneous transmissions of more than one station cause the
destruction of all packets involved. By exploiting recent advances in PHY-layer
multiuser detection (MUD) techniques, it is possible for a receiver to receive
multiple packets simultaneously. This paper argues that such multipacket
reception (MPR) capability can greatly enhance the capacity of future WLANs. In
addition, the paper provides the MAC-layer and PHY-layer designs needed to
achieve the improved capacity. First, to demonstrate MPR as a powerful
capacity-enhancement technique, we prove a "super-linearity" result, which
states that the system throughput per unit cost increases as the MPR capability
increases. Second, we show that the commonly deployed binary exponential
backoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR
system, and that the optimal backoff factor increases with the MPR capability,
the number of packets that can be received simultaneously. Third, based on the
above insights, we design a joint MAC-PHY layer protocol for an IEEE
802.11-like WLAN that incorporates advanced PHY-layer signal processing
techniques to implement MPR.

ABSTRACT_BEGIN
  Network latency and packet loss are considered to be an important requirement
for realistic evaluation of Peer-to-Peer protocols. Dedicated clusters, such as
Grid'5000, do not provide the variety of network latency and packet loss rates
that can be found in the Internet. However, compared to the experiments
performed on testbeds such as PlanetLab, the experiments performed on dedicated
clusters are reproducible, as the computational resources are not shared. In
this paper, we perform experiments to study the impact of network latency and
packet loss on the time required to download a file using BitTorrent. In our
experiments, we observe a less than 15% increase on the time required to
download a file when we increase the round-trip time between any two peers,
from 0 ms to 400 ms, and the packet loss rate, from 0% to 5%. Our main
conclusion is that the underlying network latency and packet loss have a
marginal impact on the time required to download a file using BitTorrent.
Hence, dedicated clusters such as Grid'5000 can be safely used to perform
realistic and reproducible BitTorrent experiments.

ABSTRACT_BEGIN
  The increasing demand for reliable Web applications gives a central role to
Web testing. Most of the existing works are focused on the definition of novel
testing techniques, specifically tailored to the Web. However, no attempt was
carried out so far to understand the specific nature of Web faults. This paper
presents a user session based testing technique that clusters user sessions
based on the service profile and selects a set of representative user sessions
from each cluster and tailored by augmentation with additional requests to
cover the dependence relationships between web pages. The created suite not
only can significantly reduce the size of the collected user sessions, also
viable to exercise fault sensitive paths. The results demonstrate that our
approach consistently detected the majority of known faults using a relatively
small number of test cases and will be a powerful system when more and more
user sessions are being clustered.

ABSTRACT_BEGIN
  A mobile agent is a program that is not bound to the system on which it began
execution, but rather travels amongst the hosts in the network with its code
and current execution state (i.e. Distributed Environment).The implementation
of distributed applications can be based on a multiplicity of technologies,
e.g. plain sockets, Remote Procedure Call (RPC), Remote Method Invocation
(RMI), Java Message Service (JMS), .NET Remoting, or Web Services. These
technologies differ widely in complexity, interoperability, standardization,
and ease of use. The Mobile Agent technology is emerging as an alternative to
build a smart generation of highly distributed systems. In this work, we
investigate the performance aspect of agent-based technologies for information
retrieval. We present a comparative performance evaluation model of Mobile
Agents versus .Net remoting by means of an analytical approach. A quantitative
measurements are performed to compare .Net remoting and mobile agents using
communication time, code size (agent code), Data size, number of node as
performance parameters in this research work. The results depict that Mobile
Agent paradigm offers a superior performance compared to .Net remoting
paradigm, offers fast computational speed; procure lower invocation cost by
making local invocations instead of remote invocations over the network,
thereby reducing network bandwidth.

ABSTRACT_BEGIN
  Peer-to-peer (P2P) networks establish loosely coupled application-level
overlays on top of the Internet to facilitate efficient sharing of resources.
It can be roughly classified as either structured or unstructured networks.
Without stringent constraints over the network topology, unstructured P2P
networks can be constructed very efficiently and are therefore considered
suitable to the Internet environment. However, the random search strategies
adopted by these networks usually perform poorly with a large network size. To
enhance the search performance in unstructured P2P networks through exploiting
users' common interest patterns captured within a probability-theoretic
framework termed the user interest model (UIM). A search protocol and a routing
table updating protocol are further proposed in order to expedite the search
process through self organizing the P2P network into a small world. Both
theoretical and experimental analyses are conducted and demonstrated the
effectiveness and efficiency of the approach.

ABSTRACT_BEGIN
  These Network Coding improves the network operation beyond the traditional
routing or store-and-forward, by mixing of data stream within a network.
Network coding techniques explicitly minimizes the total no of transmission in
wireless network. The Coding-aware routing maximizes the coding opportunity by
finding the coding possible path for every packet in the network. Here we
propose CORMEN: a new coding-aware routing mechanism based on opportunistic
routing. In CORMEN, every node independently can take the decision whether to
code packets or not and forwarding of packets is based on the coding
opportunity available.

ABSTRACT_BEGIN
  Wireless ad hoc networks are power constrained since nodes operate with
limited battery energy. Thus, energy consumption is crucial in the design of
new ad hoc routing protocols. In order to maximize the lifetime of ad hoc
networks, traffic should be sent via a route that can be avoid nodes with low
energy. In addition, considering that the nodes of ad hoc networks are mobile,
it is possible that a created path is broken because of nodes mobility and
establishment of a new path would be done again. This is because of sending
additional control packets, accordingly, energy consumption increases. Also, it
should avoid nodes which have more buffered packets. Maybe, because of long
queue, some of these packets are dropped and transmitted again. This is the
reason for wasting of energy. In this paper we propose a new energy efficient
algorithm, that uses a new cost function and avoid nodes with characteristics
which mentioned above. We show that this algorithm improves the network energy
consumption by using this new cost function.

ABSTRACT_BEGIN
  We consider the problem of scheduling in multihop wireless networks subject
to interference constraints. We consider a graph based representation of
wireless networks, where scheduled links adhere to the K-hop link interference
model. We develop a distributed greedy heuristic for this scheduling problem.
Further, we show that this distributed greedy heuristic computes the exact same
schedule as the centralized greedy heuristic.

ABSTRACT_BEGIN
  Spectrum is a scarce commodity, and considering the spectrum scarcity faced
by the wireless-based service providers led to high congestion levels.
Technical inefficiencies from pooled spectrum (this is nothing but the "common
carrier principle" adopted in oil/gas/electricity pipelines/networks.), since
all ad hoc networks share a common pool of channels, exhausting the available
channels will force ad hoc networks to block the services. Researchers found
that cognitive radio (CR) technology may resolve the spectrum scarcity. CR
network proved to next generation wireless communication system that proposed
as a way to reuse under-utilised spectrum of licensee user (primary network) in
an opportunistic and non-interfering basis. A CR is a self-configuring entity
in a wireless networking that senses its environment, tracks changes, and
frequently exchanges information with their networks. Adding this layer of such
intelligence to the ad hoc network by looking at the overall geography of the
network known as cognitive radio ad hoc networks (CRAHNs). However, CRAHN
facing challenges and condition become worst while tracks changes i.e.
reallocation of another under-utilised channels while primary network user
arrives. In this paper, channels or resource reallocation technique based on
bio-inspired computing algorithm for CRAHN has been proposed.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks is one of the most challenging research area in the
field of Mobile Ad Hoc Networks, in this research we propose a flexible,
simple, and scalable design for revocation list distribution in VANET, which
will reduce channel overhead and eliminate the use of CRL. Also it will
increase the security of the network and helps in identifying the adversary
vehicles.

ABSTRACT_BEGIN
  Inferring plausible node mobility based only on information from wireless
contact traces is a difficult problem. Working with mobility information allows
richer protocol simulations, particularly in dense networks, but requires
complex set-ups to measure. On the other hand, contact information is easier to
measure but only allows for simplistic simulation models. In a contact trace a
lot of node movement information is irretrievably lost so the original
positions and velocities are in general out of reach. In this paper, we propose
a fast heuristic algorithm, inspired by dynamic force-based graph drawing,
capable of inferring a plausible movement from any contact trace, and evaluate
it on both synthetic and real-life contact traces. Our results reveal that (i)
the quality of the inferred mobility is directly linked to the precision of the
measured contact trace, and (ii) the simple addition of appropriate
anticipation forces between nodes leads to an accurate inferred mobility.

ABSTRACT_BEGIN
  Use of ARIMA model in Sensor network The basic idea of our energy efficient
information collection scheme is to suppress data transmission if the data
sampled by sensor nodes are predictable by the sink node. This is done in two
phases 1) Preliminary Data Collection- During this phase sink node collects
enough data so that it can build up ARIMA model for each node. Then sink node
selects a model for the particular node and sends back the corresponding model
parameters to the node and also keeps them with it. Selecting the model for a
node there is a tradeoff between energy consumption and accuracy of prediction.
So we choose the model according to C = {\alpha} xMAE + (1 - {\alpha}) x rtran
0=< {\alpha} =<1 where the model should minimize C. Here MAE is Mean Absolute
Error which is normalized by some predefined error tolerance and rtran is the
ratio of number of samples transmitted over total number of samples. 2)
Adaptive Data Collection- After the sensor node has received the model
parameters it checks each actual data value with the data value calculated from
the parameters received. If there is deviation beyond some predefined error
tolerance then only it sends the original data value to the sink node.

ABSTRACT_BEGIN
  In this paper, we propose a channel assortment strategy for Reliable
Communication in Multi-Hop Cognitive Radio Networks.

ABSTRACT_BEGIN
  In this paper, we propose an adaptive and occupancy-based channel selection
for unreliable cognitive radio networks.

ABSTRACT_BEGIN
  In this paper, we propose a cognitive radio based Internet access framework
for disaster response network deployment in challenged environments. The
proposed architectural framework is designed to help the existent but partially
damaged networks to restore their connectivity and to connect them to the
global Internet. This architectural framework provides the basis to develop
algorithms and protocols for the future cognitive radio network deployments in
challenged environments.

ABSTRACT_BEGIN
  This paper introduces a new channel selection strategy for reliable
contentionaware data dissemination in multi-hop cognitive radio network. The
key challenge here is to select channels providing a good tradeoff between
connectivity and contention. In other words, channels with good opportunities
for communication due to (1) low primary radio nodes (PRs) activities, and (2)
limited contention of cognitive ratio nodes (CRs) acceding that channel, have
to be selected. Thus, by dynamically exploring residual resources on channels
and by monitoring the number of CRs on a particular channel, SURF allows
building a connected network with limited contention where reliable
communication can take place. Through simulations, we study the performance of
SURF when compared with three other related approaches. Simulation results
confirm that our approach is effective in selecting the best channels for
efficient and reliable multi-hop data dissemination.

ABSTRACT_BEGIN
  The high-level contribution of this paper is a simulation-based detailed
performance comparison of three different classes of routing protocols for
mobile ad hoc networks: stability-based routing, power-aware routing and
load-balanced routing. We choose the Flow-Oriented Routing protocol (FORP), the
traffic interference based Load Balancing Routing (LBR) protocol and Min-Max
Battery Cost Routing (MMBCR) as representatives of the stability-based routing,
load-balancing and power-aware routing protocols respectively. Among the three
routing protocols, FORP incurs the least number of route transitions; while LBR
incurs the smallest hop count and lowest end-to-end delay per data packet.
Energy consumed per node is the least for MMBCR, closely followed by LBR. MMBCR
is the most fair in terms of node usage and hence it incurs the largest time
for first node failure. FORP tends to repeatedly use nodes lying on the stable
path and hence is the most unfair of the three routing protocols and it incurs
the smallest value for the time of first node failure. As we measure the
failure times of up to the first five nodes in the network, we observe that LBR
incurs the maximum improvement in the lifetime of the nodes and MMBCR incurs
the least improvement beyond the time of first node failure.

ABSTRACT_BEGIN
  In IEEE 802.11 Wireless Local Area Networks (WLANs), network nodes
experiencing collisions on the shared channel need to backoff for a random
period of time, which is uniformly selected from the Contention Window (CW).
This contention window is dynamically controlled by the Backoff algorithm.
First step to design a an efficient backoff algorithm for multi-hop ad hoc
network is to analysis of the existing backoff algorithms in multi-hop ad hoc
networks. Thus, in this paper, we considered two important multi-hop adhoc
network scenarios: (a) Node Mobility Scenario and (b) Transmission Range
Scenario and analyze and evaluate both the impact of mobility (i.e. node speed)
and the impact of transmission range of nodes on the performance of various
backoff algorithms

ABSTRACT_BEGIN
  Encryption study basically deals with three levels of algorithms. The first
algorithm deals with encryption mechanism, second deals with decryption
Mechanism and the third discusses about the generation of keys and sub keys
used in the encryption study. In the given study, a new algorithm is discussed.
The algorithm executes a series of steps and generates a sequence. This
sequence is being used as sub key to be mapped to plain text to generate cipher
text. The strength of the encryption & Decryption process depends on the
strength of sequence generated against crypto analysis.. In this part of work
some statistical tests like Uniformity tests, Universal tests & Repetition
tests are tried on the sequence generated to test the strength of it.

ABSTRACT_BEGIN
  Security and Privacy are two important parameters that need to be considered
when dealing with wireless sensor networks as WSN operate in an unattended
environment and carry sensitive information critical to the application.
However, applying security techniques that consume minimum resources is still a
challenge and this paper makes an attempt to address the same. One of the major
attacks in sensor network is denial of service attack that not only diminishes
the network capacity but also affects the reliability of information being
transmitted. This work is an extension of our previous work which could
successfully detect DDoS using ants. However, no emphasis was made towards the
prevention mechanism. In this paper an ant-based framework that exploits the
significance of stateless and stateful signatures and hence preserving the
legtimate packets only, thereby discarding the contaminated packets has been
proposed.

ABSTRACT_BEGIN
  A significant technical challenge in deploying femtocells is controlling the
interference from the underlay of femtos onto the overlay of macros. This paper
presents a novel interference control method where the macrocell bandwidth is
partitioned into subbands, and the short-range femtocell links adaptively
allocate their power across the subbands based on a load-spillage power control
method. The scheme can improve rate distribution in the macro network while
also providing opportunities for short-range communication as well. Moreover,
the proposed scheme requires minimal interference coordination communication
between the femtos and macros, which is one of the main challenges in femtocell
systems. Also, simulations show certain advantages over simpler
orthogonalization schemes or power control schemes without subband
partitioning. Further modest gains may also be possible with interference
cancelation.

ABSTRACT_BEGIN
  The common utilization-based definition of available bandwidth and many of
the existing tools to estimate it suffer from several important weaknesses: i)
most tools report a point estimate of average available bandwidth over a
measurement interval and do not provide a confidence interval; ii) the commonly
adopted models used to relate the available bandwidth metric to the measured
data are invalid in almost all practical scenarios; iii) existing tools do not
scale well and are not suited to the task of multi-path estimation in
large-scale networks; iv) almost all tools use ad-hoc techniques to address
measurement noise; and v) tools do not provide enough flexibility in terms of
accuracy, overhead, latency and reliability to adapt to the requirements of
various applications. In this paper we propose a new definition for available
bandwidth and a novel framework that addresses these issues. We define
probabilistic available bandwidth (PAB) as the largest input rate at which we
can send a traffic flow along a path while achieving, with specified
probability, an output rate that is almost as large as the input rate. PAB is
expressed directly in terms of the measurable output rate and includes
adjustable parameters that allow the user to adapt to different application
requirements. Our probabilistic framework to estimate network-wide
probabilistic available bandwidth is based on packet trains, Bayesian
inference, factor graphs and active sampling. We deploy our tool on the
PlanetLab network and our results show that we can obtain accurate estimates
with a much smaller measurement overhead compared to existing approaches.

ABSTRACT_BEGIN
  Spatio-temporal preferences and encounter statistics provide realistic
measures to understand mobile user's behavioral preferences and transfer
opportunities in Delay Tolerant Networks (DTNs). The time dependent behavior
and periodic reappearances at specific locations can approximate future online
presence while encounter statistics can aid to forward the routing decisions.
It is theoretically shown that such characteristics heavily affect the
performance of routing protocols. Therefore, mobility models demonstrating such
characteristics are also expected to show identical routing performance.
However, we argue models despite capturing these properties deviate from their
expected routing performance. We use realistic traces to validate this
observation on two mobility models. Our empirical results for epidemic routing
show those models' largely differ (delay 67% & reachability 79%) from the
observed values. This in-turn call for two important activities: (i) Analogous
to routing, explore structural properties on a Global scale (ii) Design new
mobility models that capture them.

ABSTRACT_BEGIN
  The introduction of physical layer network coding gives rise to the concept
of turning a collision of transmissions on a wireless channel useful. In the
idea of physical layer network coding, two synchronized simultaneous packet
transmissions are carefully encoded such that the superimposed transmission can
be decoded to produce a packet which is identical to the bitwise binary sum of
the two transmitted packets. This paper explores the decoding of superimposed
transmission resulted by multiple synchronized simultaneous transmissions. We
devise a coding scheme that achieves the identification of individual
transmission from the synchronized superimposed transmission. A mathematical
proof for the existence of such a coding scheme is given.

ABSTRACT_BEGIN
  We consider a set of primary channels that operate in an unslotted fashion,
switching activity at random times. A secondary user senses the primary
channels searching for transmission opportunities. If a channel is sensed to be
free, the secondary terminal transmits, and if sensed to be busy, the secondary
transmitter remains silent.We solve the problem of determining the optimal time
after which a primary channel needs to be sensed again depending on the sensing
outcome. The objective is to find the inter-sensing times such that the mean
secondary throughput is maximized while imposing a constraint over the maximum
tolerable interference inflicted on the primary network. Our numerical results
show that by optimizing the sensing-dependent inter-sensing times, our proposed
scheme reduces the impact of sensing errors caused by false alarm and
misdetection and outperforms the case of a single sensing period.

ABSTRACT_BEGIN
  Network management and security is currently one of the most vibrant research
areas, among which, research on detecting and identifying anomalies has
attracted a lot of interest. Researchers are still struggling to find an
effective and lightweight method for anomaly detection purpose. In this paper,
we propose a simple, robust method that detects network anomalous traffic data
based on flow monitoring. Our method works based on monitoring the four
predefined metrics that capture the flow statistics of the network. In order to
prove the power of the new method, we did build an application that detects
network anomalies using our method. And the result of the experiments proves
that by using the four simple metrics from the flow data, we do not only
effectively detect but can also identify the network traffic anomalies.

ABSTRACT_BEGIN
  Nowadays computing becomes increasingly mobile and pervasive. One of the
important steps in pervasive computing is context-awareness. Context-aware
pervasive systems rely on information about the context and user preferences to
adapt their behavior. However, context-aware applications do not always behave
as user's desire, and can cause users to feel dissatisfied with unexpected
actions. To solve these problems, context-aware systems must provide mechanisms
to adapt automatically when the context changes significantly. The interesting
characteristic of context is its own behaviors which depend on various aspects
of the surrounding contexts. This paper uses contextual graphs to solve the
problem "the mutual relationships among the contexts". We describe the most
relevant work in this area, as well as ongoing research on developing
context-aware system for ubiquitous computing based on contextual graph. The
usage of contextual graph in context-awareness is expected to make it effective
for developers to develop various applications with the need of context
reasoning.

ABSTRACT_BEGIN
  One of the most critical tasks for network administrator is to ensure system
uptime and availability. For the network security, anomaly detection systems,
along with firewalls and intrusion prevention systems are the must-have tools.
So far in the field of network anomaly detection, people are working on two
different approaches. One is flow-based; usually rely on network elements to
make so-called flow information available for analysis. The second approach is
packet-based; which directly analyzes the data packet information for the
detection of anomalies. This paper describes the main differences between the
two approaches through an in-depth analysis. We try to answer the question of
when and why an approach is better than the other. The answer is critical for
network administrators to make their choices in deploying a defending system,
securing the network and ensuring business continuity.

ABSTRACT_BEGIN
  Context awareness is one of the important fields in ubiquitous computing.
Smart Home, a specific instance of ubiquitous computing, provides every family
with opportunities to enjoy the power of hi-tech home living. Discovering that
relationship among user, activity and context data in home environment is
semantic, therefore, we apply ontology to model these relationships and then
reason them as the semantic information. In this paper, we present the
realization of smart home's context-aware system based on ontology. We discuss
the current challenges in realizing the ontology context base. These challenges
can be listed as collecting context information from heterogeneous sources,
such as devices, agents, sensors into ontology, ontology management, ontology
querying, and the issue related to environment database explosion.

ABSTRACT_BEGIN
  Context awareness is the most important research area in ubiquitous
computing. In particular, for smart home, context awareness attempts to bring
the best services to the home habitants. However, the implementation in the
real environment is not easy and takes a long time from building the scratch.
Thus, to support the implementation in the real smart home, it is necessary to
demonstrate that thing can be done in the simulator in which context
information can be created by virtual sensors instead of physical sensors. In
this paper, we propose ISS, an Interactive Smart home Simulator system aiming
at controlling and simulating the behavior of an intelligent house. The
developed system aims to provide architects, designers a simulation and useful
tool for understanding the interaction between environment, people and the
impact of embedded and pervasive technology on in daily life. In this research,
the smart house is considered as an environment made up of independent and
distributed devices interacting to support user's goals and tasks. Therefore,
by using ISS, the developer can realize the relationship among virtual home
space, surrounded environment, use and home appliances.

ABSTRACT_BEGIN
  We consider a GI/G/c/K-type retrial queueing system with constant retrial
rate. The system consists of a primary queue and an orbit queue. The primary
queue has $c$ identical servers and can accommodate the maximal number of $K$
jobs. If a newly arriving job finds the full primary queue, it joins the orbit.
The original primary jobs arrive to the system according to a renewal process.
The jobs have general i.i.d. service times. A job in front of the orbit queue
retries to enter the primary queue after an exponentially distributed time
independent of the orbit queue length. Telephone exchange systems, Medium
Access Protocols and short TCP transfers are just some applications of the
proposed queueing system. For this system we establish minimal sufficient
stability conditions. Our model is very general. In addition, to the known
particular cases (e.g., M/G/1/1 or M/M/c/c systems), the proposed model covers
as particular cases the deterministic service model and the Erlang model with
constant retrial rate. The latter particular cases have not been considered in
the past. The obtained stability conditions have clear probabilistic
interpretation.

ABSTRACT_BEGIN
  Concern for the human security inside mines is as old as the mining itself.
However, ICT (Information and communication technologies), which has impacted
human life in so many ways has not been much used for making mines safer. We
propose a method that has been practically implemented which can enhance mine
safety enormously. It is based on integration of wireless sensor network with
an external network through a gateway.

ABSTRACT_BEGIN
  This paper studies the problem of scheduling in single-hop wireless networks
with real-time traffic, where every packet arrival has an associated deadline
and a minimum fraction of packets must be transmitted before the end of the
deadline. Using optimization and stochastic network theory we propose a
framework to model the quality of service (QoS) requirements under delay
constraints. The model allows for fairly general arrival models with
heterogeneous constraints. The framework results in an optimal scheduling
algorithm which fairly allocates data rates to all flows while meeting
long-term delay demands. We also prove that under a simplified scenario our
solution translates into a greedy strategy that makes optimal decisions with
low complexity.

ABSTRACT_BEGIN
  In this paper we investigate the evolution of the IPv4 and IPv6 Internet
topologies at the autonomous system (AS) level over a long period of time.We
provide abundant empirical evidence that there is a phase transition in the
growth trend of the two networks. For the IPv4 network, the phase change
occurred in 2001. Before then the network's size grew exponentially, and
thereafter it followed a linear growth. Changes are also observed around the
same time for the maximum node degree, the average node degree and the average
shortest path length. For the IPv6 network, the phase change occurred in late
2006. It is notable that the observed phase transitions in the two networks are
different, for example the size of IPv6 network initially grew linearly and
then shifted to an exponential growth. Our results show that following decades
of rapid expansion up to the beginning of this century, the IPv4 network has
now evolved into a mature, steady stage characterised by a relatively slow
growth with a stable network structure; whereas the IPv6 network, after a slow
startup process, has just taken off to a full speed growth. We also provide
insight into the possible impact of IPv6-over-IPv4 tunneling deployment scheme
on the evolution of the IPv6 network. The Internet topology generators so far
are based on an inexplicit assumption that the evolution of Internet follows
non-changing dynamic mechanisms. This assumption, however, is invalidated by
our results.Our work reveals insights into the Internet evolution and provides
inputs to future AS-Level Internet models.

ABSTRACT_BEGIN
  BitTorrent is the most popular P2P content delivery application where
individual users share various type of content with tens of thousands of other
users. The growing popularity of BitTorrent is primarily due to the
availability of valuable content without any cost for the consumers. However,
apart from required resources, publishing (sharing) valuable (and often
copyrighted) content has serious legal implications for user who publish the
material (or publishers). This raises a question that whether (at least major)
content publishers behave in an altruistic fashion or have other incentives
such as financial. In this study, we identify the content publishers of more
than 55k torrents in 2 major BitTorrent portals and examine their behavior. We
demonstrate that a small fraction of publishers are responsible for 66% of
published content and 75% of the downloads. Our investigations reveal that
these major publishers respond to two different profiles. On one hand,
antipiracy agencies and malicious publishers publish a large amount of fake
files to protect copyrighted content and spread malware respectively. On the
other hand, content publishing in BitTorrent is largely driven by companies
with financial incentive. Therefore, if these companies lose their interest or
are unable to publish content, BitTorrent traffic/portals may disappear or at
least their associated traffic will significantly reduce.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks (VANET) is one of the most challenging research
area in the field of Mobile Ad Hoc Networks. In this research we proposed a
dynamic power adjustment protocol that will be used for sending the periodical
safety message. (Beacon)based on the analysis of the channel status depending
on the channel congestion and the power used for transmission. The Beacon Power
Control (BPC) protocol first sensed and examined the percentage of the channel
congestion, the result obtained was used to adjust the transmission power for
the safety message to reach the optimal power. This will lead to decrease the
congestion in the channel and achieve good channel performance and beacon
dissemination.

ABSTRACT_BEGIN
  We consider several WLAN stations associated at rates r1, r2, ..., rk with an
Access Point. Each station is downloading a long file from a local server,
located on the LAN to which the AP is attached. We model these simultaneous
TCP-controlled transfers using a Markov Chain. Our analytical approach leads to
a procedure to compute aggregate download throughput numerically, and the
results match simulations very well.

ABSTRACT_BEGIN
  Our goal is to infer the topology of a network when (i) we can send probes
between sources and receivers at the edge of the network and (ii) intermediate
nodes can perform simple network coding operations, i.e., additions. Our key
intuition is that network coding introduces topology-dependent correlation in
the observations at the receivers, which can be exploited to infer the
topology. For undirected tree topologies, we design hierarchical clustering
algorithms, building on our prior work. For directed acyclic graphs (DAGs),
first we decompose the topology into a number of two-source, two-receiver
(2-by-2) subnetwork components and then we merge these components to
reconstruct the topology. Our approach for DAGs builds on prior work on
tomography, and improves upon it by employing network coding to accurately
distinguish among all different 2-by-2 components. We evaluate our algorithms
through simulation of a number of realistic topologies and compare them to
active tomographic techniques without network coding. We also make connections
between our approach and alternatives, including passive inference, traceroute,
and packet marking.

ABSTRACT_BEGIN
  The paper proposes a method for measuring available bandwidth, based on
testing network packets of various sizes (Variable Packet Size method, VPS).
The boundaries of applicability of the model have been found, which are based
on the accuracy of measurements of packet delays, also we have derived a
formula of measuring the upper limit of bandwidth. The computer simulation has
been performed and relationship between the measurement error of available
bandwidth and the number of measurements has been found. Experimental
verification with the use of RIPE Test Box measuring system has shown that the
suggested method has advantages over existing measurement techniques. Pathload
utility has been chosen as an alternative technique of measurement, and to
ensure reliable results statistics by SNMP agent has been withdrawn directly
from the router.

ABSTRACT_BEGIN
  The Network Simulator (NS-2) is a most widely used network simulator. It has
the capabilities to simulate a range of networks including wired and wireless
networks. In this tutorial, we present the implementation of Ad Hoc On-Demand
Distance Vector (AODV) Protocol in NS-2. This tutorial is targeted to the
novice user who wants to understand the implementation of AODV Protocol in
NS-2.

ABSTRACT_BEGIN
  With the proliferation of cheaper electronic devices, wireless communication
over multiple-channels in a multi-interface network is now possible. For
instace, wireless sensor nodes can now operate over multiplechannels. Moreover,
cognitive radio sensor networks are also evolving, which also operates over
multiple-channels. In the market, we can find antennas that can support the
operation of multiple channels, for e.g. the cc2420 antenna that is used for
communication between wireless sensor nodes consists of 16 programmable
channels. The proper utilization of multiple-channels reduces the interference
between the nodes and increase the network throughput. Recently, a Cognitive
Radio Cognitive Network (CRCN) patch for NS-2 simulator has proposed to support
multi-channel multi-interface capability in NS-2. In this tutorial, we consider
how to simulate a multi-channel multiinterface wireless network using the NS-2
simulator. This tutorial is trageted to the novice users who wants to
understand the implementation of multi-channel multi-interface in NS-2. We take
the Cognitive Radio Cognitive Network (CRCN) patch for NS-2 simulator and
demonstrate broadcasting over multiple-channels in a multi-interface network
setting. In our seeting, node braodcasts the Hello packets to its neighbors.
Neighboring nodes receive the Hello packets if and only if they are tuned to
the same channel. We demonstrate through example that the tuning of receivers
can be done in two fashions.

ABSTRACT_BEGIN
  In this report, firstly, we presents state of the art survey on Data
Management and Data Dissemination techniques with Mobile Sink. Moreover we
classify these techniques into two ample sub-categories. Under this
classification, we identify, review, compare, and highlight these techniques
and their pros and cons. We do a SWOT (Strength, Weaknesses, Opportunities,
Threats) analysis of each scheme. We also discuss where each scheme is
appropriate.
  Secondly, we presents a new distributed data management scheme which is based
upon Random Walk Based Membership Service to facilitate Data Dissemination in
Mobile Sink based Wireless Sensor Networks. Our proposed scheme efficiently
deals with the aforementioned problems and we also compare the characteristics
of our proposed scheme with the state-of-the-art data-dissemination schemes. We
propose using Random Walks (RWs) with uniformly distributed views to
disseminate data through the WSN with a controlled overhead. This is performed
by the use of a Random Walk Based Membership Service - the RaWMS. Our proposal
solves then the problems generated when (a) all nodes are storage motes, being
no aggregation performed (b) one center node plays the role of storage mote and
aggregates data from all the other nodes (c) replication is performed on all
nodes in the network.
  To the best of our knowledge, we are the first to propose an efficient data
dissemination approach (in terms of overhead, adaptiveness and
representativeness) to allow a mobile sink to gather a representative view of
the monitored region covered by n sensor nodes by only visiting any m nodes,
where hopefully m << n.

ABSTRACT_BEGIN
  This paper aims to propose a significant way of remote access and real time
monitoring of a particular geographic area by integrating wireless sensor
clouds with existing Telecom infrastructure and applications built around them
through a gateway. This utility is very potent for environment monitoring in
harsh and inaccessible places like mines, nuclear reactors, etc. We demonstrate
a scaled down version of multi-hop network of wireless sensor nodes and its
integration with existing telecom network infrastructure via a gateway.

ABSTRACT_BEGIN
  Recent research efforts have shown that the popular BitTorrent protocol does
not provide fair resource reciprocation and may allow free-riding. In this
paper, we propose a BitTorrent-like protocol that replaces the peer selection
mechanisms in the regular BitTorrent protocol with a novel reinforcement
learning (RL) based mechanism. Due to the inherent opration of P2P systems,
which involves repeated interactions among peers over a long period of time,
the peers can efficiently identify free-riders as well as desirable
collaborators by learning the behavior of their associated peers. Thus, it can
help peers improve their download rates and discourage free-riding, while
improving fairness in the system. We model the peers' interactions in the
BitTorrent-like network as a repeated interaction game, where we explicitly
consider the strategic behavior of the peers. A peer, which applies the
RL-based mechanism, uses a partial history of the observations on associated
peers' statistical reciprocal behaviors to determine its best responses and
estimate the corresponding impact on its expected utility. The policy
determines the peer's resource reciprocations with other peers, which would
maximize the peer's long-term performance, thereby making foresighted
decisions. We have implemented the proposed reinforcement-learning based
mechanism and incorporated it into an existing BitTorrent client. We have
performed extensive experiments on a controlled Planetlab test bed. Our results
confirm that our proposed protocol (1) promotes fairness in terms of incentives
to each peer's contribution e.g. high capacity peers improve their download
completion time by up to 33\%, (2) improves the system stability and robustness
e.g. reducing the peer selection luctuations by 57\%, and (3) discourages
free-riding e.g. peers reduce by 64\% their upload to \FR, in comparison to the
regular \BT~protocol.

ABSTRACT_BEGIN
  Estimating link capacity in a wireless network is a complex task because the
available capacity at a link is a function of not only the current arrival rate
at that link, but also of the arrival rate at links which interfere with that
link as well as of the nature of interference between these links. Models which
accurately characterize this dependence are either too computationally complex
to be useful or lack accuracy. Further, they have a high implementation
overhead and make restrictive assumptions, which makes them inapplicable to
real networks.
  In this paper, we propose CapEst, a general, simple yet accurate,
measurement-based approach to estimating link capacity in a wireless network.
To be computationally light, CapEst allows inaccuracy in estimation; however,
using measurements, it can correct this inaccuracy in an iterative fashion and
converge to the correct estimate. Our evaluation shows that CapEst always
converged to within 5% of the correct value in less than 18 iterations. CapEst
is model-independent, hence, is applicable to any MAC/PHY layer and works with
auto-rate adaptation. Moreover, it has a low implementation overhead, can be
used with any application which requires an estimate of residual capacity on a
wireless link and can be implemented completely at the network layer without
any support from the underlying chipset.

ABSTRACT_BEGIN
  In mobile ad hoc networks (MANET), nodes usually belong to different
authorities and pursue different goals. In order to maximize their own
performance, nodes in such networks tend to be selfish and are not willing to
forward packets for benefit of others. Meanwhile, some nodes may behave
maliciously and try to disrupt the network through wasting other nodes
resources in a very large scale. In this article, we present a reputation-based
attack resistant cooperation stimulation (RACS) system which ensures that
damage caused by malicious nodes can be bounded and cooperation among the
selfish nodes can be enforced. Mathematical analyses of the system as well as
the simulation results have confirmed effectiveness of our proposed system.
RACS is completely self-organizing and distributed. It does not require any
tamper-proof hardware or central management policy.

ABSTRACT_BEGIN
  The third generation partnership project (3GPP) has addressed the feasibility
of interworking and specified the interworking architecture and security
architecture for third generation (3G)-wireless local area network (WLAN), it
is developing, system architecture evolution (SAE)/ long term evolution (LTE)
architecture, for the next generation mobile communication system. To provide a
secure 3G-WLAN interworking in the SAE/LTE architecture, Extensible
authentication protocol-authentication and key agreement (EAP-AKA) is used.
However, EAP-AKA have several vulnerabilities. Therefore, this paper not only
analyses the threats and attacks in 3G-WLAN interworking but also proposes a
new authentication and key agreement protocol based on EAP-AKA. The proposed
protocol combines elliptic curve Diffie-Hellman (ECDH) with symmetric key
cryptosystem to overcome the vulnerabilities. The proposed protocol is used in
hybrid coupled 3G-WLAN convergence network to analyse its efficiency in terms
of QoS metrics, the results obtained using OPNET 14.5 shows that the proposed
protocol outperforms existing interworking protocols both in security and QoS.

ABSTRACT_BEGIN
  Power management is one of the vital issue in wireless sensor networks, where
the lifetime of the network relies on battery powered nodes. Transmitting at
high power reduces the lifetime of both the nodes and the network. One
efficient way of power management is to control the power at which the nodes
transmit. In this paper, a virtual multiple input multiple output wireless
sensor network (VMIMO-WSN)communication architecture is considered and the
power control of sensor nodes based on the approach of game theory is
formulated. The use of game theory has proliferated, with a broad range of
applications in wireless sensor networking. Approaches from game theory can be
used to optimize node level as well as network wide performance. The game here
is categorized as an incomplete information game, in which the nodes do not
have complete information about the strategies taken by other nodes. For
virtual multiple input multiple output wireless sensor network architecture
considered, the Nash equilibrium is used to decide the optimal power level at
which a node needs to transmit, to maximize its utility. Outcome shows that the
game theoretic approach considered for VMIMO-WSN architecture achieves the best
utility, by consuming less power.

ABSTRACT_BEGIN
  The belief propagation (BP) algorithm is an efficient way to solve
"inference" problems in graphical models, such as Bayesian networks and Markov
random fields. The system-state probability distribution of CSMA wireless
networks is a Markov random field. An interesting question is how BP can help
the analysis and design of CSMA wireless networks. This paper explores three
such applications. First, we show how BP can be used to compute the throughputs
of different links in the network given their access intensities, defined as
the mean packet transmission time divided by the mean backoff countdown time.
Second, we propose an inverse-BP algorithm to solve the reverse problem: how to
set the access intensities of different links to meet their target throughputs?
Third, we introduce a BP-adaptive CSMA algorithm to find the link access
intensities that can achieve optimal system utility. BP solves the three
problems with exact results in tree networks. It may, however, lose accuracy in
networks with a loopy contention graph. We show how a generalized version of
BP, GBP, can be designed to solve the three problems with high accuracy for
networks with a loopy contention graph. Importantly, we show how the BP and GBP
algorithms in this paper can be implemented in a distributed manner, making
them useful in practical CSMA network opera-tion.

ABSTRACT_BEGIN
  An interesting distributed adaptive CSMA MAC protocol, called adaptive CSMA,
was proposed recently to schedule any strictly feasible achievable rates inside
the capacity region. Of particular interest is the fact that the adaptive CSMA
can achieve a system utility arbitrarily close to that is achievable under a
central scheduler. However, a specially designed transport-layer rate
controller is needed for this result. An outstanding question is whether the
widely-installed TCP Reno is compatible with adaptive CSMA and can achieve the
same result. The answer to this question will determine how close to practical
deployment adaptive CSMA is. Our answer is yes and no. First, we observe that
running TCP Reno directly over adaptive CSMA results in severe starvation
problems. Effectively, its performance is no better than that of TCP Reno over
legacy CSMA (IEEE 802.11), and the potentials of adaptive CSMA cannot be
realized. Fortunately, we find that multi-connection TCP Reno over adaptive
CSMA with active queue management can materialize the advantages of adaptive
CSMA. NS-2 simulations demonstrate that our solution can alleviate starvation
and achieve fair and efficient rate allocation. Multi-connection TCP can be
implemented at either application or transport layer. Application-layer
implementation requires no kernel modification, making the solution readily
deployable in networks running adaptive CSMA.

ABSTRACT_BEGIN
  Several social-aware forwarding strategies have been recently introduced in
opportunistic networks, and proved effective in considerably in- creasing
routing performance through extensive simulation studies based on real-world
data. However, this performance improvement comes at the expense of storing a
considerable amount of state information (e.g, history of past encounters) at
the nodes. Hence, whether the benefits on routing performance comes directly
from the social-aware forwarding mechanism, or indirectly by the fact state
information is exploited is not clear. Thus, the question of whether
social-aware forwarding by itself is effective in improving opportunistic
network routing performance remained unaddressed so far. In this paper, we give
a first, positive answer to the above question, by investigating the expected
message delivery time as the size of the net- work grows larger.

ABSTRACT_BEGIN
  This paper proposes and analyzes the performance of a simple frequency-agile
CSMA MAC protocol. In this MAC, a node carrier-senses multiple frequency
channels simultaneously, and it takes the first opportunity to transmit on any
one of the channels when allowed by the CSMA backoff mechanism. We show that
the frequency-agile MAC can effectively 1) boost throughput and 2) remove
temporal starvation. Furthermore, the MAC can be implemented on the existing
multiple-frequency setup in Wi-Fi using multi-radio technology, and it can
co-exist with the legacy MAC using single radio. This paper provides exact
stationary throughput analysis for regular 1D and thin-strip 2D CSMA networks
using a "transfer-matrix" approach. In addition, accurate approximations are
given for 2D grid networks. Our closed-form formulas accurately quantify the
throughput gain of frequency-agile CSMA. To characterize temporal starvation,
we use the metric of "mean residual access time" (MRAT). Our simulations and
closed-form approximations indicate that the frequency-agile MAC can totally
eliminate temporal starvation in 2D grid networks, reducing its MRAT by orders
of magnitude. Finally, this paper presents a "coloring theorem" to justify the
use of the frequency-agile MAC in general network topologies. Our analysis and
theorem suggest that with enough frequency channels, the frequency-agile MAC
can effectively decouple the detrimental interactions between neighboring links
responsible for low throughput and starvation.

ABSTRACT_BEGIN
  With social networking sites providing increasingly richer context,
User-Centric Service (UCS) creation is expected to explode following a similar
success path to User-Generated Content. One of the major challenges in this
emerging highly user-centric networking paradigm is how to make these exploding
in numbers yet, individually, of vanishing demand services available in a
cost-effective manner. Of prime importance to the latter (and focus of this
paper) is the determination of the optimal location for hosting a UCS. Taking
into account the particular characteristics of UCS, we formulate the problem as
a facility location problem and devise a distributed and highly scalable
heuristic solution to it.
  Key to the proposed approach is the introduction of a novel metric drawing on
Complex Network Analysis. Given a current location of UCS, this metric helps to
a) identify a small subgraph of nodes with high capacity to act as service
demand concentrators; b) project on them a reduced yet accurate view of the
global demand distribution that preserves the key attraction forces on UCS;
and, ultimately, c) pave the service migration path towards its optimal
location in the network. The proposed iterative UCS migration algorithm, called
cDSMA, is extensively evaluated over synthetic and real-world network
topologies. Our results show that cDSMA achieves high accuracy, fast
convergence, remarkable insensitivity to the size and diameter of the network
and resilience to inaccurate estimates of demands for UCS across the network.
It is also shown to clearly outperform local-search heuristics for service
migration that constrain the subgraph to the immediate neighbourhood of the
node currently hosting UCS.

ABSTRACT_BEGIN
  Major wireless operators are nowadays facing network capacity issues in
striving to meet the growing demands of mobile users. At the same time,
3G-enabled devices increasingly benefit from ad hoc radio connectivity (e.g.,
Wi-Fi). In this context of hybrid connectivity, we propose Push-and-track, a
content dissemination framework that harnesses ad hoc communication
opportunities to minimize the load on the wireless infrastructure while
guaranteeing tight delivery delays. It achieves this through a control loop
that collects user-sent acknowledgements to determine if new copies need to be
reinjected into the network through the 3G interface. Push-and-Track includes
multiple strategies to determine how many copies of the content should be
injected, when, and to whom. The short delay-tolerance of common content, such
as news or road traffic updates, make them suitable for such a system. Based on
a realistic large-scale vehicular dataset from the city of Bologna composed of
more than 10,000 vehicles, we demonstrate that Push-and-Track consistently
meets its delivery objectives while reducing the use of the 3G network by over
90%.

ABSTRACT_BEGIN
  The characteristics of wireless communication channels may vary with time due
to fading, environmental changes and movement of mobile wireless devices.
Tracking and estimating channel gains of wireless channels is therefore a
fundamentally important element of many wireless communication systems. In
particular, the receivers in many wireless networks need to estimate the
channel gains by means of a training sequence. This paper studies the scaling
law (on the network size) of the overhead for channel gain monitoring in
wireless network. We first investigate the scenario in which a receiver needs
to track the channel gains with respect to multiple transmitters. To be
concrete, suppose that there are n transmitters, and that in the current round
of channel-gain estimation, no more than k channels suffer significant
variations since the last round. We proves that "\Theta(k\log((n+1)/k)) time
slots" is the minimum number of time slots needed to catch up with the k varied
channels. At the same time, we propose a novel channel-gain monitoring scheme
named ADMOT to achieve the overhead lower-bound. ADMOT leverages recent
advances in compressive sensing in signal processing and interference
processing in wireless communication, to enable the receiver to estimate all n
channels in a reliable and computationally efficient manner within
O(k\log((n+1)/k)) time slots. To our best knowledge, all previous
channel-tracking schemes require \Theta(n) time slots regardless of k. Note
that based on above results for single receiver scenario, the scaling law of
general setting is achieved in which there are multiple transmitters, relay
nodes and receivers.

ABSTRACT_BEGIN
  We consider a broad class of interference coordination and resource
allocation problems for wireless links where the goal is to maximize the sum of
functions of individual link rates. Such problems arise in the context of, for
example, fractional frequency reuse (FFR) for macro-cellular networks and
dynamic interference management in femtocells. The resulting optimization
problems are typically hard to solve optimally even using centralized
algorithms but are an essential computational step in implementing rate-fair
and queue stabilizing scheduling policies in wireless networks. We consider a
belief propagation framework to solve such problems approximately. In
particular, we construct approximations to the belief propagation iterations to
obtain computationally simple and distributed algorithms with low communication
overhead. Notably, our methods are very general and apply to, for example, the
optimization of transmit powers, transmit beamforming vectors, and sub-band
allocation to maximize the above objective. Numerical results for femtocell
deployments demonstrate that such algorithms compute a very good operating
point in typically just a couple of iterations.

ABSTRACT_BEGIN
  A network coding scheme for practical implementations of wireless body area
networks is presented, with the objective of providing reliability under
low-energy constraints. We propose a simple network layer protocol for star
networks, adapting redundancy based on both transmission and reception energies
for data and control packets, as well as channel conditions. Our numerical
results show that even for small networks, the amount of energy reduction
achievable can range from 29% to 87%, as the receiving energy per control
packet increases from equal to much larger than the transmitting energy per
data packet. The achievable gains increase as a) more nodes are added to the
network, and/or b) the channels seen by different sensor nodes become more
asymmetric.

ABSTRACT_BEGIN
  A macrocell superposed by indoor deployed femtocells forms a
geography-overlapped and spectrum-shared two tier network, which can
efficiently improve coverage and enhance system capacity. It is important for
reducing inter-tier co-channel interference that any femtocell user (FU) can
select suitable access channel according to the path losses between itself and
the macrocell users (MUs). Path loss should be estimated non-cooperatively
since information exchange is difficult between macrocell and femtocells. In
this paper, a novel method is proposed for FU to estimate the path loss between
itself and any MU independently. According to the adaptive modulation and
coding (AMC) mode information broadcasted by the macrocell base station (BS),
FU first estimates the path loss between BS and a MU by using Maximum a
Posteriori (MAP) method. The probability distribution function (PDF) and
statistics of the transmission power of the MU is then derived. According to
the sequence of received powers from the MU, FU estimates the path loss between
itself and the MU by using minimum mean square error (MMSE) method. Simulation
results show that the proposed method can efficiently estimate the path loss
between any FU and any MU in all kinds of conditions.

ABSTRACT_BEGIN
  Wireless Internet in the in-vehicle environment is an evolving reality that
reflects the gradual maturity of wireless technologies. Its complexity is
reflected in the diversity of wireless technologies and dynamically changing
network environments. The ability to adapt to the dynamics of such environments
and to survive transient failures due to network handoffs are fundamentally
important in failure-prone vehicular environments. In this paper we identify
several new issues arising from network heterogeneity in vehicular environments
and concentrate on designing and implementing a network-aware prototype system
that supports HTTP session continuity in the presence of network volatility,
with the emphasis on the following specifically tailored features: (1)
automatic and transparent HTTP failure recovery, (2) network awareness and
adaptation, (3) application-layer preemptive network handoff. Experimental
results gathered from real application environments based on CDMA {\it 1xRTT}
and IEEE 802 networks are presented and analyzed.

ABSTRACT_BEGIN
  In this paper, we discuss the computation of weighted max-min rate allocation
using joint TDM/FDM strategies under a PSD mask constraint. We show that the
weighted max-min solution allocates the rates according to a predetermined rate
ratio defined by the weights, a fact that is very valuable for
telecommunication service providers. Furthermore, we show that the problem can
be efficiently solved using linear programming. We also discuss the resource
allocation problem in the mixed services scenario where certain users have a
required rate, while the others have flexible rate requirements. The solution
is relevant to many communication systems that are limited by a power spectral
density mask constraint such as WiMax, Wi-Fi and UWB.

ABSTRACT_BEGIN
  P2P computing lifts taxing issues in various areas of computer science. The
largely used decentralized unstructured P2P systems are ad hoc in nature and
present a number of research challenges. In this paper, we provide a
comprehensive theoretical survey of various state-of-the-art search and
replication schemes in unstructured P2P networks for file-sharing applications.
The classifications of search and replication techniques and their advantages
and disadvantages are briefly explained. Finally, the various issues on
searching and replication for unstructured P2P networks are discussed.

ABSTRACT_BEGIN
  Handoff has become an essential criterion in mobile communication system,
specially in urban areas, owing to the limited coverage area of Access Points
(AP). Handover of calls between two Base Stations (BSs) is encountered
frequently and it is essentially required to minimize the delay of the process.
Many solutions attempting to improve this process have been proposed but only a
few use geo-location systems in the management of the handover. Here we propose
to minimize the handoff latency by minimizing the number of APs scanned by the
Mobile Node (MN) during each handoff procedure. We consider the whole
topographical area as a two dimensional plane. By GPS, we can note down the
co-ordinates of the MN at any instant. The average rate of change of its
latitudinal distance and longitudinal distance with a specific time period is
evaluated at the end of the given time period. With the knowledge of the given
parameter, it is possible to determine the latitude and longitude of the MN
after a particular instant of time. Hence, the direction of motion of the MN
can be determined, which in turns gives the AP towards which the MN is
headings. This reduces the number of APs to be scanned. Thus, on an overall
basis, the handoff latency can be reduced by almost half to one third of its
value.

ABSTRACT_BEGIN
  In this paper we analytically propose an alternative approach to achieve
better fairness in scheduling mechanisms which could provide better quality of
service particularly for real time application. Our proposal oppose the
allocation of the bandwidth which adopted by all previous scheduling mechanism.
It rather adopt the opposition approach be proposing the notion of
Maxmin-charge which fairly distribute the congestion. Furthermore, analytical
proposition of novel mechanism named as Just Queueing is been demonstrated.

ABSTRACT_BEGIN
  In time-varying wireless networks, the states of the communication channels
are subject to random variations, and hence need to be estimated for efficient
rate adaptation and scheduling. The estimation mechanism possesses inaccuracies
that need to be tackled in a probabilistic framework. In this work, we study
scheduling with rate adaptation in single-hop queueing networks under two
levels of channel uncertainty: when the channel estimates are inaccurate but
complete knowledge of the channel/estimator joint statistics is available at
the scheduler; and when the knowledge of the joint statistics is incomplete. In
the former case, we characterize the network stability region and show that a
maximum-weight type scheduling policy is throughput-optimal. In the latter
case, we propose a joint channel statistics learning - scheduling policy. With
an associated trade-off in average packet delay and convergence time, the
proposed policy has a stability region arbitrarily close to the stability
region of the network under full knowledge of channel/estimator joint
statistics.

ABSTRACT_BEGIN
  Retransmission based on packet acknowledgement (ACK/NAK) is a fundamental
error control technique employed in IEEE 802.11-2007 unicast network. However
the 802.11-2007 standard falls short of proposing a reliable MAC-level recovery
protocol for multicast frames. In this paper we propose a latency and bandwidth
efficient coding algorithm based on the principles of network coding for
retransmitting lost packets in a singlehop wireless multicast network and
demonstrate its effectiveness over previously proposed network coding based
retransmission algorithms.

ABSTRACT_BEGIN
  Managing network complexity, accommodating greater numbers of subscribers,
improving coverage to support data services (e.g. email, video, and music
downloads), keeping up to speed with fast-changing technology, and driving
maximum value from existing networks - all while reducing CapEX and OpEX and
ensuring Quality of Service (QoS) for the network and Quality of Experience
(QoE) for the user. These are just some of the pressing business issues faced
by mobileservice providers, summarized by the demand to "achieve more, for
less." The ultimate goal of optimization techniques at the network and
application layer is to ensure End-user perceived QoS. The next generation
networks (NGN), a composite environment of proven telecommunications and
Internet-oriented mechanisms have become generally recognized as the
telecommunications environment of the future. However, the nature of the NGN
environment presents several complex issues regarding quality assurance that
have not existed in the legacy environments (e.g., multi-network, multi-vendor,
and multi-operator IP-based telecommunications environment, distributed
intelligence, third-party provisioning, fixed-wireless and mobile access,
etc.). In this Research Paper, a service aware policy-based approach to NGN
quality assurance is presented, taking into account both perceptual quality of
experience and technologydependant quality of service issues. The respective
procedures, entities, mechanisms, and profiles are discussed. The purpose of
the presented approach is in research, development, and discussion of pursuing
the end-to-end controllability of the quality of the multimedia NGN-based
communications in an environment that is best effort in its nature and promotes
end user's access agnosticism, service agility, and global mobility.

ABSTRACT_BEGIN
  Next generation networks (NGN) services are assumed to be a new revenue
stream for both network operators and service providers. New services
especially focused on a mobile telecommunications that would be used not only
as a communication de vice but also as a personal gateway to order or consume a
variety of services and products [1]. This type of advanced services can be
accomplished when the adaptability of the packet-networks (Internet) and the
quality of service of the circuit switched networks are combined into one
network [2]. New challenges appear in the billing of this heterogeneous multi
services network. Some examples of such a services and possible solutions about
charging and billing are examined in this paper. The first steps of
mathematical model for billing are also considered.

ABSTRACT_BEGIN
  Mobile Communication marketplace has stressed that "content is king" ever
since the initial footsteps for Next Generation Networks like 3G, 3GPP, IP
Multimedia subsystem (IMS) services. However, many carriers and content
providers have struggled to drive revenue for content services, primarily due
to current limitations of certain types of desirable content offerings,
simplistic billing models, and the inability to support flexible pricing,
charging and settlement. Unlike wire line carriers, wireless carriers have a
limit to the volume of traffic they can carry, bounded by the finite wireless
spectrum. Event based services like calling, conferencing etc., only perceive
charge per event, while the Content based charging system attracts Mobile
Network Operators (MNOs) to maximize service delivery to customer and achieve
best ARPU. With the Next Generation Networks, the number of data related
services that can be offered, is increased significantly. The wireless carrier
will be able to move from offering wireless telecommunications services to
offering wireless telecommunication services plus a number of personalized
Value Added Services like news, games, video broadcasts, or multimedia
messaging service (MMS) through the network. The next generation Content Based
Billing systems allow the operators to maximize their revenues from such
services. These systems will enable operators to offer and bill for
application-based and content-based services, rather than for just bytes of
data. Therefore, the wireless business focus is no longer on infrastructure
build-outs but on customer retention and increased average revenue per customer
(ARPU). The mobile operator generates new revenues, strengthens brand value,
and differentiates its service to attract and retain customers.

ABSTRACT_BEGIN
  We obtain an association policy for STAs in an IEEE 802.11 WLAN by taking
into account explicitly two aspects of practical importance: (a) TCP-controlled
short file downloads interspersed with read times (motivated by web browsing),
and (b) different STAs associated with an AP at possibly different rates
(depending on distance from the AP). Our approach is based on two steps. First,
we consider an analytical model to obtain the aggregate AP throughput for long
TCP-controlled file downloads when STAs are associated at k different rates r1,
r2, : : :, rk; this extends earlier work in the literature. Second, we present
a 2-node closed queueing network model to approximate the expected
average-sized file download time for a user who shares the AP with other users
associated at a multiplicity of rates. These analytical results motivate the
proposed association policy, called the Estimated Delay based Association (EDA)
policy: Associate with the AP at which the expected file download time is the
least. Simulations indicate that for a web-browsing type traffic scenario, EDA
outperforms other policies that have been proposed earlier; the extent of
improvement ranges from 12.8% to 46.4% for a 9-AP network. To the best of our
knowledge, this is the first work that proposes an association policy tailored
specifically for web browsing. Apart from this, our analytical results could be
of independent interest

ABSTRACT_BEGIN
  Despite the tremendous success of BitTorrent, its swarming system suffers
from a fundamental limitation: lower or no availability of unpopular contents.
Recently, Menasche et al. has shown that bundling is a promising solution to
mitigate this availability problem; it improves the availability and reduces
download times for unpopular contents by combining multiple files into a single
swarm. There also have been studies on bundling strategies and performance
issues in bundled swarms. In spite of the recent surge of interest in the
benefits of and strategies for bundling, there are still little empirical
grounding for understanding, describing, and modeling it. This is the first
empirical study that measures and analyzes how prevalent contents bundling is
in BitTorrent and how peers access the bundled contents, in comparison to the
other non-bundled (i.e., single-filed) ones. To our surprise, we found that
around 70% of BitTorrent swarms contain multiple files, which indicate that
bundling has become widespread for contents sharing. We also show that the
amount of bytes shared in bundled swarms is estimated to be around 85% out of
all the BitTorrent contents logged in our datasets. Inspired from our findings,
we raise and discuss three important research questions in the field of file
sharing systems as well as future contents-oriented networking: i) bundling
strategies, ii) bundling-aware sharing systems in BitTorrent, and iii)
implications on content-oriented networking.

ABSTRACT_BEGIN
  This paper studies the problem of distributed computation over a network of
wireless sensors. While this problem applies to many emerging applications, to
keep our discussion concrete we will focus on sensor networks used for
structural health monitoring. Within this context, the heaviest computation is
to determine the singular value decomposition (SVD) to extract mode shapes
(eigenvectors) of a structure. Compared to collecting raw vibration data and
performing SVD at a central location, computing SVD within the network can
result in significantly lower energy consumption and delay. Using recent
results on decomposing SVD, a well-known centralized operation, into
components, we seek to determine a near-optimal communication structure that
enables the distribution of this computation and the reassembly of the final
results, with the objective of minimizing energy consumption subject to a
computational delay constraint. We show that this reduces to a generalized
clustering problem; a cluster forms a unit on which a component of the overall
computation is performed. We establish that this problem is NP-hard. By
relaxing the delay constraint, we derive a lower bound to this problem. We then
propose an integer linear program (ILP) to solve the constrained problem
exactly as well as an approximate algorithm with a proven approximation ratio.
We further present a distributed version of the approximate algorithm. We
present both simulation and experimentation results to demonstrate the
effectiveness of these algorithms.

ABSTRACT_BEGIN
  In this paper, we present the performance of different broadcast schemes for
multihop sensor networks based on mathematical modeling. In near future many
applications will demand multicast (Broadcast) communication feature from the
sensor networks. This broadcast feature does not use virtual carrier sensing
but relies on physical carrier sensing to reduce collision. For this paper, we
analyze the different broadcast schemes for multihop wireless sensor networks
and also calculated the achievable throughput.

ABSTRACT_BEGIN
  In low power wireless sensor networks, MAC protocols usually employ periodic
sleep/wake schedule to reduce idle listening time. Even though this mechanism
is simple and efficient, it results in high end-to-end latency and low
throughput. On the other hand, the previously proposed CSMA/CA-based MAC
protocols have tried to reduce inter-node interference at the cost of increased
latency and lower network capacity. In this paper we propose IAMAC, a CSMA/CA
sleep/wake MAC protocol that minimizes inter-node interference, while also
reduces per-hop delay through cross-layer interactions with the network layer.
Furthermore, we show that IAMAC can be integrated into the SP architecture to
perform its inter-layer interactions. Through simulation, we have extensively
evaluated the performance of IAMAC in terms of different performance metrics.
Simulation results confirm that IAMAC reduces energy consumption per node and
leads to higher network lifetime compared to S-MAC and Adaptive S-MAC, while it
also provides lower latency than S-MAC. Throughout our evaluations we have
considered IAMAC in conjunction with two error recovery methods, i.e., ARQ and
Seda. It is shown that using Seda as the error recovery mechanism of IAMAC
results in higher throughput and lifetime compared to ARQ.

ABSTRACT_BEGIN
  Dead Reckoning mechanism allows reducing the network utilization considerably
when used in Distributed Interactive Simulation Applications. However, this
technique often ignores available contextual information that may be
influential to the state of an entity, sacrificing remote predictive accuracy
in favor of low computational complexity. The remainder of this paper focuses
on the analysis of the Dead Reckoning Algorithms. Some contributions are
expected and overviews of the major bandwidth reduction techniques currently
investigated are discussed. A novel extension of Dead Reckoning based on ANFIS
systems is suggested to increase the network availability and fulfilling the
required QoS in such applications. The model shows it primary benefits
regarding the other research contributions, especially in the decision making
of the behavior of simulated entities.

ABSTRACT_BEGIN
  Real-Time availability of information is of most importance in large scale
distributed interactive simulation in network-centric communication.
Information generated from multiple federates must be distributed and made
available to interested parties and providing the required QoS for consistent
communication. The remainder of this paper discuss design alternative for
realizing high performance distributed interactive simulation (DIS) application
using the OMG Data Distribution Service (DDS), which is a QoS enabled
publish/subscribe platform standard for time-critical, data-centric and large
scale distributed networks. The considered application, in the civil domain, is
used for remote education in driving schools. An experimental design evaluates
the bandwidth and the latency performance of DDS and a comparison with the High
Level Architecture performance is given.

ABSTRACT_BEGIN
  We study a problem of quick detection of top-k Personalized PageRank lists.
This problem has a number of important applications such as finding local cuts
in large graphs, estimation of similarity distance and name disambiguation. In
particular, we apply our results to construct efficient algorithms for the
person name disambiguation problem. We argue that when finding top-k
Personalized PageRank lists two observations are important. Firstly, it is
crucial that we detect fast the top-k most important neighbours of a node,
while the exact order in the top-k list as well as the exact values of PageRank
are by far not so crucial. Secondly, a little number of wrong elements in top-k
lists do not really degrade the quality of top-k lists, but it can lead to
significant computational saving. Based on these two key observations we
propose Monte Carlo methods for fast detection of top-k Personalized PageRank
lists. We provide performance evaluation of the proposed methods and supply
stopping criteria. Then, we apply the methods to the person name disambiguation
problem. The developed algorithm for the person name disambiguation problem has
achieved the second place in the WePS 2010 competition.

ABSTRACT_BEGIN
  In various applications, the effect of errors in gradient-based iterations is
of particular importance when seeking saddle points of the Lagrangian function
associated with constrained convex optimization problems. Of particular
interest here are problems arising in power control applications, where network
utility is maximized subject to minimum signal-to-interference-plus-noise ratio
(SINR) constraints, maximum interference constraints, maximum received power
constraints, or simultaneous minimum and maximum SINR constraints. Especially
when the gradient iterations are executed in a disributed fashion, imperfect
exchanges among the link nodes may result in erroneous gradient vectors. In
order to assess and cope with such errors, two running averages (ergodic
sequences) are formed from the iterates generated by the perturbed saddle point
method, each with complementary strengths. Under the assumptions of problem
convexity and error boundedness, bounds on the constraint violation and the
suboptimality per iteration index are derived. The two types of running
averages are tested on a spectrum sharing problem with minimum and maximum SINR
constraints, as well as maximum interference constraints.

ABSTRACT_BEGIN
  We consider a cognitive radio network where primary users (PUs) employ
network coding for data transmissions. We view network coding as a spectrum
shaper, in the sense that it increases spectrum availability to secondary users
(SUs) and offers more structure of spectrum holes that improves the
predictability of the primary spectrum. With this spectrum shaping effect of
network coding, each SU can carry out adaptive channel sensing by dynamically
updating the list of the PU channels predicted to be idle while giving priority
to these channels when sensing. This dynamic spectrum access approach with
network coding improves how SUs detect and utilize temporal spectrum holes over
PU channels. Our results show that compared to the existing approaches based on
retransmission, both PUs and SUs can achieve higher stable throughput, thanks
to the spectrum shaping effect of network coding.

ABSTRACT_BEGIN
  User online behavior and interests will play a central role in future mobile
networks. We introduce a systematic method for large-scale multi-dimensional
analysis of online activity for thousands of mobile users across 79 buildings
over a variety of web domains. We propose a modeling approach based on
self-organizing maps (SOM) for discovering, organizing and visualizing
different mobile users' trends from billions of WLAN records. We find
surprisingly that users' trends based on domains and locations can be
accurately modeled using a self-organizing map with clearly distinct
characteristics. We also find many non-trivial correlations between different
types of web domains and locations. Based on our analysis, we introduce a
mixture model as an initial step towards realistic simulation of wireless
network usage.

ABSTRACT_BEGIN
  TCP performs poorly in networks with serious packet reordering. Processing
reordered packets in the TCP layer is costly and inefficient, involving
interaction of the sender and receiver. Motivated by the interrupt coalescing
mechanism that delivers packets upward for protocol processing in blocks, we
propose a new strategy, Sorting Reordered Packets with Interrupt Coalescing
(SRPIC), to reduce packet reordering in the receiver. SRPIC works in the
network device driver; it makes use of the interrupt coalescing mechanism to
sort the reordered packets belonging to the same TCP stream in a block of
packets before delivering them upward; each sorted block is internally ordered.
Experiments have proven the effectiveness of SRPIC against forward-path
reordering.

ABSTRACT_BEGIN
  In this paper we develop an intergraded model for request mechanism and data
transmission in the uplink phase in the presence of channel noise. This model
supports quality of service. The wireless channel is prone to many impairments.
Thus, certain techniques have to be developed to deliver data to the receiver.
We calculated the performance parameters for single and multichannel wireless
networks, like the requests throughput, data throughput and the requests
acceptance probability and data acceptance probability. The proposed model is
general model since it can be applied to different wireless networks such as
IEEE802.11a, IEEE802.16e, CDMA operated networks and Hiperlan\2.

ABSTRACT_BEGIN
  In this paper, we are interested in improving the performance of constructive
network coding schemes in lossy wireless environments.We propose I2NC - a
cross-layer approach that combines inter-session and intra-session network
coding and has two strengths. First, the error-correcting capabilities of
intra-session network coding make our scheme resilient to loss. Second,
redundancy allows intermediate nodes to operate without knowledge of the
decoding buffers of their neighbors. Based only on the knowledge of the loss
rates on the direct and overhearing links, intermediate nodes can make
decisions for both intra-session (i.e., how much redundancy to add in each
flow) and inter-session (i.e., what percentage of flows to code together)
coding. Our approach is grounded on a network utility maximization (NUM)
formulation of the problem. We propose two practical schemes, I2NC-state and
I2NC-stateless, which mimic the structure of the NUM optimal solution. We also
address the interaction of our approach with the transport layer. We
demonstrate the benefits of our schemes through simulations.

ABSTRACT_BEGIN
  We study the problem of maximizing the broadcast rate in peer-to-peer (P2P)
systems under \emph{node degree bounds}, i.e., the number of neighbors a node
can simultaneously connect to is upper-bounded. The problem is critical for
supporting high-quality video streaming in P2P systems, and is challenging due
to its combinatorial nature. In this paper, we address this problem by
providing the first distributed solution that achieves near-optimal broadcast
rate under arbitrary node degree bounds, and over arbitrary overlay graph. It
runs on individual nodes and utilizes only the measurement from their one-hop
neighbors, making the solution easy to implement and adaptable to peer churn
and network dynamics. Our solution consists of two distributed algorithms
proposed in this paper that can be of independent interests: a network-coding
based broadcasting algorithm that optimizes the broadcast rate given a
topology, and a Markov-chain guided topology hopping algorithm that optimizes
the topology. Our distributed broadcasting algorithm achieves the optimal
broadcast rate over arbitrary P2P topology, while previously proposed
distributed algorithms obtain optimality only for P2P complete graphs. We prove
the optimality of our solution and its convergence to a neighborhood around the
optimal equilibrium under noisy measurements or without time-scale separation
assumptions. We demonstrate the effectiveness of our solution in simulations
using uplink bandwidth statistics of Internet hosts.

ABSTRACT_BEGIN
  We propose distributed link reversal algorithms to circumvent communication
voids in geographic routing. We also solve the attendant problem of integer
overflow in these algorithms. These are achieved in two steps. First, we derive
partial and full link reversal algorithms that do not require one-hop neighbor
information, and convert a destination-disoriented directed acyclic graph (DAG)
to a destination-oriented DAG. We embed these algorithms in the framework of
Gafni and Bertsekas ("Distributed algorithms for generating loop-free routes in
networks with frequently changing topology", 1981) in order to establish their
termination properties. We also analyze certain key properties exhibited by our
neighbor oblivious link reversal algorithms, e.g., for any two neighbors, their
t-states are always consecutive integers, and for any node, its t-state size is
upper bounded by log(N). In the second step, we resolve the integer overflow
problem by analytically deriving one-bit full link reversal and two-bit partial
link reversal versions of our neighbor oblivious link reversal algorithms.

ABSTRACT_BEGIN
  Mobile Ad hoc Network (MANET) is a collection of wireless mobile nodes that
dynamically form a network temporarily without any support of central
administration. Moreover, Every node in MANET moves arbitrarily making the
multi-hop network topology to change randomly at unpredictable times. There are
several familiar routing protocols like DSDV, AODV, DSR, etc...which have been
proposed for providing communication among all the nodes in the network. This
paper presents a performance comparison of proactive and reactive protocols
DSDV, AODV and DSR based on metrics such as throughput, packet delivery ratio
and average end-to-end delay by using the NS-2 simulator.

ABSTRACT_BEGIN
  We study the asymptotic performance of two multi-hop overlaid ad-hoc networks
that utilize the same temporal, spectral, and spatial resources based on random
access schemes. The primary network consists of Poisson distributed legacy
users with density \lambda^{(p)} and the secondary network consists of Poisson
distributed cognitive radio users with density \lambda^{(s)} =
(\lambda^{(p)})^{\beta} (\beta>0, \beta \neq 1) that utilize the spectrum
opportunistically. Both networks are decentralized and employ ALOHA medium
access protocols where the secondary nodes are additionally equipped with
range-limited perfect spectrum sensors to monitor and protect primary
transmissions. We study the problem in two distinct regimes, namely \beta>1 and
0<\beta<1. We show that in both cases, the two networks can achieve their
corresponding stand-alone throughput scaling even without secondary spectrum
sensing (i.e., the sensing range set to zero); this implies the need for a more
comprehensive performance metric than just throughput scaling to evaluate the
influence of the overlaid interactions. We thus introduce a new criterion,
termed the asymptotic multiplexing gain, which captures the effect of
inter-network interferences with different spectrum sensing setups. With this
metric, we clearly demonstrate that spectrum sensing can substantially improve
primary network performance when \beta>1. On the contrary, spectrum sensing
turns out to be unnecessary when \beta<1 and setting the secondary network's
ALOHA parameter appropriately can substantially improve primary network
performance.

ABSTRACT_BEGIN
  Network virtualization allows one to build dynamic distributed systems in
which resources can be dynamically allocated at locations where they are most
useful. In order to fully exploit the benefits of this new technology,
protocols need to be devised which react efficiently to changes in the demand.
This paper argues that the field of online algorithms and competitive analysis
provides useful tools to deal with and reason about the uncertainty in the
request dynamics, and to design algorithms with provable performance
guarantees. As a case study, we describe a system (e.g., a gaming application)
where network virtualization is used to support thin client applications for
mobile devices to improve their QoS. By decoupling the service from the
underlying resource infrastructure, it can be migrated closer to the current
client locations while taking into account migration cost. This paper
identifies the major cost factors in such a system, and formalizes the
corresponding optimization problem. Both randomized and deterministic, gravity
center based online algorithms are presented which achieve a good tradeoff
between improved QoS and migration cost in the worst-case, both for service
migration within an infrastructure provider as well as for networks supporting
cross-provider migration. The paper reports on our simulation results and also
presents an explicit construction of an optimal offline algorithm which allows,
e.g., to evaluate the competitive ratio empirically.

ABSTRACT_BEGIN
  Tor is a popular low-latency anonymity network. However, Tor does not protect
against the exploitation of an insecure application to reveal the IP address
of, or trace, a TCP stream. In addition, because of the linkability of Tor
streams sent together over a single circuit, tracing one stream sent over a
circuit traces them all. Surprisingly, it is unknown whether this linkability
allows in practice to trace a significant number of streams originating from
secure (i.e., proxied) applications. In this paper, we show that linkability
allows us to trace 193% of additional streams, including 27% of HTTP streams
possibly originating from "secure" browsers. In particular, we traced 9% of Tor
streams carried by our instrumented exit nodes. Using BitTorrent as the
insecure application, we design two attacks tracing BitTorrent users on Tor. We
run these attacks in the wild for 23 days and reveal 10,000 IP addresses of Tor
users. Using these IP addresses, we then profile not only the BitTorrent
downloads but also the websites visited per country of origin of Tor users. We
show that BitTorrent users on Tor are over-represented in some countries as
compared to BitTorrent users outside of Tor. By analyzing the type of content
downloaded, we then explain the observed behaviors by the higher concentration
of pornographic content downloaded at the scale of a country. Finally, we
present results suggesting the existence of an underground BitTorrent ecosystem
on Tor.

ABSTRACT_BEGIN
  This paper formulates the authentication planning problem when network coding
is implemented in a wireless sensor network. The planning problem aims at
minimizing the energy consumed by the security application which is guarantied
using message authentication codes. This paper proposes a binary non-linear
optimization formulation for this planning problem whose decision variables are
the authentication decision of the nodes and the MAC modes of operation. It is
illustrated for a butterfly topology. Results show that there is a real
trade-off between energy efficiency and message throughput in this context.

ABSTRACT_BEGIN
  This technical report presents the implementation of a Network Coding module
in WSNet - a Wireless Sensor Network simulator. This implementation provides a
generic programming interface to allow an easy specialization of different
coding strategies: random, source/destination-oriented, intra/inter-flow, etc.

ABSTRACT_BEGIN
  A widely adopted two-dimensional Markov chain model of the IEEE 802.11 DCF
was introduced by Bianchi to characterize the backoff behavior of a single node
under a saturated traffic condition. Using this approach, we propose a queuing
model for the 802.11 DCF under a non-saturated traffic environment. The input
buffer of each node is modeled as a Geo/G/1 queue, and the packet service time
distribution is derived from Markov state space of 802.11 DCF with the
underlying scheduling algorithm. The DCF defines two access mechanisms, namely
the Basic access mechanism and the request-to-send/clear-to-send (RTS/CTS)
access mechanism. Based on our model, performance analyses of both schemes are
studied with probabilistic exponential backoff scheduling. We obtain the
characteristic equation of network throughput and expressions of packet
queueing delay. Specifically, we obtain the stable throughput and bounded delay
regions with respect to the retransmission factor according to the basic
queueing analysis. For both access schemes, the bounded delay region is a
subset of the stable throughput region. Our results show that the RTS/CTS
access mechanism is more stable and performs better than the Basic access
mechanism. The analysis in this paper is verified by simulation results.

ABSTRACT_BEGIN
  This paper presents and analyses the implementation of a novel active queue
management (AQM) named FavorQueue that aims to improve delay transfer of short
lived TCP flows over best-effort networks. The idea is to dequeue packets that
do not belong to a flow previously enqueued first. The rationale is to mitigate
the delay induced by long-lived TCP flows over the pace of short TCP data
requests and to prevent dropped packets at the beginning of a connection and
during recovery period. Although the main target of this AQM is to accelerate
short TCP traffic, we show that FavorQueue does not only improve the
performance of short TCP traffic but also improves the performance of all TCP
traffic in terms of drop ratio and latency whatever the flow size. In
particular, we demonstrate that FavorQueue reduces the loss of a retransmitted
packet, decreases the number of dropped packets recovered by RTO and improves
the latency up to 30% compared to DropTail. Finally, we show that this scheme
remains compliant with recent TCP updates such as the increase of the initial
slow-start value.

ABSTRACT_BEGIN
  Detecting misbehavior (such as transmissions of false information) in
vehicular ad hoc networks (VANETs) is very important problem with wide range of
implications including safety related and congestion avoidance applications. We
discuss several limitations of existing misbehavior detection schemes (MDS)
designed for VANETs. Most MDS are concerned with detection of malicious nodes.
In most situations, vehicles would send wrong information because of selfish
reasons of their owners, e.g. for gaining access to a particular lane. Because
of this (\emph{rational behavior}), it is more important to detect false
information than to identify misbehaving nodes. We introduce the concept of
data-centric misbehavior detection and propose algorithms which detect false
alert messages and misbehaving nodes by observing their actions after sending
out the alert messages. With the data-centric MDS, each node can independently
decide whether an information received is correct or false. The decision is
based on the consistency of recent messages and new alert with reported and
estimated vehicle positions. No voting or majority decisions is needed, making
our MDS resilient to Sybil attacks. Instead of revoking all the secret
credentials of misbehaving nodes, as done in most schemes, we impose fines on
misbehaving nodes (administered by the certification authority), discouraging
them to act selfishly. This reduces the computation and communication costs
involved in revoking all the secret credentials of misbehaving nodes.

ABSTRACT_BEGIN
  In this paper we expose a dynamic traffic-classification scheme to support
multimedia applications such as voice and broadband video transmissions over
IEEE 802.11 Wireless Local Area Networks (WLANs). Obviously, over a Wi-Fi link
and to better serve these applications - which normally have strict bounded
transmission delay or minimum link rate requirement - a service differentiation
technique can be applied to the media traffic transmitted by the same mobile
node using the well-known 802.11e Enhanced Distributed Channel Access (EDCA)
protocol. However, the given EDCA mode does not offer user differentiation,
which can be viewed as a deficiency in multi-access wireless networks.
Accordingly, we propose a new inter-node priority access scheme for IEEE
802.11e networks which is compatible with the EDCA scheme. The proposed scheme
joins a dynamic user-weight to each mobile station depending on its outgoing
data, and therefore deploys inter-node priority for the channel access to
complement the existing EDCA inter-frame priority. This provides efficient
quality of service control across multiple users within the same coverage area
of an access point. We provide performance evaluations to compare the proposed
access model with the basic EDCA 802.11 MAC protocol mode to elucidate the
quality improvement achieved for multimedia communication over 802.11 WLANs.

ABSTRACT_BEGIN
  In view of the fact that routing algorithms are network layer entities and
the varying performance of any routing algorithm depends on the underlying
networks. Localized routing algorithms avoid the problems associated with the
maintenance of global network state by using statistics of flow blocking
probabilities. We developed a new network parameter that can be used to predict
which network topology gives better performance on the quality of localized QoS
routing algorithms. Using this parameter we explore a simple model that can be
rewired to introduce increasing the performance. We find that this model have
small characteristic path length. Simulations of random and complex networks
used to show that the performance is significantly affected by the level of
connectivity.

ABSTRACT_BEGIN
  Techniques for channel allocation in cellular networks have been an area of
intense research interest for many years. An efficient channel allocation
scheme can significantly reduce call-blocking and calldropping probabilities.
Another important issue is to effectively manage the power requirements for
communication. An efficient power control strategy leads to reduced power
consumption and improved signal quality. In this paper, we present a novel
integer linear program (ILP) formulation that jointly optimizes channel
allocation and power control for incoming calls, based on the
carrier-to-interference ratio (CIR). In our approach we use a hybrid channel
assignment scheme, where an incoming call is admitted only if a suitable
channel is found such that the CIR of all ongoing calls on that channel, as
well as that of the new call, will be above a specified value. Our formulation
also guarantees that the overall power requirement for the selected channel
will be minimized as much as possible and that no ongoing calls will be dropped
as a result of admitting the new call. We have run simulations on a benchmark
49 cell environment with 70 channels to investigate the effect of different
parameters such as the desired CIR. The results indicate that our approach
leads to significant improvements over existing techniques.

ABSTRACT_BEGIN
  Next generation Internet is highly concerned about the issue of reliability.
Principally, the foundation of reliability is authentication of the source IP
address. With the signature-and-verification based defense mechanisms available
today, unfortunately, there is a lack of hierarchical architecture, which makes
the structure of the trust alliance excessively flat and single. Moreover, with
the increasing scale of the trust alliance, costs of validation grow so quickly
that they do not adapt to incremental deployment. Via comparison with
traditional solutions, this article proposes a hierarchical, inter-domain
authenticated source address validation solution named SafeZone. SafeZone
employs two intelligent designs, lightweight tag replacement and a hierarchical
partitioning scheme, each of which helps to ensure that SafeZone can construct
trustworthy and hierarchical trust alliances without the negative influences
and complex operations on de facto networks. Extensive experiments also
indicate that SafeZone can effectively obtain the design goals of a
hierarchical architecture, along with lightweight, loose coupling and
"multi-fence support" and as well as an incremental deployment scheme.

ABSTRACT_BEGIN
  The maximum traffic arrival rate at the network for a given delay guarantee
(delay constrained throughput) has been well studied for wired channels.
However, few results are available for wireless channels, especially when
multiple antennas are employed at the transmitter and receiver. In this work,
we analyze the network delay constrained throughput of a multiple input
multiple output (MIMO) wireless channel with time-varying spatial correlation.
The MIMO channel is modeled via its virtual representation, where the
individual spatial paths between the antenna pairs are Gilbert-Elliot channels.
The whole system is then described by a K-State Markov chain, where K depends
upon the degree of freedom (DOF) of the channel. We prove that the DOF based
modeling is indeed accurate. Furthermore, we study the impact of the delay
requirements at the network layer, violation probability and the number of
antennas on the throughput under different fading speeds and signal strength.

ABSTRACT_BEGIN
  This document contains the documentation of TOBI (Tools for BCI) Interface A
(TiA).
  TiA is a standardized interface to transmit raw biosignals. It is able to
deal with multirate and block-oriented data transmission. Data is distinguished
by different signal types (e.g., EEG, EOG, NIRS,...), whereby those signals can
be acquired at the same time from different acquisition devices. TiA is built
as a client-server model. Multiple clients can connect to one server.
Information is exchanged via a control- and a separated data connection.
Control commands and meta information are transmitted over the control
connection. Raw biosignal data is delivered using the data connection in a
unidirectional way. For this purpose a standardized handshaking protocol and
raw data packet have been developed. Thus, an abstraction layer between
hardware devices and data processing was evolved facilitating standardization.

ABSTRACT_BEGIN
  Widespread use of sensors and multisensory personal devices generate a lot of
personal information. Sharing this information with others could help in
various ways. However, this information may be misused when shared with all.
Sharing of information between trusted parties overcomes this problem. This
paper describes a model to share information based on interactions and opinions
to build trust among peers. It also considers institutional and other controls,
which influence the behaviour of the peers. The trust and control build
confidence. The computed confidence bespeaks whether to reveal information or
not thereby increasing trusted cooperation among peers.

ABSTRACT_BEGIN
  Target coverage problem in wireless sensor networks is concerned with
maximizing the lifetime of the network while continuously monitoring a set of
targets. A sensor covers targets which are within the sensing range. For a set
of sensors and a set of targets, the sensor-target coverage relationship is
assumed to be known. A sensor cover is a set of sensors that covers all the
targets. The target coverage problem is to determine a set of sensor covers
with maximum aggregated lifetime while constraining the life of each sensor by
its initial battery life. The problem is proved to be NP-complete and heuristic
algorithms to solve this problem are proposed. In the present study, we give a
unified interpretation of earlier algorithms and propose a new and efficient
algorithm. We show that all known algorithms are based on a common reasoning
though they seem to be derived from different algorithmic paradigms. We also
show that though some algorithms guarantee bound on the quality of the
solution, this bound is not meaningful and not practical too. Our
interpretation provides a better insight to the solution techniques. We propose
a new greedy heuristic which prioritizes sensors on residual battery life. We
show empirically that the proposed algorithm outperforms all other heuristics
in terms of quality of solution. Our experimental study over a large set of
randomly generated problem instances also reveals that a very na\"ive greedy
approach yields solutions which is reasonably (appx. 10%) close to the actual
optimal solutions.

ABSTRACT_BEGIN
  This paper considers non-cooperative and fully-distributed power-allocation
for secondary-users (SUs) in spectrum-sharing environments when
normalized-interference to each secondary-user is uncertain. We model each
uncertain parameter by the sum of its nominal (estimated) value and a bounded
additive error in a convex set, and show that the allocated power always
converges to its equilibrium, called robust Nash equilibrium (RNE). In the case
of a bounded and symmetric uncertainty set, we show that the power allocation
problem for each SU is simplified, and can be solved in a distributed manner.
We derive the conditions for RNE's uniqueness and for convergence of the
distributed algorithm; and show that the total throughput (social utility) is
less than that at NE when RNE is unique. We also show that for multiple RNEs,
the the social utility may be higher at a RNE as compared to that at the
corresponding NE, and demonstrate that this is caused by SUs' orthogonal
utilization of bandwidth for increasing the social utility. Simulations confirm
our analysis.

ABSTRACT_BEGIN
  We analyze push and pull for data collection in wireless sensor networks.
Most applications to date use the traditional push approach, where nodes
transmit sensed data immedi- ately to the sink. Using a pull approach, nodes
store the data in their local flash memory, and only engage in commu- nication
during dedicated collection phases. We show how one can transform an existing
push-based collection protocol into a pull-based one, and compare the power
consumption of both approaches on a 35-node testbed. Our results show that
substantial energy gains are possible with pull, provided that the application
can tolerate a long latency.

ABSTRACT_BEGIN
  The interference imposes a significant negative impact on the performance of
wireless networks. With the continuous deployment of larger and more
sophisticated wireless networks, reducing interference in such networks is
quickly being focused upon as a problem in today's world. In this paper we
analyze the interference reduction problem from a graph theoretical viewpoint.
A graph coloring methods are exploited to model the interference reduction
problem. However, additional constraints to graph coloring scenarios that
account for various networking conditions result in additional complexity to
standard graph coloring. This paper reviews a variety of algorithmic solutions
for specific network topologies.

ABSTRACT_BEGIN
  Now a day's many organizations are required to communicate online on a daily
basis, 24-hour, seven-days-a-week, to gain the desired competitive advantages
and profits; although there are a variety of disruptions that may occur within
business application such as broken (off-line) database-links and unhanded
database exceptions. Such cases will end the automated business work, and force
business users to continue business procedures and functionalities via paper
work, which causes additional resources with less business competitive
advantages. In this paper, we will propose a new model in which we embed short
message services (SMS) within business applications using the SMS Gateway such
as "Ozeki Message Server", and programmed application packages. By using our
proposed model, we can maintain business continuity when a partial disruption
occurs and then switch to our application model. As a result to the
experimental work, we conclude that our model supports business continuity
since it supports the account balance modification while the database link is
disrupted. In addition, we carried out each step twice and the scenario was
reliable since all of its steps were reliable.

ABSTRACT_BEGIN
  Several emerging classes of applications that run over wireless networks have
a need for mathematical models and tools to systematically characterize the
reliability of the network. We propose two metrics for measuring the
reliability of wireless mesh routing topologies, one for flooding and one for
unicast routing. The Flooding Path Probability (FPP) metric measures the
end-to-end packet delivery probability when each node broadcasts a packet after
hearing from all its upstream neighbors. The Unicast Retransmission Flow (URF)
metric measures the end-to-end packet delivery probability when a relay node
retransmits a unicast packet on its outgoing links until it receives an
acknowledgement or it tries all the links. Both metrics rely on specific packet
forwarding models, rather than heuristics, to derive explicit expressions of
the end-to-end packet delivery probability from individual link probabilities
and the underlying connectivity graph.
  We also propose a distributed, greedy algorithm that uses the URF metric to
construct a reliable routing topology. This algorithm constructs a Directed
Acyclic Graph (DAG) from a weighted, undirected connectivity graph, where each
link is weighted by its success probability. The algorithm uses a vector of
decreasing reliability thresholds to coordinate when nodes can join the routing
topology. Simulations demonstrate that, on average, this algorithm constructs a
more reliable topology than the usual minimum hop DAG.

ABSTRACT_BEGIN
  Distributed denial of service attacks are often considered a security
problem. While this may be the way to view the problem with today's Internet,
new network architectures attempting to address the issue should view it as a
scalability problem. In addition, they need to address the problem based on a
rigorous foundation.

ABSTRACT_BEGIN
  Nowadays scarcity of spectrum availability is increasing highly. Adding
cognition to the existing Wireless Sensor Network (WSN) infrastructure will
help in this situation. As sensor nodes in WSN are limited with some constrains
like power, efforts are required to increase the lifetime and other performance
measures of the network. In this paper we propose the idea of Doubly Cognitive
WSN. The basic idea is to progressively allocate the sensing resources only to
the most promising areas of the spectrum. This work is based on Artificial
Neural Network as well as on Support Vector Machine (SVM) concept. As the load
of sensing resource is reduced significantly, this approach will save the
energy of the nodes, and also reduce the sensing time dramatically. The
proposed work can be enhanced by doing the pattern analysis thing after a
sufficiently long time again and again to review the strategy of sensing. Thus
Doubly Cognitive WSN will enable current WSN to overcome the spectrum scarcity
as well as save the energy of the sensor nodes.

ABSTRACT_BEGIN
  Minimization of the number of cluster heads in a wireless sensor network is a
very important problem to reduce channel contention and to improve the
efficiency of the algorithm when executed at the level of cluster-heads. In
this paper, we propose an efficient method based on genetic algorithms (GAs) to
solve a sensor network optimization problem. Long communication distances
between sensors and a sink in a sensor network can greatly drain the energy of
sensors and reduce the lifetime of a network. By clustering a sensor network
into a number of independent clusters using a GA, we can greatly minimize the
total communication distance, thus prolonging the network lifetime. Simulation
results show that our algorithm can quickly find a good solution.

ABSTRACT_BEGIN
  Intersession network coding (NC) can provide significant performance benefits
via mixing packets at wireless routers; these benefits are especially
pronounced when NC is applied in conjunction with intelligent link scheduling.
NC however imposes certain processing operations, such as encoding, decoding,
copying and storage. When not utilized carefully, all these operations can
induce tremendous processing overheads in practical, wireless, multi-rate
settings. Our measurements with prior NC implementations suggest that such
processing operations severely degrade the router throughput, especially at
high bit rates. Motivated by this, we design {\bf NCRAWL}, a Network Coding
framework for Rate Adaptive Wireless Links. The design of NCRAWL facilitates
low overhead NC functionalities, thereby effectively approaching the
theoretically expected capacity benefits of joint NC and scheduling. We
implement and evaluate NCRAWL on a wireless testbed. Our experiments
demonstrate that NCRAWL meets the theoretical predicted throughput gain while
requiring much less CPU processing, compared to related frameworks.

ABSTRACT_BEGIN
  To understand the factors that encourage the deployment of a new networking
technology, we must be able to model how such technology gets deployed. We
investigate how network structure influences deployment with a simple
deployment model and different network models through computer simulations. The
results indicate that a realistic model of networking technology deployment
should take network structure into account.

ABSTRACT_BEGIN
  We deal with the problem of streaming multiple video streams between pairs of
nodes in a multi-hop wireless ad hoc network. The nodes are static, know their
locations, and are synchronized (via GPS). We introduce a new interference
model that uses variable interference radiuses. We present an algorithm for
computing a frequency assignment and a schedule whose goal is to maximize
throughput over all the video streams. In addition, we developed a localized
flow-control mechanism to stabilize the queue lengths.
  We simulated traffic scheduled by the algorithm using OMNET++/MixiM (i.e.,
physical SINR interference model with 802.11g) to test whether the computed
throughput is achieved. The results of the simulation show that the computed
solution is \SINR-feasible and achieves predictable stable throughputs.

ABSTRACT_BEGIN
  The mmWave communication system is operating at a regime with high number of
antennas and very limited number of RF analog chains. Large number of antennas
are used to extend the communication range for recovering the high path loss
while fewer RF analog chains are designed to reduce transmit and processing
power and hardware complexity. In this regime, typical MIMO algorithms are not
applicable.
  Before any communication starts, devices are needed to align their beam
pointing angles towards each other. An efficient searching protocol to obtain
the best beam angle pair is therefore needed. It is called BeamForming (BF)
training protocol.
  This paper presents a new BF training technique called beam coding. Each beam
angle is assigned unique signature code. By coding multiple beam angles and
steering at their angles simultaneously in a training packet, the best beam
angle pair can be obtained in a few packets. The proposed BF training technique
not only shows the robustness in non-line-of-sight environment, but also
provides very flat power variations within a packet in contrast to the IEEE
802.11ad standard whose scheme may lead to large dynamic range of signals due
to beam angles varying across a training packet.

ABSTRACT_BEGIN
  Enabling real time applications in wireless sensor networks requires certain
delay and bandwidth which pose more challenges in the design of routing
protocols. The algorithm that is used for packet routing in such applications
should be able to establish a tradeoff between end to end delay parameter and
energy consumption. In this paper, we propose a new multi path routing
algorithm for real time applications in wireless sensor networks namely QEMPAR
which is QoS aware and can increase the network lifetime. Simulation results
show that the proposed algorithm is more efficient than previous algorithms in
providing quality of service requirements of real-time applications.

ABSTRACT_BEGIN
  Nowadays, multimedia and real-time applications consume much network
resources and so, need high flow rates and very small transfer delay. The
current ad hoc networks (MANETs), in their original state, are not able to
satisfy the requirements of quality of service (QoS). Researches for improving
QoS in these networks are main topics and a subject of intensive researches. In
Adhoc networks, the routing phase plays an important role for improving QoS.
Numerous routing protocols (proactive, reactive and hybrid) were proposed. AODV
(Adhoc On demand Distance Vector) is probably the more treated in literature In
this article, we propose a new variant based on the AODV which gives better
results than the original AODV protocol with respect of a set of QoS parameters
and under different constraints, taking into account the limited resources of
mobile environments (bandwidth, energy, etc...). The proposed variant (M-AODV)
suggests that the discovering operation for paths reconstruction should be done
from the source. It also defines a new mechanism for determining multiple
disjoint (separated) routes. To validate our solution, simulations were made
under Network Simulator (NS2). We measure traffic control and packet loss rate
under diverse constraints (mobility, energy and scale).

ABSTRACT_BEGIN
  Coloring is used in wireless networks to improve communication efficiency,
mainly in terms of bandwidth, energy and possibly end-to-end delays. In this
research report, we define the h-hop node coloring problem, with h any positive
integer. We prove that the associated decision problem is NP-complete. We then
present a 3-hop distributed coloring algorithm that is optimized for dense
networks: a node does not need to exchange the priorities and colors of its
2-hop neighbors. Through simulation results, we highlight the impact of
priority assignment on the number of colors obtained for any network. We then
focus on grids and identify a color pattern that can be reproduced to color the
whole grid. We show how the coloring algorithm can use regularity properties to
obtain a periodic color pattern with the optimal number of colors. We then
consider grids with holes and study how to extend our results.

ABSTRACT_BEGIN
  With the fast development of video and voice network applications, CDN
(Content Distribution Networks) and P2P (Peer-to-Peer) content distribution
technologies have gradually matured. How to effectively use Internet resources
thus has attracted more and more attentions. For the study of resource pricing,
a whole pricing strategy containing pricing models, mechanisms and methods
covers all the related topics. We first introduce three basic Internet resource
pricing models through an Internet cost analysis. Then, with the evolution of
service types, we introduce several corresponding mechanisms which can ensure
pricing implementation and resource allocation. On network resource pricing
methods, we discuss the utility optimization in economics, and emphasize two
classes of pricing methods (including system optimization and entities'
strategic optimizations). Finally, we conclude the paper and forecast the
research direction on pricing strategy which is applicable to novel service
situation in the near future.

ABSTRACT_BEGIN
  This paper argues for the adoption of a information centric system model
instead of the current service-oriented one. We present an architecture for a
global information storage and dissemination network which provides for
efficient interaction and coordination among autonomous actors through a shared
information space. We believe that the resulting, loosely coupled systems,
while probabilistic in nature, will lead to robust outcomes at large scales.

ABSTRACT_BEGIN
  Mobile Adhoc Network is a kind of wireless ad hoc network where nodes are
connected wirelessly and the network is self configuring. MANET may work in a
standalone manner or may be a part of another network. In this paper we have
compared Random Walk Mobility Model and Random Waypoint Mobility Model over two
reactive routing protocols Dynamic Source Routing (DSR) and Adhoc On-Demand
Distance Vector Routing (AODV) protocol and one Proactive routing protocol
Distance Sequenced Distance Vector Routing (DSDV) Our analysis showed that DSR,
AODV & DSDV under Random Walk and Random Way Point Mobility models have similar
results for similar inputs however as the pause time increases so does the
difference in performance rises. They show that their motion, direction, angle
of direction, speed is same under both mobility models. We have made their
analysis on packet delivery ratio, throughput and routing overhead. We have
tested them with different criteria like different number of nodes, speed and
different maximum number of connections.

ABSTRACT_BEGIN
  Consider a wireless network of n nodes represented by a graph G=(V, E) where
an edge (i,j) models the fact that transmissions of i and j interfere with each
other, i.e. simultaneous transmissions of i and j become unsuccessful. Hence it
is required that at each time instance a set of non-interfering nodes
(corresponding to an independent set in G) access the wireless medium. To
utilize wireless resources efficiently, it is required to arbitrate the access
of medium among interfering nodes properly. Moreover, to be of practical use,
such a mechanism is required to be totally distributed as well as simple. As
the main result of this paper, we provide such a medium access algorithm. It is
randomized, totally distributed and simple: each node attempts to access medium
at each time with probability that is a function of its local information. We
establish efficiency of the algorithm by showing that the corresponding network
Markov chain is positive recurrent as long as the demand imposed on the network
can be supported by the wireless network (using any algorithm). In that sense,
the proposed algorithm is optimal in terms of utilizing wireless resources. The
algorithm is oblivious to the network graph structure, in contrast with the
so-called `polynomial back-off' algorithm by Hastad-Leighton-Rogoff (STOC '87,
SICOMP '96) that is established to be optimal for the complete graph and
bipartite graphs (by Goldberg-MacKenzie (SODA '96, JCSS '99)).

ABSTRACT_BEGIN
  We present a novel feedback protocol for wireless broadcast networks that
utilize linear network coding. We consider transmission of packets from one
source to many receivers over a single-hop broadcast erasure channel. Our
method utilizes a predictive model to request feedback only when the
probability that all receivers have completed decoding is significant. In
addition, our proposed NACK-based feedback mechanism enables all receivers to
request, within a single time slot, the number of retransmissions needed for
successful decoding. We present simulation results as well as analytical
results that show the favorable scalability of our technique as the number of
receivers, file size, and packet erasure probability increase. We also show the
robustness of this scheme to uncertainty in the predictive model, including
uncertainty in the number of receiving nodes and the packet erasure
probability, as well as to losses of the feedback itself. Our scheme, SMART, is
shown to perform nearly as well as an omniscient transmitter that requires no
feedback. Furthermore, SMART, is shown to outperform current state of the art
methods at any given erasure probability, file size, and numbers of receivers.

ABSTRACT_BEGIN
  The goal of traffic management is efficiently utilizing network resources via
adapting of source sending rates and routes selection. Traditionally, this
problem is formulated into a utilization maximization problem. The single-path
routing scheme fails to react to instantaneous network congestion. Multi-path
routing schemes thus have been proposed aiming at improving network efficiency.
Unfortunately, the natural optimization problem to consider is concave but not
strictly concave. It thus brings a huge challenge to design stable multi-path
congestion control algorithms.
  In this paper, we propose a generalized multi-path utility maximization model
to consider the problem of routes selection and flow control, and derive a
family of multi-path dual congestion control algorithms. We show that the
proposed algorithms are stable in the absence of delays. We also derive
decentralized and scalable sufficient conditions for a particular scheme when
propagation delays exist in networks. Simulations are implemented using both
Matlab and NS2, on which evaluation of the proposed multi-path dual algorithms
is exerted. The comparison results, between the proposed algorithms and the
other two existing algorithms, show that the proposed multi-path dual
algorithms with appropriate parameter settings can achieve a stable aggregated
throughput while maintaining fairness among the involved users.

ABSTRACT_BEGIN
  Mesurer avec pr\'ecision la dynamique des graphes de terrain est une t\^ache
difficile, car les propri\'et\'es observ\'ees peuvent \^etre biais\'ees pour
diff\'erentes raisons, en particulier le fait que la p\'eriode de mesure soit
finie. Dans ce papier, nous introduisons une m\'ethodologie g\'en\'erale qui
nous permet de savoir si la fen\^etre d'observation est suffisamment longue
pour caract\'eriser une propri\'et\'e donn\'ee dans n'importe quel syst\`eme
dynamique. Nous appliquons cette m\'ethodologie \`a l'\'etude des dur\'ees de
sessions et des dur\'ees de vie des fichiers sur deux jeux de donn\'ees P2P.
Nous montrons que le comportement des propri\'et\'es est diff\'erent : pour les
dur\'ees de sessions, notre m\'ethodologie nous permet de caract\'eriser avec
pr\'ecision la forme de leur distribution. Par contre, pour les dur\'ees de vie
des fichiers, nous montrons que cette propri\'et\'e ne peut pas \^etre
caract\'eris\'ee, soit parce qu'elle n'est pas stationnaire, soit parce que la
dur\'ee de notre mesure est trop courte.

ABSTRACT_BEGIN
  In this paper, we first propose a general interpolation algorithm in a free
module of a linearized polynomial ring, and then apply this algorithm to decode
several important families of codes, Gabidulin codes, KK codes and MV codes.
Our decoding algorithm for Gabidulin codes is different from the polynomial
reconstruction algorithm by Loidreau. When applied to decode KK codes, our
interpolation algorithm is equivalent to the Sudan-style list-1 decoding
algorithm proposed by K/"otter and Kschischang for KK codes. The general
interpolation approach is also capable of solving the interpolation problem for
the list decoding of MV codes proposed by Mahdavifar and Vardy, and has a lower
complexity than solving linear equations.

ABSTRACT_BEGIN
  The exponential increase of multimedia services by the mobile users requires
seamless connectivity with cost effective Quality of Service QoS provisioning.
For providing such on-demand QoS, the network needs to utilize the radio
channels among the Mobile Hosts (MHs) effectively. We use vector genetic
algorithm VGA for temporal imploration of sharable channel(s) from the
neighbouring cells to fulfill the needs of a cell. We propose a new micro-level
temporal channel imploration mechanism MiCi, which promptly allocates available
borrowing channel s of the neighbouring cell(s) to the needy cell. The novelty
of MiCi is scalability, high availability, and on demand allocation of the
channels to the desired cells. The performance of our model has been tested by
simulation against a standard FCA scheme as well as a Greedy Borrowing
Heuristic. In all the test cases MiCi shows promising results in comparison to
both the schemes.

ABSTRACT_BEGIN
  Seeders (peers that do not request anything but contribute to the system) are
a powerful concept in peer-to-peer (P2P). They allow to leverage the capacities
of a P2P system. While seeding is a natural idea for filesharing or
video-on-demand applications, it seems somehow counter-intuitive in the context
of live streaming. This paper aims at describing the feasibility and
performance of P2P live seeding. After a formal definition of "live seeding"
and efficiency, we consider the theoretical performance of systems where the
overhead is neglected. We then propose a linear overhead model and extend the
results for this model, for a single seeder and for a set of seeders as well
(it is not always possible to perfectly aggregate individual efficiencies in a
given system).

ABSTRACT_BEGIN
  In this work, we develop a distributed source routing algorithm for topology
discovery suitable for ISP transport networks, that is however inspired by
opportunistic algorithms used in ad hoc wireless networks. We propose a
plug-and-play control plane, able to find multiple paths toward the same
destination, and introduce a novel algorithm, called adaptive probabilistic
flooding, to achieve this goal. By keeping a small amount of state in routers
taking part in the discovery process, our technique significantly limits the
amount of control messages exchanged with flooding -- and, at the same time, it
only minimally affects the quality of the discovered multiple path with respect
to the optimal solution. Simple analytical bounds, confirmed by results
gathered with extensive simulation on four realistic topologies, show our
approach to be of high practical interest.

ABSTRACT_BEGIN
  The importance of wireless communication is increasing day by day throughout
the world due to cellular and broadband technologies. Everyone around the world
would like to be connected seamlessly anytime anywhere through the best
network. The 4G wireless system must have the capability to provide high data
transfer rates, quality of services and seamless mobility. In 4G, there are a
large variety of heterogeneous networks. The users for variety of applications
would like to utilize heterogeneous networks on the basis of their preferences
such as real time, high availability and high bandwidth. When connections have
to switch between heterogeneous networks for performance and high availability
reasons, seamless vertical handoff is necessary. The requirements like
capability of the network, handoff latency, network cost, network conditions,
power consumption and user's preferences must be taken into consideration
during vertical handoff. In this paper, we have extracted the requirements of a
vertical handoff from the literature surveyed. The evaluation of the existing
work is also being done on the basis of required parameters for vertical
handoff. A sophisticated, adaptive and intelligent approach is required to
implement the vertical handoff mechanism in 4G wireless networks to produce an
effective service for the user by considering dynamic and non dynamic
parameters.

ABSTRACT_BEGIN
  Loss tomography has received considerable attention in recent years and a
number of estimators based on maximum likelihood (ML) or Bayesian principles
have been proposed. Almost all of the estimators are devoted to the tree
topology despite the general topology is more common in practice. There has
been few likelihood function devoted to the general topology, not to mention
the estimator. To overcome this, two sets of sufficient statistics for the tree
and general topologies, respectively, are proposed in this paper. Using the
statistics, two likelihood functions, one for a topology, are proposed here and
subsequently two likelihood equations for the general topology, one is
link-based and the other is path-based, are obtained. In addition, a dependence
between subtrees in terms of their estimates is identified for the general
topology and a divide-and-conquer strategy is proposed to deal with the
dependence, which divides a general network into two types of independent
trees. Further, two algorithms, one for a type of the independent trees, are
proposed to estimate the loss rates of each type.

ABSTRACT_BEGIN
  Loss tomography has been studied for more than 10 years and a number of
estimators have been proposed. The estimators can be divided into two classes:
maximum likelihood and non-maximum likelihood. The maximum likelihood
estimators rely on the maximum likelihood principle to ensure the accuracy of
the estimates obtained by the estimators. Unfortunately, all of the maximum
likelihood estimators need to use an iterative procedure to search the solution
space for the maximum or to solve a high degree polynomial. An iterative
procedure can be computationally expensive and may even converge to a local
maximum. On the other hand, the non-maximum likelihood estimators pursue closed
form solutions by scarifying the accuracy of estimates. To overcome the
pitfalls, we, in this paper, propose a closed form and maximum likelihood
estimator to estimate the loss rate of a link in a network. The closed form
solution is built on the discovery of a connection between the number of probes
passing a link and the number of probes passing its parent. The proposed
estimator is applicable to both the tree topology and the general one.

ABSTRACT_BEGIN
  Communication protocols for mobile ad hoc networks (MANETs) follow either an
Optimum Routing Approach (ORA) or the Least Overhead Routing Approach (LORA):
With ORA, protocols tend to determine and use the optimal communication
structure at every time instant; whereas with LORA, a protocol tends to use a
chosen communication structure as long as it exists. In this paper, we study
the impact of the ORA and LORA strategies on minimum hop routes and minimum
connected dominating sets (MCDS) in MANETs. Our primary hypothesis is that the
LORA strategy could yield routes with a larger time-averaged hop count and MCDS
node size when compared to the minimum hop count of routes and the node size of
the MCDS determined using the ORA strategy. Our secondary hypothesis is that
the impact of ORA vs. LORA also depends on how long the communication structure
is being used. Our hypotheses are evaluated using extensive simulations under
diverse conditions of network density, node mobility and mobility models such
as the Random Waypoint model, City Section model and the Manhattan model. In
the case of minimum hop routes, which exist for relatively a much longer time
compared to the MCDS, the hop count of routes maintained according to LORA,
even though not dramatically high, is appreciably larger (6-12%) than those
maintained according to ORA; on the other hand, the number of nodes
constituting a MCDS maintained according to LORA is only at most 6% larger than
the node size of a MCDS maintained under the ORA strategy.

ABSTRACT_BEGIN
  The interest in virtualization has been growing rapidly in the IT industry
because of inherent benefits like better resource utilization and ease of
system manageability. The experimentation and use of virtualization as well as
the simultaneous deployment of virtual software are increasingly getting
popular and in use by educational institutions for research and teaching. This
paper stresses on the potential advantages associated with virtualization and
the use of virtual machines for scenarios, which cannot be easily implemented
and/or studied in a traditional academic network environment, but need to be
explored and experimented by students to meet the raising needs and
knowledge-base demanded by the IT industry. In this context, we discuss various
aspects of virtualization - starting from the working principle of virtual
machines, installation procedure for a virtual guest operating system on a
physical host operating system, virtualization options and a performance study
measuring the throughput obtained on a network of virtual machines and physical
host machines. In addition, the paper extensively evaluates the use of virtual
machines and virtual networks in an academic environment and also specifically
discusses sample projects on network security, which may not be feasible enough
to be conducted in a physical network of personal computers; but could be
conducted only using virtual machines.

ABSTRACT_BEGIN
  The objective of this research is to integrate an RFID (Radio Frequency
Identification) reader into a Wireless Sensor Network (WSN) to authorize or
keep track of people carrying RFID tags. The objective was accomplished by
integrating hardware and software. The hardware consisted of two WSN nodes -
the RFID node connected to one of the WSN nodes, and a computer connected to
the other WSN node. For the RFID equipment, we used the SM130-EK kit, which
included the RFID reader and the RFID tags; and for the WSN, we used the
Synapse Network Evaluation kit, which included the two sensor nodes. The
software consisted of a program module developed in Python to control the
microprocessors of the nodes; and a database controlled by a simple program to
manage the tag IDs of people wearing them. The WSN and RFID nodes were
connected through I2C interfacing. Also, the work of sending commands to the
RFID node, to make it read a tag and send it back to the computer, was
accomplished by the Python code developed which also controls the data signals.
At the computer, the received tag ID is evaluated with other existing tag IDs
on the database, to check if that tag has authorization or not to be in the
covered area. Our research has the potential of being adapted for use with
secure real-time access control applications involving WSN and RFID
technologies.

ABSTRACT_BEGIN
  Code-division multiple-access (CDMA) has the potential to support traffic
sources with a wide range of quality of service (QoS) requirements. The traffic
carrying capacity of CDMA channels under QoS constraints (such as delay
guarantee) is, however, less well-understood. In this work, we propose a method
based on stochastic network calculus and large system analysis to quantify the
maximum traffic that can be carried by a multiuser CDMA network under the QoS
constraints. At the physical layer, we have linear minimum-mean square error
receivers and adaptive modulation and coding, while the channel service process
is modeled by using a finite-state Markov chain. We study the impact of delay
requirements, violation probability and the user load on the traffic carrying
capacity under different signal strengths. A key insight provided by the
numerical results is as to how much one has to back-off from capacity under the
different delay requirements.

ABSTRACT_BEGIN
  Hard handover mechanism is adopted to be used in 3GPP Long Term Evolution
(3GPP LTE) in order to reduce the complexity of the LTE network architecture.
This mechanism comes with degradation in system throughput as well as a higher
system delay. This paper proposes a new handover algorithm known as LTE Hard
Handover Algorithm with Average Received Signal Reference Power (RSRP)
Constraint (LHHAARC) in order to minimize number of handovers and the system
delay as well as maximize the system throughput. An optimized system
performance of the LHHAARC is evaluated and compared with three well-known
handover algorithms via computer simulation. The simulation results show that
the LHHAARC outperforms three well-known handover algorithms by having less
number of average handovers per UE per second, shorter total system delay
whilst maintaining a higher total system throughput.

ABSTRACT_BEGIN
  In a mobile ad hoc network, temporary link failures and route changes occur
frequently. With the assumption that all packet losses are due to congestion,
TCP performs poorly in such an environment. There are many versions of TCP
which modified time to time as per need. In this paper modifications introduced
on TCP New Reno over mobile ad-hoc networks using calculation of New
Retransmission Time out (RTO), to improve performance in term of congestion
control. To calculate New RTO, adaptive backoff response approach (ABRA) in TCP
New Reno was applied which suggest ABRA New Reno. It utilizes an ABRA by which
congestion window and slow start threshold values were decreased whenever an
acknowledgement is received and new backoff value calculate from smoothed round
trip time. Evaluation based on comparative study of ABRA New Reno with other
TCP Variants like New Reno and Reno was done using realistic parameters like
TCP Packet Received, Packet Drop, Packets Retransmitted, Throughput, and Packet
Delivery Ratio calculated by varying attributes of Node Speed, Number of Nodes
and Pause Time. Implementation and simulations were performed in QualNet 4.0
simulator.

ABSTRACT_BEGIN
  2010 has witnessed many public consultations around the world concerning Net
neutrality. A second legislative phase that may follow, could involve various
structural changes in the Internet. The status that the Internet access has in
Europe as a universal service evolves as the level of quality of service (QoS)
to be offered improves. If guarantees on QoS are to be imposed, as requested by
several economic actors, it would require introducing new indicators of quality
of services, as well as regulation legislation and monitoring of the offered
levels of QoS. This tendency in Europe may change the nature of the Internet
from a best effort network to, perhaps, a more expensive one, that offers
guaranteed performance. This paper presents an overview of the above issues as
well as an overview of recent research on net-neutrality, with an emphasis on
game theoretical approaches.

ABSTRACT_BEGIN
  Due to the increasing demand of capacity in wireless cellular networks, the
small cells such as pico and femto cells are becoming more popular to enjoy a
spatial reuse gain, and thus cells with different sizes are expected to coexist
in a complex manner. In such a heterogeneous environment, the role of
interference management (IM) becomes of more importance, but technical
challenges also increase, since the number of cell-edge users, suffering from
severe interference from the neighboring cells, will naturally grow. In order
to overcome low performance and/or high complexity of existing static and other
dynamic IM algorithms, we propose a novel low-complex and fully distributed IM
scheme, called REFIM, in the downlink of heterogeneous multi-cell networks. We
first formulate a general optimization problem that turns out to require
intractable computation complexity for global optimality. To have a practical
solution with low computational and signaling overhead, which is crucial for
low-cost small-cell solutions, e.g., femto cells, in REFIM, we decompose it
into per-BS problems based on the notion of reference user and reduce feedback
overhead over backhauls both temporally and spatially. We evaluate REFIM
through extensive simulations under various configurations, including the
scenarios from a real deployment of BSs. We show that, compared to the schemes
without IM, REFIM can yield more than 40% throughput improvement of cell-edge
users while increasing the overall performance by 10~107%. This is equal to
about 95% performance of the existing centralized IM algorithm that is known to
be near-optimal but hard to implement in practice due to prohibitive
complexity. We also present that as long as interference is managed well, the
spectrum sharing policy can outperform the best spectrum splitting policy where
the number of subchannels is optimally divided between macro and femto cells.

ABSTRACT_BEGIN
  Unlike the traditional model of information pull, matchmaking is base on a
cooperative partnership between information providers and consumers, assisted
by an intelligent facilitator (the matchmaker). Refer to some experiments, the
matchmaking to be most useful in two different ways: locating information
sources or services that appear dynamically and notification of information
changes. Effective information and services sharing in distributed such as P2P
based environments raises many challenges, including discovery and localization
of resources, exchange over heterogeneous sources, and query processing. One
traditional approach for dealing with some of the above challenges is to create
unified integrated schemas or services to combine the heterogeneous sources.
This approach does not scale well when applied in dynamic distributed
environments and has many drawbacks related to the large numbers of sources.
The main issues in matchmaking are how to represent advertising and request,
and how to calculate possibility matching between advertising and request. The
advertising and request can represent data or services by using many model of
representation. In this paper, we address an approach of matchmaking by
considering semantic agreement between sources.

ABSTRACT_BEGIN
  Understanding queuing dynamics of TCP is important for correct router buffer
sizing as well as for optimizing the performance of the TCP protocol itself.
However, modeling of buffer content dynamics under TCP has received relatively
little attention given its importance. Commonly used queuing models are based
on overly simplistic assumptions about the packet arrival process. As a
consequence, there are no quantitatively accurate closed loop TCP models
capable of predicting performance even for a single link shared by multiple
flows. Our present paper aims to close this gap by proposing a simple TCP
queuing model, which is based on experimental observations and validated by
extensive packet level simulations.

ABSTRACT_BEGIN
  A Fibonacci string is a length n binary string containing no two consecutive
1s. Fibonacci cubes (FC), Extended Fibonacci cubes (ELC) and Lucas cubes (LC)
are subgraphs of hypercube defined in terms of Fibonacci strings. All these
cubes were introduced in the last ten years as models for interconnection
networks and shown that their network topology posseses many interesting
properties that are important in parallel processor network design and parallel
applications. In this paper, we propose a new family of Fibonacci-like cube,
namely Extended Lucas Cube (ELC). We address the following network simulation
problem : Given a linear array, a ring or a two-dimensional mesh; how can its
nodes be assigned to ELC nodes so as to keep their adjacent nodes near each
other in ELC ?. We first show a simple fact that there is a Hamiltonian path
and cycle in any ELC. We prove that any linear array and ring network can be
embedded into its corresponding optimum ELC (the smallest ELC with at least the
number of nodes in the ring) with dilation 1, which is optimum for most cases.
Then, we describe dilation 1 embeddings of a class of meshes into their
corresponding optimum ELC.

ABSTRACT_BEGIN
  In a multi-cell scenario, the inter-cell interference (ICI) is detrimental in
achieving the intended system performance, in particular for the edge users.
There is paucity of work available in literature on ICI coordination (ICIC) for
relay-assisted cellular networks (RACN). In this paper, we do a survey on the
ICIC schemes in cellular networks and RACN. We then propose a self-organized
resource allocation plan for RACN to improve the edge user's performance by
ICIC. We compare the performance of reuse-1, reuse-3, soft frequency reuse
(SFR) scheme, proposed plan with and without relays. The performance metrics
for comparison are edge user's spectral efficiency, their
signal-to-interference-and-noise ratio (SINR) and system's area spectral
efficiency. We show by the simulation results that our proposed plan performs
better than the existing resource allocation schemes in static allocation
scenario. Next, we propose to make our resource allocation plan dynamic and
self-organized. The distinct features of our proposed plan are: One, it
achieves a trade-off between the system's area spectral efficiency and the edge
user's spectral efficiency performance. Secondly, it introduces a novel concept
of interfering neighbor set to achieve ICIC by local interaction between the
entities.

ABSTRACT_BEGIN
  Forthcoming wireless communications will be characterized by the ubiquity of
multiaccess. Despite the inherently increased complexity, end-users should be
able to take advantage of the most suitable access network. Thus, access
selection in an environment with different overlapping radio technologies is of
central interest and an architecture is needed that performs equally well on
single- and multi-operator scenarios, considers several parameters, and
respects the principle of layering. In this paper, we introduce the Ambient
Networks heterogeneous access selection architecture explaining how it meets
such requirements. We present the essential architectural components and
explain their interactions. We illustrate how the proposed architecture works
in practice and discuss recent results form our prototype-based validation.

ABSTRACT_BEGIN
  In recent years several wireless communication standards have been developed
and more are expected, each with different scope in terms of spatial coverage,
radio access capabilities, and mobility support. Heterogeneous networks combine
multiple of these radio interfaces both in network infrastructure and in user
equipment which requires a new multi-radio framework, enabling mobility and
handover management for multiple RATs. The use of heterogeneous networks can
capitalize on the overlapping coverage and allow user devices to take advantage
of the fact that there are multiple radio interfaces. This paper presents the
functional architecture for such a framework and proposes a generic signaling
exchange applicable to a range of different handover management protocols that
enables seamless mobility. The interworking of radio resource management,
access selection and mobility management is defined in a generic and modular
way, which is extensible for future protocols and standards.

ABSTRACT_BEGIN
  Solutions for mobility management in wireless networks have been investigated
and proposed in various research projects and standardization bodies. With the
continuing deployment of different access networks, the wider range of
applications tailored for a mobile environment, and a larger diversity of
wireless end systems, it emerged that a single mobility protocol (such as
Mobile IP) is not sufficient to handle the different requirements adequately.
Thus a solution is needed to manage multiple mobility protocols in end systems
and network nodes, to detect and select the required protocols, versions and
optional features, and enable control on running daemons. For this purpose a
mobility toolbox has been developed as part of the EU funded Ambient Networks
project. This paper describes this modular management approach and illustrates
the additional benefits a mobility protocol can gain by using state transfer as
an example.

ABSTRACT_BEGIN
  We study the power-aware buffering problem in battery-powered sensor
networks, focusing on the fixed-size and fixed-interval buffering schemes. The
main motivation is to address the yet poorly understood size variation-induced
effect on power-aware buffering schemes. Our theoretical analysis elucidates
the fundamental differences between the fixed-size and fixed-interval buffering
schemes in the presence of data size variation. It shows that data size
variation has detrimental effects on the power expenditure of the fixed-size
buffering in general, and reveals that the size variation induced effects can
be either mitigated by a positive skewness or promoted by a negative skewness
in size distribution. By contrast, the fixed-interval buffering scheme has an
obvious advantage of being eminently immune to the data-size variation. Hence
the fixed-interval buffering scheme is a risk-averse strategy for its
robustness in a variety of operational environments. In addition, based on the
fixed-interval buffering scheme, we establish the power consumption
relationship between child nodes and parent node in a static data collection
tree, and give an in-depth analysis of the impact of child bandwidth
distribution on parent's power consumption.
  This study is of practical significance: it sheds new light on the
relationship among power consumption of buffering schemes, power parameters of
radio module and memory bank, data arrival rate and data size variation,
thereby providing well-informed guidance in determining an optimal buffer size
(interval) to maximize the operational lifespan of sensor networks.

ABSTRACT_BEGIN
  Multi-channel multi-interface Wireless Mesh Networks permit to spread the
load across orthogonal channels to improve network capacity. Although broadcast
is vital for many layer-3 protocols, proposals for taking advantage of multiple
channels mostly focus on unicast transmissions. In this paper, we propose
broadcast algorithms that fit any channel and interface assignment strategy.
They guarantee that a broadcast packet is delivered with a minimum probability
to all neighbors. Our simulations show that the proposed algorithms efficiently
limit the overhead.

ABSTRACT_BEGIN
  The proliferation of overlapping, always-on IEEE 802.11 Access Points (APs)
in urban areas can cause spectrum sharing conflicts, inefficient bandwidth
usage and power waste. Cooperation among APs could address these problems (i)
by allowing under-used devices to hand over their clients to nearby APs and
temporarily switch off, (ii) by balancing the load of clients among APs and
thus offloading congested APs. The federated houses model provides an appealing
backdrop to implement cooperation among APs. In this paper, we outline a
framework that, assuming the presence of a multipurpose gateway with AP
capabilities in every household, allows such cooperation through the monitoring
of local wireless resources and the triggering of offloading requests toward
other federated gateways. We then present simulation results in realistic
settings that provide some insight on the capabilities of our framework.

ABSTRACT_BEGIN
  Clustering is an important research topic for wireless sensor networks
(WSNs). A large variety of approaches has been presented focusing on different
performance metrics. Even though all of them have many practical applications,
an extremely limited number of software implementations is available to the
research community. Furthermore, these very few techniques are implemented for
specific WSN systems or are integrated in complex applications. Thus it is very
difficult to comparatively study their performance and almost impossible to
reuse them in future applications under a different scope. In this work we
study a large body of well established algorithms. We identify their main
building blocks and propose a component-based architecture for developing
clustering algorithms that (a) promotes exchangeability of algorithms thus
enabling the fast prototyping of new approaches, (b) allows cross-layer
implementations to realize complex applications, (c) offers a common platform
to comparatively study the performance of different approaches, (d) is hardware
and OS independent. We implement 5 well known algorithms and discuss how to
implement 11 more. We conduct an extended simulation study to demonstrate the
faithfulness of our implementations when compared to the original
implementations. Our simulations are at very large scale thus also
demonstrating the scalability of the original algorithms beyond their original
presentations. We also conduct experiments to assess their practicality in real
WSNs. We demonstrate how the implemented clustering algorithms can be combined
with routing and group key establishment algorithms to construct WSN
applications. Our study clearly demonstrates the applicability of our approach
and the benefits it offers to both research & development communities.

ABSTRACT_BEGIN
  Relay-assisted cooperative wireless communication has been shown to have
significant performance gains over the legacy direct transmission scheme.
Compared with single relay based cooperation schemes, utilizing multiple relays
further improves the reliability and rate of transmissions. Distributed
space-time coding (DSTC), as one of the schemes to utilize multiple relays,
requires tight coordination between relays and does not perform well in a
distributed environment with mobility. In this paper, a cooperative medium
access control (MAC) layer protocol, called \emph{STiCMAC}, is designed to
allow multiple relays to transmit at the same time in an IEEE 802.11 network.
The transmission is based on a novel DSTC scheme called \emph{randomized
distributed space-time coding} (\emph{R-DSTC}), which requires minimum
coordination. Unlike conventional cooperation schemes that pick nodes with good
links, \emph{STiCMAC} picks a \emph{transmission mode} that could most improve
the end-to-end data rate. Any station that correctly receives from the source
can act as a relay and participate in forwarding. The MAC protocol is
implemented in a fully decentralized manner and is able to opportunistically
recruit relays on the fly, thus making it \emph{robust} to channel variations
and user mobility. Simulation results show that the network capacity and delay
performance are greatly improved, especially in a mobile environment.

ABSTRACT_BEGIN
  The concept of physical-layer network coding (PNC) was proposed in 2006 for
application in wireless networks. Since then it has developed into a subfield
of network coding with wide followings. The basic idea of PNC is to exploit the
network coding operation that occurs naturally when electromagnetic (EM) waves
are superimposed on one another. This simple idea turns out to have profound
and fundamental ramifications. Subsequent works by various researchers have led
to many new results in the domains of 1) wireless communication; 2) wireless
information theory; and 3) wireless networking. The purpose of this paper is
fourfold. First, we give a brief tutorial on the basic concept of PNC. Second,
we survey and discuss recent key results in the three aforementioned areas.
Third, we examine a critical issue in PNC: synchronization. It has been a
common belief that PNC requires tight synchronization. Our recent results
suggest, however, that PNC may actually benefit from asynchrony. Fourth, we
propose that PNC is not just for wireless networks; it can also be useful in
optical networks. We provide an example showing that the throughput of a
passive optical network (PON) could potentially be raised by 100% with PNC.

ABSTRACT_BEGIN
  Traceroute measurements are one of our main instruments to shed light onto
the structure and properties of today's complex networks such as the Internet.
This paper studies the feasibility and infeasibility of inferring the network
topology given traceroute data from a worst-case perspective, i.e., without any
probabilistic assumptions on, e.g., the nodes' degree distribution. We attend
to a scenario where some of the routers are anonymous, and propose two
fundamental axioms that model two basic assumptions on the traceroute data: (1)
each trace corresponds to a real path in the network, and (2) the routing paths
are at most a factor 1/alpha off the shortest paths, for some parameter alpha
in (0,1]. In contrast to existing literature that focuses on the cardinality of
the set of (often only minimal) inferrable topologies, we argue that a large
number of possible topologies alone is often unproblematic, as long as the
networks have a similar structure. We hence seek to characterize the set of
topologies inferred with our axioms. We introduce the notion of star graphs
whose colorings capture the differences among inferred topologies; it also
allows us to construct inferred topologies explicitly. We find that in general,
inferrable topologies can differ significantly in many important aspects, such
as the nodes' distances or the number of triangles. These negative results are
complemented by a discussion of a scenario where the trace set is best
possible, i.e., "complete". It turns out that while some properties such as the
node degrees are still hard to measure, a complete trace set can help to
determine global properties such as the connectivity.

ABSTRACT_BEGIN
  Transmission of video over a limited bandwidth network is challenging due to
the natural variability of video, and link characteristics. Video smoothing
techniques can be used to facilitate more effective transmission and to
preserve better quality. In this paper we develop a semi-optimal video
smoothing approach to manage the transmission of MPEG-4 and H.264 video while
mapping it to be more suitable for a QoS based network. The proposed technique
utilizes a smoothing buffer with pre-defined thresholds to smooth the
transmission rates while assuming minimal information about the video to be
transmitted. The results obtained showed a significant improvements in
transmission rate variability while guaranteeing no buffer overflows or
underflows. In addition, a queuing model is developed for the used smoothing
algorithm for H.264 video streams with optimized encoding and packetization,
utilizing the available H.264 macroblock ordering option.

ABSTRACT_BEGIN
  The current Internet is based on a fundamental assumption of reliability and
good intent among actors in the network. Unfortunately, unreliable and
malicious behaviour is becoming a major obstacle for Internet communication. In
order to improve the trustworthiness and reliability of the network
infrastructure, we propose a novel trust model to be incorporated into BGP
routing. In our approach, trust model is defined by combining voting and
recommendation to direct trust estimation for neighbour routers located in
different autonomous systems. We illustrate the impact of our approach with
cases that demonstrate the indication of distrusted paths beyond the nearest
neighbours and the detection of a distrusted neighbour advertising a trusted
path. We simulated the impact of weighting voted and direct trust in a
rectangular grid of 15*15 nodes (autonomous systems) with a randomly connected
topology.

ABSTRACT_BEGIN
  IEEE 802.11 is a widely used wireless LAN standard which offers a good
bandwidth at low cost In an ESS, multiple APs can co-exist with overlapping
coverage area. A mobile node connects to the AP from which it receives the best
signal. Changes in traffic to and from different MNs occur over time. Load
imbalance may develop on different APs. Throughput and delay of the different
flows passing through the APs, where the load has increased beyond certain
limit, may degrade. Different MNs associated to the overloaded APs will
experience performance degradation. Overall performance of the ESS will also
drop. In this paper we propose a scheme where MNs experiencing degraded
performance will initiate action and with assistance from the associate AP
perform handoff to less loaded AP within its range to improve performance.

ABSTRACT_BEGIN
  Deployment of multimedia applications warrants provisioning of Quality of
Service (QoS) in MANET. However, limited battery power, other resource
constraints and mobility of nodes make QoS provisioning difficult to achieve in
MANET. This difficulty can be overcome by using a cross-layer approach for
routing. In [1] Patil et al., proposed a cross-layer routing protocol named
Cost Based Power Aware Cross Layer - AODV (CPACL-AODV) which overcomes the
limitation of battery power of nodes. Though many similar energy efficient and
cross-layer routing protocols have been proposed for MANET, none of them
handles QoS. A novel MANET routing protocol, Type of Service, Power and
Bandwidth Aware AODV (TSPBA-AODV), which overcomes resource constraints and
simultaneously provides QoS guarantees using a cross-layer approach, is
proposed in this paper. In addition the effect of variation in nodes' mobility
on performance of TSPBA-AODV is compared with that of CPACL-AODV [1] for two
different types of network traffic. As shown by the results of simulations
performed, TSPBA-AODV performs better than CPACL-AODV for MANET in which nodes
move with small speeds (speeds up to 40 Km/hr approx.). In addition the effect
of variation in data sending rate of nodes on performance of the protocols is
also studied. As shown by the results of simulations performed, TSPBA-AODV
performs better than CPACL-AODV for all variations in data sending rate of
nodes.

ABSTRACT_BEGIN
  Mobile ad hoc networks (MANETs) are a set of mobile nodes which are
self-configuring and connected by wireless links automatically as per the
defined routing protocol. The absence of a central management agency or a fixed
infrastructure is a key feature of MANETs. These nodes communicate with each
other by interchange of packets, which for those nodes not in wireless range
goes hop by hop. Due to lack of a defined central authority, securitizing the
routing process becomes a challenging task thereby leaving MANETs vulnerable to
attacks, which results in deterioration in the performance characteristics as
well as raises a serious question mark about the reliability of such networks.
In this paper we have attempted to present an overview of the routing
protocols, the known routing attacks and the proposed countermeasures to these
attacks in various works.

ABSTRACT_BEGIN
  Classifying network traffic according to their application-layer protocols is
an important task in modern networks for traffic management and network
security. Existing payload-based or statistical methods of application
identification cannot meet the demand of both high performance and accurate
identification at the same time. We propose an application identification
framework that classifies traffic at aggregate-flow level leveraging
aggregate-flow cache. A detailed traffic classifier designed based on this
framework is illustrated to improve the throughput of payload-based
identification methods. We further optimize the classifier by proposing an
efficient design of aggregate-flow cache. The cache design employs a
frequency-based, recency-aware replacement algorithm based on the analysis of
temporal locality of aggregate-flow cache. Experiments on real-world traces
show that our traffic classifier with aggregate-flow cache can reduce up to 95%
workload of backend identification engine. The proposed cache replacement
algorithm outperforms well-known replacement algorithms, and achieves 90% of
the optimal performance using only 15% of memory. The throughput of a
payload-based identification system, L7-filter [1], is increased by up to 5.1
times by using our traffic classifier design.

ABSTRACT_BEGIN
  We consider the problem of quickest event detection with sleep-wake
scheduling in small extent wireless sensor networks in which, at each time
slot, each sensor node in the awake state observes a sample and communicates
the information to the fusion centre. The sensor nodes in the sleep state do
not sample or communicate any information to the fusion centre (FC), thereby
conserving energy. At each time slot, the FC, after having received the samples
from the sensor nodes in the wake state, makes a decision to stop (and thus
declare that the event has occurred) or to continue observing. If it decides to
continue, the FC also makes the decision of choosing the number of sensor nodes
to be in the wake state in the next time slot. We consider three alternative
approaches to the problem of choosing the number of sensor nodes to be in the
wake state in time slot k+1, based on the information available at time slot k,
namely, 1. optimal control of M_{k+1}, the number of sensor nodes to be in the
awake state in time slot k+1, 2. optimal control of q_{k+1}, the probability of
a sensor node to be in the awake state in time slot k+1, and 3. optimal
probability q that a sensor node is in the awake state in any time slot. In
each case, we formulate the problem as a sequential decision process. We show
that a sufficient statistic for the decision at time k is the a posteriori
probability of change Pi_k. Also, we show that the optimal stopping rule is a
threshold rule on Pi_k. The optimal policy for M_{k+1} can keep very few
sensors wake during the prechange phase and then quickly increase the number of
sensors in the wake state when a change is "suspected". Among the three
sleep-wake algorithms described, we observe that the total cost is minimum for
the optimum control of M_{k+1} and is maximum for the optimum control on q.

ABSTRACT_BEGIN
  We consider a small extent sensor network for event detection, in which nodes
take samples periodically and then contend over a {\em random access network}
to transmit their measurement packets to the fusion center. We consider two
procedures at the fusion center to process the measurements. The Bayesian
setting is assumed; i.e., the fusion center has a prior distribution on the
change time. In the first procedure, the decision algorithm at the fusion
center is \emph{network-oblivious} and makes a decision only when a complete
vector of measurements taken at a sampling instant is available. In the second
procedure, the decision algorithm at the fusion center is \emph{network-aware}
and processes measurements as they arrive, but in a time causal order. In this
case, the decision statistic depends on the network delays as well, whereas in
the network-oblivious case, the decision statistic does not depend on the
network delays. This yields a Bayesian change detection problem with a tradeoff
between the random network delay and the decision delay; a higher sampling rate
reduces the decision delay but increases the random access delay. Under
periodic sampling, in the network--oblivious case, the structure of the optimal
stopping rule is the same as that without the network, and the optimal change
detection delay decouples into the network delay and the optimal decision delay
without the network. In the network--aware case, the optimal stopping problem
is analysed as a partially observable Markov decision process, in which the
states of the queues and delays in the network need to be maintained. A
sufficient statistic for decision is found to be the network-state and the
posterior probability of change having occurred given the measurements received
and the state of the network. The optimal regimes are studied using simulation.

ABSTRACT_BEGIN
  In recent years, efficient energy utilization becomes an essential
requirement for data centers, especially in data centers of world-leading
companies, where "Green Data Center" defines a new term for an
environment-concerned data center. Solutions to change existing a data center
to the green one may vary. In the big company, high-cost approaches including
re-planning server rooms, changing air-conditioners, buying low-powered
servers, and equipping sophisticating environmental control equipments are
possible, but not for small to medium enterprises (SMEs) and academic sectors
which have limited budget. In this paper, we propose a novel system, SENVM,
used to monitor and control air temperature in a server room to be in
appropriate condition, not too cold, where very unnecessary cooling leads to
unnecessary extra electricity expenses, and also inefficient in energy
utilization. With implementing on an emerging technology, Wireless Sensor
Network (WSN), Green Data Center is feasible to every small data center with no
wiring installation, easy deployment, and low maintenance fee. In addition, the
prototype of the system has been tested, and the first phase of the project is
deployed in a real-world data center.

ABSTRACT_BEGIN
  In designing wireless sensor networks, it is important to reduce energy
dissipation and prolong network lifetime. In this paper, a new model with
energy and monitored objects heterogeneity is proposed for heterogeneous
wireless sensor networks. We put forward an energy-efficient prediction
clustering algorithm, which is adaptive to the heterogeneous model. This
algorithm enables the nodes to select the cluster head according to factors
such as energy and communication cost, thus the nodes with higher residual
energy have higher probability to become a cluster head than those with lower
residual energy, so that the network energy can be dissipated uniformly. In
order to reduce energy consumption when broadcasting in clustering phase and
prolong network lifetime, an energy consumption prediction model is established
for regular data acquisition nodes. Simulation results show that compared with
current clustering algorithms, this algorithm can achieve longer sensor network
lifetime, higher energy efficiency and superior network monitoring quality.

ABSTRACT_BEGIN
  Recent work has shown the feasibility of single-channel full-duplex wireless
physical layer, allowing nodes to send and receive in the same frequency band
at the same time. In this report, we first design and implement a real-time
64-subcarrier 10 MHz full-duplex OFDM physical layer, FD-PHY. The proposed
FD-PHY not only allows synchronous full-duplex transmissions but also selective
asynchronous full-duplex modes. Further, we show that in over-the-air
experiments using optimal antenna placement on actual devices, the
self-interference can be suppressed upto 80dB, which is 10dB more than prior
reported results. Then we propose a full-duplex MAC protocol, FD-MAC, which
builds on IEEE 802.11 with three new mechanisms -- shared random backoff,
header snooping and virtual backoffs. The new mechanisms allow FD-MAC to
discover and exploit full-duplex opportunities in a distributed manner. Our
over-the-air tests show over 70% throughput gains from using full-duplex over
half-duplex in realistically used cases.

ABSTRACT_BEGIN
  To achieve the requirement of high data rate, low latency, user fairness for
next generation wireless networks, proper designing of cross-layer optimized
dynamic resource allocation algorithm is prerequisite. In this paper, we
propose a dynamic resource allocation scheme in Orthogonal Frequency Division
Multiple Access (OFDMA) systems to optimize the non real-time (NRT) traffic,
which requires allocation of minimum quantum of data within a predefined time
that does not incur packet loss. Most existing and proposed works on resource
allocation schemes focused on traffic consisting of delay constraint real-time
(RT) or delay-tolerant (NRT, Best-Effort (BE)) applications in a single scheme.
In this work, we investigate the resource allocation problem in heterogeneous
multiuser OFDMA system with the objective of optimizing the aggregate data
delivery of NRT and BE traffic to maximize the overall system performance, by
exploiting the inherent time-diversity gain in mobile wireless environment for
delay-tolerant applications. Simulation results show that the proposed
algorithm greatly enhances the system capacity, when compared to traditional
proportional fair resource allocation algorithm.

ABSTRACT_BEGIN
  This work considers a single-cell random access channel (RACH) in cellular
wireless networks. Communications over RACH take place when users try to
connect to a base station during a handover or when establishing a new
connection. Within the framework of Self-Organizing Networks (SONs), the system
should self- adapt to dynamically changing environments (channel fading,
mobility, etc.) without human intervention. For the performance improvement of
the RACH procedure, we aim here at maximizing throughput or alternatively
minimizing the user dropping rate. In the context of SON, we propose protocols
which exploit information from measurements and user reports in order to
estimate current values of the system unknowns and broadcast global
action-related values to all users. The protocols suggest an optimal pair of
user actions (transmission power and back-off probability) found by minimizing
the drift of a certain function. Numerical results illustrate considerable
benefits of the dropping rate, at a very low or even zero cost in power
expenditure and delay, as well as the fast adaptability of the protocols to
environment changes. Although the proposed protocol is designed to minimize
primarily the amount of discarded users per cell, our framework allows for
other variations (power or delay minimization) as well.

ABSTRACT_BEGIN
  We consider the problem of estimating sparse communication channels in the
MIMO context. In small to medium bandwidth communications, as in the current
standards for OFDM and CDMA communication systems (with bandwidth up to 20
MHz), such channels are individually sparse and at the same time share a common
support set. Since the underlying physical channels are inherently
continuous-time, we propose a parametric sparse estimation technique based on
finite rate of innovation (FRI) principles. Parametric estimation is especially
relevant to MIMO communications as it allows for a robust estimation and
concise description of the channels. The core of the algorithm is a
generalization of conventional spectral estimation methods to multiple input
signals with common support. We show the application of our technique for
channel estimation in OFDM (uniformly/contiguous DFT pilots) and CDMA downlink
(Walsh-Hadamard coded schemes). In the presence of additive white Gaussian
noise, theoretical lower bounds on the estimation of SCS channel parameters in
Rayleigh fading conditions are derived. Finally, an analytical spatial channel
model is derived, and simulations on this model in the OFDM setting show the
symbol error rate (SER) is reduced by a factor 2 (0 dB of SNR) to 5 (high SNR)
compared to standard non-parametric methods - e.g. lowpass interpolation.

ABSTRACT_BEGIN
  It is known that link throughputs of CSMA wireless networks can be computed
from a time-reversible Markov chain arising from an ideal CSMA network model
(ICN). In particular, this model yields general closed-form equations of link
throughputs. However, an idealized and important assumption made in ICN is that
the backoff countdown process is in "contiuous-time" and carrier sensing is
instantaneous. As a result, there is no collision in ICN. In practical CSMA
protocols such as IEEE 802.11, the stations count down in "mini-timeslot" and
the process is therefore a "discrete-time" process. In particular, two stations
may end their backoff process in the same mini-timeslot and then transmit
simultaneously, resulting in a packet collision. This paper is an attempt to
study how to compute link throughputs after taking such backoff collision
effects into account. We propose a generalized ideal CSMA network model (GICN)
to characterize the collision states as well as the interactions and dependency
among links in the network. We show that link throughputs and collision
probability can be computed from GICN. Simulation results validate GICN's
accuracy. Interestingly, we also find that the original ICN model yields fairly
accurate results despite the fact that collisions are not modeled.

ABSTRACT_BEGIN
  This paper investigates the problem of link scheduling to meet traffic
demands with minimum airtime in a multi-transmit-receive (MTR) wireless
network. MTR networks are a new class of networks, in which each node can
simultaneously transmit to a number of other nodes, or simultaneously receive
from a number of other nodes. The MTR capability can be enabled by the use of
multiple directional antennas or multiple channels. Potentially, MTR can boost
the network capacity significantly. However, link scheduling that makes full
use of the MTR capability must be in place before this can happen. We show that
optimal link scheduling can be formulated as a linear program (LP). However,
the problem is NP-hard because we need to find all the maximal independent sets
in a graph first. We propose two computationally efficient algorithms, called
Heavy-Weight-First (HWF) and Max-Degree-First (MDF) to solve this problem.
Simulation results show that both HWF and MDF can achieve superior performance
in terms of runtime and optimality.

ABSTRACT_BEGIN
  Many research works have been carried out recently to find the optimal path
in network routing. Among them the evolutionary algorithms is an area where
work is carried out extensively. We in this paper, have used PSO for finding
the optimal path and the concept of region based network is introduced along
with the use of indirect encoding. A comparative study of genetic algorithm
(GA) and particle swarm optimization (PSO) is carried out, and it was found
that PSO performed better than GA.

ABSTRACT_BEGIN
  Significant research has been carried out in the recent years for generating
systems exhibiting intelligence for realizing optimized routing in networks. In
this paper, a grade based twolevel based node selection method along with
Particle Swarm Optimization (PSO) technique is proposed. It assumes that the
nodes are intelligent and there exist a knowledge base about the environment in
their local memory. There are two levels for approaching the effective route
selection process through grading. At the first level, grade based selection is
applied and at the second level, the optimum path is explored using PSO. The
simulation has been carried out on different topological structures and it is
observed that a graded network produces a significant reduction in number of
iteration to arrive at the optimal path selection.

ABSTRACT_BEGIN
  One of the fundamental data transmission mechanisms in Ethernet LAN is
broadcasting. Flooding is a direct broadcasting technique used in these
networks. A significant drawback of this method is that it can lead to
broadcast storms. This phenomenon is more common in multivendor switch
environment. Broadcast storms usually results in dissension, collision and
redundancy leading to degradation of the network performance. Most of the
storms appear without much warning and it affects the efficiency of network
even in situations when the network is expected to work most efficiently. There
are several characteristic patterns by which storm can appear in a LAN, like
rate monotonic repetition, transient appearances with different types of growth
properties and decay profiles. In this paper we discuss the storm build up
pattern in an industry and present various reasons for storm in LAN. We have
identified a strategy for controlling network storms, using multiple static
agents. These agents inhibit storm packet regeneration in the network using the
knowledge of storm growth pattern. A model developed out of empirical studies
is used to differentiate normal packet growth from storm packet growth and used
in control mechanism of storms.

ABSTRACT_BEGIN
  The paper presents a method which shows a significant improvement in
discovering the path over the distance vector protocol. The proposed method is
a multi-parameter QoS along with the fitness function which shows that it
overcomes the limitation of DV like routing loops by spanning tree approach,
count-to-infinity problem by decision attribute. The input considered is a
topology satisfying the QoS parameters of size 1 to 64 nodes and it was shown
that an optimal path selection was obtained efficiently over the classical
distance vector algorithm

ABSTRACT_BEGIN
  Data communication in sensor networks can have timing constraints like end to
end deadlines. If the deadlines are not met either a catastrophe can happen in
hard real time systems or performance deterioration can occur in soft real time
systems. In real time sensor networks, the recovery of data through
retransmission should be minimized due to the stringent requirements on the
worst case time delays. This paper presents the application of Stop and Go
Multihop protocol (SGMH) at node level in wireless sensor networks for
scheduling and hence to meet the hard real time routing requirements. SGMH is a
distributed multihop packet delivery algorithm. The fractions of the total
available bandwidth on each channel is assigned to several traffic classes by
which the time it takes to traverse each of the hops from the source to the
destination is bounded. It is based on the notion of time frames (Tfr). In
sensor networks packets can have different delay guarantees. Multiple frame
sizes can be assigned for different traffic classes.

ABSTRACT_BEGIN
  To overcome the problem of unavailability of grid power in rural India, we
explore the possibility of powering WSN gateways using a bicycle dynamo. The
"Data mule" bicycle generates its own power to ensure a self sustainable data
transfer for information dissemination to small and marginal farmers. Our
multi-interface WSN gateway is equipped with Bluetooth, Wi-Fi and GPRS
technologies. To achieve our goal, we exploit the DTN stack in the energy sense
and introduce necessary modifications to its configuration.

ABSTRACT_BEGIN
  Security remains as a major concern in the mobile ad hoc networks. This paper
presents a new protocol SD-AODV, which is an extension of the exiting protocol
AODV. The proposed protocol is made secure and dynamic against three main types
of routing attacks- wormhole attack, byzantine attack and blackhole attack.
SD-AODV protocol was evaluated through simulation experiments done on Glomosim
and performance of the network was measured in terms of packet delivery
fraction, average end-to-end delay, global throughput and route errors of a
mobile ad hoc network where a defined percentage of nodes behave maliciously.
Experimentally it was found that the performance of the network did not degrade
in the presence of the above said attacks indicating that the proposed protocol
was secure against these attacks.

ABSTRACT_BEGIN
  Ad-hoc routing protocols use a number of algorithms for route discovery. Some
use flooding in which a route request packet (RREQ) is broadcasted from a
source node to other nodes in the network. This often leads to unnecessary
retransmissions, causing congestion and packet collisions in the network, a
phenomenon called a broadcast storm. This paper presents a RREQ message
forwarding scheme for AODV that reduces routing overheads. This has been called
AODV_EXT. Its performance is compared to that of AODV, DSDV, DSR and OLSR
protocols. Simulation results show that AODV_EXT achieves 3% energy efficiency,
19.5% improvement in data throughput and 69.5% reduction in the number of
dropped packets for a network of 50 nodes. Greater efficiency is achieved in
high density network and marginal improvement in networks with a small number
of nodes.

ABSTRACT_BEGIN
  Mobile Ad-Hoc network is a collection of mobile nodes in communication
without using infrastructure. As the real-time applications used in today's
wireless network grow, we need some schemes to provide more suitable service
for them. We know that most of actual schemes do not perform well on traffic
which is not strictly CBR. Therefore, in this paper we have studied the impact,
respectively, of mobility models and the density of nodes on the performances
(End-to-End Delay, Throughput and Packet Delivery ratio) of routing protocol
(Optimized Link State Routing) OLSR by using in the first a real-time VBR
(MPEG-4) and secondly the Constant Bit Rate (CBR) traffic. Finally we compare
the performance on both cases. Experimentally, we considered the three mobility
models as follows Random Waypoint, Random Direction and Mobgen Steady State.
The experimental results illustrate that the behavior of OLSR change according
to the model and the used traffics.

ABSTRACT_BEGIN
  The issue of mobility is important in wireless network because internet
connectivity can only be effective if it's available during the movement of
node. To enhance mobility, wireless access systems are designed such as IEEE
802.16e to operate on the move without any disruption of services. In this
paper we are analyzing the impact of mobility on the QoS parameters
(Throughput, Average Jitter and Average end to end Delay) of a mobile WiMAX
network (IEEE 802.16e) with CBR application.

ABSTRACT_BEGIN
  Recent research in ubiquitous computing uses technologies of Body Area
Networks (BANs) to monitor the person's kinematics and physiological
parameters. In this paper we propose a real time mobile health system for
monitoring elderly patients from indoor or outdoor environments. The system
uses a bio- signal sensor worn by the patient and a Smartphone as a central
node. The sensor data is collected and transmitted to the intelligent server
through GPRS/UMTS to be analyzed. The prototype (UMHMSE) monitors the elderly
mobility, location and vital signs such as Sp02 and Heart Rate. Remote users
(family and medical personnel) might have a real time access to the collected
information through a web application.

ABSTRACT_BEGIN
  We design a cross-layer approach to aid in develop- ing a cooperative
solution using multi-packet reception (MPR), network coding (NC), and medium
access (MAC). We construct a model for the behavior of the IEEE 802.11 MAC
protocol and apply it to key small canonical topology components and their
larger counterparts. The results obtained from this model match the available
experimental results with fidelity. Using this model, we show that fairness
allocation by the IEEE 802.11 MAC can significantly impede performance; hence,
we devise a new MAC that not only substantially improves throughput, but
provides fairness to flows of information rather than to nodes. We show that
cooperation between NC, MPR, and our new MAC achieves super-additive gains of
up to 6.3 times that of routing with the standard IEEE 802.11 MAC. Furthermore,
we extend the model to analyze our MAC's asymptotic and throughput behaviors as
the number of nodes increases or the MPR capability is limited to only a single
node. Finally, we show that although network performance is reduced under
substantial asymmetry or limited implementation of MPR to a central node, there
are some important practical cases, even under these conditions, where MPR, NC,
and their combination provide significant gains.

ABSTRACT_BEGIN
  Loss tomography has received considerable attention in recent years and a
number of estimators have been proposed. Although most of the estimators claim
to be the maximum likelihood estimators, the claim is only partially true since
the maximum likelihood estimate can be obtained at most for a class of data
sets. Unfortunately, few people are aware of this restriction that leads to a
misconception that an estimator is applicable to all data sets as far as it
returns a unique solution. To correct this, we in this paper point out the risk
of this misconception and illustrate the inconsistency between data and model
in the most influential estimators. To ensure the model used in estimation
consistent with the data collected from an experiment, the data sets used in
estimation are divided into 4 classes according to the characteristics of
observations. Based on the classification, the validity of an estimator is
defined and the validity of the most influential estimators is evaluated. In
addition, a number of estimators are proposed, one for a class of data sets
that have been overlooked. Further, a general estimator is proposed that is
applicable to all data classes. The discussion starts from the tree topology
and end at the general topology.

ABSTRACT_BEGIN
  This paper proposes a handover scheme supporting Multi-Protocol Label
Switching (MPLS) in a Proxy Mobile IPv6 (PMIPv6) domain that improves the
mobility and gives Quality of Service (QoS) and Traffic Engineering (TE)
capabilities in wireless access networks. The proposed scheme takes advantages
of both PMIPv6 and MPLS. PMIPv6 was designed to provide NETwork-based Localized
Mobility Management (NETLMM) support to a Mobile Node (MN); therefore, the MN
does not perform any mobility related signaling, while MPLS is used as an
alternative tunneling technology between the Mobile Access Gateway (MAG) and
the Local Mobility Anchor (LMA) replacing the IP-in-IP tunnels with Label
Switched Path (LSP) tunnels. It can also be integrated with other QoS
architectures such as Differentiated Services (DiffServ) and/or Integrated
Services (IntServ). In this study, we used MATLAB to perform an analysis to
evaluate the impact of introducing MPLS technology in PMIPv6 domain based on
handover latency, operational overhead and packet loss during the handover.
This was compared with PMIPv6, and a PMIPv6/MPLS integration. We proved that
the proposed scheme can give better performance than other schemes.

ABSTRACT_BEGIN
  When a long-term energy constraint is imposed to a transmitter, the average
energy-efficiency of a transmitter is, in general, not maximized by always
transmitting. In a cognitive radio context, this means that a secondary link
can re-exploit the non-used time-slots. In the case where the secondary link is
imposed to generate no interference on the primary link, a relevant issue is
therefore to know the fraction of time-slots available to the secondary
transmitter, depending on the system parameters. On the other hand, if the
secondary transmitter is modeled as a selfish and free player choosing its
power control policy to maximize its average energy-efficiency, resulting
primary and secondary signals are not necessarily orthogonal and studying the
corresponding Stackelberg game is relevant to know the outcome of this
interactive situation in terms of power control policies.

ABSTRACT_BEGIN
  Distributed Opportunistic Scheduling (DOS) is inherently harder than
conventional opportunistic scheduling due to the absence of a central entity
that has knowledge of all the channel states. With DOS, stations contend for
the channel using random access; after a successful contention, they measure
the channel conditions and only transmit in case of a good channel, while
giving up the transmission opportunity when the channel conditions are poor.
The distributed nature of DOS systems makes them vulnerable to selfish users:
by deviating from the protocol and using more transmission opportunities, a
selfish user can gain a greater share of the wireless resources at the expense
of the well-behaved users. In this paper, we address the selfishness problem in
DOS from a game theoretic standpoint. We propose an algorithm that satisfies
the following properties: (i) when all stations implement the algorithm, the
wireless network is driven to the optimal point of operation, and (ii) one or
more selfish stations cannot gain any profit by deviating from the algorithm.
The key idea of the algorithm is to react to a selfish station by using a more
aggressive configuration that (indirectly) punishes this station. We build on
multivariable control theory to design a mechanism for punishment that on the
one hand is sufficiently severe to prevent selfish behavior while on the other
hand is light enough to guarantee that, in the absence of selfish behavior, the
system is stable and converges to the optimum point of operation. We conduct a
game theoretic analysis based on repeated games to show the algorithm's
effectiveness against selfish stations. These results are confirmed by
extensive simulations.

ABSTRACT_BEGIN
  This paper shows that the deployment of an opportunistic network on any
public transportation system leads to obtain a scalable and efficient urban
communication platform. We use the term Bus Switched Networks (BSNs) to
indicate this urban backbone that complements the services of 3G networks and
enables to meet the application level requirements for a large class of
applications by ensuring high delivery ratio and acceptable delays under
different conditions of packet load. We sustain these arguments by providing
three contributions. The first contribution is a novel and lightweight
probabilistic routing protocol for BSN which we prove to be highly effective in
satisfying the loose QoS required by urban-wide delay-tolerant information
services. The second contribution is the proposal of URBeS, an analysis
platform that, given a specific city served by public transportation, produces
real bus mobility traces and traffic analysis for any given routing protocol.
The last contribution is an extensive benchmark analysis on three real cities
which have been selected to explore geo and structural diversity.

ABSTRACT_BEGIN
  This paper presents background theories and required steps towards
preparation of a WLAN location system. This paper targets on a software project
and intention behind this paper is to motivate the young researchers in the
area.

ABSTRACT_BEGIN
  Although wireless sensor networks (WSNs) are powerful in monitoring physical
events, the data collected from a WSN are almost always incomplete if the
surveyed physical event spreads over a wide area. The reason for this
incompleteness is twofold: i) insufficient network coverage and ii) data
aggregation for energy saving. Whereas the existing recovery schemes only
tackle the second aspect, we develop Dual-lEvel Compressed Aggregation (DECA)
as a novel framework to address both aspects. Specifically, DECA allows a high
fidelity recovery of a widespread event, under the situations that the WSN only
sparsely covers the event area and that an in-network data aggregation is
applied for traffic reduction. Exploiting both the low-rank nature of
real-world events and the redundancy in sensory data, DECA combines matrix
completion with a fine-tuned compressed sensing technique to conduct a
dual-level reconstruction process. We demonstrate that DECA can recover a
widespread event with less than 5% of the data, with respect to the dimension
of the event, being collected. Performance evaluation based on both synthetic
and real data sets confirms the recovery fidelity and energy efficiency of our
DECA framework.

ABSTRACT_BEGIN
  In this report, we first describe the problem that we are dealing with i.e.
data dissemination in multi-hop cognitive radio networks. To address this
problem, we propose a channel selection strategy named 'SURF'. We evaluate the
proposed channel selection strategy in both single-hop and multi-hop scenarios
and compared it with relevant approaches. So far, one technical report and a
poster is published as part of this work, while two publications are under
review; one is in IEEE Communications Letters and the second one is in IEEE
WoWMoM conference. In on-going works sections, we first mention some possible
directions in the context of SURF. In addition to that, we mention different
research problems that we are planning to deal during the course of this PhD
dissertation.

ABSTRACT_BEGIN
  In this paper we consider the problem of communication scheduling in wireless
networks with respect to the SINR(Signal to Interference plus Noise Ratio)
constraint in metric spaces. The nodes are assigned linear powers, i.e. for
each sender node the power is constant times the path loss between the sender
and corresponding receiver. This is the minimal power for a successful
transmission. We present a constant factor deterministic approximation
algorithm, which works for at least Euclidean fading metrics. Simultaneously we
obtain the approximate value of the optimal schedule length with error at most
a constant factor. To give an insight into the complexity of the problem, we
show that in some metric spaces the problem is NP-hard and cannot be
approximated within a factor less than 1.5.

ABSTRACT_BEGIN
  We present an algorithm which computes a planar 2-spanner from an Unit Disk
Graph when the node density is sufficient. The communication complexity in
terms of number of node's identifier sent by the algorithm is $6n$, while the
computational complexity is $O(n\Delta)$, with $\Delta$ the maximum degree of
the communication graph. Furthermore, we present a simple and efficient routing
algorithm dedicated to the computed graph.
  Last but not least, using traditional Euclidean coordinates, our algorithm
needs the broadcast of as few as $3n$ node's identifiers. Under the hypothesis
of sufficient node density, no broadcast at all is needed, reducing the
previous best known complexity of an algorithm to compute a planar spanner of
an Unit Disk Graph which was of $5n$ broadcasts.

ABSTRACT_BEGIN
  In multi-access wireless networks, transmission scheduling is a key component
that determines the efficiency and fairness of wireless spectrum allocation. At
one extreme, greedy opportunistic scheduling that allocates airtime to the user
with the largest instantaneous channel gain achieves the optimal spectrum
efficiency and transmission reliability but the poorest user-level fairness. At
the other extreme, fixed TDMA scheduling achieves the fairest airtime
allocation but the lowest spectrum efficiency and transmission reliability. To
balance the two competing objectives, extensive research efforts have been
spent on designing opportunistic scheduling schemes that reach certain tradeoff
points between the two extremes. In this paper and in contrast to the
conventional wisdom, we find that in relay-assisted cellular networks, fixed
TDMA achieves the same optimal diversity gain as greedy opportunistic
scheduling. In addition, by incorporating very limited opportunism, a simple
relaxed-TDMA scheme asymptotically achieves the same optimal system reliability
in terms of outage probability as greedy opportunistic scheduling. This reveals
a surprising fact: transmission reliability and user fairness are no longer
contradicting each other in relay-assisted systems. They can be both achieved
by the simple TDMA schemes. For practical implementations, we further propose a
fully distributed algorithm to implement the relaxed-TDMA scheme. Our results
here may find applications in the design of next-generation wireless
communication systems with relay architectures such as LTE-advanced and WiMAX.

ABSTRACT_BEGIN
  Wireless 802.11 links operate in unlicensed spectrum and so must accommodate
other unlicensed transmitters which generate pulsed interference. We propose a
new approach for detecting the presence of pulsed interference affecting 802.11
links, and for estimating temporal statistics of this interference. This
approach builds on recent work on distinguishing collision losses from noise
losses in 802.11 links. When the intervals between interference pulses are
i.i.d., the approach is not confined to estimating the mean and variance of
these intervals but can recover the complete probability distribution. The
approach is a transmitter-side technique that provides per-link information and
is compatible with standard hardware. We demonstrate the effectiveness of the
proposed approach using extensive experimental measurements. In addition to
applications to monitoring, management and diagnostics, the fundamental
information provided by our approach can potentially be used to adapt the frame
durations used in a network so as to increase capacity in the presence of
pulsed interference.

ABSTRACT_BEGIN
  In geographical forwarding of packets in a large wireless sensor network
(WSN) with sleep-wake cycling nodes, we are interested in the local decision
problem faced by a node that has custody of a packet and has to choose one
among a set of next-hop relay nodes to forward the packet towards the sink.
Each relay is associated with a reward that summarizes the benefit of
forwarding the packet through that relay. We seek a solution to this local
problem, the idea being that such a solution, if adopted by every node, could
provide a reasonable heuristic for the end-to-end forwarding problem. Towards
this end, we propose a relay selection problem comprising a forwarding node and
a collection of relay nodes, with the relays waking up sequentially at random
times. At each relay wake-up instant the forwarder can choose to probe a relay
to learn its reward value, based on which the forwarder can then decide whether
to stop (and forward its packet to the chosen relay) or to continue to wait for
further relays to wake-up. The forwarder's objective is to select a relay so as
to minimize a combination of waiting-delay, reward and probing cost. Our
problem can be considered as a variant of the asset selling problem studied in
the operations research literature. We formulate our relay selection problem as
a Markov decision process (MDP) and obtain some interesting structural results
on the optimal policy (namely, the threshold and the stage-independence
properties). We also conduct simulation experiments and gain valuable insights
into the performance of our local forwarding-solution.

ABSTRACT_BEGIN
  We consider Multi-User MIMO (MU-MIMO) scheduling in the 3GPP LTE-Advanced
(3GPP LTE-A) cellular uplink. The 3GPP LTE-A uplink allows for precoded
multi-stream (precoded MIMO) transmission from each scheduled user and also
allows flexible multi-user (MU) scheduling wherein multiple users can be
assigned the same time-frequency resource. However, exploiting these features
is made challenging by certain practical constraints that have been imposed in
order to maintain a low signaling overhead. We show that while the scheduling
problem in the 3GPP LTE-A cellular uplink is NP-hard, it can be formulated as
the maximization of a submodular set function subject to one matroid and
multiple knapsack constraints. We then propose constant-factor polynomial-time
approximation algorithms and demonstrate their superior performance via
simulations.

ABSTRACT_BEGIN
  We note a fact which is simple, but may be useful for the networking research
community: essentially any change to BGP's decision process can cause
divergence --- or convergence when BGP would otherwise diverge.

ABSTRACT_BEGIN
  This paper describes the background of smart information infrastructure and
the needs for smart grid information security. It introduces the conceptual
analysis to the methodology with the application of hermeneutic circle and
information security functional requirement identification. Information
security for the grid market cover matters includes automation and
communications industry that affects the operation of electric power systems
and the functioning of the utilities that manage them and its awareness of this
information infrastructure has become critical to the reliability of the power
system. Community benefits from of cost savings, flexibility and deployment
along with the establishment of wireless communications. However, concern
revolves around the security protections for easily accessible devices such as
the smart meter and the related communications hardware. On the other hand, the
changing points between traditional versus smart grid networking trend and the
information security importance on the communication field reflects the
criticality of grid information security functional requirement identification.
The goal of this paper is to identify the functional requirement and relate its
significance addresses to the consumer requirement of an information security
of a smart grid. Vulnerabilities may bring forth possibility for an attacker to
penetrate a network, make headway admission to control software, alter it to
load conditions that destabilize the grid in unpredictable ways. Focusing on
the grid information security functional requirement is stepping ahead in
developing consumer trust and satisfaction toward smart grid completeness.

ABSTRACT_BEGIN
  Policy-based management (PBM) is being used as technological solution on the
managing and controlling complex networks and systems. One of the most
important issues involved in the life-cycle of PBM is the policies creation
because the future decisions made by the management system depend on this, and
therefore, the network behavior. In this paper we present a novel model for
creating management policies in telecommunications networks. We propose a model
which includes a Policy Creation Process, Actors, Policy Abstraction Levels and
a Procedure for Creating Policies. An implementation of the proposed model over
the Technology Division at University of Cauca is included.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a novel trajectory clustering technique for selecting the cluster
heads in WSNs. Our algorithm selects the cluster heads based on traffic and
rotates periodically. It provides the first trajectory based clustering
technique for selecting the cluster heads and to extenuate the hot spot problem
by prolonging the network lifetime.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a novel trajectory clustering technique for selecting the cluster
heads in WSNs. Our algorithm selects the cluster heads based on traffic and
rotates periodically. It provides the first trajectory based clustering
technique for selecting the cluster heads and to extenuate the hot spot problem
by prolonging the network lifetime.

ABSTRACT_BEGIN
  Wireless sensor networks (WSNs) suffers from the hot spot problem where the
sensor nodes closest to the base station are need to relay more packet than the
nodes farther away from the base station. Thus, lifetime of sensory network
depends on these closest nodes. Clustering methods are used to extend the
lifetime of a wireless sensor network. However, current clustering algorithms
usually utilize two techniques; selecting cluster heads with more residual
energy, and rotating cluster heads periodically to distribute the energy
consumption among nodes in each cluster and lengthen the network lifetime. Most
of the algorithms use random selection for selecting the cluster heads. Here,
we propose a Fault Tolerant Trajectory Clustering (FTTC) technique for
selecting the cluster heads in WSNs. Our algorithm selects the cluster heads
based on traffic and rotates periodically. It provides the first Fault Tolerant
Trajectory based clustering technique for selecting the cluster heads and to
extenuate the hot spot problem by prolonging the network lifetime.

ABSTRACT_BEGIN
  In this paper, we study the optimal placement and optimal number of base
stations added to an existing wireless data network through the interference
gradient method. This proposed method considers a sub-region of the existing
wireless data network, hereafter called region of interest. In this region, the
provider wants to increase the network coverage and the users throughput. In
this aim, the provider needs to determine the optimal number of base stations
to be added and their optimal placement. The proposed approach is based on the
Delaunay triangulation of the region of interest and the gradient descent
method in each triangle to compute the minimum interference locations. We
quantify the increase of coverage and throughput.

ABSTRACT_BEGIN
  The analysis of large-scale complex networks is a major challenge in the Big
Data domain. Given the large-scale of the complex networks researchers commonly
deal with nowadays, the use of localized information (i.e. restricted to a
limited neighborhood around each node of the network) for centrality-based
analysis is gaining momentum in the recent literature. In this context, we
propose a framework for the Distributed Assessment of Network Centralities
(DANCE) in complex networks. DANCE offers a single environment that allows the
use of different localized centrality proposals, which can be tailored to
specific applications. This environment can be thus useful given the vast
potential applicability of centrality-based analysis on large-scale complex
networks found in different areas, such as Biology, Physics, Sociology, or
Computer Science. Since the localized centrality proposals DANCE implements
employ only localized information, DANCE can easily benefit from parallel
processing environments and run on different computing architectures. To
illustrate this, we present a parallel implementation of DANCE and show how it
can be applied to the analysis of large-scale complex networks using different
kinds of network centralities. This implementation is made available to complex
network researchers and practitioners interested in using it through a
scientific web portal.

ABSTRACT_BEGIN
  For two-tier networks consisting of macrocells and femtocells, the channel
access mechanism can be configured to be open access, closed access, or hybrid
access. Hybrid access arises as a compromise between open and closed access
mechanisms, in which a fraction of available spectrum resource is shared to
nonsubscribers while the remaining reserved for subscribers. This paper focuses
on a hybrid access mechanism for multi-channel femtocells which employ
orthogonal spectrum access schemes. Considering a randomized channel assignment
strategy, we analyze the performance in the downlink. Using stochastic geometry
as technical tools, we model the distribution of femtocells as Poisson point
process or Neyman-Scott cluster process and derive the distributions of
signal-to-interference-plus-noise ratios, and mean achievable rates, of both
nonsubscribers and subscribers. The established expressions are amenable to
numerical evaluation, and shed key insights into the performance tradeoff
between subscribers and nonsubscribers. The analytical results are corroborated
by numerical simulations.

ABSTRACT_BEGIN
  Within the wireless mesh network, a bottleneck problem arises as the number
of concurrent traffic flows (NCTF) increases over a single common control
channel, as it is for most conventional networks. To alleviate this problem,
this paper proposes a two-stage coordination multi-radio multi-channel MAC
(TSC-M2MAC) protocol that designates all available channels as both control
channels and data channels in a time division manner through a two-stage
coordination. At the first stage, a load balancing breadth-first-search-based
vertex coloring algorithm for multi-radio conflict graph is proposed to
intelligently allocate multiple control channels. At the second stage, a
REQ/ACK/RES mechanism is proposed to realize dynamical channel allocation for
data transmission. At this stage, the Channel-and-Radio Utilization Structure
(CRUS) maintained by each node is able to alleviate the hidden nodes problem;
also, the proposed adaptive adjustment algorithm for the Channel Negotiation
and Allocation (CNA) sub-interval is able to cope with the variation of NCTF.
In addition, we design a power saving mechanism for the TSC-M2MAC to decrease
its energy consumption. Simulation results show that the proposed protocol is
able to achieve higher throughput and lower end-to-end packet delay than
conventional schemes. They also show that the TSC-M2MAC can achieve load
balancing, save energy, and remain stable when the network becomes saturated.

ABSTRACT_BEGIN
  Peer-to-Peer (P2P) content sharing systems are susceptible to the content
pollution attack, in which attackers aggressively inject polluted contents into
the systems to reduce the availability of authentic contents, thus decreasing
the confidence of participating users.
  In this paper, we design a pollution-free P2P content sharing system, Green,
by exploiting the inherent content-based information and the social-based
reputation. In Green, a content provider (i.e., creator or sharer) publishes
the information of his shared contents to a group of content maintainers
self-organized in a security overlay for providing the mechanisms of redundancy
and reliability, so that a content requestor can obtain and filter the
information of his requested content from the associated maintainers. We employ
a reputation model to help the requestor better identify the polluted contents,
and then utilize the social (friend-related) information to enhance the
effectiveness and efficiency of our reputation model. Now, the requestor could
easily select an authentic content version for downloading. While downloading,
each requestor performs a realtime integrity verification and takes prompt
protection to handle the content pollution. To further improve the system
performance, we devise a scalable probabilistic verification scheme.
  Green is broadly applicable for both structured and unstructured overlay
applications, and moreover, it is able to defeat various kinds of content
pollution attacks without incurring significant overhead on the participating
users. The evaluation in massive-scale networks validates the success of Green
against the content pollution.

ABSTRACT_BEGIN
  We study network loss tomography based on observing average loss rates over a
set of paths forming a tree -- a severely underdetermined linear problem for
the unknown link loss probabilities. We examine in detail the role of sparsity
as a regularising principle, pointing out that the problem is technically
distinct from others in the compressed sensing literature. While sparsity has
been applied in the context of tomography, key questions regarding uniqueness
and recovery remain unanswered. Our work exploits the tree structure of path
measurements to derive sufficient conditions for sparse solutions to be unique
and the condition that $\ell_1$ minimization recovers the true underlying
solution. We present a fast single-pass linear algorithm for $\ell_1$
minimization and prove that a minimum $\ell_1$ solution is both unique and
sparsest for tree topologies. By considering the placement of lossy links
within trees, we show that sparse solutions remain unique more often than is
commonly supposed. We prove similar results for a noisy version of the problem.

ABSTRACT_BEGIN
  The performances of the routing protocols are important since they compute
the primary path between source and destination. In addition, routing protocols
need to detect failure within a short period of time when nodes move to start
updating the routing table in order to find a new primary path to the
destination. Meantime, loss of packets and end-to- end delays will increase
thereby reducing throughput and degrading the performance of the network. This
paper proposes a new algorithm, DBRT (Driven Backup Routing Table), to improve
the existing proactive protocols such as DSDV (Destination Sequenced Distance
Vector) protocol by creating a backup routing table to provide multiple
alternative routes. The DBRT algorithm identifies adjacent nodes for each node
in the same range and then selects one of these as a backup next hop according
to the available path to the destination. The results show that loss of data
packets, throughput and end-to-end delay times between source and destination
are improved. The results show that the new protocol does not degrade the
network's performance despite sending extra messages to construct and update
the new backup routing table. Simulations (using an NS2 simulator) are
undertaken to demonstrate the difference between using a DSDV protocol with or
without the proposed schema.

ABSTRACT_BEGIN
  With the increasing demands for real-time applications traffic in net- works
such as video and voice a high convergence time for the existing routing
protocols when failure occurred is required. These applications can be very
sensitive to packet loss when link/node goes down. In this paper, we propose
two algorithms schemas for the link state protocol to reroute the traffic in
two states; first, pre-calculated an alternative and disjoint path with the
primary one from the source to the destination by re-routing traffic through
it, regardless of the locations of failure and the number of failed links.
Second, rerouting the traffic via an alternative path from a node whose local
link is down without the need to wait until the source node knows about the
failure. This is achieved by creating a new backup routing table based on the
original routing table which is computed by the dijkstra algorithm. The goal of
these algorithms is to reduce loss of packets, end-to-end delay time, improve
throughput and avoiding local loop when nodes re-converge the topology in case
of failure.

ABSTRACT_BEGIN
  Network traffic is difficult to monitor and analyze, especially in
high-bandwidth networks. Performance analysis, in particular, presents extreme
complexity and scalability challenges. GPU (Graphics Processing Unit)
technology has been utilized recently to accelerate general purpose scientific
and engineering computing. GPUs offer extreme thread-level parallelism with
hundreds of simple cores. Their data-parallel execution model can rapidly solve
large problems with inherent data parallelism. At Fermilab, we have prototyped
a GPU-accelerated network performance monitoring system, called G-NetMon, to
support large-scale scientific collaborations. In this work, we explore new
opportunities in network traffic monitoring and analysis with GPUs. Our system
exploits the data parallelism that exists within network flow data to provide
fast analysis of bulk data movement between Fermilab and collaboration sites.
Experiments demonstrate that our G-NetMon can rapidly detect sub-optimal bulk
data movements.

ABSTRACT_BEGIN
  Beaconless position-based forwarding protocols have recently evolved as a
promising solution for packet forwarding in wireless sensor networks. However,
as the node density grows, the overhead incurred in the process of relay
selection grows significantly. As such, end-to-end performance in terms of
energy and latency is adversely impacted. With the motivation of developing a
packet forwarding mechanism that is tolerant to node density, an alternative
position-based protocol is proposed in this paper. In contrast to existing
beaconless protocols, the proposed protocol is designed such that it eliminates
the need for potential relays to undergo a relay selection process. Rather, any
eligible relay may decide to forward the packet ahead, thus significantly
reducing the underlying overhead. The operation of the proposed protocol is
empowered by exploiting favorable features of orthogonal frequency division
multiplexing (OFDM) at the physical layer. The end-to-end performance of the
proposed protocol is evaluated against existing beaconless position-based
protocols analytically and as well by means of simulations. The proposed
protocol is demonstrated in this paper to be more efficient. In particular, it
is shown that for the same amount of energy the proposed protocol transports
one bit from source to destination much quicker.

ABSTRACT_BEGIN
  Common WLAN pathologies include low signal-to-noise ratio, congestion, hidden
terminals or interference from non-802.11 devices and phenomena. Prior work has
focused on the detection and diagnosis of such problems using layer-2
information from 802.11 devices and special-purpose access points and monitors,
which may not be generally available. Here, we investigate a userlevel
approach: is it possible to detect and diagnose 802.11 pathologies with
strictly user-level active probing, without any cooperation from, and without
any visibility in, layer-2 devices? In this paper, we present preliminary but
promising results indicating that such diagnostics are feasible.

ABSTRACT_BEGIN
  Vehicular Ad Hoc Networks (VANET) is a subclass of Mobile ad hoc networks
which provides a distinguished approach for Intelligent Transport System (ITS).
The survey of routing protocols in VANET is important and necessary for smart
ITS. This paper discusses the advantages / disadvantages and the applications
of various routing protocols for vehicular ad hoc networks. It explores the
motivation behind the designed, and traces the evolution of these routing
protocols. F inally the paper concludes by a tabular comparison of the various
routing protocols for VANET.

ABSTRACT_BEGIN
  In this paper, we develop a partner selection protocol for enhancing the
network lifetime in cooperative wireless networks. The case-study is the
cooperative relayed transmission from fixed indoor nodes to a common outdoor
access point. A stochastic bivariate model for the spatial distribution of the
fading parameters that govern the link performance, namely the Rician K-factor
and the path-loss, is proposed and validated by means of real channel
measurements. The partner selection protocol is based on the real-time
estimation of a function of these fading parameters, i.e., the coding gain. To
reduce the complexity of the link quality assessment, a Bayesian approach is
proposed that uses the site-specific bivariate model as a-priori information
for the coding gain estimation. This link quality estimator allows network
lifetime gains almost as if all K-factor values were known. Furthermore, it
suits IEEE 802.15.4 compliant networks as it efficiently exploits the
information acquired from the receiver signal strength indicator. Extensive
numerical results highlight the trade-off between complexity, robustness to
model mismatches and network lifetime performance. We show for instance that
infrequent updates of the site-specific model through K-factor estimation over
a subset of links are sufficient to at least double the network lifetime with
respect to existing algorithms based on path loss information only.

ABSTRACT_BEGIN
  We construct a simple network model to provide insight into network design
strategies. We show that the model can be used to address various approaches to
network coding, MAC, and multi-packet reception so that their effects on
network throughput can be evaluated. We consider several topology components
which exhibit the same non-monotonic saturation behavior found within the Katti
et. al. COPE experiments. We further show that fairness allocation by the MAC
can seriously impact performance and cause this non-monotonic saturation. Using
our model, we develop a MAC that provides monotonic saturation, higher
saturation throughput gains and fairness among flows rather than nodes. The
proposed model provides an estimate of the achievable gains for the cross-layer
design of network coding, multi-packet reception, and MAC showing that
super-additive throughput gains on the order of six times that of routing are
possible.

ABSTRACT_BEGIN
  Through several studies, it has been highlighted that mobility patterns in
mobile networks are driven by human behaviors. This effect has been
particularly observed in intermittently connected networks like DTN (Delay
Tolerant Networks). Given that common social intentions generate similar human
behavior, it is relevant to exploit this knowledge in the network protocols
design, e.g. to identify the closeness degree between two nodes. In this paper,
we propose a temporal link prediction technique for DTN which quantifies the
behavior similarity between each pair of nodes and makes use of it to predict
future links. Our prediction method keeps track of the spatio-temporal aspects
of nodes behaviors organized as a third-order tensor that aims to records the
evolution of the network topology. After collapsing the tensor information, we
compute the degree of similarity for each pair of nodes using the Katz measure.
This metric gives us an indication on the link occurrence between two nodes
relying on their closeness. We show the efficiency of this method by applying
it on three mobility traces: two real traces and one synthetic trace. Through
several simulations, we demonstrate the effectiveness of the technique
regarding another approach based on a similarity metric used in DTN. The
validity of this method is proven when the computation of score is made in a
distributed way (i.e. with local information). We attest that the tensor-based
technique is effective for temporal link prediction applied to the
intermittently connected networks. Furthermore, we think that this technique
can go beyond the realm of DTN and we believe this can be further applied on
every case of figure in which there is a need to derive the underlying social
structure of a network of mobile users.

ABSTRACT_BEGIN
  Objective: The main objective of this paper is to construct a distributed
clustering algorithm based upon spatial data correlation among sensor nodes and
perform data accuracy for each distributed cluster at their respective cluster
head node. Design Procedure/Approach: We investigate that due to deployment of
high density of sensor nodes in the sensor field, spatial data are highly
correlated among sensor nodes in spatial domain. Based on high data correlation
among sensor nodes, we propose a non -overlapping irregular distributed
clustering algorithm with different sizes to collect most accurate or precise
data at the cluster head node for each respective distributed cluster. To
collect the most accurate data at the cluster head node for each distributed
cluster in sensor field, we propose a Data accuracy model and compare the
results with Information accuracy model. Finding: Simulation results shows that
our propose Data accuracy model collects more accurate data and gives better
performance than Information accuracy model at the cluster head node for each
respective distributed cluster in our propose distributed clustering
algorithm.Morover there exist a optimal cluster of sensor nodes which is
adequate to perform approximately the same data accuracy achieve by a cluster.
Practical Implementation: Measuring humidity and moisture content in an
agricultural field, measuring temperature in physical environment. Inventive
/Novel Idea: A distributed clustering algorithm is proposed based on spatial
data correlation among sensor nodes with Data accuracy model.

ABSTRACT_BEGIN
  The rapid increase of vehicular traffic and congestion on the highways began
hampering the safe and efficient movement of traffic. Consequently, year by
year, we see the ascending rate of car accidents and casualties in most of the
countries. Therefore, exploiting the new technologies, e.g. wireless sensor
networks, is required as a solution of reduction of these saddening and
reprehensible statistics. This has motivated us to propose a novel and
comprehensive system to utilize Wireless Sensor Networks for vehicular
networks. We coin the vehicular network employing wireless Sensor networks as
Vehicular Ad Hoc and Sensor Network, or VASNET in short. The proposed VASNET is
particularly for highway traffic .VASNET is a self-organizing Ad Hoc and sensor
network comprised of a large number of sensor nodes. In VASNET there are two
kinds of sensor nodes, some are embedded on the vehicles-vehicular nodes- and
others are deployed in predetermined distances besides the highway road, known
as Road Side Sensor nodes (RSS). The vehicular nodes are used to sense the
velocity of the vehicle for instance. We can have some Base Stations (BS) such
as Police Traffic Station, Firefighting Group and Rescue Team. The base
stations may be stationary or mobile. VASNET provides capability of wireless
communication between vehicular nodes and stationary nodes, to increase safety
and comfort for vehicles on the highway roads. In this paper we explain main
fundamentals and challenges of VASNET.

ABSTRACT_BEGIN
  Many energy-aware routing protocols have been proposed for wireless sensor
networks. Most of them are only energy savers and do not take care about energy
balancing. The energy saver protocols try to decrease the energy consumption of
the network as a whole; however the energy manager protocols balance the energy
consumption in the network to avoid network partitioning. This means that
energy saver protocols are not necessarily energy balancing and vice versa.
However, the lifetime of wireless sensor network is strictly depending on
energy consumption; therefore, energy management is an essential task to be
considered. This paper proposes an energy aware routing protocol, named FEAR,
which considers energy balancing and energy saving. It finds a fair trade-off
between energy balancing and energy saving by fuzzy set concept. FEAR routing
protocol is simulated and evaluated by Glomosim simulator.

ABSTRACT_BEGIN
  Network of Information (NetInf) is a term coined for networks which unlike
contemporary network are not node centric. As the name indicates, information
supersedes nodes in the network. In this report, we propose an architecture of
mobile node for NetInf. We call it NetInf Mobile Node. It is an extension of
the basic node architecture proposed for NetInf. It is compatible to NetInf and
TCP/IP based networks. The Virtual Node Layer modules in the architecture
provide support for managing mobility, power consumption of the node as well
data relaying/storing services. In- ner/Outer Locator Construction Routers (I/O
LCTR) are two functions introduced in NetInf mobile nodes to operate between
NetInf and non- NetInf sites. The basic purpose of NetInf mobile node is to
maintain the QoS during mobility events. The handoff/handover are critical
situations during mobility where chances of QoS degradation of an ongoing
session are high. This report presents one such scenario in which QoS of an
appli- cation is maintained during a handoff situations in heterogeneous
wireless network environment through our proposed algorithm.

ABSTRACT_BEGIN
  In this paper, we have modeled the routing over- head generated by three
reactive routing protocols; Ad-hoc On-demand Distance Vector (AODV), Dynamic
Source Routing (DSR) and DYnamic MANET On-deman (DYMO). Routing performed by
reactive protocols consists of two phases; route discovery and route
maintenance. Total cost paid by a protocol for efficient routing is sum of the
cost paid in the form of energy consumed and time spent. These protocols
majorly focus on the optimization performed by expanding ring search algorithm
to control the flooding generated by the mechanism of blind flooding. So, we
have modeled the energy consumed and time spent per packet both for route
discovery and route maintenance. The proposed framework is evaluated in NS-2 to
compare performance of the chosen routing protocols.

ABSTRACT_BEGIN
  In this paper, we identify and analyze the requirements to design a new
routing link metric for wireless multihop networks. Considering these
requirements, when a link metric is proposed, then both the design and
implementation of the link metric with a routing protocol become easy.
Secondly, the underlying network issues can easily be tackled. Thirdly, an
appreciable performance of the network is guaranteed. Along with the existing
implementation of three link metrics Expected Transmission Count (ETX), Minimum
Delay (MD), and Minimum Loss (ML), we implement inverse ETX; invETX with
Optimized Link State Routing (OLSR) using NS-2.34. The simulation results show
that how the computational burden of a metric degrades the performance of the
respective protocol and how a metric has to trade-off between different
performance parameters.

ABSTRACT_BEGIN
  In this paper, we evaluate, analyze, and compare the impact of mobility on
the behavior of three reactive protocols (AODV, DSR, DYMO) and three proactive
protocols (DSDV, FSR, OLSR) in multi-hop wireless networks. We take into
account throughput, end-to-end delay, and normalized routing load as
performance parameters. Based upon the extensive simulation results in NS-2, we
rank all of six protocols according to the performance parameters. Besides
providing the interesting facts regarding the response of each protocol on
varying mobilities and speeds, we also study the trade-offs, the routing
protocols have to make. Such as, to achieve throughput, a protocol has to pay
some cost in the form of increased end-to-end delay or routing overhead.

ABSTRACT_BEGIN
  We consider the scheduling problem in downlink wireless networks with
heterogeneous, Markov-modulated, ON/OFF channels. It is well-known that the
performance of scheduling over fading channels heavily depends on the accuracy
of the available Channel State Information (CSI), which is costly to acquire.
Thus, we consider the CSI acquisition via a practical ARQ-based feedback
mechanism whereby channel states are revealed at the end of only scheduled
users' transmissions. In the assumed presence of temporally-correlated channel
evolutions, the desired scheduler must optimally balance the
exploitation-exploration trade-off, whereby it schedules transmissions both to
exploit those channels with up-to-date CSI and to explore the current state of
those with outdated CSI.
  In earlier works, Whittle's Index Policy had been suggested as a
low-complexity and high-performance solution to this problem. However,
analyzing its performance in the typical scenario of statistically
heterogeneous channel state processes has remained elusive and challenging,
mainly because of the highly-coupled and complex dynamics it possesses. In this
work, we overcome these difficulties to rigorously establish the asymptotic
optimality properties of Whittle's Index Policy in the limiting regime of many
users. More specifically: (1) we prove the local optimality of Whittle's Index
Policy, provided that the initial state of the system is within a certain
neighborhood of a carefully selected state; (2) we then establish the global
optimality of Whittle's Index Policy under a recurrence assumption that is
verified numerically for the problem at hand. These results establish, for the
first time to the best of our knowledge, that Whittle's Index Policy possesses
analytically provable optimality characteristics for scheduling over
heterogeneous and temporally-correlated channels.

ABSTRACT_BEGIN
  The number of contending neighbors of a node in a multihop ad hoc network has
to be adjusted while analyzing the performance of the network such as computing
the end-to-end delays along a path from a given source to a destination. In
this paper, we describe a method to adjust the number of contending neighbors
of a node in a multihop wireless ad hoc network. Our method is based on the
minimum number of neighbors that has to be common between two consecutive nodes
along a path. We derive an analytical expression for the adjustment factor.

ABSTRACT_BEGIN
  We propose a new model for peer-to-peer networking which takes the network
bottlenecks into account beyond the access. This model allows one to cope with
key features of P2P networking like degree or locality constraints or the fact
that distant peers often have a smaller rate than nearby peers. We show that
the spatial point process describing peers in their steady state then exhibits
an interesting repulsion phenomenon. We analyze two asymptotic regimes of the
peer-to-peer network: the fluid regime and the hard--core regime. We get closed
form expressions for the mean (and in some cases the law) of the peer latency
and the download rate obtained by a peer as well as for the spatial density of
peers in the steady state of each regime, as well as an accurate approximation
that holds for all regimes. The analytical results are based on a mix of
mathematical analysis and dimensional analysis and have important design
implications. The first of them is the existence of a setting where the
equilibrium mean latency is a decreasing function of the load, a phenomenon
that we call super-scalability.

ABSTRACT_BEGIN
  Mobile Ad-hoc Network (MANET) is the self organizing collection of mobile
nodes. The communication in MANET is done via a wireless media. Ad hoc wireless
networks have massive commercial and military potential because of their
mobility support. Due to demanding real time multimedia applications, Quality
of Services (QoS) support in such infrastructure less networks have become
essential. QoS routing in mobile Ad-Hoc networks is challenging due to rapid
change in network topology. Consequently, the available state information for
routing is inherently imprecise. QoS routing may suffer badly due to several
factors including radio interference on available bandwidth, and inefficient
flooding of information to the adjacent nodes. As a result the performance of
the network degrades substantially. This paper aims at the solution for energy
efficient QoS routing by best utilization of network resources such as energy
and bandwidth. A comparative study shows that despite the overhead due to QoS
management, this solution performs better than classical OLSR protocol in terms
of QoS and efficient utilization of energy.

ABSTRACT_BEGIN
  Currently, the development of research around VoIP experience a tremendous
growth. In the community of open source Asterisk represents a reliable
alternative for a lower cost solution. In this same community as the SIP
protocol is a supplement to the more asterisk PBX. to share the benefits
claimed by proponents of free software co-existence with other Asterisk server
is not yet proven. In this context this paper we show a comparison of the use
of simplified resource material for the apache server using the HTTP protocol
and server that uses the asterisk SIP.

ABSTRACT_BEGIN
  Stochastic network calculus is an evolving theory which accounts for
statistical multiplexing and uses an envelope approach for probabilistic delay
and backlog analysis of networks. One of the key ideas of stochastic network
calculus is the possibility to describe service offered at network node as a
stochastic service envelope, which in turn can be used to describe the
stochastic service available in a network of nodes and determine end-to-end
probabilistic delay and backlog bounds. This paper introduces a new definition
of stochastic service envelopes which yield a simple network service envelope
and tighter end-to-end performance bounds. It is shown for ($\sigma(\theta),
\rho(\theta)$) - constrained traffic model that the end-to-end performance
measures computed using the new stochastic network service envelope are tight
in comparison to the ones obtained using the existing start-of-the-art
definition of statistical network service envelope and are bounded by ${\cal
O}(H \log{H})$, where $H$ is the number of nodes traversed by the arrival
traffic.

ABSTRACT_BEGIN
  In wireless networks, the transmission rate of a link is determined by
received signal strength, interference from simultaneous transmissions, and
available coding-modulation schemes. Rate allocation is a key problem in
wireless network design, but a very challenging problem because: (i) wireless
interference is global, i.e., a transmission interferes all other simultaneous
transmissions, and (ii) the rate-power relation is non-convex and
non-continuous, where the discontinuity is due to limited number of
coding-modulation choices in practical systems. In this paper, we propose a
distributed power control and coding-modulation adaptation algorithm using
annealed Gibbs sampling, which achieves throughput optimality in an arbitrary
network topology. We consider a realistic
Signal-to-Interference-and-Noise-Ratio (SINR) based interference model, and
assume continuous power space and finite rate options (coding-modulation
choices). Our algorithm first decomposes network-wide interference to local
interference by properly choosing a "neighborhood" for each transmitter and
bounding the interference from non-neighbor nodes. The power update policy is
then carefully designed to emulate a Gibbs sampler over a Markov chain with a
continuous state space. We further exploit the technique of simulated annealing
to speed up the convergence of the algorithm to the optimal power and
coding-modulation configuration. Finally, simulation results demonstrate the
superior performance of the proposed algorithm.

ABSTRACT_BEGIN
  Energy efficiency in cellular networks is a growing concern for cellular
operators to not only maintain profitability, but also to reduce the overall
environment effects. This emerging trend of achieving energy efficiency in
cellular networks is motivating the standardization authorities and network
operators to continuously explore future technologies in order to bring
improvements in the entire network infrastructure. In this article, we present
a brief survey of methods to improve the power efficiency of cellular networks,
explore some research issues and challenges and suggest some techniques to
enable an energy efficient or "green" cellular network. Since base stations
consume a maximum portion of the total energy used in a cellular system, we
will first provide a comprehensive survey on techniques to obtain energy
savings in base stations. Next, we discuss how heterogeneous network deployment
based on micro, pico and femto-cells can be used to achieve this goal. Since
cognitive radio and cooperative relaying are undisputed future technologies in
this regard, we propose a research vision to make these technologies more
energy efficient. Lastly, we explore some broader perspectives in realizing a
"green" cellular network technology

ABSTRACT_BEGIN
  To reduce datacenter energy consumption and cost, current practice has
considered demand-proportional resource provisioning schemes, where servers are
turned on/off according to the load of requests.
  Most existing work considers instantaneous (Internet) requests only, which
are explicitly or implicitly assumed to be delay-sensitive. On the other hand,
in datacenters, there exist a vast amount of delay-tolerant jobs, such as
background/maintainance jobs. In this paper, we explicitly differentiate
delay-sensitive jobs and delay tolerant jobs. We focus on the problem of using
delay-tolerant jobs to fill the extra capacity of datacenters, referred to as
trough/valley filling. Giving a higher priority to delay-sensitive jobs, our
schemes complement to most existing demand-proportional resource provisioning
schemes. Our goal is to design intelligent trough filling mechanisms that are
energy efficient and also achieve good delay performance. Specifically, we
propose two joint dynamic speed scaling and traffic shifting schemes, one
subgradient-based and the other queue-based. Our schemes assume little
statistical information of the system, which is usually difficult to obtain in
practice. In both schemes, energy cost saving comes from dynamic speed scaling,
statistical multiplexing, electricity price diversity, and service efficiency
diversity. In addition, good delay performance is achieved in the queue-based
scheme via load shifting and capacity allocation based on queue conditions.
Practical issues that may arise in datacenter networks are considered,
including capacity and bandwidth constraint, service agility constraint, and
load shifting cost. We use both artificial and real datacenter traces to
evaluate the proposed schemes.

ABSTRACT_BEGIN
  Frame Synchronization (FS) is required in several communication standards in
order to recover the individual frames that have been aggregated in a burst.
This paper proposes a low-delay and reducedcomplexity Sliding Trellis
(ST)-based FS technique, compared to our previously proposed trellis-based FS
method. Each burst is divided into overlapping windows in which FS is
performed. Useful information is propagated from one window to the next. The
proposed method makes use of soft information provided by the channel, but also
of all sources of redundancy present in the protocol stack. An illustration of
our STbased approach for the WiMAX Media Access Control (MAC) layer is
provided. When FS is performed on bursts transmitted over Rayleigh fading
channel, the ST-based approach reduces the FS latency and complexity at the
cost of a very small performance degradation compared to our full complexity
trellis-based FS and outperforms state-of-the-art FS techniques.

ABSTRACT_BEGIN
  This paper introduces a reconstruction approach for the input signal of an
oversampled filter bank (OFB) when the sub-bands generated at its output are
quantized and transmitted over a noisy channel. This approach exploits the
redundancy introduced by the OFB and the fact that the quantization noise is
bounded. A maximum-likelihood estimate of the input signal is evaluated, which
only considers the vectors of quantization indexes corresponding to subband
signals that could have been generated by the OFB and that are compliant with
the quantization errors. When considering an OFB with an oversampling ratio of
3/2 and a transmission of quantized subbands on an AWGN channel, compared to a
classical decoder, the performance gains are up to 9 dB in terms of SNR for the
reconstructed signal, and 3 dB in terms of channel SNR.

ABSTRACT_BEGIN
  Systems of networked mobile robots, such as unmanned aerial or ground
vehicles, will play important roles in future military and commercial
applications. The communications for such systems will typically be over
wireless links and may require that the robots form an ad hoc network and
communicate on a peer-to-peer basis. In this paper, we consider the problem of
optimizing the network topology to minimize the total traffic in a network
required to support a given set of data flows under constraints on the amount
of movement possible at each mobile robot. In this paper, we consider a
subclass of this problem in which the initial and final topologies are trees,
and the movement restrictions are given in terms of the number of edges in the
graph that must be traversed. We develop algorithms to optimize the network
topology while maintaining network connectivity during the topology
reconfiguration process. Our topology reconfiguration algorithm uses the
concept of prefix labelling and routing to move nodes through the network while
maintaining network connectivity. We develop two algorithms to determine the
final network topology: an optimal, but computationally complex algorithm, and
a greedy suboptimal algorithm that has much lower complexity. We present
simulation results to compare the performance of these algorithm.

ABSTRACT_BEGIN
  Recovery from transient failures is one of the prime issues in the context of
distributed systems. These systems demand to have transparent yet efficient
techniques to achieve the same. Checkpoint is defined as a designated place in
a program where normal processing of a system is interrupted to preserve the
status information. Checkpointing is a process of saving status information.
Mobile computing systems often suffer from high failure rates that are
transient and independent in nature. To add reliability and high availability
to such distributed systems, checkpoint based rollback recovery is one of the
widely used techniques for applications such as scientific computing, database,
telecommunication applications and mission critical applications. This paper
surveys the algorithms which have been reported in the literature for
checkpointing in Mobile Computing Systems.

ABSTRACT_BEGIN
  Recent trends show that there are swift developments and fast convergence of
wireless and mobile communication networks with internet services to provide
the quality of ubiquitous access to network users. Most of the wireless
networks and mobile cellular networks are moving to be all IP based. These
networks are connected through the private IP core networks using the TCP/IP
protocol or through the Internet. As such, there is room to improve the
mobility support through the Internet and support ubiquitous network access by
providing seamless handover. This is especially true with the invention of
portable mobile and laptop devices that can be connected almost everywhere at
any time. However, the recent explosion on the usage of mobile and laptop
devices has also generated several issues in terms of performance and quality
of service. Nowadays, mobile users demand high quality performance, best
quality of services and seamless connections that support real-time application
such as audio and video streaming. The goal of this paper is to study the
impact and evaluate the mobility management protocols under micro mobility
domain on link layer and network layer handover performance. Therefore, this
paper proposes an integration solution of network-based mobility management
framework, based on Proxy Mobile IPv6, to alleviate handover latency, packet
loss and increase throughput and the performance of video transmission when
mobile host moves to new network during handover on high speed mobility.
Simulations are conducted to analyze the relationship between the network
performances with the moving speed of mobile host over mobility protocols.
Based on simulation results, we presented and analyzed the results of mobility
protocols under intra-domain traffics in micro mobility domain.

ABSTRACT_BEGIN
  The study of the weather is performed using instruments termed weather
stations. These weather stations are distributed around the world, collecting
the data from the different phenomena. Several weather organizations have been
deploying thousands of these instruments, creating big networks to collect
weather data. These instruments are collecting the weather data and delivering
it for later processing in the collections points. Nevertheless, all the
methodologies used to transmit the weather data are based in protocols non
adapted for this purpose. Thus, the weather stations are limited by the data
formats and protocols used in them, not taking advantage of the real-time data
available on them. We research the weather instruments, their technology and
their network capabilities, in order to provide a solution for the mentioned
problem. OpenWeather is the protocol proposed to provide a more optimum and
reliable way to transmit the weather data. We evaluate the environmental
factors, such as location or bandwidth availability, in order to design a
protocol adapted to the requirements established by the automatic weather
stations. A peer to peer architecture is proposed, providing a functional
implementation of OpenWeather protocol. The evaluation of the protocol is
executed in a real scenario, providing the hints to adapt the protocol to a
common automatic weather station.

ABSTRACT_BEGIN
  The goal of this research report is to present OSERENA "Optimized SchEduling
RoutEr Node Activity", a distributed coloring algorithm optimized for dense
wireless networks. Network density has an extremely reduced impact on the size
of the messages exchanged to color the network. Furthermore, the number of
colors used to color the network is not impacted by this optimization. We
describe in this research report the properties of the algorithm and prove its
correctness and termination. Simulation results point out the considerable
gains in bandwidth.

ABSTRACT_BEGIN
  Most disruption-tolerant networking (DTN) protocols available in the
literature have focused on mere contact and intercontact characteristics to
make forwarding decisions. Nevertheless, there is a world behind contacts: just
because one node is not in contact with some potential destination, it does not
mean that this node is alone. There may be interesting end-to-end transmission
opportunities through other nearby nodes. Existing protocols miss such
possibilities by maintaining a simple contact-based view of the network. In
this paper, we investigate how the vicinity of a node evolves through time and
whether such information can be useful when routing data. We observe a clear
tradeoff between routing performance and the cost for monitoring the
neighborhood. Our analyses suggest that limiting a node's neighborhood view to
three or four hops is more than enough to significantly improve forwarding
efficiency without incurring prohibitive overhead.

ABSTRACT_BEGIN
  "Web Of Things" evolved from "Internet Of Things". Lot of research has been
done in designing architecture for "Web Of Things". Two main architectures are
Smart gateway based architecture and embedded Web Server based architecture.
These architectures address some of the basic and essential issues relating to
Service Oriented Architecture for "Web Of Things". Taking into consideration
the period of coexistence of IPv4 and IPv6 we propose an architecture using
SOCKS protocol based IPv6/IPv4 gatewaying and refinements which facilitates
smooth heterogeneous communications between the IPv6 and IPv4 enabled embedded
nodes and can potentially be used to prevent security threats like
Denial-of-Service (DoS) attacks on embedded devices attached to the web and
increase its performance. Our architecture provides a way for caching responses
from device and thereby increasing its efficiency and performance and yielding
quick response times.

ABSTRACT_BEGIN
  In this paper we give a brief introduction to five different distance vector
routing protocols (RIP, AODV, EIGRP, RIP-MTI and Babel) and give some of our
thoughts on how to solve the count to infinity problem. Our focus is how
distance vector routing protocols, based on limited information, can prevent
routing loops and the count to infinity problem.

ABSTRACT_BEGIN
  Multiple Input Multiple Output (MIMO) technology is going to be a viable
alternative for future generation wireless broadband services in order to meet
the striving requirements for throughput and system robustness. In Long Term
Evolution (LTE), MIMO technologies have been broadly used to get better
downlink peak rate, cell coverage, as well as average cell throughput. In the
present paper a 2x2 MIMO is taken as baseline configuration for a LTE downlink
under a Microcellular propagation scenario considering a non physical
correlation based channel with Poor and rich scattering environment. The
throughput capacity of the downlink is obtained for poor and rich scattering
environments. Besides, two vital aspects of MIMO technique viz Spatial
Multiplexing (SM) and Transmit Diversity (TD) are investigated in order to see
their effect on throughput of the system. The effect of parameters like Speed
of mobile station, number of Multipath, Rician factor (K) on throughput of such
systems is reported and discussed. The investigations reported in this paper
helps in estimating the throughput capacity of LTE downlink under SM and TD
mode.

ABSTRACT_BEGIN
  The main purpose of this paper is to study the performance of two linear
channel estimators for LTE Downlink systems, the Least Square Error (LSE) and
the Linear Minimum Mean Square Error (LMMSE). As LTE is a MIMO-OFDM based
system, a cyclic prefix is inserted at the beginning of each transmitted OFDM
symbol in order to completely suppress both inter-carrier interference (ICI)
and inter-symbol interference (ISI). Usually, the cyclic prefix is equal to or
longer than the channel length but in some cases and because of some unforeseen
channel behaviour, the cyclic prefix can be shorter. Therefore, we propose to
study the performance of the two linear estimators under the effect of the
channel length. Computer simulations show that, in the case where the cyclic
prefix is equal to or longer than the channel length,LMMSE performs better than
LSE but at the cost of computational complexity.In the other case, LMMSE
continue to improve its performance only for low SNR values but it degrades for
high SNR values in which LS shows better performance for LTE Downlink systems.
MATLAB Monte-Carlo simulations are used to evaluate the performance of the
studied estimators in terms of Mean Square Error (MSE) and Bit Error Rate (BER)
for 2x2 LTE Downlink systems.

ABSTRACT_BEGIN
  There has been a tremendous effort in improving wireless LAN for supporting
the demanding multimedia application. Many new protocols or ideas have been
proposed and proved by using a mathematical model or running a simulation
program. That is satisfactory but these proposed designs might not work in the
real world situation. Testbed is an option to alleviate this gap and present
the opportunity to see the real problem and ensure that the design works. A
framework architecture for building a testbed to test a new concept or design
is presented in this paper. The framework is designed in the modularity style
in such a way that can be easily exchanged or modified. A testbed based on the
framework that implements the polling based mechanism has been created and the
results have shown that the QoS of the real time traffic can be maintained in
the presence of the high non-real time traffic.

ABSTRACT_BEGIN
  Recently mobile agents are used to discover services in mobile ad-hoc network
(MANET) where agents travel through the network, collecting and sometimes
spreading the dynamically changing service information. But it is important to
investigate how reliable the agents are for this application as the
dependability issues(reliability and availability) of MANET are highly affected
by its dynamic nature.The complexity of underlying MANET makes it hard to
obtain the route reliability of the mobile agent systems (MAS); instead we
estimate it using Monte Carlo simulation. Thus an algorithm for estimating the
task route reliability of MAS (deployed for discovering services) is proposed,
that takes into account the effect of node mobility in MANET. That mobility
pattern of the nodes affects the MAS performance is also shown by considering
different mobility models. Multipath propagation effect of radio signal is
considered to decide link existence. Transient link errors are also considered.
Finally we propose a metric to calculate the reliability of service discovery
protocol and see how MAS performance affects the protocol reliability. The
experimental results show the robustness of the proposed algorithm. Here the
optimum value of network bandwidth (needed to support the agents) is calculated
for our application. However the reliability of MAS is highly dependent on link
failure probability.

ABSTRACT_BEGIN
  Heterogeneous wireless sensor networks (HWSNET) are more suitable for real
life applications as compared to the homogeneous counterpart. Security of
HWSNET becomes a very important issue with the rapid development of HWSNET.
Intrusion detection system is one of the major and efficient defensive methods
against attacks in HWSNET. Because of different constraints of sensor networks,
security solutions have to be designed with limited usage of computation and
resources. A particularly devastating attack is the sleep deprivation attack.
Here a malicious node forces legitimate nodes to waste their energy by
resisting the sensor nodes from going into low power sleep mode. The target of
this attack is to maximize the power consumption of the affected node, thereby
decreasing its battery life. Existing works on sleep deprivation attack have
mainly focused on mitigation using MAC based protocols, such as S-MAC (sensor
MAC), T-MAC (timeout MAC), B-MAC (Berkley MAC), G-MAC (gateway MAC). In this
article, a brief review of some of the recent intrusion detection systems in
wireless sensor network environment is presented. Finally, a framework of
cluster based layered countermeasure for Insomnia Detection has been proposed
for heterogeneous wireless sensor network (HWSNET) to efficiently detect sleep
deprivation attack. Simulation results on MATLAB exhibit the effectiveness of
the proposed model.

ABSTRACT_BEGIN
  As cellular networks are turning into a platform for ubiquitous data access,
cellular operators are facing a severe data capacity crisis due to the
exponential growth of traffic generated by mobile users. In this work, we
investigate the benefits of sharing infrastructure and spectrum among two
cellular operators. Specifically, we provide a multi-cell analytical model
using stochastic geometry to identify the performance gain under different
sharing strategies, which gives tractable and accurate results. To validate the
performance using a realistic setting, we conduct extensive simulations for a
multi-cell OFDMA system using real base station locations. Both analytical and
simulation results show that even a simple cooperation strategy between two
similar operators, where they share spectrum and base stations, roughly
quadruples capacity as compared to the capacity of a single operator. This is
equivalent to doubling the capacity per customer, providing a strong incentive
for operators to cooperate, if not actually merge.

ABSTRACT_BEGIN
  Bluetooth is developed for short range communication. Bluetooth Devices are
normally having low power and low cost. This is a wireless communication
technology designed to connect phones, laptops and PDAs. The greater
availability of portable devices with Bluetooth connectivity imposes wireless
connection between enabled devices. On an average the range of Bluetooth
devices is about 10 meters. The basic limitation of the Bluetooth communication
is this range limitation. In this paper I have studied the limitations of
Bluetooth communication and consider range constraint as the major limitation.
I propose a new expanded Blue tooth network to overcome the range constraint of
Bluetooth device. This creates a network of Bluetooth enabled devices that will
include laptops, set top devices and also mobile phones. The main purpose of
this proposal is to establish a network will enable the users to communicate
outside the range without any range constraint.

ABSTRACT_BEGIN
  This paper investigates the adaptive subcarrier and bit allocation algorithm
for OFDMA systems. To minimize overall transmitted power, we propose a novel
adaptive subcarrier and bit allocation algorithm based on channel state
information (CSI) and quality state information (QSI). A suboptimal approach
that separately performs subcarrier allocation and bit loading is proposed. It
is shown that a near optimal solution is obtained by the proposed algorithm
which has low complexity compared to that of other conventional algorithm. We
will study the problem of finding an optimal sub-carrier and power allocation
strategy for downlink communication to multiple users in an OFDMA based
wireless system. Assuming knowledge of the instantaneous channel gains for all
users, we propose a multiuser OFDMA subcarrier, and bit allocation algorithm to
minimize the total transmit power. This is done by assigning each user a set of
subcarriers and by determining the number of bits and the transmit power level
for each subcarrier. The objective is to minimize the total transmitted power
over the entire network to satisfy the application layer and physical layer. We
formulate this problem as a constrained optimization problem and present
centralized algorithms. The simulation results will show that our approach
results in an efficient assignment of subcarriers and transmitter power levels
in terms of the energy required for transmitting each bit of information, to
address this need, we also present a bit loading algorithm for allocating
subcarriers and bits in order to satisfy the rate requirements of the links.

ABSTRACT_BEGIN
  We quantify the throughput capacity of wireless multi-hop networks with
several medium access schemes. We analyze pure ALOHA scheme where simultaneous
transmitters are dispatched according to a uniform Poisson distribution and
exclusion schemes where simultaneous transmitters are dispatched according to
an exclusion rule such as node coloring and carrier sense based schemes. We
consider both no-fading and standard Rayleigh fading channel models. Our
results show that, under no-fading, slotted ALOHA can achieve at least
one-third (or half under Rayleigh fading) of the throughput capacity of node
coloring scheme whereas carrier sense based scheme can achieve almost the same
throughput capacity as node coloring.

ABSTRACT_BEGIN
  We study the optimal transmission scheme that maximizes the local capacity in
two-dimensional (2D) wireless networks. Local capacity is defined as the
average information rate received by a node randomly located in the network.
Using analysis based on analytical and numerical methods, we show that maximum
local capacity can be obtained if simultaneous emitters are positioned in a
grid pattern based on equilateral triangles. We also compare this maximum local
capacity with the local capacity of slotted ALOHA scheme and our results show
that slotted ALOHA can achieve at least half of the maximum local capacity in
wireless networks.

ABSTRACT_BEGIN
  In this work, we evaluate local capacity of wireless ad hoc networks with
several medium access protocols and identify the most optimal protocol. We
define local capacity as the average information rate received by a receiver
randomly located in the network. We analyzed grid pattern protocols where
simultaneous transmitters are positioned in a regular grid pattern, pure ALOHA
protocols where simultaneous transmitters are dispatched according to a uniform
Poisson distribution and exclusion protocols where simultaneous transmitters
are dispatched according to an exclusion rule such as node coloring and carrier
sense protocols. Our analysis allows us to conjecture that local capacity is
optimal when simultaneous transmitters are positioned in a grid pattern based
on equilateral triangles and our results show that this optimal local capacity
is at most double the local capacity of simple ALOHA protocol. Our results also
show that node coloring and carrier sense protocols approach the optimal local
capacity by an almost negligible difference.

ABSTRACT_BEGIN
  We investigate the design of a broadcast system where the aim is to maximise
the throughput. This task is usually challenging due to the channel
variability. Modern satellite communications systems such as DVB-SH and DVB-S2
mainly rely on time sharing strategy to optimize throughput. They consider
hierarchical modulation but only for unequal error protection or backward
compatibility purposes. We propose in this article to combine time sharing and
hierarchical modulation together and show how this scheme can improve the
performance in terms of available rate. We present the gain on a simple channel
modeling the broadcasting area of a satellite. Our work is applied to the
DVB-SH standard, which considers hierarchical modulation as an optional
feature.

ABSTRACT_BEGIN
  We propose an integrated architecture for smart grids, that supports data
aggregation and access control. Data can be aggregated by home area network,
building area network and neighboring area network in such a way that the
privacy of customers is protected. We use homomorphic encryption technique to
achieve this. The consumer data that is collected is sent to the substations
where it is monitored by remote terminal units (RTU). The proposed access
control mechanism gives selective access to consumer data stored in data
repositories and used by different smart grid users. Users can be maintenance
units, utility centers, pricing estimator units or analyzing and prediction
groups. We solve this problem of access control using cryptographic technique
of attribute-based encryption. RTUs and users have attributes and cryptographic
keys distributed by several key distribution centers (KDC). RTUs send data
encrypted under a set of attributes. Users can decrypt information provided
they have valid attributes. The access control scheme is distributed in nature
and does not rely on a single KDC to distribute keys. Bobba \emph{et al.}
\cite{BKAA09} proposed an access control scheme, which relies on a centralized
KDC and is thus prone to single-point failure. The other requirement is that
the KDC has to be online, during data transfer which is not required in our
scheme. Our access control scheme is collusion resistant, meaning that users
cannot collude and gain access to data, when they are not authorized to access.
We theoretically analyze our schemes and show that the computation overheads
are low enough to be carried out in smart grids. To the best of our knowledge,
ours is the first work on smart grids, which integrates these two important
security components (privacy preserving data aggregation and access control)
and presents an overall security architecture in smart grids.

ABSTRACT_BEGIN
  In this paper, a simple network management architecture for supporting both
QoS requirements and organization network management policies is purposed. By
grouping the traffic flows according to the QoS requirements or certain network
management policies, the network resources are effectively controlled. The
purposed architecture is easy to deploy; the gateway is the only equipment that
needs installation, leaving the rest of the system untouched. The architecture
has not significantly degraded the overall system utilization when applying it
to the outgoing bound of the gateway. The architecture can also be implemented
on the wireless LAN at the access point because the architecture is designed in
such the way that it is independent to both the lower and upper protocol
layers.

ABSTRACT_BEGIN
  Cognitive Radio Networks allow unlicensed users to opportunistically access
the licensed spectrum without causing disruptive interference to the primary
users (PUs). One of the main challenges in CRNs is the ability to detect PU
transmissions. Recent works have suggested the use of secondary user (SU)
cooperation over individual sensing to improve sensing accuracy. In this paper,
we consider a CRN consisting of a single PU and multiple SUs to study the
problem of maximizing the total expected system throughput. We propose a
Bayesian decision rule based algorithm to solve the problem optimally with a
constant time complexity. To prioritize PU transmissions, we re-formulate the
throughput maximization problem by adding a constraint on the PU throughput.
The constrained optimization problem is shown to be NP-hard and solved via a
greedy algorithm with pseudo-polynomial time complexity that achieves strictly
greater than 1/2 of the optimal solution. We also investigate the case for
which a constraint is put on the sensing time overhead, which limits the number
of SUs that can participate in cooperative sensing. We reveal that the system
throughput is monotonic over the number of SUs chosen for sensing. We
illustrate the efficacy of the performance of our algorithms via a numerical
investigation.

ABSTRACT_BEGIN
  Stochastic network calculus is the probabilistic version of the network
calculus, which uses envelopes to perform probabilistic analysis of queueing
networks. The accuracy of probabilistic end-to-end delay or backlog bounds
computed using network calculus has always been a concern. In this paper, we
propose novel end-to-end probabilistic bounds based on demisubmartingale
inequalities which improve the existing bounds for the tandem networks of
GI/GI/1 queues. In particular, we show that reasonably accurate bounds are
achieved by comparing the new bounds with the existing results for a network of
M/M/1 queues.

ABSTRACT_BEGIN
  A VoIP based call has stringent QoS requirements with respect to delay,
jitter, loss, MOS and R-Factor. Various QoS mechanisms implemented to satisfy
these requirements must be adaptive under diverse network scenarios and applied
in proper sequence, otherwise they may conflict with each other. The objective
of this paper is to address the problem of adaptive QoS maintenance and
sequential execution of available QoS implementation mechanisms with respect to
VoIP under varying network conditions. In this paper, we generalize this
problem as state-space problem and solve it. Firstly, we map the problem of QoS
optimization into state-space domain and apply incremental heuristic search. We
implement the proposed algorithm under various network and user scenarios in a
VoIP test-bed for QoS enhancement. Then learning strategy is implemented for
refinement of knowledge base to improve the performance of call quality over
time. Finally, we discuss the advantages and uniqueness of our approach.

ABSTRACT_BEGIN
  Security in mobile AD HOC network is a big challenge as it has no centralized
authority which can supervise the individual nodes operating in the network.
The attacks can come from both inside the network and from the outside. We are
trying to classify the existing attacks into two broad categories: DATA traffic
attacks and CONTROL traffic attacks. We will also be discussing the presently
proposed methods of mitigating those attacks.

ABSTRACT_BEGIN
  The main objective of this paper is to reduce the number of sensor nodes by
estimating a trade off between data accuracy and energy consumption for
selecting nodes in probabilistic approach in distributed networks. Design
Procedure/Approach: Observed data are highly correlated among sensor nodes in
the spatial domain due to deployment of high density of sensor nodes. These
sensor nodes form non-overlapping distributed clusters due to high data
correlation among them. We develop a probabilistic model for each distributed
cluster to perform data accuracy and energy consumption model in the network.
Finally we find a trade off between data accuracy and energy consumption model
to select an optimal number of sensor nodes in each distributed cluster. We
also compare the performance for our data accuracy estimation model with
information accuracy model for each distributed cluster in the network.
Practical Implementation: Measuring temperature in physical environment and
measuring moisture content in agricultural field. Inventive /Novel Idea:
Optimal node selection in probabilistic approach using the trade of between
data accuracy and energy consumption in cluster-based distributed network.

ABSTRACT_BEGIN
  Delay-capacity tradeoffs for mobile networks have been analyzed through a
number of research work. However, L\'{e}vy mobility known to closely capture
human movement patterns has not been adopted in such work. Understanding the
delay-capacity tradeoff for a network with L\'{e}vy mobility can provide
important insights into understanding the performance of real mobile networks
governed by human mobility. This paper analytically derives an important point
in the delay-capacity tradeoff for L\'{e}vy mobility, known as the critical
delay. The critical delay is the minimum delay required to achieve greater
throughput than what conventional static networks can possibly achieve (i.e.,
$O(1/\sqrt{n})$ per node in a network with $n$ nodes). The L\'{e}vy mobility
includes L\'{e}vy flight and L\'{e}vy walk whose step size distributions
parametrized by $\alpha \in (0,2]$ are both heavy-tailed while their times
taken for the same step size are different. Our proposed technique involves (i)
analyzing the joint spatio-temporal probability density function of a
time-varying location of a node for L\'{e}vy flight and (ii) characterizing an
embedded Markov process in L\'{e}vy walk which is a semi-Markov process. The
results indicate that in L\'{e}vy walk, there is a phase transition such that
for $\alpha \in (0,1)$, the critical delay is always $\Theta (n^{1/2})$ and for
$\alpha \in [1,2]$ it is $\Theta(n^{\frac{\alpha}{2}})$. In contrast, L\'{e}vy
flight has the critical delay $\Theta(n^{\frac{\alpha}{2}})$ for
$\alpha\in(0,2]$.

ABSTRACT_BEGIN
  It is highly desirable and challenging for a wireless ad hoc network to have
self-organization properties in order to achieve network wide characteristics.
Studies have shown that Small World properties, primarily low average path
length and high clustering coefficient, are desired properties for networks in
general. However, due to the spatial nature of the wireless networks, achieving
small world properties remains highly challenging. Studies also show that,
wireless ad hoc networks with small world properties show a degree distribution
that lies between geometric and power law. In this paper, we show that in a
wireless ad hoc network with non-uniform node density with only local
information, we can significantly reduce the average path length and retain the
clustering coefficient. To achieve our goal, our algorithm first identifies
logical regions using Lateral Inhibition technique, then identifies the nodes
that beamform and finally the beam properties using Flocking. We use Lateral
Inhibition and Flocking because they enable us to use local state information
as opposed to other techniques. We support our work with simulation results and
analysis, which show that a reduction of up to 40% can be achieved for a
high-density network. We also show the effect of hopcount used to create
regions on average path length, clustering coefficient and connectivity.

ABSTRACT_BEGIN
  The technology related to networking moves wired connection to wireless
connection.The basic problem concern in the wireless domain, random packet loss
for the end to end connection. In this paper we show the performance and the
impact of the packet loss and delay, by the bit error rate throughput etc with
respect to the real world scenario vehicular ad hoc network in 3-dimension
space (VANET in 3D). Over the years software development has responded to the
increasing growth of wireless connectivity in developing network enabled
software. In this paper we consider the real world physical problem in three
dimensional wireless domain and map the problem to analytical problem . In this
paper we simulate that analytic problem with respect to real world scenario by
using enhanced antenna position system (EAPS) mounted over the mobile node in
3D space. In this paper we convert the real world problem into lab oriented
problem by using the EAPS -system and shown the performance in wireless domain
in 3 dimensional space.

ABSTRACT_BEGIN
  Dynamic spectrum access in cognitive radio networks can greatly improve the
spectrum utilization efficiency. Nevertheless, interference may be introduced
to the Primary User (PU) when the Secondary Users (SUs) dynamically utilize the
PU's licensed channels. If the SUs can be synchronous with the PU's time slots,
the interference is mainly due to their imperfect spectrum sensing of the
primary channel. However, if the SUs have no knowledge about the PU's exact
communication mechanism, additional interference may occur. In this paper, we
propose a dynamic spectrum access protocol for the SUs confronting with unknown
primary behavior and study the interference caused by their dynamic access.
Through analyzing the SUs' dynamic behavior in the primary channel which is
modeled as an ON-OFF process, we prove that the SUs' communication behavior is
a renewal process. Based on the Renewal Theory, we quantify the interference
caused by the SUs and derive the corresponding close-form expressions. With the
interference analysis, we study how to optimize the SUs' performance under the
constraints of the PU's communication quality of service (QoS) and the
secondary network's stability. Finally, simulation results are shown to verify
the effectiveness of our analysis.

ABSTRACT_BEGIN
  Collaborative spectrum sensing has been recognized as a promising approach to
improve the sensing performance via exploiting the spatial diversity of the
secondary users. In this study, a new selfishness issue is identified, that
selfish users sense no spectrum in collaborative sensing. For easier
presentation, it's denoted as entropy selfishness. This selfish behavior is
difficult to distinguish, making existing detection based incentive schemes
fail to work. To thwart entropy selfishness in distributed collaborative
sensing, we propose YouSense, a One-Time Pad (OTP) based incentive design that
could naturally isolate entropy selfish users from the honest users without
selfish node detection. The basic idea of YouSense is to construct a trapdoor
one-time pad for each sensing report by combining the original report and a
random key. Such a one-time pad based encryption could prevent entropy selfish
users from accessing the original sensing report while enabling the honest
users to recover the report. Different from traditional cryptography based OTP
which requires the key delivery, YouSense allows an honest user to recover the
pad (or key) by exploiting a unique characteristic of collaborative sensing
that different secondary users share some common observations on the same radio
spectrum. We further extend YouSense to improve the recovery successful rate by
reducing the cardinality of set of the possible pads. By extensive USRP based
experiments, we show that YouSense can successfully thwart entropy selfishness
with low system overhead.

ABSTRACT_BEGIN
  Wireless sensor networks consisting of great number of cheap and tiny sensor
nodes which are used for military environment controlling, natural events
recording, traffic monitoring, robot navigation, and etc. Such a networks
encounter with various types of challenges like energy consumption, routing,
coverage, reliability. The most significant types of these problems are
coverage that originated from the nodes energy consumption constrained. In
order to dominate this problem different kinds of methods has been presented
where the majority of them based on theoretical methods and used unbalanced and
calculated distributions. In all of the proposed methods a large numbers of
nodes are used. In this paper our attempt is based on using a few numbers of
sensors in order to cover the vast area of environment. We proposed an
algorithm that divides the desired environment to several areas and in each of
these areas by using the genetic algorithm improve the coverage. The proposed
method is simulated in MATLAB software and the obtained results are compared
with the existing algorithms. Results show that the presented algorithm has a
substantial coverage in compare with its previous counterparts.

ABSTRACT_BEGIN
  Relaying is a promising enhancement to current radio access networks. Relay
enhanced LTE-Advanced networks are expected to fulfill the demanding coverage
and capacity requirements in a cost-efficient way. However, due to low transmit
power, the coverage areas of the relay nodes will be small. Therefore, the
performance of relay deployments may be limited by load imbalances. In this
study, we present a practical solution for this problem by introducing a bias
to cell selection and handover decisions along with a reduction in eNB transmit
power. This method results in an extension of the relay cells and an
appropriate load balance can then be achieved. Moreover, it is shown that a
proper power control setting is necessary in the uplink and that power control
optimization can further enhance the system performance. Comprehensive system
level simulations confirm that the proposed solution yields significant user
throughput gains both in the uplink and the downlink.

ABSTRACT_BEGIN
  ulticasting is an important communication paradigm for enabling the
dissemination of information selectively. This paper considers the problem of
optimal secure multicasting in a communication network captured through a graph
(optimal is in an interesting sense) and provides a doubly optimal solution
using results from source coding. It is realized that the solution leads to
optimal design (in a well defined optimality sense) of organizational
hierarchies captured through a graph. In this effort two novel concepts :
prefix free path, graph entropy are introduced. Some results of graph entropy
are provided. Also some results on Kraft inequality are discussed. As an
application Hierarchical Hybrid Communication Network is utilized as a model of
structured Mobile Adhoc network for utility in Disaster Management. Several new
research problems that naturally emanate from this research are summarized.

ABSTRACT_BEGIN
  In the years, routing protocols in wireless sensor networks (WSN) have been
substantially investigated by researches. Most state-of-the-art surveys have
focused on reviewing of wireless sensor network .In this paper we review the
existing secure geographic routing protocols for wireless sensor network (WSN)
and also provide a qualitative comparison of them.

ABSTRACT_BEGIN
  In this paper, we study the performance of greedy scheduling in multihop
wireless networks, where the objective is aggregate utility maximization.
Following standard approaches, we consider the dual of the original
optimization problem. The dual can be solved optimally, only with the knowledge
of the maximal independent sets in the network. But computation of maximal
independent sets is known to be NP-hard. Motivated by this, we propose a
distributed greedy heuristic to address the problem of link scheduling. We
evaluate the effect of the distributed greedy heuristic on aggregate utility
maximization in detail, for the case of an arbitrary graph. We provide some
insights into the factors affecting aggregate utility maximization in a
network, by providing bounds on the same. We give simulation results for the
approximate aggregate utility maximization achieved under distributed
implementation of the greedy heuristic and find them close to the maximum
aggregate utility obtained using optimal scheduling.

ABSTRACT_BEGIN
  Static analysis (aka offline analysis) of a model of an IP network is useful
for understanding, debugging, and verifying packet flow properties of the
network. There have been static analysis approaches proposed in the literature
for networks based on model checking as well as graph reachability. Abstract
interpretation is a method that has typically been applied to static analysis
of programs. We propose a new, abstract-interpretation based approach for
analysis of networks. We formalize our approach, mention its correctness
guarantee, and demonstrate its flexibility in addressing multiple
network-analysis problems that have been previously solved via tailor-made
approaches. Finally, we investigate an application of our analysis to a novel
problem -- inferring a high-level policy for the network -- which has been
addressed in the past only in the restricted single-router setting.

ABSTRACT_BEGIN
  This article proposes a stochastic model to obtain the end-to-end delay law
between two nodes of a Delay Tolerant Network (DTN). We focus on the commonly
used Binary Spray and Wait (BSW) routing protocol and propose a model that can
be applied to homogeneous or heterogeneous networks (i.e. when the
inter-contact law parameter takes one or several values). To the best of our
knowledge, this is the first model allowing to estimate the delay distribution
of Binary Spray and Wait DTN protocol in heterogeneous networks. We first
detail the model and propose a set of simulations to validate the theoretical
results.

ABSTRACT_BEGIN
  Controlling the global statuses of a network by its local dynamic parameters
is an important issue, and it is difficult to obtain the direct solution for.
The transformation method, which is originally used to control physical field
by designing material parameters, is proposed to obtain the necessary local
dynamic parameters when the global statuses of a network system are prescribed
in a space. The feasibility of this transformation method is demonstrated and
verified by two examples (a communication field cloak and a communication field
bender) in the network system. It is shown that the global system state can be
controlled by adjusting the local nodes dynamics with the transformation
method. Simulation results also show that the transformation method provides a
powerful, intuitive and direct way for the global statuses controlling of
network systems.

ABSTRACT_BEGIN
  More and more critical Wireless Sensor Networks (WSNs) applications are
emerging. Those applications need reliability and respect of time constraints.
The underlying mechanisms such as MAC and routing must handle such
requirements. Our approach to the time constraint problem is to bound the
hop-count between a node and the sink and the time it takes to do a hop so the
end-to-end delay can be bounded and the communications are thus real-time. For
reliability purpose we propose to select forwarder nodes depending on how they
are connected in the direction of the sink. In order to be able to do so we
need a coordinate (or a metric) that gives information on hop-count, that
allows to strongly differentiate nodes and gives information on the
connectivity of each node keeping in mind the intrinsic constraints of WSWs
such as energy consumption, autonomy, etc. Due to the efficiency and
scalability of greedy routing in WSNs and the financial cost of GPS chips,
Virtual Coordinate Systems (VCSs) for WSNs have been proposed. A category of
VCSs is based on the hop-count from the sink, this scheme leads to many nodes
having the same coordinate. The main advantage of this system is that the hops
number of a packet from a source to the sink is known. Nevertheless, it does
not allow to differentiate the nodes with the same hop-count. In this report we
propose a novel hop-count-based VCS which aims at classifying the nodes having
the same hop-count depending on their connectivity and at differentiating nodes
in a 2-hop neighborhood. Those properties make the coordinates, which also can
be viewed as a local identifier, a very powerful metric which can be used in
WSNs mechanisms.

ABSTRACT_BEGIN
  Now days, interests in the application of Wireless Body Area Network (WBAN)
have grown considerably. A number of tiny wireless sensors, strategically
placed on the human body, create a wireless body area network that can monitor
various vital signs, providing real-time feedback to the user and medical
personnel. This communication needs to be energy efficient and highly reliable
while keeping delays low. In this paper we present hardware and software
architecture for BAN and also we offer reliable communication and data
aggregation.

ABSTRACT_BEGIN
  XML and XML Schema are widely used in different domains for the definition of
standards that enhance the interoperability between parts exchanging
information through the Internet. The size and complexity of some standards,
and their associated schemas, have been growing with time as new use case
scenarios and data models are added to them. The common approach to deal with
the complexity of producing XML processing code based on these schemas is the
use of XML data binding generators. Unfortunately, these tools do not always
produce code that ?ts the limitations of resource-constrained devices, such as
mobile phones, in the presence of large schemas. In this paper we present
Instance-based XML data binding, an approach to produce compact
application-specific XML processing code for mobile devices. The approach
utilises information extracted from a set of XML documents about how the
application make use of the schemas.

ABSTRACT_BEGIN
  Formal analysis techniques are widely used today in order to verify and
analyze communication protocols. In this work, we launch a quantitative
verification analysis for the low- cost Radio Frequency Identification (RFID)
protocol proposed by Song and Mitchell. The analysis exploits a Discrete-Time
Markov Chain (DTMC) using the well-known PRISM model checker. We have managed
to represent up to 100 RFID tags communicating with a reader and quantify each
RFID session according to the protocol's computation and transmission cost
requirements. As a consequence, not only does the proposed analysis provide
quantitative verification results, but also it constitutes a methodology for
RFID designers who want to validate their products under specific cost
requirements.

ABSTRACT_BEGIN
  Smart grid, regarded as the next generation power grid, uses two-way flows of
electricity and information to create a widely distributed automated energy
delivery network. In this work we present our vision on smart grid from the
perspective of wireless communications and networking technologies. We present
wireless communication and networking paradigms for four typical scenarios in
the future smart grid and also point out the research challenges of the
wireless communication and networking technologies used in smart grid

ABSTRACT_BEGIN
  Subscriber satisfaction and maximum radio resource utilization are the
pivotal criteria in communication system design. In multi-Carrier CDMA system,
different paging algorithms are used for locating user within the shortest
possible time and best possible utilization of radio resources. Different
paging algorithms underscored different techniques based on the different
purposes. However, low servicing time of sequential search and better
utilization of radio resources of concurrent search can be utilized
simultaneously by swapping of the algorithms. In this paper, intelligent
mechanism has been developed for dynamic algorithm assignment basing on
time-varying traffic demand, which is predicted by radial basis neural network;
and its performance has been analyzed are based on prediction efficiency of
different types of data. High prediction efficiency is observed with a good
correlation coefficient (0.99) and subsequently better performance is achieved
by dynamic paging algorithm assignment. This claim is substantiated by the
result of proposed intelligent paging strategy.

ABSTRACT_BEGIN
  Modern radio communication is faced with a problem about how to distribute
restricted frequency to users in a certain space. Since our task is to minimize
the number of repeaters, a natural idea is enlarging coverage area. However,
coverage has restrictions. First, service area has to be divided economically
as repeater's coverage is limited. In this paper, our fundamental method is to
adopt seamless cellular network division. Second, underlying physics content in
frequency distribution problem is interference between two close frequencies.
Consequently, we choose a proper frequency width of 0.1MHz and a relevantly
reliable setting to apply one frequency several times.
  We make a few general assumptions to simplify real situation. For instance,
immobile users yield to homogenous distribution; repeaters can receive and
transmit information in any given frequency in duplex operation; coverage is
mainly decided by antenna height.
  Two models are built up to solve 1000 users and 10000 users situations
respectively. In order to utilize restricted frequency and PL code, three
stratified terms - "cell", "cluster", "group" - are introduced to describe the
models in detail. Under our analysis, 91 repeaters for 1000 users and 469
repeaters for 10000 users are viable results.
  Next, to test stability and sensitivity of models, we give total
consideration to the variation of sum of users, antenna height, and frequency
width and service radius. Evaluation about models is offered qualitatively.
Finally, two practical cases are put forward to gain a partial knowledge of
mountainous area. The brief method in dealing with mountains is classified
discussion in two ideal conditions. It may provide some constructive
suggestions to avoid shortcomings or take proper measures in similar locations.

ABSTRACT_BEGIN
  We present an implementation of Multipath TCP (MPTCP) under the NS-3 open
source network simulator. MPTCP is a promising extension of TCP currently
considered by the recent eponymous IETF working group, with the objective of
improving the performance of TCP, especially its robustness to variable network
conditions. We describe this new protocol, its main functions and our
implementation in NS-3. Besides this implementation compliant to the current
versions of the IETF drafts, we have also added and compared various packet
reordering mechanisms. We indeed notice that such mechanisms highly improve the
performance of MPTCP. We believe that our implementation could be useful for
future works in MPTCP performance evaluation, especially to compare packet
reordering algorithms or coupling congestion control mechanisms between
subfows.

ABSTRACT_BEGIN
  The concepts of MIMO MC-CDMA are not new but the new technologies to improve
their functioning are an emerging area of research. In general, most mobile
communication systems transmit bits of information in the radio space to the
receiver. The radio channels in mobile radio systems are usually multipath
fading channels, which cause inter-symbol interference (ISI) in the received
signal. To remove ISI from the signal, there is a need of strong equalizer. In
this thesis we have focused on simulating the MIMO MC-CDMA systems in MATLAB
and designed the channel estimation for them.

ABSTRACT_BEGIN
  In this paper, the performance of high speed optical fiber based network is
analysed by using dispersion compensating module (DCM). The optimal operating
condition of the DCM is obtained by considering dispersion management
configurations for the symmetrical system i.e Pre-compensation &
Post-compensation. The dispersion compensating fiber (DCF) is tested for a
single span, single channel system operating at a speed of 10 Gb/s with a
transmitting wavelength of 1550 nm, over 120 km single mode fibre by using the
compensating fiber for 24 km,30km and 35Km. So far, most of the investigations
for single mode fiber (SMF) transmission at high amplifier spacings in the
order of 90 km to 120 km is focused on conventional Non Return to Zero(NRZ)
format. The simulation results are validated by analysing the Q-factor and Bit
error rate (BER) in the numerical simulator OptSim.

ABSTRACT_BEGIN
  High Peak to Average Power Ratio (PAPR) of the transmitted signal is a
serious problem in multicarrier modulation systems. In this paper a new
technique for reduction in PAPR of the Multicarrier Code Division Multiple
Access (MC CDMA) signals based on combining the Discrete Transform either
Discrete Cosine Transform (DCT) or multi-resolution Discrete Wavelet Transform
(DWT) with companding is proposed. It is analyzed and implemented using MATLAB.
Simulation results of reduction in PAPR and power Spectral Density (PSD) of the
MC CDMA with companding and without companding are compared with the MC CDMA
with DCT and companding, DWT and companding systems. The new technique proposed
is to make use of multi-resolution DWT in combination with companding in order
to achieve a very substantial reduction in PAPR of the MC CDMA signal

ABSTRACT_BEGIN
  Cooperative diversity is a technique in which various radio terminals relay
signals for each other. Cooperative diversity results when cooperative
communications is used primarily to leverage the spatial diversity available
among distributed radios. In this paper different cooperative diversity schemes
and their applications in various wireless networks are discussed. In this
paper the impact of cooperative diversity on the energy consumption and
lifetime of sensor network and the impact of cooperation in cognitive radio are
discussed. Here, user scheduling and radio resource allocation techniques are
also discussed which are developed in order to efficiently integrate various
cooperative diversity schemes for the emerging IEEE 802.16j based systems.

ABSTRACT_BEGIN
  Segment retransmissions are an essential tool in assuring reliable end-to-end
communication in the Internet. Their crucial role in TCP design and operation
has been studied extensively, in particular with respect to identifying
non-conformant, buggy, or underperforming behaviour. However, TCP segment
retransmissions are often overlooked when examining and analyzing large traffic
traces. In fact, some have come to believe that retransmissions are a rare
oddity, characteristically associated with faulty network paths, which,
typically, tend to disappear as networking technology advances and link
capacities grow. We find that this may be far from the reality experienced by
TCP flows. We quantify aggregate TCP segment retransmission rates using
publicly available network traces from six passive monitoring points attached
to the egress gateways at large sites. In virtually half of the traces examined
we observed aggregate TCP retransmission rates exceeding 1%, and of these,
about half again had retransmission rates exceeding 2%. Even for sites with low
utilization and high capacity gateway links, retransmission rates of 1%, and
sometimes higher, were not uncommon. Our results complement, extend and bring
up to date partial and incomplete results in previous work, and show that TCP
retransmissions continue to constitute a non-negligible percentage of the
overall traffic, despite significant advances across the board in
telecommunications technologies and network protocols. The results presented
are pertinent to end-to-end protocol designers and evaluators as they provide a
range of "realistic" scenarios under which, and a "marker" against which,
simulation studies can be configured and calibrated, and future protocols
evaluated.

ABSTRACT_BEGIN
  In this paper, we investigate the performance of bidirectional relay
selection using amplify-and-forward protocol with imperfect channel state
information, i.e., delay effect and channel estimation error. The asymptotic
expression of end-to-end SER in high SNR regime is derived in a closed form,
which indicates that the delay effect causes the loss of both coding gain and
diversity order, while the channel estimation error merely affects the coding
gain. Finally, analytical results are verified by Monte-Carlo simulations.

ABSTRACT_BEGIN
  Typical applications of the mobile ad-hoc network, MANET, are in disaster
recovery operations which have to respect time constraint needs. Since MANET is
affected by limited resources such as power constraints, it is a challenge to
respect the deadline of a real-time data. This paper proposes the Energy and
Delay aware based on Dynamic Source Routing protocol, ED-DSR. ED-DSR
efficiently utilizes the network resources such as the intermediate mobile
nodes energy and load. It ensures both timeliness and energy efficiency by
avoiding low-power and overloaded intermediate mobile nodes. Through
simulations, we compare our proposed routing protocol with the basic routing
protocol Dynamic Source Routing, DSR. Weighting factors are introduced to
improve the route selection. Simulation results, using the NS-2 simulator, show
that the proposed protocol prolongs the network lifetime (up to 66%), increases
the volume of packets delivered while meeting the data flows real-time
constraints and shortens the endto- end delay.

ABSTRACT_BEGIN
  In this paper, we dynamically select the transmission rate and design
wireless network coding to improve the quality of services such as delay for
time critical applications. With low transmission rate, and hence longer
transmission range, more packets may be encoded together, which increases the
coding opportunity. However, low transmission rate may incur extra transmission
delay, which is intolerable for time critical applications. We design a novel
joint rate selection and wireless network coding (RSNC) scheme with delay
constraint, so as to minimize the total number of packets that miss their
deadlines at the destination nodes. We prove that the proposed problem is
NPhard, and propose a novel graph model and transmission metric which consider
both the heterogenous transmission rates and the packet deadline constraints
during the graph construction. Using the graph model, we mathematically
formulate the problem and design an efficient algorithm to determine the
transmission rate and coding strategy for each transmission. Finally,
simulation results demonstrate the superiority of the RSNC scheme.

ABSTRACT_BEGIN
  In this paper the variation principles from theoretical physics is considered
that would describe the process of routing in computer networks. The total
traffic which is currently served on all hops of the route has been chosen as
the quantity to minimize. Universal metric function has been found for dynamic
routing taking into account the packet loss effect. An attempt to derive the
metric of the most popular dynamic routing protocols such as RIP, OSPF, EIGRP
from universal metric was made.

ABSTRACT_BEGIN
  Mobile browser is known to be slow because of the bottleneck in resource
loading. Client-only solutions to improve resource loading are attractive
because they are immediately deployable, scalable, and secure. We present the
first publicly known treatment of client-only solutions to understand how much
they can improve mobile browser speed without infrastructure support.
Leveraging an unprecedented set of web usage data collected from 24 iPhone
users continuously over one year, we examine the three fundamental, orthogonal
approaches a client-only solution can take: caching, prefetching, and
speculative loading, which is first proposed and studied in this work.
Speculative loading predicts and speculatively loads the subresources needed to
open a web page once its URL is given. We show that while caching and
prefetching are highly limited for mobile browsing, speculative loading can be
significantly more effective. Empirically, we show that client-only solutions
can improve the browser speed by about 1.4 second on average for web sites
visited by the 24 iPhone users. We also report the design, realization, and
evaluation of speculative loading in a WebKit-based browser called Tempo. On
average, Tempo can reduce browser delay by 1 second (~20%).

ABSTRACT_BEGIN
  ISPs are increasingly selling "tiered" contracts, which offer Internet
connectivity to wholesale customers in bundles, at rates based on the cost of
the links that the traffic in the bundle is traversing. Although providers have
already begun to implement and deploy tiered pricing contracts, little is known
about how such pricing affects ISPs and their customers. While contracts that
sell connectivity on finer granularities improve market efficiency, they are
also more costly for ISPs to implement and more difficult for customers to
understand. In this work we present two contributions: (1) we develop a novel
way of mapping traffic and topology data to a demand and cost model; and (2) we
fit this model on three large real-world networks: an European transit ISP, a
content distribution network, and an academic research network, and run
counterfactuals to evaluate the effects of different pricing strategies on both
the ISP profit and the consumer surplus. We highlight three core findings.
First, ISPs gain most of the profits with only three or four pricing tiers and
likely have little incentive to increase granularity of pricing even further.
Second, we show that consumer surplus follows closely, if not precisely, the
increases in ISP profit with more pricing tiers. Finally, the common ISP
practice of structuring tiered contracts according to the cost of carrying the
traffic flows (e.g., offering a discount for traffic that is local) can be
suboptimal and that dividing contracts based on both traffic demand and the
cost of carrying it into only three or four tiers yields near-optimal profit
for the ISP.

ABSTRACT_BEGIN
  Given the rapid increase in traffic, greater demands have been put on
research in high-speed switching systems. Such systems have to simultaneously
meet several constraints, e.g., high throughput, low delay and low complexity.
This makes it challenging to design an efficient scheduling algorithm, and has
consequently drawn considerable research interest. However, previous results
either cannot provide a 100% throughput guarantee without a speedup, or require
a complex centralized scheduler. In this paper, we design a distributed 100%
throughput algorithm for crosspoint buffered switches, called DISQUO, with very
limited message passing. We prove that DISQUO can achieve 100% throughput for
any admissible Bernoulli traffic, with a low time complexity of O(1) per port
and a few bits message exchanging in every time slot. To the best of our
knowledge, it is the first distributed algorithm that can provide a 100%
throughput for a crosspoint buffered switch.

ABSTRACT_BEGIN
  A Multipath Transport Protocol for Future Internet

ABSTRACT_BEGIN
  Vehicular sensor network (VSN) is an emerging technology, which combines
wireless communication offered by vehicular ad hoc networks (VANET) with
sensing devices installed in vehicles. VSN creates a huge opportunity to extend
the road-side sensor infrastructure of existing traffic control systems. The
efficient use of the wireless communication medium is one of the basic issues
in VSN applications development. This paper introduces a novel method of
selective data collection for traffic control applications, which provides a
significant reduction in data amounts transmitted through VSN. The underlying
idea is to detect the necessity of data transfers on the basis of uncertainty
determination of the traffic control decisions. According to the proposed
approach, sensor data are transmitted from vehicles to the control node only at
selected time moments. Data collected in VSN are processed using on-line
traffic simulation technique, which enables traffic flow prediction,
performance evaluation of control strategies and uncertainty estimation. If
precision of the resulting information is insufficient, the optimal control
strategy cannot be derived without ambiguity. As a result the control decision
becomes uncertain and it is a signal informing that new traffic data from VSN
are necessary to provide more precise prediction and to reduce the uncertainty
of decision. The proposed method can be applied in traffic control systems of
different types e.g. traffic signals, variable speed limits, and dynamic route
guidance. The effectiveness of this method is illustrated in an experimental
study on traffic control at signalised intersection.

ABSTRACT_BEGIN
  Many works have studied the Internet topology, but few have investigated the
question of how it evolves over time. This paper focuses on the Internet
routing IP-level topology and proposes a first step towards realistic modeling
of its dynamics. We study periodic measurements of routing trees from a single
monitor to a fixed destination set and identify invariant properties of its
dynamics. We then propose a simple model for the underlying mechanisms of the
topology dynamics. Simulations show that it effectively captures the observed
behaviors, thus providing key insights of relevant mechanisms governing the
Internet routing dynamics.

ABSTRACT_BEGIN
  Our current world is revolutionned by the networks which are interconnecting
any machine to any other one. Nowadays equipments are plugged to the network by
the way of many different network adapters (Ethernet, wifi, GSM), and network
equipments are more and more interconnected to each other creating a highly
mesh network. Thus, redundant paths are created between any two endpoints
enabling multipath or the use of multiple paths to handle a communication. The
benefits of multipath are the enhancement of robustness against failure, and
minimizing communication cost. In the present work, we will talk about some
solutions of multipath applied to the transport level. We will expose the most
important problem facing multipath which is packet reordering, and we will also
expose results of our simulations.

ABSTRACT_BEGIN
  With the escalation of the IEEE 802.11 based wireless networks, voice over IP
and analogous applications are also used over wireless networks. Recently, the
wireless LAN systems are spaciously deployed for public Internet services. In
public wireless LAN systems, reliable user authentication and mobility support
are indispensable issues. When a mobile device budges out the range of one
access point (AP) and endeavor to connect to new AP, it performs handoff.
Contemporarily, PNC and SNC were proposed to propagate the MN context to the
entire neighboring AP's on the wireless network with the help of neighbor
graph. In this paper, we proposed a non-overlapping AP's caching scheme (NACS),
which propagates the mobile node context to those AP's which do not overlap
with the current AP. To capture the topology of non-overlapping AP's in the
wireless network, non-overlapping graph (NOG) is generated at each AP.
Simulation results shows that NACS reduces the signaling cost of propagating
the MN context to the neighbor AP's in the wireless network.

ABSTRACT_BEGIN
  We investigate the design of a broadcast system in order to maximise the
throughput. This task is usually challenging due to the channel variability.
Forty years ago, Cover introduced and compared two schemes: time sharing and
superposition coding. Even if the second scheme was proved to be optimal for
some channels, modern satellite communications systems such as DVB-SH and
DVB-S2 mainly rely on time sharing strategy to optimize the throughput. They
consider hierarchical modulation, a practical implementation of superposition
coding, but only for unequal error protection or backward compatibility
purposes. We propose in this article to combine time sharing and hierarchical
modulation together and show how this scheme can improve the performance in
terms of available rate. We introduce the hierarchical 16-APSK to boost the
performance of the DVB-S2 standard. We also evaluate various strategies to
group the receivers in pairs when using hierarchical modulation. Finally, we
show in a realistic use case based on DVB-S2 that the combined scheme can
provide throughput gains greater than 10% compared to the best time sharing
strategy.

ABSTRACT_BEGIN
  Link failures in wide area networks are common and cause significant data
losses. Mesh-based protection schemes offer high capacity efficiency but they
are slow and require complex signaling. Additionally, real-time reconfiguration
of a cross-connect threatens their transmission integrity. On the other hand,
coding-based protection schemes are proactive. Therefore, they have higher
restoration speed, lower signaling complexity, and higher transmission
integrity. This paper introduces a coding-based protection scheme, named Coded
Path Protection (CPP). In CPP, a backup copy of the primary data is encoded
with other data streams, resulting in capacity savings. This paper presents an
optimal and simple capacity placement and coding group formation algorithm. The
algorithm converts the sharing structure of any solution of a Shared Path
Protection (SPP) technique into a coding structure with minimum extra capacity.
We conducted quantitative and qualitative comparisons of our technique with the
SPP and, another technique, known as p-cycle protection. Simulation results
confirm that the CPP is significantly faster than the SPP and p-cycle
techniques. CPP incurs marginal extra capacity on top of SPP. Its capacity
efficiency is lower than the p-cycle technique for dense networks but can be
higher for sparse networks. In addition, unlike p-cycle protection, CPP is
inherently suitable for the wavelength continuity constraint in optical
networks.

ABSTRACT_BEGIN
  We address a major flaw in the abovementioned paper, which proposes to
calculate effective capacity of random channels by the use of central limit
theorem. We analytically show that the authors are incorrect in finding the
effective capacity by first taking the limit of cumulative random process
rather than taking the limit of moment generating function of the same process.
We later quantify our results over a correlated ON-OFF process.

ABSTRACT_BEGIN
  The paper presents a methodology of transmitting voice in SMS (Short Message
Service) over GSM network. Usually SMS contents are text based and limited to
140 bytes. It supports national and international roaming, but also supported
by other telecommunication such as TDMA (Time Division Multiple Access), CDMA
(Code Division Multiple Access) as well. It can sent/ receive simultaneously
with other services. Such features make it favorable for this methodology. For
this an application is developed using J2ME platform which is supported by all
mobile phones in the world. This algorithm's test is conducted on N95 having
Symbian Operating System (OS).

ABSTRACT_BEGIN
  A mobile Ad-hoc network (MANET) is a dynamic multi hop wireless network
established by a group of nodes in which there is no central administration.
Due to mobility of nodes and dynamic network topology, the routing is one of
the most important challenges in ad-hoc networks. Several routing algorithms
for MANETs have been proposed by the researchers which have been classified
into various categories, however, the most prominent categories are proactive,
reactive and hybrid. The performance comparison of routing protocols for MANETs
has been presented by other researcher also, however, none of these works
considers proactive, reactive and hybrid protocols together. In this paper, the
performance of proactive (DSDV), reactive (DSR and AODV) and hybrid (ZRP)
routing protocols has been compared. The performance differentials are analyzed
on the basis of throughput, average delay, routing overhead and number of
packets dropped with a variation of number of nodes, pause time and mobility.

ABSTRACT_BEGIN
  As wireless access technologies grow rapidly, the recent studies have focused
on granting mobile users the ability of roaming across different wireless
networks in a seamless manner thus offering seamless mobility. The different
characteristics of each wireless technology with regards to QoS brought many
challenges for provisioning the continuous services (audio/video streaming) in
a seamless way. In this paper, we intend to review the existing context-aware
methods which offered solutions for service continuity. We looked at the types
of context information used in each solution. Through this study, it is clear
that context awareness plays a significant role in handover process in order to
satisfy users demanding seamless services. Therefore, the goal of this paper is
to compare the existing methods grouped as general, IMS based, and WLAN/WiMAX
solutions in terms of several criteria, such as interworking architecture,
service continuity, and QoS provisioning.

ABSTRACT_BEGIN
  In this paper, we study a new approach to model the overall Energy
Consumption (EC) in Wireless Sensor Networks (WSN). First, we extract
parameters involving in the EC of WSNs. The dependency between configuration
parameters and the average residual energy of a specific application is then
investigated. Our approach has three key steps: profiling, parameter reduction,
and modeling. In profiling, a sensor network simulator is re-run 800 times with
different values of the configuration parameters in order to profile the
average residual energy in nodes. In the parameter reduction, three statistical
analyses (p-value, linear and non-linear correlation) are applied to the
outcome of profiled experiments in order to separate the effective parameters
on WSN residual energy. Finally, linear regression is used to model the
relation between the chosen effective parameters and the residual energy. The
evaluation based on running the simulator for another 200 times with different
values of the effective parameters shows that the model can predict the
residual energy of nodes in WSN with average error of less than 13%.

ABSTRACT_BEGIN
  Minimizing the energy consumption of a wireless sensor network application is
crucial for effective realization of the intended application in terms of cost,
lifetime, and functionality. However, the minimizing task is hardly possible as
no overall energy cost function is available for optimization. Optimizing a
specific component of the total energy cost does not help in reducing the total
energy cost as this reduction may be negated by an increase in the energy
consumption of other components of the application. Recently we proposed
Hierarchy Energy Driven Architecture as a robust architecture that takes into
account all principal energy constituents of wireless sensor network
applications. Based on the proposed architecture, this paper presents a single
overall model and proposes a feasible formulation to express the overall energy
consumption of a generic wireless sensor network application in terms of its
energy constituents. The formulation offers a concrete expression for
evaluating the performance of a wireless sensor network application, optimizing
its constituent's operations, and designing more energy-efficient applications.
The paper also presents simulation results to demonstrate the feasibility of
our model and energy formulation

ABSTRACT_BEGIN
  Wireless mesh cubes are used to improve the channel signaling.

ABSTRACT_BEGIN
  In randomly deployed networks, such as sensor networks, an important problem
for each node is to discover its \textit{neighbor} nodes so that the
connectivity amongst nodes can be established. In this paper, we consider this
problem by incorporating the physical layer parameters in contrast to the most
of the previous work which assumed a collision channel. Specifically, the pilot
signals that nodes transmit are successfully decoded if the strength of the
received signal relative to the interference is sufficiently high. Thus, each
node must extract signal parameter information from the superposition of an
unknown number of received signals. This problem falls naturally in the purview
of random set theory (RST) which generalizes standard probability theory by
assigning \textit{sets}, rather than values, to random outcomes. The
contributions in the paper are twofold: first, we introduce the realistic
effect of physical layer considerations in the evaluation of the performance of
\textit{logical} discovery algorithms; such an introduction is necessary for
the accurate assessment of how an algorithm performs. Secondly, given the
\textit{double} uncertainty of the environment (that is, the lack of knowledge
of the number of neighbors along with the lack of knowledge of the individual
signal parameters), we adopt the viewpoint of RST and demonstrate its advantage
relative to classical matched filter detection method.

ABSTRACT_BEGIN
  In this paper, a family of ant colony algorithms called DAACA for data
aggregation has been presented which contains three phases: the initialization,
packet transmission and operations on pheromones. After initialization, each
node estimates the remaining energy and the amount of pheromones to compute the
probabilities used for dynamically selecting the next hop. After certain rounds
of transmissions, the pheromones adjustment is performed periodically, which
combines the advantages of both global and local pheromones adjustment for
evaporating or depositing pheromones. Four different pheromones adjustment
strategies are designed to achieve the global optimal network lifetime, namely
Basic-DAACA, ES-DAACA, MM-DAACA and ACS-DAACA. Compared with some other data
aggregation algorithms, DAACA shows higher superiority on average degree of
nodes, energy efficiency, prolonging the network lifetime, computation
complexity and success ratio of one hop transmission. At last we analyze the
characteristic of DAACA in the aspects of robustness, fault tolerance and
scalability.

ABSTRACT_BEGIN
  Many localization algorithms and systems have been developed by means of
wireless sensor networks for both indoor and outdoor environments. To achieve
higher localization accuracy, extra hardware equipments are utilized by most of
the existing localization solutions, which increase the cost and considerably
limit the location-based applications. The Internet of Things (IOT) integrates
many technologies, such as Internet, Zigbee, Bluetooth, infrared, WiFi, GPRS,
3G, etc, which can enable different ways to obtain the location information of
various objects. Location-based service is a primary service of the IOT, while
localization accuracy is a key issue. In this paper, a higher accuracy
localization scheme is proposed which can effectively satisfy diverse
requirements for many indoor and outdoor location services. The proposed scheme
composes of two phases: 1) partition phase, in which the target region is split
into small grids; 2) localization refinement phase, in which a higher accuracy
of localization can be obtained by applying an algorithm designed in the paper.
A trial system is set up to verify correctness of the proposed scheme and
furthermore to illustrate its feasibility and availability. The experimental
results show that the proposed scheme can improve the localization accuracy.

ABSTRACT_BEGIN
  One of the key issues in mobile communication is to find the current location
of mobile terminal (MT) to deliver the services, which is called as location
management (LM). Increasing users and diverse services demand for a
high-quality skeleton for LM. As an MT moves within a cellular network, it
registers its new location to the nearest base station (BS). When a call
arrives for an MT, the network searches the target MT in the area where it was
last registered. This paper presents comprehensive classification of existing
major LM schemes, their comparative study and factors influencing their
performance. Finally, guidelines for developing and rating a LM scheme are
suggested with the help of LPCIC rule, which is the main contribution of this
paper.

ABSTRACT_BEGIN
  In energy constrained wireless sensor networks, it is significant to make
full use of the limited energy and maximize the network lifetime even when
facing some unexpected situation. In this paper, all sensor nodes are grouped
into clusters, and for each cluster, it has a mobile cluster head to manage the
whole cluster. We consider an emergent situation that one of the mobile cluster
heads is broken down, and hence the whole cluster is consequently out of work.
An efficient approach is proposed for recovering the failure cluster by
selecting multiple static sensor nodes as the cluster heads to collect packets
and transmit them to the sink node. Improved simulated annealing algorithm is
utilized to achieve the uniform deployment of the cluster heads. The new
cluster heads are dynamically changed in order to keep balanced energy
consumption. Among the new cluster heads, packets are transmitted through
multi-hop forwarding path which is cost-lowest path found by Dijkstra's
algorithm. A balanced energy consumption model is provided to help find the
cost-lowest path and prolong the lifetime of the network. The forwarding path
is updated dynamically according to the cost of the path and residual energy of
the node in that path. The experimental results show that the failure cluster
is recovered and the lifetime of the cluster is prolonged.

ABSTRACT_BEGIN
  Congestions in wireless sensor networks (WSNs) could potentially cause packet
loss, throughput impairment and energy waste. To address this issue, a
hop-by-hop cross-layer congestion control scheme (HCCC) built on
contention-based MAC protocol is proposed in this paper. According to MAC-layer
channel information including buffer occupancy ratio and congestion degree of
local node, HCCC dynamically adjusts channel access priority in MAC layer and
data transmission rate of the node to tackle the problem of congestion.
Simulations have been conducted to compare HCCC against closely-related
existing schemes. The results show that HCCC exhibits considerable superiority
in terms of packets loss ratio, throughput and energy efficiency.

ABSTRACT_BEGIN
  With the increasing popularity of wireless networks, wireless local area
networks (WLANs) have attracted significant research interest, which play a
critical role in providing anywhere and anytime connectivity. For WLANs the
IEEE 802.11 standard is the most mature technology and has been widely adopted
for wireless networks. This paper analyzes real-time performance of the IEEE
802.11 standard that adopts the MAC protocol of Distributed Coordination
Function (DCF) operating in infrastructure mode. Extensive simulations have
been done to examine how the network performance in terms of realtime metrics
including effective data rate, latency and packet loss rate will be impacted by
some critical parameters (e.g. CWmin and packet payload). The results are
presented and analyzed. The analysis of simulation results can provide support
for parameter configuration and optimization of WLANs for realtime
applications.

ABSTRACT_BEGIN
  Cyber-physical systems (CPS) can be viewed as a new generation of systems
with integrated control, communication and computational capabilities. Like the
internet transformed how humans interact with one another, cyber-physical
systems will transform how people interact with the physical world. Currently,
the study of CPS is still in its infancy and there exist many research issues
and challenges ranging from electricity power, health care, transportation and
smart building etc. In this paper, an introduction of CPeSC3 (cyber physical
enhanced secured wireless sensor networks (WSNs) integrated cloud computing for
u-life care) architecture and its application to the health care monitoring and
decision support systems is given. The proposed CPeSC3 architecture is composed
of three main components, namely 1) communication core, 2) computation core,
and 3) resource scheduling and management core. Detailed analysis and
explanation are given for relevant models such as cloud computing, real time
scheduling and security models. Finally, a medical health care application
scenario is presented based on our practical test-bed which has been built for
3 years.

ABSTRACT_BEGIN
  IEEE 802.15.4 supports a Guaranteed Time Slot (GTS) allocation mechanism for
time-critical and delay-sensitive data transmissions in Wireless Personal Area
Networks (WPANs). However, the inflexible first-come-first-served GTS
allocation policy and the passive deallocation mechanism significantly reduce
network efficiency. In this paper, we propose an Adaptive and Real-Time GTS
Allocation Scheme (ART-GAS) to provide differentiated services for devices with
different priorities, which guarantees data transmissions for time-sensitive
and high-traffic devices. The bandwidth utilization in IEEE 802.15.4-based PAN
is improved. Simulation results show that our ART-GAS algorithm significantly
outperforms the existing GTS mechanism specified in IEEE 802.15.4.

ABSTRACT_BEGIN
  With the rapid development of new and innovative applications for mobile
devices like smartphones, advances in battery technology have not kept pace
with rapidly growing energy demands. Thus energy consumption has become a more
and more important issue of mobile devices. To meet the requirements of saving
energy, it is critical to monitor and analyze the energy consumption of
applications on smartphones. For this purpose, we develop a smart energy
monitoring system called SEMO for smartphones using Android operating system.
It can profile mobile applications with battery usage information, which is
vital for both developers and users.

ABSTRACT_BEGIN
  Mobile devices have been shipped with multiple wireless network interfaces in
order to meet their diverse communication and networking demands. In this
paper, we propose an A-GPS assisted scheme that discovers the nearest Wi-Fi
network access points (APs) by using user's location information. This allows
the user to switch to the Wi-Fi interface in an intelligent manner when she/he
arrives at the nearest Wi-Fi network AP. Therefore, it avoids the long periods
in idle state and greatly reduces the number of unnecessary Wi-Fi scans on the
mobile device. The experimental results demonstrate that our scheme effectively
saves energy for mobile devices integrated with Wi-Fi and cellular interfaces.

ABSTRACT_BEGIN
  We design and analyze a mechanism for forming coalitions of peers in a data
swarming system where peers have heterogeneous upload capacities. A coalition
is a set of peers that explicitly cooperate with other peers inside the
coalition via choking, data replication, and capacity allocation strategies.
Further, each peer interacts with other peers outside its coalition via
potentially distinct choking, data replication, and capacity allocation
strategies. Following on our preliminary work in IEEE ICNP 2011 that
demonstrated significant performance benefits of coalitions, we present here a
comprehensive analysis of the choking and data replication strategies for
coalitions.
  We first develop an analytical model to understand a simple random choking
strategy as a within-coalition strategy and show that it accurately predicts a
coalition's performance. Our analysis formally shows that the random choking
strategy can help a coalition achieve near-optimal performance by optimally
choosing the re-choking interval lengths and the number unchoke slots. Further,
our analytical model can be easily adapted to model a BitTorrent-like swarm. We
also introduce a simple data replication strategy which significantly improves
data availability within a coalition as compared to the rarest-first piece
replication strategy employed in BitTorrent systems. We further propose a
cooperation-aware better response strategy that achieves convergence of the
dynamic coalition formation process when peers freely join or leave any
coalition. Finally, using extensive simulations, we demonstrate improvements in
the performance of a swarming system due to coalition formation.

ABSTRACT_BEGIN
  Sensors have limited resources so it is important to manage the resources
efficiently to maximize their use. A sensor's battery is a crucial resource as
it singly determines the lifetime of sensor network applications. Since these
devices are useful only when they are able to communicate with the world, radio
transceiver of a sensor as an I/O and a costly unit plays a key role in its
lifetime. This resource often consumes a big portion of the sensor's energy as
it must be active most of the time to announce the existence of the sensor in
the network. As such the radio component has to deal with its embedded sensor
network whose parameters and operations have significant effects on the
sensor's lifetime. In existing energy models, hardware is considered, but the
environment and the network's parameters did not receive adequate attention.
Energy consumption components of traditional network architecture are often
considered individually and separately, and their influences on each other have
not been considered in these approaches. In this paper we consider all possible
tasks of a sensor in its embedded network and propose an energy management
model. We categorize these tasks in five energy consuming constituents. The
sensor's Energy Consumption (EC) is modeled on its energy consuming
constituents and their input parameters and tasks. The sensor's EC can thus be
reduced by managing and executing efficiently the tasks of its constituents.
The proposed approach can be effective for power management, and it also can be
used to guide the design of energy efficient wireless sensor networks through
network parameterization and optimization.

ABSTRACT_BEGIN
  In contrast to the classical cyclic prefix (CP)-OFDM, the time domain
synchronous (TDS)-OFDM employs a known pseudo noise (PN) sequence as guard
interval (GI). Conventional channel estimation methods for TDS-OFDM are based
on the exploitation of the PN sequence and consequently suffer from intersymbol
interference (ISI). This paper proposes a novel dataaided channel estimation
method which combines the channel estimates obtained from the PN sequence and,
most importantly, additional channel estimates extracted from OFDM data
symbols. Data-aided channel estimation is carried out using the rebuilt OFDM
data symbols as virtual training sequences. In contrast to the classical turbo
channel estimation, interleaving and decoding functions are not included in the
feedback loop when rebuilding OFDM data symbols thereby reducing the
complexity. Several improved techniques are proposed to refine the data-aided
channel estimates, namely one-dimensional (1-D)/two-dimensional (2-D) moving
average and Wiener filtering. Finally, the MMSE criteria is used to obtain the
best combination results and an iterative process is proposed to progressively
refine the estimation. Both MSE and BER simulations using specifications of the
DTMB system are carried out to prove the effectiveness of the proposed
algorithm even in very harsh channel conditions such as in the single frequency
network (SFN) case.

ABSTRACT_BEGIN
  In this paper, we propose LMEEC, a cluster-based routing protocol with low
energy consumption for wireless sensor networks. Our protocol is based on a
strategy which aims to provide a more reasonable exploitation of the selected
nodes (cluster-heads) energy. Simulation results show the effectiveness of
LMEEC in decreasing the energy consumption, and in prolonging the network
lifetime, compared to LEACH.

ABSTRACT_BEGIN
  The Internet is constantly changing, and its hierarchy was recently shown to
become flatter. Recent studies of inter-domain traffic showed that large
content providers drive this change by bypassing tier-1 networks and reaching
closer to their users, enabling them to save transit costs and reduce reliance
of transit networks as new services are being deployed, and traffic shaping is
becoming increasingly popular.
  In this paper we take a first look at the evolving connectivity of large
content provider networks, from a topological point of view of the autonomous
systems (AS) graph. We perform a 5-year longitudinal study of the topological
trends of large content providers, by analyzing several large content providers
and comparing these trends to those observed for large tier-1 networks. We
study trends in the connectivity of the networks, neighbor diversity and
geographical spread, their hierarchy, the adoption of IXPs as a convenient
method for peering, and their centrality. Our observations indicate that
content providers gradually increase and diversify their connectivity, enabling
them to improve their centrality in the graph, and as a result, tier-1 networks
lose dominance over time.

ABSTRACT_BEGIN
  A wireless network is realized by mobile devices which communicate over radio
channels. Since, experiments of real life problem with real devices are very
difficult, simulation is used very often. Among many other important properties
that have to be defined for simulative experiments, the mobility model and the
radio propagation model have to be selected carefully. Both have strong impact
on the performance of mobile wireless networks, e.g., the performance of
routing protocols varies with these models. There are many mobility and radio
propagation models proposed in literature. Each of them was developed with
different objectives and is not suited for every physical scenario. The radio
propagation models used in common wireless network simulators, in general
researcher consider simple radio propagation models and neglect obstacles in
the propagation environment. In this paper, we study the performance of
wireless networks simulation by consider different Radio propagation models
with considering obstacles in the propagation environment. In this paper we
analyzed the performance of wireless networks by OPNET Modeler .In this paper
we quantify the parameters such as throughput, packet received attenuation.

ABSTRACT_BEGIN
  In this paper we propose a new routing protocol with low energy consumption
for wireless sensor networks based on the clustering approach. Our protocol is
based on a strategy which aims at providing a more equitable exploitation of
the selected nodes (cluster-heads) energy by distributing their load of the
managed sensors during the clustering process. In order to save the energy
dissipated while transmitting sensed data to the base station, the multi-hops
routing strategy is used to arrange the communication of the data between
cluster-heads nodes. Simulation results demonstrate that our proposed protocol
decreases the energy consumption and prolongs the network lifetime.

ABSTRACT_BEGIN
  The vision of next generation wireless network (NGWN) is to integrate
different wireless access technologies, each with its own characteristics, into
a common IP-based core network to provide mobile user with service continuity
and seamless roaming. One of the major issues for the converged heterogeneous
networks is providing a seamless vertical handover (VHO) with QoS support. In
this paper we have reviewed the various interworking architectures and handover
scenarios between UMTS and WiMAX. Also, we have compared the proposed solutions
based on different criteria and revealed the pros and cons of each scheme. The
comparison aids to adopt a better interworking and handover mechanism in NGWN.

ABSTRACT_BEGIN
  The knowledge of end-to-end network distances is essential to many Internet
applications. As active probing of all pairwise distances is infeasible in
large-scale networks, a natural idea is to measure a few pairs and to predict
the other ones without actually measuring them. This paper formulates the
distance prediction problem as matrix completion where unknown entries of an
incomplete matrix of pairwise distances are to be predicted. The problem is
solvable because strong correlations among network distances exist and cause
the constructed distance matrix to be low rank. The new formulation circumvents
the well-known drawbacks of existing approaches based on Euclidean embedding.
  A new algorithm, so-called Decentralized Matrix Factorization by Stochastic
Gradient Descent (DMFSGD), is proposed to solve the network distance prediction
problem. By letting network nodes exchange messages with each other, the
algorithm is fully decentralized and only requires each node to collect and to
process local measurements, with neither explicit matrix constructions nor
special nodes such as landmarks and central servers. In addition, we compared
comprehensively matrix factorization and Euclidean embedding to demonstrate the
suitability of the former on network distance prediction. We further studied
the incorporation of a robust loss function and of non-negativity constraints.
Extensive experiments on various publicly-available datasets of network delays
show not only the scalability and the accuracy of our approach but also its
usability in real Internet applications.

ABSTRACT_BEGIN
  Multistage Interconnection Networks (MINs) are very popular in switching and
communication applications. A MIN connects N inputs to N outputs and is
referred as an N \times N MIN; having size N. Optical Multistage
Interconnection Network (OMIN) represents an important class of Interconnection
networks. Crosstalk is the basic problem of OMIN. Switch Conflict and Link
Conflict are the two main reason of crosstalk. In this paper, we are
considering both problems. A number of techniques like Optical window, Improved
Window, Heuristic, Genetic, and Zero have been proposed earlier in this
research domain. In this paper, we have proposed two algorithms called Address
Selection Algorithm and Route Selection Algorithm (RSA). RSA is based on
Improved Window Method. We have applied the proposed algorithms on existing
Omega network, having shuffle-exchange connection pattern. The main
functionality of ASA and RSA is to minimize the number of switch and link
conflicts in the network and to provide conflict free routes.

ABSTRACT_BEGIN
  We provide an analytical study of the impact of packet skipping and
opportunistic network coding on the timely communication of messages through a
single network element. In a first step, we consider a single-server queueing
system with Poisson arrivals, exponential service times, and a single buffer
position. Packets arriving at a network node have a fixed deadline before which
they should reach the destination. To preserve server capacity, we introduce a
thresholding policy, based on remaining time until deadline expiration, to
decide whether to serve a packet or skip its service. The obtained goodput
improvement of the system is derived, as well as the operating conditions under
which thresholding can enhance performance. Subsequently, we focus our analysis
on a system that supports network coding instead of thresholding. We
characterize the impact of network coding at a router node on the delivery of
packets associated with deadlines. We model the router node as a queueing
system where packets arrive from two independent Poisson flows and undergo
opportunistic coding operations. We obtain an exact expression for the goodput
of the system and study the achievable gain. Finally, we provide an analytical
model that considers both network coding and packet skipping, capturing their
joint performance. A comparative analysis between the aforementioned approaches
is provided.

ABSTRACT_BEGIN
  Source-controlled routing has been proposed as a way to improve flexibility
of future network architectures, as well as simplifying the data plane.
However, if a packet specifies its path, this precludes fast local re-routing
within the network. We propose SlickPackets, a novel solution that allows
packets to slip around failures by specifying alternate paths in their headers,
in the form of compactly-encoded directed acyclic graphs. We show that this can
be accomplished with reasonably small packet headers for real network
topologies, and results in responsiveness to failures that is competitive with
past approaches that require much more state within the network. Our approach
thus enables fast failure response while preserving the benefits of
source-controlled routing.

ABSTRACT_BEGIN
  In this paper, we propose to improve the performance of the channel
estimation for LTE Downlink systems under the effect of the channel length. As
LTE Downlink system is a MIMO-OFDMA based system, a cyclic prefix (CP) is
inserted at the beginning of each transmitted OFDM symbol in order to mitigate
both inter-carrier interference (ICI) and inter-symbol interference (ISI). The
inserted CP is usually equal to or longer than the channel length. However, the
cyclic prefix can be shorter because of some unforeseen channel behaviour.
Previous works have shown that in the case where the cyclic prefix is equal to
or longer than the channel length, LMMSE performs better than LSE but at the
cost of computational complexity .In the other case, LMMSE performs also better
than LS only for low SNR values. However, LS shows better performance for LTE
Downlink systems for high SNR values. Therefore, we propose a hybrid LS-LMMSE
channel estimation technique robust to the channel length effect. MATLAB
Monte-Carlo simulations are used to evaluate the performance of the proposed
estimator in terms of Mean Square Error (MSE) and Bit Error Rate (BER) for 2x2
LTE Downlink systems.

ABSTRACT_BEGIN
  Location information is the major component in location based applications.
This information is used in different safety and service oriented applications
to provide users with services according to their Geolocation. There are many
approaches to locate mobile nodes in indoor and outdoor environments. In this
paper, we are interested in outdoor localization particularly in cellular
networks of mobile nodes and presented a localization method based on cell and
user location information. Our localization method is based on hello message
delay (sending and receiving time) and coordinate information of Base
Transceiver Station (BTSs). To validate our method across cellular network, we
implemented and simulated our method in two scenarios i.e. maintaining database
of base stations in centralize and distributed system. Simulation results show
the effectiveness of our approach and its implementation applicability in
telecommunication systems.

ABSTRACT_BEGIN
  In this paper we present a model for the lifetime of wireless sensor
networks. The model takes into consideration several parameters such as the
total number of sensors, network size, percentage of sink nodes, location of
sensors, the mobility of sensors, and power consumption. A definition of the
life time of the network based on three different criteria is introduced;
percentage of available power to total power, percentage of alive sensors to
total sensors, and percentage of alive sink sensors to total sink sensors. A
Matlab based simulator is developed for the introduced model. A number of
wireless sensor networks scenarios are presented and discussed.

ABSTRACT_BEGIN
  The focus of user behavior in the Internet has changed over the recent years
towards being driven by exchanging and accessing information. Many advances in
networking technologies have utilized this change by focusing on the content of
an exchange rather than the endpoints exchanging the content. Network coding
and information centric networking are two examples of these technology trends,
each being developed largely independent so far. This paper brings these areas
together in an evolutionary as well as explorative setting for a new
internetworking architecture. We outline opportunities for applying network
coding in a novel and performance-enhancing way that could eventually push
forward the case for information centric network itself.

ABSTRACT_BEGIN
  This paper is a thorough study of a digital broadcasting system adapted to
the small mountainous island of Mauritius. A digital LAN was designed with
MPEG-2 signals. The compressed signals were transmitted using DVB-T and QAM
modulators. QAM-16 and QAM-64 modulators were designed and tested with a
simulator under critical conditions of AWGN and phase noises. Results obtained
from simulation have shown that Digital video broadcast with a single frequency
network (SFN) is possible in Mauritius with QAM-64 and QAM-16 modulators
applying COFDM mode of transmission. However, this study has also shown that
QAM-16 modulator had a better performance at low AWGN values (less than 12 dB)
and can be adopted for Mauritius Island, provided that the number of
transmitted channels is not high enough.

ABSTRACT_BEGIN
  The Internet Threat Monitoring (ITM),is a globally scoped Internet monitoring
system whose goal is to measure, detect, characterize, and track threats such
as distribute denial of service(DDoS) attacks and worms. To block the
monitoring system in the internet the attackers are targeted the ITM system. In
this paper we address flooding attack against ITM system in which the attacker
attempt to exhaust the network and ITM's resources, such as network bandwidth,
computing power, or operating system data structures by sending the malicious
traffic. We propose an information-theoretic frame work that models the
flooding attacks using Botnet on ITM. Based on this model we generalize the
flooding attacks and propose an effective attack detection using Honeypots.

ABSTRACT_BEGIN
  The explosive increase in data demand coupled with the rapid deployment of
various wireless access technologies have led to the increase of number of
multi-homed or multi-interface enabled devices. Fully exploiting these
interfaces has motivated researchers to propose numerous solutions that
aggregate their available bandwidths to increase overall throughput and satisfy
the end-user's growing data demand. These solutions, however, have faced a
steep deployment barrier that we attempt to overcome in this paper. We propose
a Deployable Bandwidth Aggregation System (DBAS) for multi-interface enabled
devices. Our system does not introduce any intermediate hardware, modify
current operating systems, modify socket implementations, nor require changes
to current applications or legacy servers. The DBAS architecture is designed to
automatically estimate the characteristics of applications and dynamically
schedule various connections or packets to different interfaces. Since our main
focus is deployability, we fully implement DBAS on the Windows operating system
and evaluate various modes of operation. Our implementation and simulation
results show that DBAS achieves throughput gains up to 193% compared to current
operating systems, while operating as an out-of-the-box standard Windows
executable, highlighting its deployability and ease of use.

ABSTRACT_BEGIN
  Combining cognitive radio technology with user cooperation could be
advantageous to both primary and secondary transmissions. In this paper, we
propose a first relaying scheme for cognitive radio networks (called "Adaptive
relaying scheme 1"), where one relay node can assist the primary or the
secondary transmission with the objective of improving the outage probability
of the secondary transmission with respect to a primary outage probability
threshold. Upper bound expressions of the secondary outage probability using
the proposed scheme are derived over Rayleigh fading channels. Numerical and
simulation results show that the secondary outage probability using the
proposed scheme is lower than that of other relaying schemes. Then, we extend
the proposed scheme to the case where the relay node has the ability to decode
both the primary and secondary signals and also can assist simultaneously both
transmissions. Simulations show the performance improvement that can be
obtained due to this extension in terms of secondary outage probability.

ABSTRACT_BEGIN
  We develop an approximate analytical technique for evaluating the performance
of multi-hop networks based on beacon-less CSMA/CA as standardised in IEEE
802.15.4, a popular standard for wireless sensor networks. The network
comprises sensor nodes, which generate measurement packets, and relay nodes
which only forward packets. We consider a detailed stochastic process at each
node, and analyse this process taking into account the interaction with
neighbouring nodes via certain unknown variables (e.g., channel sensing rates,
collision probabilities, etc.). By coupling these analyses of the various
nodes, we obtain fixed point equations that can be solved numerically to obtain
the unknown variables, thereby yielding approximations of time average
performance measures, such as packet discard probabilities and average queueing
delays. Different analyses arise for networks with no hidden nodes and networks
with hidden nodes. We apply this approach to the performance analysis of tree
networks rooted at a data sink. Finally, we provide a validation of our
analysis technique against simulations.

ABSTRACT_BEGIN
  In this paper we consider the Max-Weight protocol for routing and scheduling
in wireless networks under an adversarial model. This protocol has received a
significant amount of attention dating back to the papers of Tassiulas and
Ephremides. In particular, this protocol is known to be throughput-optimal
whenever the traffic patterns and propagation conditions are governed by a
stationary stochastic process.
  However, the standard proof of throughput optimality (which is based on the
negative drift of a quadratic potential function) does not hold when the
traffic patterns and the edge capacity changes over time are governed by an
arbitrary adversarial process. Such an environment appears frequently in many
practical wireless scenarios when the assumption that channel conditions are
governed by a stationary stochastic process does not readily apply.
  In this paper we prove that even in the above adversarial setting, the
Max-Weight protocol keeps the queues in the network stable (i.e. keeps the
queue sizes bounded) whenever this is feasible by some routing and scheduling
algorithm. However, the proof is somewhat more complex than the negative
potential drift argument that applied in the stationary case. Our proof holds
for any arbitrary interference relationships among edges. We also prove the
stability of $\ep$-approximate Max-Weight under the adversarial model. We
conclude the paper with a discussion of queue sizes in the adversarial model as
well as a set of simulation results.

ABSTRACT_BEGIN
  We present DISco, a storage and communication middleware designed to enable
distributed and task-centric autonomic control of networks.
  DISco is designed to enable multi-agent identification of anomalous
situations -- so-called "challenges" -- and assist coordinated remediation that
maintains degraded -- but acceptable -- service level, while keeping a track of
the challenge evolution in order to enable human-assisted diagnosis of flaws in
the network. We propose to use state-of-art peer-to-peer publish/subscribe and
distributed storage as core building blocks for the DISco service.

ABSTRACT_BEGIN
  We investigate on the scalability of multihop wireless communications, a
major concern in networking, for the case that users access content replicated
across the nodes. In contrast to the standard paradigm of randomly selected
communicating pairs, content replication is efficient for certain regimes of
file popularity, cache and network size. Our study begins with the detailed
joint content replication and delivery problem on a 2D square grid, a hard
combinatorial optimization. This is reduced to a simpler problem based on
replication density, whose performance is of the same order as the original.
Assuming a Zipf popularity law, and letting the size of content and network
both go to infinity, we identify the scaling laws and regimes of the required
link capacity, ranging from O(\sqrt{N}) down to O(1).

ABSTRACT_BEGIN
  Node misbehavior due to selfish or malicious behavior could significantly
degrade the performance of MANET because most existing routing protocols in
MANET aim to find the most efficient path. Overhearing and reputation based
cooperation schemes have been used to detect and isolate the misbehaving nodes
as well as to force them to cooperate. Performance analysis has been done for
the network traffic using OCEAN over DSR on ns2 while considering the low
energy levels for mobile nodes. Throughput, energy level, routing packets and
normalized routing overhead are analyzed for OCEAN and normal DSR to show the
impact of OCEAN on the overall network performance.

ABSTRACT_BEGIN
  In this paper, we consider resource allocation in the 3GPP Long Term
Evolution (LTE) cellular uplink, which will be the most widely deployed next
generation cellular uplink. The key features of the 3GPP LTE uplink (UL) are
that it is based on a modified form of the orthogonal frequency division
multiplexing based multiple access (OFDMA) which enables channel dependent
frequency selective scheduling, and that it allows for multi-user (MU)
scheduling wherein multiple users can be assigned the same time-frequency
resource. In addition to the considerable spectral efficiency improvements that
are possible by exploiting these two features, the LTE UL allows for transmit
antenna selection together with the possibility to employ advanced receivers at
the base-station, which promise further gains. However, several practical
constraints that seek to maintain a low signaling overhead, are also imposed.
In this paper, we show that the resulting resource allocation problem is
APX-hard and then propose a local ratio test (LRT) based constant-factor
polynomial-time approximation algorithm. We then propose two enhancements to
this algorithm as well as a sequential LRT based MU scheduling algorithm that
offers a constant-factor approximation and is another useful choice in the
complexity versus performance tradeoff. Further, user pre-selection, wherein a
smaller pool of good users is pre-selected and a sophisticated scheduling
algorithm is then employed on the selected pool, is also examined. We suggest
several such user pre-selection algorithms, some of which are shown to offer
constant-factor approximations to the pre-selection problem. Detailed
evaluations reveal that the proposed algorithms and their enhancements offer
significant gains.

ABSTRACT_BEGIN
  Thanks to the potential they hold and the variety of their application
domains, Multimedia Wireless Sensor Networks (MWSN) are forecast to become
highly integrated into our daily activities. Due to the carried content nature,
mainly composed of images and/or video streams with high throughput and delay
constraints, Quality of Service in the context of MWSN is a crucial issue. In
this paper, we propose a QoS and energy aware geographic routing protocol for
MWSN: QGRP. The proposed protocol addresses bandwidth, delay and energy
constraints associated with MWSN. QGRP adopts an analytical model of IEEE
802.11 Distributed Coordination Function (DCF) to estimate available bandwidth
and generates loop-free routing paths.

ABSTRACT_BEGIN
  This dissertation is a study on the design and analysis of novel, optimal
routing and rate control algorithms in wireless, mobile communication networks.
Congestion control and routing algorithms upto now have been designed and
optimized for wired or wireless mesh networks. In those networks, optimal
algorithms (optimal in the sense that either the throughput is maximized or
delay is minimized, or the network operation cost is minimized) can be
engineered based on the classic time scale decomposition assumption that the
dynamics of the network are either fast enough so that these algorithms
essentially see the average or slow enough that any changes can be tracked to
allow the algorithms to adapt over time. However, as technological advancements
enable integration of ever more mobile nodes into communication networks, any
rate control or routing algorithms based, for example, on averaging out the
capacity of the wireless mobile link or tracking the instantaneous capacity
will perform poorly. The common element in our solution to engineering
efficient routing and rate control algorithms for mobile wireless networks is
to make the wireless mobile links seem as if they are wired or wireless links
to all but few nodes that directly see the mobile links (either the mobiles or
nodes that can transmit to or receive from the mobiles) through an appropriate
use of queuing structures at these selected nodes. This approach allows us to
design end-to-end rate control or routing algorithms for wireless mobile
networks so that neither averaging nor instantaneous tracking is necessary.

ABSTRACT_BEGIN
  A typical web search engine consists of three principal parts: crawling
engine, indexing engine, and searching engine. The present work aims to
optimize the performance of the crawling engine. The crawling engine finds new
web pages and updates web pages existing in the database of the web search
engine. The crawling engine has several robots collecting information from the
Internet. We first calculate various performance measures of the system (e.g.,
probability of arbitrary page loss due to the buffer overflow, probability of
starvation of the system, the average time waiting in the buffer). Intuitively,
we would like to avoid system starvation and at the same time to minimize the
information loss. We formulate the problem as a multi-criteria optimization
problem and attributing a weight to each criterion. We solve it in the class of
threshold policies. We consider a very general web page arrival process modeled
by Batch Marked Markov Arrival Process and a very general service time modeled
by Phase-type distribution. The model has been applied to the performance
evaluation and optimization of the crawler designed by INRIA Maestro team in
the framework of the RIAM INRIA-Canon research project.

ABSTRACT_BEGIN
  Major wireless operators are nowadays facing network capacity issues in
striving to meet the growing demands of mobile users. At the same time,
3G-enabled devices increasingly benefit from ad hoc radio connectivity (e.g.,
Wi-Fi). In this context of hybrid connectivity, we propose Push-and-track, a
content dissemina- tion framework that harnesses ad hoc communication
opportunities to minimize the load on the wireless infrastructure while
guaranteeing tight delivery delays. It achieves this through a control loop
that collects user-sent acknowledgements to determine if new copies need to be
reinjected into the network through the 3G interface. Push-and-Track is
flexible and can be applied to a variety of scenarios, including periodic
message flooding and floating data. For the former, this paper examines
multiple strategies to determine how many copies of the content should be
injected, when, and to whom; for the latter, it examines the achievable offload
ratio depending on the freshness constraints. The short delay-tolerance of
common content, such as news or road traffic updates, make them suitable for
such a system. Use cases with a long delay-tolerance, such as software updates,
are an even better fit. Based on a realistic large-scale vehicular dataset from
the city of Bologna composed of more than 10,000 vehicles, we demonstrate that
Push-and-Track consistently meets its delivery objectives while reducing the
use of the 3G network by about 90%.

ABSTRACT_BEGIN
  The proposed system highlights a novel approach of exclusive verification
process using gain protocol for ensuring security among both the parties
(client-service provider) in m-commerce application with cloud enabled service.
The proposed system is based on the potential to verify the clients with
trusted hand held device depending on the set of frequent events and actions to
be carried out. The framework of the proposed work is design after collecting a
real time data sets from an android enabled hand set, which when subjected to
gain protocol, will result in detection of malicious behavior of illegal
clients in the network. The real time experiment is performed with applicable
datasets gather, which show the best result for identifying threats from last 2
months data collected.

ABSTRACT_BEGIN
  We give a livelock free routing algorithm for any allowed network. Unlike
some other solutions to this problem:
  1) packets entering the network have an absolute upper bound on the time to
reach their destination; 2) under light loads, packets are delivered to their
destinations in nearly optimal time; 3) packets with desired paths far away
from congested areas will have routing times far shorter than packets wanting
to access congested areas; 4) if the network becomes congested and later
clears, the network operates just as it would have when it was initially under
a light load.
  The main ideas of this note appear in a different form in my 1994 patent
5,369,745. This note adds to those results and makes them more mathematical.

ABSTRACT_BEGIN
  In this paper we introduce a novel Automatic Repeat reQuest (ARQ) scheme for
cooperative wireless networks. Our scheme adopts network coding techniques in
order to enhance the total bandwidth of the network by minimizing the total
number of transmissions. The performance of the proposed approach is evaluated
by means of computer simulations and compared to other cooperative schemes,
while an analytical solution is provided to validate the results.

ABSTRACT_BEGIN
  A wireless sensor network has a wide application domain which is expanding
everyday and they have been deployed pertaining to their application area. An
application independent approach is yet to come to terms with the ongoing
exploitation of the WSNs. In this paper we propose a decentralized lifetime
maximizing tree for application independent data aggregation scheme using the
clustering for data delivery in WSNs. The proposed tree will minimize the
energy consumption which has been a resisting factor in the smooth working of
WSNs as well as minimize the distance between the communicating nodes under the
control of a sub-sink which further communicate and transfer data to the sink
node.

ABSTRACT_BEGIN
  Real-time applications are performance critical applications that require
bounded service latency. In multi-hop wireless ad-hoc and sensor networks,
communication delays are dominant over processing delays. Therefore, to enable
real-time applications in such networks, the communication latency must be
bounded. In this paper, we derive expressions of real-time capacity that
characterize the ability of a network to deliver data on time as well as
develop network protocols that achieve this capacity. Real-time capacity
expressions are obtained and analyzed for the earliest deadline first, deadline
monotonic. This paper presents a treatment of the real-time capacity limits.
The limits are derived for two extreme traffic topologies namely, the load
balanced topology and the convergecast (i.e., many-to-one) topology. It
considers DM and EDF scheduling algorithms, and discusses the implications of
the capacity limit expressions.

ABSTRACT_BEGIN
  We show that, for independent interfering sources and a signal link with
exponentially distributed received power, the total probability of outage can
be decomposed as a simple expression of the outages from the individual
interfering sources. We give a mathematical proof of this result, and discuss
some immediate implications, showing how it results in important
simplifications to statistical outage analysis. We also discuss its application
to two active topics of study: spectrum sharing, and sum of interference powers
(e.g., lognormal) analysis.

ABSTRACT_BEGIN
  This Article presents a thorough overview of QoS routing metrics, resources
and factors affecting performance of QoS routing protocols. The relative
strength, weakness, and applicability of existing QoS routing protocols are
also studied and compared. QoS routing protocols are classified according to
the QoS metrics used type of QoS guarantee assured.

ABSTRACT_BEGIN
  In this paper we argue that contextual multi-armed bandit algorithms could
open avenues for designing self-learning security modules for computer networks
and related tasks. The paper has two contributions: a conceptual one and an
algorithmical one. The conceptual contribution is to formulate -- as an example
-- the real-world problem of preventing SPIT (Spam in VoIP networks), which is
currently not satisfyingly addressed by standard techniques, as a sequential
learning problem, namely as a contextual multi-armed bandit. Our second
contribution is to present CMABFAS, a new algorithm for general contextual
multi-armed bandit learning that specifically targets domains with finite
actions. We illustrate how CMABFAS could be used to design a fully
self-learning SPIT filter that does not rely on feedback from the end-user
(i.e., does not require labeled data) and report first simulation results.

ABSTRACT_BEGIN
  This paper presents a formal framework for identifying and filtering SPIT
calls (SPam in Internet Telephony) in an outbound scenario with provable
optimal performance. In so doing, our work is largely different from related
previous work: our goal is to rigorously formalize the problem in terms of
mathematical decision theory, find the optimal solution to the problem, and
derive concrete bounds for its expected loss (number of mistakes the SPIT
filter will make in the worst case).
  This goal is achieved by considering an abstracted scenario amenable to
theoretical analysis, namely SPIT detection in an outbound scenario with pure
sources. Our methodology is to first define the cost of making an error (false
positive and false negative), apply Wald's sequential probability ratio test to
the individual sources, and then determine analytically error probabilities
such that the resulting expected loss is minimized.
  The benefits of our approach are: (1) the method is optimal (in a sense
defined in the paper); (2) the method does not rely on manual tuning and
tweaking of parameters but is completely self-contained and mathematically
justified; (3) the method is computationally simple and scalable. These are
desirable features that would make our method a component of choice in larger,
autonomic frameworks.

ABSTRACT_BEGIN
  Orthogonal Frequency Division Multiple Access (OFDMA) is a multi-user version
of the Orthogonal Frequency Division Multiplexing (OFDM) transmission
technique, which divides a wideband channel into a number of orthogonal
narrowband subchannels, called subcarriers. An OFDMA system takes advantage of
both frequency diversity (FD) gain and frequency-selective scheduling (FSS)
gain. A FD gain is achieved by allocating a user the subcarriers distributed
over the entire frequency band whereas a FSS gain is achieved by allocating a
user adjacent subcarriers located within a subband of a small bandwidth having
the most favorable channel conditions among other subbands in the entire
frequency band. Multi-User Multiple Input Multiple Output (MU-MIMO) is a
promising technology to increase spectral efficiency. A well-known MU-MIMO mode
is Space-Division Multiple Access (SDMA) which can be used in the downlink
direction to allow a group of spatially separable users to share the same
time/frequency resources. In this paper, we study the gain from FSS in
SDMA-OFDMA systems using the example of WiMAX. Therefore, a complete SDMA-OFDMA
MAC scheduling solution supporting both FD and FSS is proposed. The proposed
solution is analyzed in a typical urban macro-cell scenario by means of
system-level packet-based simulations, with detailed MAC and physical layer
abstractions. By explicitly simulating the MAC layer overhead (MAP) which is
required to signal every packed data burst in the OFDMA frame we can present
the overall performance to be expected at the MAC layer. Our results show that
in general the gain from FSS when applying SDMA is low. However, under specific
conditions, small number of BS antennas or large channel bandwidth, a
significant gain can be achieved from FSS.

ABSTRACT_BEGIN
  For a realistic traffic mix, we evaluate the hit rates attained in a
two-layer cache hierarchy designed to reduce Internet bandwidth requirements.
The model identifies four main types of content, web, file sharing, user
generated content and video on demand, distinguished in terms of their traffic
shares, their population and object sizes and their popularity distributions.
Results demonstrate that caching VoD in access routers offers a highly
favorable bandwidth memory tradeoff but that the other types of content would
likely be more efficiently handled in very large capacity storage devices in
the core. Evaluations are based on a simple approximation for LRU cache
performance that proves highly accurate in relevant configurations.

ABSTRACT_BEGIN
  Relaying is a promising enhancement to current radio technologies, which has
been considered in IMT-Advanced candidate technologies such as 3GPP
LTE-Advanced and IEEE 802.16m. Relay enhanced networks are expected to fulfill
the demanding coverage and capacity requirements in a cost efficient way. Among
various relaying architectures multi-hop moving relays can provide additional
capacity for the cases when fixed relays are inaccessible or not able to
provide adequate solutions in terms of cost. In this paper, an overview of
multi-hop moving relays along with some of the envisioned deployment scenarios
is presented. Furthermore, different types of multi-hop moving relays are
discussed and the challenges are addressed.

ABSTRACT_BEGIN
  Relaying is considered a promising cost-efficient solution in 3GPP
LTE-Advanced for coverage extension and throughput enhancement. The compact
physical characteristics and low power requirements of the relay nodes offer
more flexible deployment options than traditional macro evolved Node Bs. This
paper provides an overview of general relaying concepts and presents the relay
deployment within the LTE-Advanced framework. Furthermore, the impact of relay
backhauling on envisioned relaying gains is discussed and the methods to
improve the performance of the backhauling are included.

ABSTRACT_BEGIN
  We consider the channel access problem under imperfect sensing of channel
state in a multi-channel opportunistic communication system, where the state of
each channel evolves as an independent and identically distributed Markov
process. The considered problem can be cast into a restless multi-armed bandit
(RMAB) problem that is of fundamental importance in decision theory. It is
well-known that solving the RMAB problem is PSPACE-hard, with the optimal
policy usually intractable due to the exponential computation complexity. A
natural alternative is to consider the easily implementable myopic policy that
maximizes the immediate reward but ignores the impact of the current strategy
on the future reward. In this paper, we perform an analytical study on the
optimality of the myopic policy under imperfect sensing for the considered RMAB
problem. Specifically, for a family of generic and practically important
utility functions, we establish the closed-form conditions under which the
myopic policy is guaranteed to be optimal even under imperfect sensing. Despite
our focus on the opportunistic channel access, the obtained results are generic
in nature and are widely applicable in a wide range of engineering domains.

ABSTRACT_BEGIN
  IEEE 802.11 DCF is the MAC protocol currently used in wireless LANs. 802.11
DCF is inefficient due to two types of overhead; channel idle time and
collision time. This paper presents the design and performance evaluation of an
efficient MAC protocol for wireless networks, called Token-DCF. Token-DCF
decreases both idle time and collision time. In Token-DCF, each station keeps
track of neighboring links' queue length by overhearing of transmitted packets
on the wireless medium. The result is then used to assign privileges to the
network stations. A privileged station does not follow the backoff mechanism
and transmits immediately after the channel is sensed idle. Our simulation
results show that Token-DCF can significantly improve channel utilization,
system throughput and channel access delay over 802.11 DCF.

ABSTRACT_BEGIN
  Throughput and per-packet delay can present strong trade-offs that are
important in the cases of delay sensitive applications.We investigate such
trade-offs using a random linear network coding scheme for one or more
receivers in single hop wireless packet erasure broadcast channels. We capture
the delay sensitivities across different types of network applications using a
class of delay metrics based on the norms of packet arrival times. With these
delay metrics, we establish a unified framework to characterize the rate and
delay requirements of applications and optimize system parameters. In the
single receiver case, we demonstrate the trade-off between average packet
delay, which we view as the inverse of throughput, and maximum ordered
inter-arrival delay for various system parameters. For a single broadcast
channel with multiple receivers having different delay constraints and feedback
delays, we jointly optimize the coding parameters and time-division scheduling
parameters at the transmitters. We formulate the optimization problem as a
Generalized Geometric Program (GGP). This approach allows the transmitters to
adjust adaptively the coding and scheduling parameters for efficient allocation
of network resources under varying delay constraints. In the case where the
receivers are served by multiple non-interfering wireless broadcast channels,
the same optimization problem is formulated as a Signomial Program, which is
NP-hard in general. We provide approximation methods using successive
formulation of geometric programs and show the convergence of approximations.

ABSTRACT_BEGIN
  This paper proposes a simple and stateless active queue management (AQM)
scheme, called geometric CHOKe (gCHOKe), to protect responsive flows from
unresponsive ones. The proposed gCHOKe has its root on and is a generalization
of the original CHOKe. It provides an extended power of flow protection,
achieved by introducing an extra flow matching trial upon each successful
matching of packets. Compared to the plain CHOKe, analysis and simulation show
that gCHOKe can achieve over 20% improvement in the bounds of both bandwidth
and buffer space used by an aggressive flow. In addition, up to 14% of the
total link capacity can be saved from the unresponsive flow, allowing
responsive or rate-adaptive flows to obtain a better share of resources in the
router.

ABSTRACT_BEGIN
  Physical layer features of Ethernet from the first realization towards the
100 Gb Ethernet (100 GbE) development have been considered. Comparisons of
these features are made according to the standardized data rates. Feasible
physical layer options are then discussed for high data rates.

ABSTRACT_BEGIN
  Cloud computing can and does mean different things to different people. The
common characteristics most shares are on-demand secure access to metered
services from nearly anywhere and dislocation of data from inside to outside
the organization. Vision of cloud computing as a new IT procurement model. The
system lifecycle, risks that are identified must be carefully balanced against
the security and privacy controls available and the expected benefits from
their utilization. Too many controls can be inefficient and ineffective, if the
benefits outweigh the costs and associated risks. In this micro research, we
characterize the problems related to security challenges.

ABSTRACT_BEGIN
  CHOKe is a simple and stateless active queue management (AQM) scheme. Apart
from low operational overhead, a highly attractive property of CHOKe is that it
can protect responsive TCP flows from unresponsive UDP flows. Particularly,
previous works have proven that CHOKe is able to bound both bandwidth share and
buffer share of (a possible aggregate) UDP traffic (flow) on a link. However,
these studies consider, and pertain only to, a steady state where the queue
reaches equilibrium in the presence of many (long-lived) TCP flows and an
unresponsive UDP flow of fixed arrival rate. If the steady state conditions are
perturbed, particularly when UDP traffic rate changes over time, it is unclear
whether the protection property of CHOKe still holds. Indeed, it can be
examined, for example, that when UDP rate suddenly becomes 0 (i.e., flow
stops), the unresponsive flow may assume close to full utilization in sub-RTT
scales, potentially starving out the TCP flows. To explain this apparent
discrepancy, this paper investigates CHOKe queue properties in a transient
regime, which is the time period of transition between two steady states of the
queue, initiated when the rate of the unresponsive flow changes. Explicit
expressions that characterize flow throughputs in transient regimes are
derived. These results provide additional understanding of CHOKe, and give some
explanation on its intriguing behavior in the transient regime.

ABSTRACT_BEGIN
  Today, a large fraction of Internet traffic is originated by Content
Providers (CPs) such as content distribution networks and hyper-giants. To cope
with the increasing demand for content, CPs deploy massively distributed
infrastructures. This poses new challenges for CPs as they have to dynamically
map end-users to appropriate servers, without being fully aware of network
conditions within an ISP as well as the end-users network locations.
Furthermore, ISPs struggle to cope with rapid traffic shifts caused by the
dynamic server selection process of CPs.
  In this paper, we argue that the challenges that CPs and ISPs face separately
today can be turned into an opportunity. We show how they can jointly take
advantage of the deployed distributed infrastructures to improve their
operation and end-user performance. We propose Content-aware Traffic
Engineering (CaTE), which dynamically adapts the traffic demand for content
hosted on CPs by utilizing ISP network information and end-user location during
the server selection process. As a result, CPs enhance their end-user to server
mapping and improve end-user experience, thanks to the ability of
network-informed server selection to circumvent network bottlenecks. In
addition, ISPs gain the ability to partially influence the traffic demands in
their networks. Our results with operational data show improvements in path
length and delay between end-user and the assigned CP server, network wide
traffic reduction of up to 15%, and a decrease in ISP link utilization of up to
40% when applying CaTE to traffic delivered by a small number of major CPs.

ABSTRACT_BEGIN
  Given the increasing demand from wireless applications, designing
energy-efficient group communication protocols is of great importance to
multi-hop wireless networks. A group communication session involves a set of
member nodes, each of them needs to send a certain number of data packets to
all other members. In this paper, we consider the problem of building a shared
multicast tree spanning the member nodes such that the total energy consumption
of a group communication session using the shared multicast tree is minimized.
Since this problem was proven as NP-complete, we propose, under our Min-Energy
Group COMmunication (MEGCOM) framework, three distributed approximation
algorithms with provable approximation ratios. When the transmission power of
each wireless node is fixed, our first two algorithms have the approximation
ratios of O(ln(\Delta+ 1)) and O(1), respectively, where \Delta is the maximum
node degree in the network. When the transmission power of each wireless node
is adjustable, our third algorithm again delivers a constant approximation
ratio. We also use extensive simulations to verify the practical performance of
our algorithms.

ABSTRACT_BEGIN
  A mobile ad-hoc network (MANET) is a peer-to-peer wireless network where
nodes can communicate with each other without the use of infrastructure such as
access points or base stations. These networks are self-configuring, capable of
self-directed operation and hastily deployable. Nodes cooperate to provide
connectivity, operates without centralized administration. Nodes are itinerant,
topology can be very dynamic and nodes must be able to relay traffic since
communicating nodes might be out of range. The dynamic nature of MANET makes
network open to attacks and unreliability. Routing is always the most
significant part for any networks. Each node should not only work for itself,
but should be cooperative with other nodes. Node misbehaviour due to selfish or
malicious intention could significantly degrade the performance of MANET. The
Qos parameters like PDR, throughput and delay are affected directly due to such
misbehaving nodes. We focus on trust management framework, which is intended to
cope with misbehaviour problem of node and increase the performance of MANETs.
A trust-based system can be used to track this misbehaving of nodes, spot them
and isolate them from routing and provide reliability. In this paper a Trust
Based Reliable AODV [TBRAODV] protocol is presented which implements a trust
value for each node. For every node trust value is calculated and based trust
value nodes are allowed to participate in routing or else identified to become
a misbehaving node. This enhances reliability in AODV routing and results in
increase of PDR, decrease in delay and throughput is maintained. This work is
implemented and simulated on NS-2. Based on simulation results, the proposed
protocol provides more consistent and reliable data transfer compared with
general AODV, if there are misbehaving nodes in the MANET

ABSTRACT_BEGIN
  Wireless networks are characterized by a dynamic topology triggered by the
nodes mobility. Thus, the wireless multi-hops connection and the channel do not
have a determinist behaviour such as: interference or multiple paths. Moreover,
the nodes' invisibility makes the wireless channel difficult to detect. This
wireless networks' behaviour should be scrutinized. In our study, we mainly
focus on radio propagation models by observing the evolution of the routing
layer's performances in terms of the characteristics of the physical layer. For
this purpose, we first examine and then display the simulation findings of the
impact of different radio propagation models on the performance of ad hoc
networks. To fully understand how these various radio models influence the
networks performance, we have compared the performances of several routing
protocols (DSR, AODV, and DSDV) for each propagation model. To complete our
study, a comparison of energy performance based routing protocols and
propagation models are presented. In order to reach credible results, we
focused on the notion of nodes' speed and the number of connections by using
the well known network simulator NS-2.

ABSTRACT_BEGIN
  Quality of Service(QoS) in Mobile Ad Hoc Networks (MANETs) though a
challenge, becomes a necessity because of its applications in critical
scenarios. Providing QoS for users belonging to various profiles and playing
different roles, becomes the need of the hour. In this paper, we propose
proportional share scheduling and MAC protocol (PS2-MAC) model. It classifies
users based on their profile as High Profiled users (HP), Medium Profiled users
(MP) and Low profiled users (LP) and assigns proportional weights. Service
Differentiation for these three service classes is achieved through, rationed
dequeuing algorithm, variable inter frame space, proportionate prioritized
backoff timers and enhanced RTS/CTS control packets. Differentiated services is
simulated in ns2 and results show that 9.5% control overhead is reduced in our
proposed scheme than the existing scheme and results also justify that,
differentiated services have been achieved for the different profiles of users
with proportionate shares and thereby reducing starvation.

ABSTRACT_BEGIN
  In VANET high speed is the real characteristics which leads to frequent
breakdown, interference etc. Therefore Performance of adhoc routing protocols
is helpful to improve the Quality of Service (QOS). In this paper we studied
various adhoc routing protocols, Reactive, Proactive & Hybrid, taking in to
consideration parameters like speed, altitude, mobility etc in real VANET
scenario. The AODV and DYMO (Reactive), OLSR (Proactive) and ZRP (hybrid)
protocols are compared for IEEE 802.11(MAC) and IEEE 802.11(DCF) standard using
Qualnet as a Simulation tool. Since IEEE 802.11, covers both physical and data
link layer. Hence performance of the protocols in these layers helps to make a
right selection of Protocol for high speed mobility. Varying parameters of
VANET shows that in the real traffic scenarios proactive protocol performs more
efficiently for IEEE 802.11 (MAC) and IEEE 802.11(DCF).

ABSTRACT_BEGIN
  One of the most challenges of 4G network is to have a unified network of
heterogeneous wireless networks. To achieve seamless mobility in such a diverse
environment, vertical hand off is still a challenging problem. In many
situations handover failures and unnecessary handoffs are triggered causing
degradation of services, reduction in throughput and increase the blocking
probability and packet loss. In this paper a new vertical handoff decision
algorithm handover necessity estimation (HNE), is proposed to minimize the
number of handover failure and unnecessary handover in heterogeneous wireless
networks. we have proposed a multi criteria vertical handoff decision algorithm
based on two parts: traveling time estimation and time threshold calculation.
Our proposed methods are compared against two other methods: (a) the fixed RSS
threshold based method, in which handovers between the cellular network and the
WLAN are initiated when the RSS from the WLAN reaches a fixed threshold, and
(b) the hysteresis based method, in which a hysteresis is introduced to prevent
the ping-pong effect. Simulation results show that, this method reduced the
number of handover failures and unnecessary handovers up to 80% and 70%,
respectively.

ABSTRACT_BEGIN
  In the world of the Internet, Web Servers such as Apache and Internet
Information Server (IIS) were developed to exchange information among client
computers having different Operation System. They have only the function of
displaying static information such as HTML files and image files into the Web
Browser. However, when the information is updated, the administrator updates it
by manual operation. In some cases, because it is necessary to update several
places about the same information, the work load becomes high than it is assume
and update error and update omission may occur. These problems were solved by
use of a Common Gateway Interface (CGI) program such as a bulletin board system
and a Blog system. However, these programs opened to Internet have often no
user authentication mechanism and no access control mechanism. That is, they
have the problem that user can access it freely only by getting the URL and
inputting it to a Web Browser. Therefore, in this paper, we show a method to
add the user authentication and access control mechanism for them. It is called
virtual use method of CGI and is realized in the case of introducing the
Destination Addressing Control System (DACS) Scheme, which is a kind of Policy
Based Network Management Scheme (PBNM). As the result, this kind of the CGI
program can be used in the organization with the above two functions.

ABSTRACT_BEGIN
  This paper is about the wireless sensor network in environmental monitoring
applications. A Wireless Sensor Network consists of many sensor nodes and a
base station. The number and type of sensor nodes and the design protocols for
any wireless sensor network is application specific. The sensor data in this
application may be light intensity, temperature, pressure, humidity and their
variations .Clustering and routing are the two areas which are given more
attention in this paper.

ABSTRACT_BEGIN
  Now a day's Heterogeneous wireless network is a promising field of research
interest. Various challenges exist in this hybrid combination like load
balancing, resource management and so on. In this paper we introduce a reliable
load balancing architecture for heterogeneous wireless communications to ensure
certain level of quality of service. To conquer the problem of centralized and
distributed design, a semi distributed load balancing architecture for multiple
access networks is introduced. In this grid based design multiple Load and
Mobile Agent Management Units is incorporated. To prove the compactness of the
design, integrated reliability, signalling overhead and total processing time
is calculated. And finally simulation result shows that overall system
performance is improved by enhancing reliability, reducing signalling overhead
and processing time.

ABSTRACT_BEGIN
  Generally traffic and the sensor network security have many challenges in the
transmission of data in the network. The existing schemes consider homogeneous
sensor networks which have poor performance and scalability. Due to many-to-one
traffic pattern, sensors may communicate with small portion of its neighbours.
Key management is the critical process in sensor nodes to secure the data. Most
existing schemes establish shared keys for all the sensors no matter whether
they are communicating or not. Hence it leads to large storage overhead.
Another problem in sensor network is compromised node attack and denial of
service attack which occurs because of its wireless nature. Existing multi path
routing algorithms are vulnerable to these attacks. So once an adversary
acquires the routing algorithm, it can compute the same routes known to the
source, and hence endanger all information sent over these routes. If an
adversary performs node compromise attack, they can easily get the encryption/
decryption keys used by that node and hence they can intercept the information
easily. In this paper we are proposing a key management scheme which only
establishes shared keys with their communicating neighbour and a mechanism to
generate randomized multipath routes for secure transmission of data to the
sink. Here we are adopting heterogeneous sensor networks and we are utilizing
elliptic curve cryptography for efficient key management which is more
efficient, scalable, and highly secure and reduces communication overhead. The
routes generated by our mechanism are highly dispersive, energy efficient and
making them quite capable of bypassing the back holes at low energy cost.

ABSTRACT_BEGIN
  One of the critical threat to internet security is Distributed Denial of
Service (DDoS). This paper by the introduction of automated online attack
classification and attack packet discarding helps to resolve the network
security issue by certain level. The incoming packets are assigned scores based
on the priority associated with the attributes and on comparison with
probability distribution of arriving packets on per packet basis.

ABSTRACT_BEGIN
  The main problem in designing DWDM transport networks is the wavelength
assignment of light paths. One way of solving this problem is to use the
algorithm BCO-RWA. However, BCO-RWA has the following disadvantages: algorithm
not solved the problem of choosing the location of the optical convector in the
network; base algorithm ignores placement optical convector during calculating
route selecting probability; base algorithm do not take into account the
nonlinear four-wave mixing phenomenon. In this paper we present an algorithm
which takes into account a number of disadvantages due to modifications
introduced in the algorithm.

ABSTRACT_BEGIN
  Mobile devices integrating wireless short-range communication technologies
make possible new applications for spontaneous communication, interaction and
collaboration. An interesting approach is to use collaboration to facilitate
communication when mobile devices are not able to establish direct
communication paths. Opportunistic networks, formed when mobile devices
communicate with each other while users are in close proximity, can help
applications still exchange data in such cases. In opportunistic networks
routes are built dynamically, as each mobile device acts according to the
store-carry-and-forward paradigm. Thus, contacts between mobile devices are
seen as opportunities to move data towards destination. In such networks data
dissemination is done using forwarding and is usually based on a
publish/subscribe model. Opportunistic data dissemination also raises questions
concerning user privacy and incentives. Such problems are addressed di?erently
by various opportunistic data dissemination techniques. In this paper we
analyze existing relevant work in the area of data dissemination in
opportunistic networks. We present the categories of a proposed taxonomy that
captures the capabilities of data dissemination techniques used in such
networks. Moreover, we survey relevant data dissemination techniques and
analyze them using the proposed taxonomy.

ABSTRACT_BEGIN
  Mobile Advertisement is a location-aware dissemination solution built on top
of a vehicular ad-hoc network. We envision a network of WiFi access points that
dynamically disseminate data to clients running on the car's smart device. The
approach can be considered an alternative to the static advertisement
billboards and can be useful to business companies wanting to dynamically
advertise their products and offers to people driving their car. The clients
can subscribe to information based on specific topics. We present design
solutions that use access points as emitters for transmitting messages to
wireless-enabled devices equipped on vehicles. We also present implementation
details for the evaluation of the proposed solution using a simulator designed
for VANET application. The results show that the application can be used for
transferring a significant amount of data even under difficult conditions, such
as when cars are moving at increased speeds, or the congested Wi-Fi network
causes significant packet loss.

ABSTRACT_BEGIN
  Wireless networking has become an important area of research in academic and
industry. The main objectives of this paper is to gain in-depth knowledge about
the Wi-Fi- WiMAX technology and how it works and understand the problems about
the WiFi- WiMAX technology in maintaining and deployment. The challenges in
wireless networks include issues like security, seamless handover, location and
emergency services, cooperation, and QoS.The performance of the WiMAX is better
than the Wi-Fi and also it provide the good response in the access. It's
evaluated the Quality of Service (Qos) in Wi-Fi compare with WiMAX and provides
the various kinds of security Mechanisms. Authentication to verify the identity
of the authorized communicating client stations. Confidentiality (Privacy) to
secure that the wirelessly conveyed information will remain private and
protected. Take necessary actions and configurations that are needed in order
to deploy Wi-Fi -WiMAX with increased levels of security and privacy

ABSTRACT_BEGIN
  Network calculus is a powerful methodology of characterizing queueing
processes and has wide applications, but few works on applying it to 802.11 by
far. In this paper, we take one of the first steps to analyze the backlog
bounds of an 802.11 wireless LAN using stochastic network calculus. In
particular, we want to address its effectiveness on bounding backlogs. We model
a wireless node as a single server with impairment service based on two
best-known models in stochastic network calculus: Jiang's and Ciucu's.
Interestingly, we find that the two models can derive equivalent stochastic
service curves and backlog bounds in our studied case. We prove that the
network-calculus bounds imply stable backlogs as long as the average rate of
traffic arrival is less than that of service, indicating the theoretical
effectiveness of stochastic network calculus in bounding backlogs. From A.
Kumar's 802.11 model, we derive the concrete stochastic service curve of an
802.11 node and its backlog bounds. We compare the derived bounds with ns-2
simulations and find that the former are very loose and we discuss the reasons.
And we show that the martingale and independent case analysis techniques can
improve the bounds significantly. Our work offers a good reference to applying
stochastic network calculus to practical scenarios.

ABSTRACT_BEGIN
  In this paper we investigate the usage of cognitive radio devices within the
service area of TV broadcast stations. Until now the main approach for a
cognitive radio to operate in the TV bands has been to register TV broadcast
stations locations and thus protecting the broadcast stations service area.
Through information about TV receivers location, we show that a cognitive radio
should be able to operate within this service area without causing harmful
interference to the TV receivers as defined by Ofcom and FCC. We provide
simulations based on real statistics from Norway that show that especially in
rural areas TV receiver registration can provide a substantial gain in terms of
exploitable frequencies for a cognitive radio.

ABSTRACT_BEGIN
  Service-oriented wireless sensor network(WSN) has been recently proposed as
an architecture to rapidly develop applications in WSNs. In WSNs, a query task
may require a set of services and may be carried out repetitively with a given
frequency during its lifetime. A service composition solution shall be provided
for each execution of such a persistent query task. Due to the energy saving
strategy, some sensors may be scheduled to be in sleep mode periodically. Thus,
a service composition solution may not always be valid during the lifetime of a
persistent query. When a query task needs to be conducted over a new service
composition solution, a routing update procedure is involved which consumes
energy. In this paper, we study service composition design which minimizes the
number of service composition solutions during the lifetime of a persistent
query. We also aim to minimize the total service composition cost when the
minimum number of required service composition solutions is derived. A greedy
algorithm and a dynamic programming algorithm are proposed to complete these
two objectives respectively. The optimality of both algorithms provides the
service composition solutions for a persistent query with minimum energy
consumption.

ABSTRACT_BEGIN
  By analyzing the source codes of ns-2, we discuss the simulated
implementations of wireless channels, network interfaces and mostly the 802.11
MAC protocol in ns-2. We also notice the "bugs" of the 802.11 simulation
compared with the reality, and present an extension to fading channels as well.

ABSTRACT_BEGIN
  Infrastructure-as-a-Service (IaaS) providers need to offer richer services to
be competitive while optimizing their resource usage to keep costs down. Richer
service offerings include new resource request models involving bandwidth
guarantees between virtual machines (VMs). Thus we consider the following
problem: given a VM request graph (where nodes are VMs and edges represent
virtual network connectivity between the VMs) and a real data center topology,
find an allocation of VMs to servers that satisfies the bandwidth guarantees
for every virtual network edge---which maps to a path in the physical
network---and minimizes congestion of the network.
  Previous work has shown that for arbitrary networks and requests, finding the
optimal embedding satisfying bandwidth requests is $\mathcal{NP}$-hard.
However, in most data center architectures, the routing protocols employed are
based on a spanning tree of the physical network. In this paper, we prove that
the problem remains $\mathcal{NP}$-hard even when the physical network topology
is restricted to be a tree, and the request graph topology is also restricted.
We also present a dynamic programming algorithm for computing the optimal
embedding in a tree network which runs in time $O(3^kn)$, where $n$ is the
number of nodes in the physical topology and $k$ is the size of the request
graph, which is well suited for practical requests which have small $k$. Such
requests form a large class of web-service and enterprise workloads. Also, if
we restrict the requests topology to a clique (all VMs connected to a virtual
switch with uniform bandwidth requirements), we show that the dynamic
programming algorithm can be modified to output the minimum congestion
embedding in time $O(k^2n)$.

ABSTRACT_BEGIN
  In a 2002 paper, Che and co-authors proposed a simple approach for estimating
the hit rates of a cache operating the least recently used (LRU) replacement
policy. The approximation proves remarkably accurate and is applicable to quite
general distributions of object popularity. This paper provides a mathematical
explanation for the success of the approximation, notably in configurations
where the intuitive arguments of Che, et al clearly do not apply. The
approximation is particularly useful in evaluating the performance of current
proposals for an information centric network where other approaches fail due to
the very large populations of cacheable objects to be taken into account and to
their complex popularity law, resulting from the mix of different content types
and the filtering effect induced by the lower layers in a cache hierarchy.

ABSTRACT_BEGIN
  In this paper, we evaluate and analyze the impact of different network loads
and varying no. of nodes on distance vector and link state routing algorithms.
We select three well known proactive protocols; Destination Sequenced Distance
Vector (DSDV) operates on distance vector routing, while Fisheye State Routing
(FSR) and Optimized Link State Routing (OLSR) protocols are based on link state
routing. Further, we evaluate and compare the effects on the performance of
protocols by changing the routing strategies of routing algorithms. We also
enhance selected protocols to achieve high performance. We take throughput,
End-to-End Delay (E2ED) and Normalized Routing Load (NRL) as performance
metrics for evaluation and comparison of chosen protocols both with default and
enhanced versions. Based upon extensive simulations in NS-2, we compare and
discuss performance trade-offs of the protocols, i.e., how a protocol achieves
high packet delivery by paying some cost in the form of increased E2ED and/or
routing overhead. FSR due to scope routing technique performs well in high data
rates, while, OLSR is more scalable in denser networks due to limited
retransmissions through Multi-Point Relays (MPRs).

ABSTRACT_BEGIN
  In this paper, we experimentally investigate the effect of fast moving object
on the RSSI in the wireless sensor networks in presence of the ground effect
and antenna orientation in elevation direction. In experimental setup, MICAz
mote pair was placed on the ground, where one mote acts as a transmitter and
the other as a receiver. The trans- mitter mote's antenna was oriented in
elevation direction with respect to the receiver mote's antenna. The fast
moving object i.e. car, was passed between the motes and the fluctuations in
the RSSI are observed. The experimental results show some sequential pattern in
RSSI fluctuations when car moves at some relatively slow speed. However, some
irregu- larities were also observed when antenna was oriented at 45 and 90 in
elevation direction.

ABSTRACT_BEGIN
  This paper considers peer-to-peer scheduling for a network with multiple
wireless devices. A subset of the devices are mobile users that desire specific
files. Each user may already have certain popular files in its cache. The
remaining devices are access points that typically have access to a larger set
of files. Users can download packets of their requested file from an access
point or from a nearby user. Our prior work optimizes peer scheduling in a
general setting, but the resulting delay can be large when applied to mobile
networks. This paper focuses on the mobile case, and develops a new algorithm
that reduces delay by opportunistically grabbing packets from current
neighbors. However, it treats a simpler model where each user desires a single
file with infinite length. An algorithm that provably optimizes throughput
utility while incentivizing participation is developed for this case. The
algorithm extends as a simple heuristic in more general cases with finite file
sizes and random active and idle periods.

ABSTRACT_BEGIN
  The Internet Threat Monitoring (ITM) is an efficient monitoring system used
globally to measure, detect, characterize and track threats such as denial of
service (DoS) and distributed Denial of Service (DDoS) attacks and worms. . To
block the monitoring system in the internet the attackers are targeted the ITM
system. In this paper we address the flooding attack of DDoS against ITM
monitors to exhaust the network resources, such as bandwidth, computing power,
or operating system data structures by sending the malicious traffic. We
propose an information-theoretic frame work that models the flooding attacks
using Botnet on ITM. One possible way to counter DDoS attacks is to trace the
attack sources and punish the perpetrators. we propose a novel traceback method
for DDoS using Honeypots. IP tracing through honeypot is a single packet
tracing method and is more efficient than commonly used packet marking
techniques.

ABSTRACT_BEGIN
  In recent years, the static shortest path (SP) problem has been well
addressed using intelligent optimization techniques, e.g., artificial neural
networks, genetic algorithms (GAs), particle swarm optimization, etc. However,
with the advancement in wireless communications, more and more mobile wireless
networks appear, e.g., mobile networks [mobile ad hoc networks (MANETs)],
wireless sensor networks, etc. One of the most important characteristics in
mobile wireless networks is the topology dynamics, i.e., the network topology
changes over time due to energy conservation or node mobility. Therefore, the
SP routing problem in MANETs turns out to be a dynamic optimization problem.
GA's are able to find, if not the shortest, at least an optimal path between
source and destination in mobile ad-hoc network nodes. And we obtain the
alternative path or backup path to avoid reroute discovery in the case of link
failure or node failure.

ABSTRACT_BEGIN
  In the recent evolution of wireless technologies, the power management has
been a worrying factor. In order to overcome the power shortage, steps are
taken to find new kind of energy harvesting methods, power attenuation
reduction methods and power saving techniques. Wireless routers even though
consume not much of power, battery powered devices require a lot. Omni
directional antenna embedded with multiple antennae focusing the beam of radio
wave signals in the direction of nodes with least transmission angle can be a
solution for this problem which is called as "Smart Antenna". To reduce power
maceration we are going for adaptive and dynamic transmission wherein the
transmission angle of antennae is varied in accordance with the movement of
nodes. Apart from saving the power considerably, it also improves the signal
strength

ABSTRACT_BEGIN
  Wireless low-power transceivers used in sensor networks such as IEEE 802.15.4
typically operate in unlicensed frequency bands that are subject to external
interference from devices transmitting at much higher power. Communication
protocols should therefore be designed to be robust against such interference.
A critical building block of many protocols at all layers is agreement on a
piece of information among a set of nodes. At the MAC layer, nodes may need to
agree on a new time slot or frequency channel; at the application layer nodes
may need to agree on handing over a leader role from one node to another.
Message loss caused by interference may break agreement in two different ways:
none of the nodes use the new information (time slot, channel, leader) and
stick with the previous assignment, or - even worse - some nodes use the new
information and some do not. This may lead to reduced performance or failures.
In this paper we investigate the problem of agreement under interference and
point out the limitations of the traditional message-based n-way handshake
approach. We propose novel protocols that use jamming instead of message
transmissions and show that they outperform the n-way handshake in terms of
agreement probability, energy consumption, and time-to-completion both in the
unicast case (two neighboring nodes agree) as well as in the broadcast case
(any number of neighboring nodes agree).

ABSTRACT_BEGIN
  IEEE 802.11 is a widely used wireless LAN standard for medium access control.
TCP is a prominent transport protocol originally designed for wired networks.
TCP treats packet loss as congestion and reduces the data rate. In wireless
networks packets are lost not only due to congestion but also due to various
other reasons. Hence there is need for making TCP adaptable to wireless
networks. Various parameters of TCP and IEEE 802.11 can be set to appropriate
values to achieve optimum performance results. In this paper optimum values for
various parameters of IEEE 802.11 are determined. Network simulator NS2 is used
for simulation.

ABSTRACT_BEGIN
  The performance of a cellular network can be significantly improved by
employing many base stations (BSs), which shortens transmission distances.
However, there exist no known results on quantifying the performance gains from
deploying many BSs. To address this issue, we adopt a stochastic-geometry model
of the downlink cellular network and analyze the mobile outage probability.
Specifically, given Poisson distributed BSs, the outage probability is shown to
diminish inversely with the increasing ratio between the BS and mobile
densities. Furthermore, we analyze the optimal tradeoff between the performance
gain from increasing the BS density and the resultant network cost accounting
for energy consumption, BS hardware and backhaul cables. The optimal BS density
is proved to be proportional to the square root of the mobile density and the
inverse of the square root of the cost factors considered.

ABSTRACT_BEGIN
  Deployment of sensor network in hostile environment makes it mainly
vulnerable to battery drainage attacks because it is impossible to recharge or
replace the battery power of sensor nodes. Among different types of security
threats, low power sensor nodes are immensely affected by the attacks which
cause random drainage of the energy level of sensors, leading to death of the
nodes. The most dangerous type of attack in this category is sleep deprivation,
where target of the intruder is to maximize the power consumption of sensor
nodes, so that their lifetime is minimized. Most of the existing works on sleep
deprivation attack detection involve a lot of overhead, leading to poor
throughput. The need of the day is to design a model for detecting intrusions
accurately in an energy efficient manner. This paper proposes a hierarchical
framework based on distributed collaborative mechanism for detecting sleep
deprivation torture in wireless sensor network efficiently. Proposed model uses
anomaly detection technique in two steps to reduce the probability of false
intrusion.

ABSTRACT_BEGIN
  Security of Wireless sensor network (WSN) becomes a very important issue with
the rapid development of WSN that is vulnerable to a wide range of attacks due
to deployment in the hostile environment and having limited resources.
Intrusion detection system is one of the major and efficient defensive methods
against attacks in WSN. A particularly devastating attack is the sleep
deprivation attack, where a malicious node forces legitimate nodes to waste
their energy by resisting the sensor nodes from going into low power sleep
mode. The goal of this attack is to maximize the power consumption of the
target node, thereby decreasing its battery life. Existing works on sleep
deprivation attack have mainly focused on mitigation using MAC based protocols,
such as S-MAC, T-MAC, B-MAC, etc. In this article, a brief review of some of
the recent intrusion detection systems in wireless sensor network environment
is presented. Finally, we propose a framework of cluster based layered
countermeasure that can efficiently mitigate sleep deprivation attack in WSN.
Simulation results on MATLAB exhibit the effectiveness of the proposed model in
detecting sleep-deprivation attacks.

ABSTRACT_BEGIN
  It is suggested to use multi-layer graphs as a mathematical model in the
design of MPLS networks. The application of this model makes it possible to
design multi-service telecommunication systems simultaneously at several levels
and to reduce the problem to the search of the minimum weight graph.

ABSTRACT_BEGIN
  This study is devoted to the problem of parametric synthesis of multi-service
telecommunication sys-tems. The main characteristics of telecommunication
systems, which are brought to account in an article, are multilayer structure
formed by the overlayed networks and presence flows with self-similarity
effect. For accounting these features of modern telecommunications systems is
proposed to use a multi-layered graph for describing the system structure, and
self-similar processes model for modeling flows in a network. Solution of
parametric synthesis problem is reduced to a nonlinear programming problem
which is solved by using gradient descent method.

ABSTRACT_BEGIN
  Considered in this paper is the method of describingc of telecommunications
systems providing VoD service using multi-layer graph. The paper describes the
relations between the structural elements at each hierarchical level of the
multi-layer graph. Transfer of video is a resource consuming task, and it
requires an optimal configuration of the studied system. The usage of the
multi-layer graph makes it possible to consider the telecommunication system as
a whole and avoid falling in the local optimums when solving optimization
problems.

ABSTRACT_BEGIN
  In this paper, we analyze the performance of cooperative content caching in
vehicular ad hoc networks (VANETs). In particular, we characterize, using
analysis and simulations, the behavior of the probability of outage (i.e. not
finding a requested data chunk at a neighbor) under freeway vehicular mobility.
First, we introduce a formal definition for the probability of outage in the
context of cooperative content caching. Second, we characterize, analytically,
the outage probability under vehicular and random mobility scenarios. Next, we
verify the analytical results using simulations and compare the performance
under a number of plausible mobility scenarios. This provides key insights into
the problem and the involved trade-offs and enable us to assess the potential
opportunity offered by the, somewhat structured, vehicular mobility that can be
exploited by cooperative content caching schemes. The presented numerical
results exhibit complete agreement between the analytical and simulation
studies. Finally, we observe that vehicular mobility creates opportunities for
enhanced outage performance under practically relevant scenarios.

ABSTRACT_BEGIN
  We consider the problem of routing packets across a multi-hop network
consisting of multiple sources of traffic and wireless links while ensuring
bounded expected delay. Each packet transmission can be overheard by a random
subset of receiver nodes among which the next relay is selected
opportunistically.
  The main challenge in the design of minimum-delay routing policies is
balancing the trade-off between routing the packets along the shortest paths to
the destination and distributing traffic according to the maximum backpressure.
Combining important aspects of shortest path and backpressure routing, this
paper provides a systematic development of a distributed opportunistic routing
policy with congestion diversity ({D-ORCD}).
  {D-ORCD} uses a measure of draining time to opportunistically identify and
route packets along the paths with an expected low overall congestion. {D-ORCD}
is proved to ensure a bounded expected delay for all networks and under any
admissible traffic. Furthermore, this paper proposes a practical implementation
which empirically optimizes critical algorithm parameters and their effects on
delay as well as protocol overhead. Realistic Qualnet simulations for
802.11-based networks demonstrate a significant improvement in the average
delay over comparative solutions in the literature. %Finally, various practical
modifications to {D-ORCD} are proposed and their performance are evaluated.

ABSTRACT_BEGIN
  Motivated by the benefits of small world networks, we propose a
self-organization framework for wireless ad hoc networks. We investigate the
use of directional beamforming for creating long-range short cuts between
nodes. Using simulation results for randomized beamforming as a guideline, we
identify crucial design issues for algorithm design. Our results show that,
while significant path length reduction is achievable, this is accompanied by
the problem of asymmetric paths between nodes. Subsequently, we propose a
distributed algorithm for small world creation that achieves path length
reduction while maintaining connectivity. We define a new centrality measure
that estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. We show,
using simulations, that this leads to significant reduction in path length
while maintaining connectivity.

ABSTRACT_BEGIN
  Due to deployment in hostile environment, wireless sensor network is
vulnerable to various attacks. Exhausted sensor nodes in sensor network become
a challenging issue because it disrupts the normal connectivity of the network.
Affected nodes give rise to denial of service that resists to get the objective
of sensor network in real life. A mathematical model based on Absorbing Markov
Chain (AMC)is proposed for Denial of Sleep attack detection in sensor network.
In this mechanism, whether sensor network is affected by denial of sleep attack
or not can be decided by considering expected death time of sensor network
under normal scenario.

ABSTRACT_BEGIN
  An increasing number of retail energy markets show price fluctuations,
providing users with the opportunity to buy energy at lower than average
prices. We propose to temporarily store this inexpensive energy in a battery,
and use it to satisfy demand when energy prices are high, thus allowing users
to exploit the price variations without having to shift their demand to the
low-price periods. We study the battery control policy that yields the best
performance, i.e., minimizes the total discounted costs. The optimal policy is
shown to have a threshold structure, and we derive these thresholds in a few
special cases. The cost savings obtained from energy storage are demonstrated
through extensive numerical experiments, and we offer various directions for
future research.

ABSTRACT_BEGIN
  We consider wireless mesh networks and the problem of routing end-to-end
traffic over multiple paths for the same origin-destination pair with minimal
interference. We introduce a heuristic for path determination with two
distinguishing characteristics. First, it works by refining an extant set of
paths, determined previously by a single- or multi-path routing algorithm.
Second, it is totally local, in the sense that it can be run by each of the
origins on information that is available no farther than the node's immediate
neighborhood. We have conducted extensive computational experiments with the
new heuristic, using AODV and OLSR, as well as their multi-path variants, as
underlying routing methods. For two different CSMA settings (as implemented by
802.11) and one TDMA setting running a path-oriented link scheduling algorithm,
we have demonstrated that the new heuristic is capable of improving the average
throughput network-wide. When working from the paths generated by the
multi-path routing algorithms, the heuristic is also capable to provide a more
evenly distributed traffic pattern.

ABSTRACT_BEGIN
  Greedy Maximal Scheduling (GMS) is an attractive low-complexity scheme for
scheduling in wireless networks. Recent work has characterized its throughput
for the case when there is no fading/channel variations. This paper aims to
understand the effect of channel variations on the relative throughput
performance of GMS vis-a-vis that of an optimal scheduler facing the same
fading. The effect is not a-priori obvious because, on the one hand, fading
could help by decoupling/precluding global states that lead to poor GMS
performance, while on the other hand fading adds another degree of freedom in
which an event unfavourable to GMS could occur.
  We show that both these situations can occur when fading is adversarial. In
particular, we first define the notion of a {\em Fading Local Pooling factor
(F-LPF)}, and show that it exactly characterizes the throughput of GMS in this
setting. We also derive general upper and lower bounds on F-LPF. Using these
bounds, we provide two example networks - one where the relative performance of
GMS is worse than if there were no fading, and one where it is better.

ABSTRACT_BEGIN
  Opportunistic Routing (OR) is a novel routing technique for wireless mesh
networks that exploits the broadcast nature of the wireless medium. OR combines
frames from multiple receivers and therefore creates a form of Spatial
Diversity, called MAC Diversity. The gain from OR is especially high in
networks where the majority of links has a high packet loss probability. The
updated IEEE 802.11n standard improves the physical layer with the ability to
use multiple transmit and receive antennas, i.e. Multiple-Input and
Multiple-Output (MIMO), and therefore already offers spatial diversity on the
physical layer, i.e. called Physical Diversity, which improves the reliability
of a wireless link by reducing its error rate. In this paper we quantify the
gain from MAC diversity as utilized by OR in the presence of PHY diversity as
provided by a MIMO system like 802.11n. We experimented with an IEEE 802.11n
indoor testbed and analyzed the nature of packet losses. Our experiment results
show negligible MAC diversity gains for both interference-prone 2.4 GHz and
interference-free 5 GHz channels when using 802.11n. This is different to the
observations made with single antenna systems based on 802.11b/g, as well as in
initial studies with 802.11n.

ABSTRACT_BEGIN
  The growing diffusion of wireless-enabled portable devices and the recent
advances in Mobile Ad-hoc NETworks (MANETs) open new scenarios where users can
benefit from anywhere and at any time for impromptu collaboration. However,
energy constrained nodes, low channel bandwidth, node mobility, high channel
error rates, channel variability and packet loss are some of the limitations of
MANETs. MANETs presents also security challenges. These networks are prone to
malicious users attack, because any device within the frequency range can get
access to the MANET. There is a need for security mechanisms aware of these
challenges. Thus, this work aims to provide a secure MANET by changing the
frequency of data transmission. This security approach was tested, and the
results shows an interesting decreased of throughput from malicious node when
the number of frequency used is increased, that way the MANET will not waste
it's resources treating malicious packets. The other contribution of this work
is a mobility aware routing approach, which aims to provide a more reliable
routing by handling effectively the nodes mobility.

ABSTRACT_BEGIN
  Operational network data, management data such as customer care call logs and
equipment system logs, is a very important source of information for network
operators to detect problems in their networks. Unfortunately, there is lack of
efficient tools to automatically track and detect anomalous events on
operational data, causing ISP operators to rely on manual inspection of this
data. While anomaly detection has been widely studied in the context of network
data, operational data presents several new challenges, including the
volatility and sparseness of data, and the need to perform fast detection
(complicating application of schemes that require offline processing or
large/stable data sets to converge).
  To address these challenges, we propose Tiresias, an automated approach to
locating anomalous events on hierarchical operational data. Tiresias leverages
the hierarchical structure of operational data to identify high-impact
aggregates (e.g., locations in the network, failure modes) likely to be
associated with anomalous events. To accommodate different kinds of operational
network data, Tiresias consists of an online detection algorithm with low time
and space complexity, while preserving high detection accuracy. We present
results from two case studies using operational data collected at a large
commercial IP network operated by a Tier-1 ISP: customer care call logs and
set-top box crash logs. By comparing with a reference set verified by the ISP's
operational group, we validate that Tiresias can achieve >94% accuracy in
locating anomalies. Tiresias also discovered several previously unknown
anomalies in the ISP's customer care cases, demonstrating its effectiveness.

ABSTRACT_BEGIN
  The IEEE 802.15.4 is a wireless standard introduced for low power, low cost
wireless communication with moderate data rates. In the next few years, it is
expected that Low Rate Wireless Personal Area Networks (LR-WPAN) will be used
in a wide variety of embedded applications, including home automation,
industrial sensing and control, environmental monitoring and sensing. In these
applications, numerous embedded devices running on batteries are distributed in
an area communicating via wireless radios. This work presents a method which
can be used for comparing current consumption of wireless data transfer
embedded systems. This paper implements a small subset of the IEEE 802.15.4
protocol to achieve a point to point communication. The implemented protocol
uses 802.15.4 MAC compliant data and acknowledgment packets. Current
consumption is measured while doing one data packet transmission. Measurements
are compared with existing work. IEEE 802.15.4 protocol implementation is done
using Verilog language. Code implementation is done in such a manner so that it
can be ported to any platform with minimal changes. It can also be modified to
suit any special experimental setup requirements.

ABSTRACT_BEGIN
  In the cloud computing application area of accomplish, we find the fact that
cloud computing covers a lot of areas are its main asset. At a top level, it is
an approach to IT where many users, some even from different companies get
access to shared IT resources such as servers, routers and various file
extensions, instead of each having their own dedicated servers. This offers
many advantages like lower costs and higher efficiency. Unfortunately there
have been some high profile incidents where some of the largest cloud providers
have had outages and even lost data, and this underscores that it is important
to have backup, security and disaster recovery capabilities. In education
field, it gives better choice and flexibility to IT departments than others.
The platform and applications you use can be on-premises, off-premises, or a
combination of both, depending on your academic organization's needs. With
cloud computing in education, you get powerful software and massive computing
resources where and when you need them. Use cloud services to best combine:
*On-demand computing and storage. *A familiar development experience with
on-demand scalability. *Online services for anywhere, anytime access to
powerful web-based tools.

ABSTRACT_BEGIN
  One of the major task of wireless sensor network is to sense accurate data
from the physical environment. Hence in this paper, we develop an estimated
data accuracy model for randomly deployed sensor nodes which can sense more
accurate data from the physical environment. We compare our results with other
information accuracy models and shows that our estimated data accuracy model
performs better than the other models. Moreover we simulate our estimated data
accuracy model under such situation when some of the sensor nodes become
malicious due to extreme physical environment. Finally using our estimated data
accuracy model we construct a probabilistic approach for selecting an optimal
set of sensor nodes from the randomly deployed maximal set of sensor nodes in
the network.

ABSTRACT_BEGIN
  This paper deals with multi-user detection techniques in asynchronous
multibeam satellite communications. The proposed solutions are based on
successive interference cancellation architecture (SIC) and channel decoding
algorithms. The aim of these detection methods is to reduce the effect of
cochannel interference due to co-frequency access, and consequently, improves
the capacity of the mulitbeam communications systems, by improving frequency
reuse. Channel estimation allows the determination of interference
coefficients, which helps their effects compensation. The developed multiuser
detections techniques are iterative. Therefore, detection quality is improved
from a stage to another. Moreover, a signals combining method, which is
integrated into these detection solutions, enhances their capability. The
proposed solutions are evaluated through computer simulations, where an
asynchronous multibeam satellite link is considered over an AWGN channel. The
obtained simulation results showed the robustness of these multi-user detection
techniques.

ABSTRACT_BEGIN
  Accurate estimation of licensed channel Primary User's (PU) temporal
statistics is important for Dynamic Spectrum Access (DSA) systems. With
accurate estimation of the mean duty cycle, u, and the mean off- and on-times
of PUs, DSA systems can more efficiently assign PU resources to its
subscribers, thus, increasing channel utilization. This paper presents a
mathematical analysis of the accuracy of estimating u, as well as the PU mean
off- and on-times, where the estimation accuracy is expressed as the mean
squared estimation error. The analysis applies for the traffic model assuming
exponentially distributed PU off- and on-times, which is a common model in
traffic literature. The estimation accuracy is quantified as a function of the
number of samples and observation window length, hence, this work provides
guidelines on traffic parameters estimation for both energy-constrained and
delay-constrained applications. For estimating u, we consider uniform,
non-uniform, and weighted sample stream averaging, as well as maximum
likelihood estimation. The estimation accuracy of the mean PU off- and on-times
is studied when maximum likelihood estimation is employed. Furthermore, we
develop algorithms for the blind estimation of the traffic parameters based on
the derived theoretical estimation accuracy expressions. We show that the
estimation error for all traffic parameters is lower bounded for a fixed
observation window length due to the correlation between the traffic samples.
Moreover, we prove that for estimating u, maximum likelihood estimation can
yield the same estimation error as weighted sample averaging using only half
the observation window length.

ABSTRACT_BEGIN
  In this paper, we compare and analyze performance of five quality link
metrics forWireless Multi-hop Networks (WMhNs). The metrics are based on loss
probability measurements; ETX, ETT, InvETX, ML and MD, in a distance vector
routing protocol; DSDV. Among these selected metrics, we have implemented ML,
MD, InvETX and ETT in DSDV which are previously implemented with different
protocols; ML, MD, InvETX are implemented with OLSR, while ETT is implemented
in MR-LQSR. For our comparison, we have selected Throughput, Normalized Routing
Load (NRL) and End-to-End Delay (E2ED) as performance parameters. Finally, we
deduce that InvETX due to low computational burden and link asymmetry
measurement outperforms among all metrics.

ABSTRACT_BEGIN
  In case of high dynamic topology, reactive routing protocols provide quick
convergence by faster route discoveries and route maintenance. Frequent
roadcasts reduce routing efficiency in terms of broadcast cost; Bk, and
expected time cost; E[t]. These costs are optimized using different mechanisms.
So, we select three reactive routing protocols; Ad-hoc On-demand Distance
Vector (AODV), Dynamic Source Routing (DSR), and DYnamic Manet On-demad (DYMO).
We model expanding Ring Search (ERS); an optimization mechanism in the selected
protocols to reduce Bk and E[t]. A novel contribution of this work is
enhancement of default ERS in the protocols to optimize Bk and E[t]. Using
NS-2, we evaluate and compare default-ERS used by these protocols; AODV-ERS1,
DSR-ERS1 and DYMO-ERS1 with enhanced-ERS; AODVERS2, DSR-ERS2 and DYMO-ERS2.
From modeling and analytical comparison, we deduce that by adjusting
Time-To-Live (T TL) value of a network, efficient optimizations of Bk and E[t]
can be achieved.

ABSTRACT_BEGIN
  Recently, low-complexity and distributed Carrier Sense Multiple Access
(CSMA)-based scheduling algorithms have attracted extensive interest due to
their throughput-optimal characteristics in general network topologies.
However, these algorithms are not well-suited for serving real-time traffic
under time-varying channel conditions for two reasons: (1) the mixing time of
the underlying CSMA Markov Chain grows with the size of the network, which, for
large networks, generates unacceptable delay for deadline-constrained traffic;
(2) since the dynamic CSMA parameters are influenced by the arrival and channel
state processes, the underlying CSMA Markov Chain may not converge to a
steady-state under strict deadline constraints and fading channel conditions.
  In this paper, we attack the problem of distributed scheduling for serving
real-time traffic over time-varying channels. Specifically, we consider
fully-connected topologies with independently fading channels (which can model
cellular networks) in which flows with short-term deadline constraints and
long-term drop rate requirements are served. To that end, we first characterize
the maximal set of satisfiable arrival processes for this system and, then,
propose a Fast-CSMA (FCSMA) policy that is shown to be optimal in supporting
any real-time traffic that is within the maximal satisfiable set. These
theoretical results are further validated through simulations to demonstrate
the relative efficiency of the FCSMA policy compared to some of the existing
CSMA-based algorithms.

ABSTRACT_BEGIN
  We study the cost of improving the goodput, or the useful data rate, to user
in a wireless network. We measure the cost in terms of number of base stations,
which is highly correlated to the energy cost as well as capital and
operational costs of a network provider.We show that increasing the available
bandwidth, or throughput, may not necessarily lead to increase in goodput,
particularly in lossy wireless networks in which TCP does not perform well. As
a result, much of the resources dedicated to the user may not translate to high
goodput, resulting in an inefficient use of the network resources. We show that
using protocols such as TCP/NC, which are more resilient to erasures and
failures in the network, may lead to a goodput commensurate the throughput
dedicated to each user. By increasing goodput, users' transactions are
completed faster; thus, the resources dedicated to these users can be released
to serve other requests or transactions. Consequently, we show that translating
efficiently throughput to goodput may bring forth better connection to users
while reducing the cost for the network providers.

ABSTRACT_BEGIN
  In 802.11 WLANs, adapting the contention parameters to network conditions
results in substantial performance improvements. Even though the ability to
change these parameters has been available in standard devices for years, so
far no adaptive mechanism using this functionality has been validated in a
realistic deployment. In this paper we report our experiences with implementing
and evaluating two adaptive algorithms based on control theory, one centralized
and one distributed, in a large-scale testbed consisting of 18 commercial
off-the-shelf devices. We conduct extensive measurements, considering different
network conditions in terms of number of active nodes, link qualities and
traffic generated. We show that both algorithms significantly outperform the
standard configuration in terms of total throughput. We also identify the
limitations inherent in distributed schemes, and demonstrate that the
centralized approach substantially improves performance under a large variety
of scenarios, which confirms its suitability for real deployments.

ABSTRACT_BEGIN
  VANET (Vehicular Ad-hoc Network) is a new technology which has taken enormous
attention in the recent years. Vehicular ad hoc network is formed by cars which
are called nodes; allow them to communicate with one another without using any
fixed road side unit. It has some unique characteristics which make it
different from other ad hoc network as well as difficult to define any exact
mobility model and routing protocols because of their high mobility and
changing mobility pattern. Hence performance of routing protocols can vary with
the various parameters such as speed, pause time, node density and traffic
scenarios. In this research paper, the performance of two on-demand routing
protocols AODV & DSR has been analyzed by means of packet delivery ratio, loss
packet ratio & average end-to-end delay with varying pause time, speed time and
node density under TCP & CBR connection.

ABSTRACT_BEGIN
  The vehicle-to-vehicle (V2V) propagation channel has significant implications
on the design and performance of novel communication protocols for vehicular ad
hoc networks (VANETs). Extensive research efforts have been made to develop V2V
channel models to be implemented in advanced VANET system simulators for
performance evaluation. The impact of shadowing caused by other vehicles has,
however, largely been neglected in most of the models, as well as in the system
simulations. In this paper we present a shadow fading model targeting system
simulations based on real measurements performed in urban and highway
scenarios. The measurement data is separated into three categories,
line-of-sight (LOS), obstructed line-of-sight (OLOS) by vehicles, and non
line-of-sight due to buildings, with the help of video information recorded
during the measurements. It is observed that vehicles obstructing the LOS
induce an additional attenuation of about 10 dB in the received signal power.
An approach to incorporate the LOS/OLOS model into existing VANET simulators is
also provided. Finally, system level VANET simulation results are presented,
showing the difference between the LOS/OLOS model and a channel model based on
Nakagami-m fading.

ABSTRACT_BEGIN
  In order to curtail the escalating packet loss rates caused by an exponential
increase in network traffic, active queue management techniques such as Random
Early Detection (RED) have come into picture. Flow Random Early Drop (FRED)
keeps state based on instantaneous queue occupancy of a given flow. FRED
protects fragile flows by deterministically accepting flows from low bandwidth
connections and fixes several shortcomings of RED by computing queue length
during both arrival and departure of the packet. Stochastic Fair Queuing (SFQ)
ensures fair access to network resources and prevents a busty flow from
consuming more than its fair share. In case of (Random Exponential Marking)
REM, the key idea is to decouple congestion measure from performance measure
(loss, queue length or delay). Stabilized RED (SRED) is another approach of
detecting nonresponsive flows. In this paper, we have shown a comparative
analysis of throughput, delay and queue length for the various congestion
control algorithms RED, SFQ and REM. We also included the comparative analysis
of loss rate having different bandwidth for these algorithms.

ABSTRACT_BEGIN
  In this paper we describe three methods for localizing a wireless sensor
network node, using anchor nodes in its neighbourhood, when there is an error
in distance estimation present. We use the intersection points of the circles
formed with the estimated distances from each anchors and we apply different
methods to form clusters. We then use the cluster points to calculate the final
position.

ABSTRACT_BEGIN
  The emerging technology of wireless sensor network (WSN) is expected to
provide a broad range of applications, such as battlefield surveillance,
environmental monitoring, smart spaces and so on. The coverage problem is a
fundamental issue in WSN, which mainly concerns with a fundamental question:
How well a sensor field is observed by the deployed sensors? Mobility is
exploited to improve area coverage in a kind of hybrid sensor networks. The
main objective for using mobile sensor nodes is to heal coverage holes after
the initial network deployment, when designing a hole healing algorithm, the
following issues need to be addressed. First, how to decide the existence of a
coverage hole and how to estimate the size of a hole. Second, what are the best
target locations to relocate mobile nodes to repair coverage holes? We use the
triangular oriented diagram (HSTT) for aim to goal where its simple, have low
calculation among construction and it is great to calculate the size of hole
exactly .

ABSTRACT_BEGIN
  Simulation results for Mobile Ad-Hoc Networks (MANETs) are fundamentally
governed by the underlying Mobility Model. Thus it is imperative to find
whether events functionally dependent on the mobility model 'converge' to well
defined functions or constants. This shall ensure the long-run consistency
among simulation performed by disparate parties. This paper reviews a work on
the discrete Random Waypoint Mobility Model (RWMM), addressing its long run
stochastic stability. It is proved that each model in the targeted discrete
class of the RWMM satisfies Birkhoff's pointwise ergodic theorem [13], and
hence time averaged functions on the mobility model surely converge. We also
simulate the most common and general version of the RWMM to give insight into
its working.

ABSTRACT_BEGIN
  We study link scheduling in wireless networks under stochastic arrival
processes of packets, and give an algorithm that achieves stability in the
physical (SINR) interference model. The efficiency of such an algorithm is the
fraction of the maximum feasible traffic that the algorithm can handle without
queues growing indefinitely. Our algorithm achieves two important goals: (i)
efficiency is independent of the size of the network, and (ii) the algorithm is
fully distributed, i.e., individual nodes need no information about the overall
network topology, not even local information.

ABSTRACT_BEGIN
  Modern consumer devices, like smartphones and tablets, have multiple
interfaces (e.g., WiFi and 3G) that attach to new access points as users move.
These mobile, multi-homed computers are a poor match with an Internet
architecture that binds connections to fixed end-points with topology-
dependent addresses. As a result, hosts typically cannot spread a connection
over multiple interfaces or paths, or change locations without breaking
existing connections.
  In this paper, we introduce ECCP, an end-host connection control protocol
that allows hosts to communicate over mul- tiple interfaces with
dynamically-changing IP addresses. Each ECCP connection consists of one or more
flows, each associated with an interface or path. A host can move an existing
flow from one interface to another or change the IP address using in-band
signaling, without any support from the underlying network. We use formal
models to verify that ECCP works correctly in the presence of packet loss,
out-of-order delivery, and frequent mobility, and to identify bugs and design
limitations in earlier mobility protocols.

ABSTRACT_BEGIN
  In this paper, we describe the implementation of a real time visualization
and feedback system for Wireless Sensor Network algorithms. The system is based
on a fixed hardware testbed, which is deployed on a vertical flat surface and a
feedback loop system that takes information about the current state of the
network and projects this state, in a visual way, on the surface itself using a
video projector. The protocol used is open and simple to use, and can be easily
adapted for different hardware configurations. We call our system Irida.

ABSTRACT_BEGIN
  In this paper, we consider a single-cell IEEE 802.16 environment in which the
base station allocates subchannels to the subscriber stations in its coverage
area. The subchannels allocated to a subscriber station are shared by multiple
connections at that subscriber station. To ensure the Quality of Service (QoS)
performances, two Connection Admission Control (CAC) mechanisms, namely,
threshold-based and queue-aware CAC mechanisms are considered at a subscriber
station. A queuing analytical framework for these admission control mechanisms
is presented considering Orthogonal Frequency Division Multiple Access (OFDMA)
based transmission at the physical layer. Then, based on the queuing model,
both the connection-level and the packet-level performances are studied and
compared with their analogues in the case without CAC. The connection arrival
is modeled by a Poisson process and the packet arrival for a connection by
Batch Markov Arrival Process (BMAP). We determine analytically and numerically
different QoS performance measures (connection blocking probability, average
number of ongoing connections, average queue length, packet dropping
probability, queue throughput and average packet delay).

ABSTRACT_BEGIN
  In this paper, we propose an enhancement of a blind channel estimator based
on a subspace approach in a MIMO OFDM context (Multi Input Multi Output
Orthogonal Frequency Division Multiplexing) in high mobility scenario. As
known, the combination between the MIMO context and the OFDM system has
stimulated mainly the evolution of the fourth generation broadband wireless
communications. The simulations results have demonstrated the effectiveness of
the approach for a 16 QAM modulation scheme and had been evaluated in term of
bit error rate BER and mean square error MSE versus the signal to noise ratio
SNR.

ABSTRACT_BEGIN
  Cellular network resources are essential to be optimized in Femto cells
equipped macro cell networks. This is achieved by increasing the cellular
coverage and channel capacity, and reducing power usage and interference
between femto cells and macro cells. In this paper, the optimization approach
for cellular resources with installed femto cells in macro cell networks has
been addressed by deploying smart antennas applications and effect power
adaptation method which significantly optimize the cellular coverage, channel
capacity, power usage, and intra and inter tier interference. The simulation
results also illustrate the outstanding performance of this optimization
methodology.

ABSTRACT_BEGIN
  Dynamic Position Location and Tracking (PL&T) is proposed deploying the
integrated approach of zone finding and triangulation using two friendly nodes
equipped with Steered Directional Antenna (DA) in Mobile Ad hoc Networks
(MANET). This approach allows the system to use only two references instead of
a typical 3 references for a straight triangulation. Moreover, the performance
of the proposed algorithm with references using directional antennas shows
significant improvement over triangulation using references with
Omnidirectional antennas as the beam power is concentrated. However, dynamic
switching of reference nodes is frequently required as the target moves outside
the predicted zone. This paper presents a better tracking accuracy in using
proposed dynamic PL&T as compared to other PL&T techniques. The multipath
fading is also addressed with the use of KV transform coding technique which
uses forward error correction and sample interleaving achieves greater than 90%
tracking accuracy with BERs of 10-6 or better.

ABSTRACT_BEGIN
  The nature of pre-determined and on-demand mobile network fabrics can be
exploited for real time Position Location and Tracking (PL&T) of radios and
sensors (nodes) for Global Positioning System (GPS) denied or GPS-free systems.
This issue is addressed by a novel system of integrated zone finding and
triangulation method for determining the PL&T of nodes when mobile network
fabrics are employed based on using directional antennas for radio
communications. Each mobile node is switched dynamically between being a
reference and a target node in PL&T operation to improve the PL&T accuracy of a
target node. This paper presents the Baseline PL&T with predictive Kalman
filter and Integrated Zone based PL&T algorithm design that integrates zone
finding and triangulation method. The performance of the proposed algorithm is
analysed using Interleaving-KV sample coding & error correction in Rayleigh and
Rician channel using Orthogonal Frequency Division Multiplexing (OFDM) system
under the severe multipath fading.

ABSTRACT_BEGIN
  Secured Position Location and Tracking (PL&T) scheme is developed for
multiple malicious radios or nodes detection using integrated key based strict
friendly scheme and position location and tracking by multi-sectored based
multiple target's PL&T. The friendly and malicious nodes detection is based on
the integrated key consisting of symmetric keys, geographic location and round
trip response time. Two strictly friend references dynamically form the
tracking zone over the detected multiple malicious nodes using the
multi-sectored adaptive beam forming. This PL&T technique is robust, precise,
scalable, and faster than using the single reference, two reference and three
reference nodes based PL&T method in the battlefield oriented Mobile Ad hoc
Networks. The simulation results show that the lower relative speed bound of
any participating node increased the switching overhead, the decreasing
received energy with increasing number of the multi-sectored beams reduced
tracking accuracy and the strict friendly authentication overhead depends upon
the time period between two latest periodic authentication failures.

ABSTRACT_BEGIN
  In this work we propose an UML modeling of the "Greedy Perimeter Stateless
Routing" (GPSR) protocol that integrate this geographic routing protocol, into
"JavaNetwork Simulator" to simulate and study this protocol in a first time and
offer some improvement in these features. Java Network Simulator (JNS) is a
project of "translation" of Network Simulator (NS) in Java initiated by "the
UCL Department of Computer Science". This simulator is not as complete as ns-2,
but it is much more accessible to programmers unfamiliar with Tcl. Java Network
Simulator does not support so far, no routing protocol for vehicular ad hoc
networks and all the routing decisions are made statically or using RIP and
OSPF. By modeling and integrating the routing protocol GPSR to JNS, users will
be able to understand the concept of the geographic routing and how the routing
information is transmitted and updated between nodes in vehicular ad hoc
network. The article first examines the architecture of the Java Network
Simulator, then gives a brief review of the routing protocol GPSR and finally
presents our UML modeling incorporating GPSR in the Java Network Simulator.

ABSTRACT_BEGIN
  This paper reports on the first systematic study of congestion-aware routing
algorithms for wireless mesh networks to achieve an improved end-end delay
performance. In particular, we compare 802.11 compatible implementations of a
set of congestion-aware routing protocols against our implementation of state
of the art shortest path routing protocol (SRCR). We implement congestion-aware
routing algorithms Backpressure (BP), Enhanced-Backpressure (E-BP) adapted from
[1], [2] suitably adjusted for 802.11 implementation. We then propose and
implement Congestion Diversity Protocol (CDP) adapted from [3] recognizing the
limitations of BP and E-BP for 802.11-based wireless networks. SRCR solely
utilizes link qualities, while BP relies on queue differential to route
packets. CDP and E-BP rely on distance metrics which take into account queue
backlogs and link qualities in the network. E-BP computes its metric by summing
the ETX and queue differential, while CDP determines its metric by calculating
the least draining time to the destination. Our small testbed consisting of
twelve 802.11g nodes enables us to empirically compare the performance of
congestion-aware routing protocols (BP, E-BP and CDP) against benchmark SRCR.
For medium to high load UDP traffic, we observe that CDP exhibits significant
improvement with respect to both end-end delay and throughput over other
protocols with no loss of performance for TCP traffic. Backpressure-based
routing algorithms (BP and E-BP) show poorer performance for UDP and TCP
traffic. Finally, we carefully study the effects of the modular approach to
congestion-aware routing design in which the MAC layer is left intact

ABSTRACT_BEGIN
  In cognitive radio networks, channel aggregation (CA) and channel
fragmentation (CF) techniques have been proposed to enhance the spectrum
utilization. While most of the literature studies CA and CF independently, in
this paper we combine CA and CF innovatively and present a new spectrum sharing
strategy named CAF (Channel Aggregation and Fragmentation). We elaborate on the
proposed CAF strategy and derive the balance equation by a continuous time
Markov chain (CTMC) model. Then various system performance metrics including
blocking probability, dropping probability, spectrum utilization and throughput
of the secondary network are evaluated. Both analytical and simulation results
show that our strategy lowers the blocking and dropping probabilities and
enhances the spectrum utilization and throughput effectively. Moreover, by
tuning the bandwidth requirement of each secondary user, different system
performance can be achieved.

ABSTRACT_BEGIN
  In this paper, we propose a system for wireless video transmission with a
wireless physical layer (PHY) that supports cooperative forwarding of
interfered/superimposed packets. Our system model considers multiple and
independent unicast transmissions between network nodes while a number of them
serve as relays of the interfered/superimposed signals. For this new PHY the
average transmission rate that each node can achieve is estimated first. Next,
we formulate a utility optimization framework for the video transmission
problem and we show that it can be simplified due to the features of the new
PHY. Simulation results reveal the system operating regions for which
superimposing wireless packets is a better choice than a typical cooperative
PHY.

ABSTRACT_BEGIN
  We study the problem of medium access control in domain of event-driven
wireless sensor networks (WSNs). In this kind of WSN, sensor nodes send data to
sink node only when an event occurs in the monitoring area. The nodes in this
kind of WSNs encounter correlated traffic as a subset of nodes start sending
data by sensing a common event simultaneously. We wish to rethink of medium
access control (MAC) for this type of traffic characteristics. For WSNs, many
existing MAC protocols utilize the basic CSMA/CA strategy such as IEEE 802.11
Binary Exponential Backoff (BEB) algorithm to handle the collisions among
packets when more than one node need to access the channel. We show that this
BEB algorithm does not work well without incurring access delay or performance
degradation due to increased number of collisions and retransmissions when
nodes encounter correlated traffic. Based on above observations in mind, We
present a Adaptive Random Backoff (ARB) algorithm that is capable of mitigating
the impact of correlated traffic and capable of minimizing the chance of
collisions. ARB is based on minor modifications of BEB. We show using numerical
analysis that our proposals improve the channel access in terms of latency,
throughput, and frame dropping probability as compared with IEEE 802.11 DCF.
Simulations using NS-2 network simulator are conducted to validate the
analytical results.

ABSTRACT_BEGIN
  Interconnecting multiple sensor networks is a relatively new research field
which has emerged in the Wireless Sensor Network domain. Wireless Sensor
Networks (WSNs) have typically been seen as logically separate, and few works
have considered interconnection and interaction between them. Interconnecting
multiple heterogeneous sensor networks therefore opens up a new field besides
more traditional research on, e.g., routing, self organization, or MAC layer
development. Up to now, some approaches have been proposed for interconnecting
multiple sensor networks with goals like information sharing or monitoring
multiple sensor networks. In this paper, we propose to utilize inter-WSN
communication to enable Collaborative Performance Optimization, i.e., our
approach aims to optimize the performance of individual WSNs by taking into
account measured information from others. The parameters to be optimized are
energy consumption on the one hand and sensing quality on the other.

ABSTRACT_BEGIN
  In this paper, we present a new queueing model providing the accurate average
system time for packets transmitted over a cognitive radio (CR) link for
multiple traffic classes with the preemptive and non-preemptive priority
service disciplines. The analysis considers general packet service time,
general distributions for the channel availability periods and service
interruption periods, and a service-resume transmission. We further introduce
and analyze two novel priority service disciplines for opportunistic spectrum
access (OSA) networks which take advantage of interruptions to preempt low
priority traffic at a low cost. Analytical results, in addition to simulation
results to validate their accuracy, are also provided and illustrate the impact
of different OSA network parameters on the average system time. We particularly
show that, for the same average CR transmission link availability, the packet
system time significantly increases in a semi-static network with long
operating and interruption periods compared to an OSA network with fast
alternating operating and interruption periods. We also present results
indicating that, due to the presence of interruptions, priority queueing
service disciplines provide a greater differentiated service in OSA networks
than in traditional networks. The analytical tools presented in this paper are
general and can be used to analyze the traffic metrics of most OSA networks
carrying multiple classes of traffic with priority queueing service
differentiation.

ABSTRACT_BEGIN
  This paper aims at designing of adaptive framework for supporting
collaborative work of different actors in public safety and disaster recovery
missions. In such scenarios, firemen and robots interact to each other to reach
a common goal; firemen team is equipped with smart devices and robots team is
supplied with communication technologies, and should carry on specific tasks.
Here, reliable connection is mandatory to ensure the interaction between
actors. But wireless access network and communication resources are vulnerable
in the event of a sudden unexpected change in the environment. Also, the
continuous change in the mission requirements such as inclusion/exclusion of
new actor, changing the actor's priority and the limitations of smart devices
need to be monitored. To perform dynamically in such case, the presented
framework is based on a generic multi-level modeling approach that ensures
adaptation handled by semantic modeling. Automated self-configuration is driven
by rule-based reconfiguration policies through ontology.

ABSTRACT_BEGIN
  In the past decade, significant research has been carried out for realizing
intelligent network routing using advertisement, position and near-optimum node
selection schemes. In this paper, a grade-based two-level node selection method
along with genetic algorithm (GA) is proposed for realizing an efficient
routing scheme. This method assumes that the nodes are intelligent and that
there exists a knowledge base about the environment in their local memory.
There are two levels for approaching the effective route selection process
through grading. At the first level, grade-based selection is applied and at
the second level, the optimum path is explored using GA. The simulation has
been carried out on different topological structures, and a significant
reduction in time is achieved for determining the optimal path through this
method compared to the non-graded networks.

ABSTRACT_BEGIN
  Peer-to-Peer (P2P) systems have proved to be the most effective and popular
file sharing applications in recent years. Previous studies mainly focus on the
equal service and the differentiated service strategies when peers have no
initial data before their download. In an upload-constrained P2P file sharing
system, we model both the equal service process and the differentiated service
process when peers' initial data distribution satisfies some special
conditions, and also show how to minimize the time to get the file to any
number of peers. The proposed models can reveal the intrinsic relations among
the initial data amount, the size of peer set and the minimum last finish time.
By using the models, we can also provide arbitrary degree of differentiated
service to a certain number of peers. We believe that our analysis process and
achieved theoretical results could provide fundamental insights into studies on
bandwidth allocation and data scheduling, and can give helpful reference both
for improving system performance and building effective incentive mechanism in
P2P file sharing systems.

ABSTRACT_BEGIN
  In this work we study energy-efficient transmission for Cognitive Radio (CR)
which opportunistically operates on Primary User's (PU's) channel through
spectrum sensing. Spectrum sensing and compulsory idling (for incumbent
protection) introduce energy-overheads for Secondary User's (SU's) operations,
and thus an appropriate balance between energy consumption in data transmission
and energy-overheads is required. We formulate this problem as a discrete-time
Markov Decision Process (MDP) in which the SU aims at minimizing its average
cost (including both energy consumption and delay cost) to finish a target
traffic payload through an appropriate rate allocation. Based on Certainty
Equivalent Control, we propose a low-complexity rate-adaptation policy that
achieves comparable performance as the optimal policy. With the low-complexity
policy, we quantify the impact of energy-overheads (including the power
consumption for spectrum sensing and compulsory idling) on the SU transmission
strategy. Specifically, the SU rate increases with the increase of
energy-overheads, whose marginal impact, however, diminishes. Moreover, the
marginal impact of energy-overheads is more significant for delay-insensitive
traffic compared to that for delay-sensitive traffic. To mitigate the loss due
to imperfect spectrum sensing, we quantify that the SU decreases (increases)
its rate with a larger mis-detection probability (false alarm probability).

ABSTRACT_BEGIN
  From the viewpoint of service operators the Total Cost of Ownership (TCO) for
developing a communication service comprises from two parts; CAPital
EXpenditure (CAPEX) and OPerational EXpenditure (OPEX). These two types of
costs are interrelated and affect any service provider's deployment strategy.
In many traditional methods, selection of critical elements of a new service is
performed in a heuristic manner aimed at reducing only the OPEX part of the TCO
which is not necessarily optimal. Furthermore, exact cost modeling for such
services is not always possible and contains some uncertainties. In the current
work, after cost modeling of each video streaming element by capturing the
effect of the model uncertainties, the TCO optimization problem for video
streaming over IP networks is formulated as a stochastic optimization problem.
The solution of the proposed optimization problem can cope with the cost
modeling uncertainties and track the dynamism in the TCO and lead to a
time-varying optimal solution. Numerical analysis results verify the developed
method.

ABSTRACT_BEGIN
  In recent years, Session Initiation Protocol (SIP) has become widely used in
current internet protocols. It is a text-based protocol much like Hyper Text
Transport Protocol (HTTP) and Simple Mail Transport Protocol (SMTP). SIP is a
strong enough signaling protocol on the internet for establishing, maintaining,
and terminating session. In this paper the areas of security and attacks in SIP
are discussed. We consider attacks from diverse related perspectives. The
authentication schemes are compared, the representative existing solutions are
highlighted, and several remaining research challenges are identified. Finally,
the taxonomy of SIP threat will be presented.

ABSTRACT_BEGIN
  Multihomed services can load-balance their incoming connection requests using
DNS, resolving the name of the server with different addresses depending on the
link load that corresponds to each address. Previous work has studied a number
of problems with this approach, e.g., due to Time-to-Live duration violations
and client proximity to local DNS servers. In this paper, we experimentally
evaluate a DNS-based ingress traffic engineering system that we deployed at
Georgia Tech. Our objective is to understand whether simple and robust load
balancing algorithms can be accurate in practice, despite aforementioned
problems with DNS-based load balancing methods. In particular, we examine the
impact of various system parameters and of the main workload characteristics.
We show that a window-based measurement scheme can be fairly accurate in
practice, as long as its window duration has been appropriately configured.

ABSTRACT_BEGIN
  In this paper we discuss the representation of the joint probability density
function of perfectly correlated continuous random variables, i.e., with
correlation coefficients $\rho=pm1$, by Dirac's $\delta$-function. We also show
how this representation allows to define Dirac's $\delta$-function as the ratio
between bivariate distributions and the marginal distribution in the limit
$\rho\rightarrow \pm1$, whenever this limit exists. We illustrate this with the
example of the bivariate Rice distribution

ABSTRACT_BEGIN
  Indoor cell phone users often suffer from poor connectivity. One promising
solution, femtocell technology, has been rapidly developed and deployed over
the past few years. One of the biggest challenges for femtocell deployment is
lack of a clear business model. This paper investigates the economic incentive
for the cellular operator (also called macrocell operator) to enable femtocell
service by leasing spectrum resource to an independent femtocell operator. On
the one hand, femtocell services can increase communication service quality and
thus increase the efficiency of the spectrum resource. On the other hand,
femtocell services may introduce more competition to the market. We model the
interactions between a macrocell operator, a femtocell operator, and users as a
three-stage dynamic game, and derive the equilibrium pricing and capacity
allocation decisions. We show that when spectrum resources are very limited,
the macrocell operator has incentive to lease spectrum to femtocell operators,
as femtocell service can provide access to more users and efficiently increase
the coverage. However, when the total spectrum resource is large, femtocell
service offers significant competition to macrocell service. Macrocell operator
thus has less incentive to enable femtocell service. We also investigate the
issue of additional operational cost and limited coverage of femtocell service
on equilibrium decisions, consumer surplus and social welfare.

ABSTRACT_BEGIN
  The paper presents two mechanisms for designing an on-demand, reliable and
efficient collection protocol for Wireless Sensor Networks. The former is the
Bidirectional Link Quality Estimation, which allows nodes to easily and quickly
compute the quality of a link between a pair of nodes. The latter, Hierarchical
Range Sectoring, organizes sensors in different sectors based on their location
within the network. Based on this organization, nodes from each sector are
coordinated to transmit in specific periods of time to reduce the hidden
terminal problem. To evaluate these two mechanisms, a protocol called HBCP
(Hierarchical-Based Collection Protocol), that implements both mechanisms, has
been implemented in TinyOS 2.1, and evaluated in a testbed using TelosB motes.
The results show that the HBCP protocol is able to achieve a very high
reliability, especially in large networks and in scenarios with bottlenecks.

ABSTRACT_BEGIN
  Diversity coding is a network restoration technique which offers near-hitless
restoration, while other state-of-the art techniques are signi?cantly slower.
Furthermore, the extra spare capacity requirement of diversity coding is
competitive with the others. Previously, we developed heuristic algorithms to
employ diversity coding structures in networks with arbitrary topology. This
paper presents two algorithms to solve the network design problems using
diversity coding in an optimal manner. The first technique pre-provisions
static traffic whereas the second technique carries out the dynamic
provisioning of the traffic on-demand. In both cases, diversity coding results
in smaller restoration time, simpler synchronization, and much reduced
signaling complexity than the existing techniques in the literature. A Mixed
Integer Programming (MIP) formulation and an algorithm based on Integer Linear
Programming (ILP) are developed for pre-provisioning and dynamic provisioning,
respectively. Simulation results indicate that diversity coding has
signi?cantly higher restoration speed than Shared Path Protection (SPP) and
p-cycle techniques. It requires more extra capacity than the p-cycle technique
and SPP. However, the increase in the total capacity is negligible compared to
the increase in the restoration speed.

ABSTRACT_BEGIN
  A properly designed handoff algorithm is essential in reducing the connection
quality deterioration when a mobile node moves across the cell boundaries.
Therefore, to improve communication quality, we identified three goals in our
paper. The first goal is to minimize unnecessary handovers and increase
communication quality by reducing misrepresentations of RSSI readings due to
multipath and shadow effect with the use of additional parameters. The second
goal is to control the handover decisions depending on the users' mobility by
utilizing location factors as one of the input parameters in a fuzzy logic
handover algorithm. The third goal is to minimize false handover alarms caused
by sudden fluctuations of parameters by monitoring the trend of fuzzy logic
outputs for a period of time before making handover decision. In this paper, we
use RSSI, speed and distance as the input decision criteria of a handover
trigger algorithm by means of fuzzy logic. The fuzzy logic output trend is
monitored for a period of time before handover is triggered. Finally, through
simulations, we show the effectiveness of the proposed handover algorithm in
achieving better communication quality.

ABSTRACT_BEGIN
  Mobile ad hoc network (MANET) is a collection of wireless mobile nodes. It
dynamically forms a temporary network without using any pre existing network
infrastructure or centralized administration i.e. with minimal prior planning.
All nodes have routing capabilities and forward data packets to other nodes in
multi-hop fashion. As the network is dynamic, the network topology continuously
experiences alterations during deployment. The biggest challenge in MANETs is
to find a path between communicating nodes. The considerations of the MANET
environment and the nature of the mobile nodes create further complications
which results in the need to develop special routing algorithms to meet these
challenges. Swarm intelligence, a bio-inspired technique, which has proven to
be very adaptable in other problem domains, has been applied to the MANET
routing problem as it forms a good fit to the problem. In ant societies the
activities of the individuals are not regulated by any explicit form of
centralized control but are the result of self-organizing dynamics driven by
local interactions and communications among a number of relatively simple
individuals. This unique characteristic has made ant societies an attractive
and inspiring model for building new algorithms and new multi-agent systems. In
this paper, we have studied and reviewed Ant Colony based routing algorithms
and its variants. Finally, a performance evaluation of the original ARA and the
EARA is carried out with respect to each other.

ABSTRACT_BEGIN
  Vehicular Ad Hoc Networks (VANET) is a very promising research venue that can
offers many useful and critical applications including the safety applications.
Most of these applications require that each vehicle knows precisely its
current position in real time. GPS is the most common positioning technique for
VANET. However, it is not accurate. Moreover, the GPS signals cannot be
received in the tunnels, undergrounds, or near tall buildings. Thus, no
positioning service can be obtained in these locations. Even if the Deferential
GPS (DGPS) can provide high accuracy, but still no GPS converge in these
locations. In this paper, we provide positioning techniques for VANET that can
provide accurate positioning service in the areas where GPS signals are
hindered by the obstacles. Experimental results show significant improvement in
the accuracy. This allows when combined with DGPS the continuity of a precise
positioning service that can be used by most of the VANET applications.

ABSTRACT_BEGIN
  The aim of this work is to change the routing strategy of AODV protocol (Ad
hoc On Demand Vector) in order to improve the energy consumption in mobile ad
hoc networks (MANET). The purpose is to minimize the regular period of HELLO
messages generated by the AODV protocol used for the research, development and
maintenance of routes. This information is useful to have an idea about battery
power levels of different network hosts. After storing this information, the
node elect the shortest path following the classical model used this
information to elect safest path (make a compromise) in terms of energy.
Transmitter node does not select another node as its battery will be exhausted
soon. Any node of the network can have the same information's about the
neighborhoods as well as other information about the energy level of the
different terminal to avoid routing using a link that will be lost due to an
exhausted battery of a node in this link. Analytical study and simulations by
Jist/SWANS have been conducted to note that no divergence relatively to the
classical AODV, a node can have this type of information that improves the
energy efficiency in ad hoc networks.

ABSTRACT_BEGIN
  We propose a collision recovery scheme for symbol-synchronous slotted ALOHA
(SA) based on physical layer network coding over extended Galois Fields.
Information is extracted from colliding bursts allowing to achieve higher
maximum throughput with respect to previously proposed collision recovery
schemes. An energy analysis is also performed, and it is shown that, by
adjusting the transmission probability, high energy efficiency can be achieved.
The paper also addresses several practical aspects, namely frequency, phase,
and amplitude estimation, as well as partial symbol asynchronism. A performance
evaluation is carried out using the proposed algorithms, revealing remarkable
performance in terms of normalized throughput.

ABSTRACT_BEGIN
  In Wireless Sensor Network, sensor nodes life time is the most critical
parameter. Many researches on these lifetime extension are motivated by LEACH
scheme, which by allowing rotation of cluster head role among the sensor nodes
tries to distribute the energy consumption over all nodes in the network.
Selection of clusterhead for such rotation greatly affects the energy
efficiency of the network. Different communication protocols and algorithms are
investigated to find ways to reduce power consumption. In this paper brief
survey is taken from many proposals, which suggests different clusterhead
selection strategies and a global view is presented. Comparison of their costs
of clusterhead selection in different rounds, transmission method and other
effects like cluster formation, distribution of clusterheads and creation of
clusters shows a need of a combined strategy for better results.

ABSTRACT_BEGIN
  Energy conservation has been an important area of interest in Wireless Sensor
networks (WSNs). Medium Access Control (MAC) protocols play an important role
in energy conservation. In this paper, we describe CSMA based MAC protocols for
WSN and analyze the simulation results of these protocols. We implemented
S-MAC, T-MAC, B-MAC, B-MAC+, X-MAC, DMAC and Wise-MAC in TOSSIM, a simulator
which unlike other simulators simulates the same code running on real hardware.
Previous surveys mainly focused on the classification of MAC protocols
according to the techniques being used or problem dealt with and presented a
theoretical evaluation of protocols. This paper presents the comparative study
of CSMA based protocols for WSNs, showing which MAC protocol is suitable in a
particular environment and supports the arguments with the simulation results.
The comparative study can be used to find the best suited MAC protocol for
wireless sensor networks in different environments.

ABSTRACT_BEGIN
  Opportunistic network is a type of Delay Tolerant Network which is
characterized by intermittent connectivity amongst the nodes and communication
largely depends upon the mobility of the participating nodes. The network being
highly dynamic, traditional MANET protocols cannot be applied and the nodes
must adhere to store-carry-forward mechanism. Nodes do not have the information
about the network topology, number of participating nodes and the location of
the destination node. Hence, message transfer reliability largely depends upon
the mobility pattern of the nodes. In this paper we have tried to find the
impact of RWP (Random Waypoint) mobility on packet delivery ratio. We estimate
mobility factors like number of node encounters, contact duration(link time)
and inter-contact time which in turn depends upon parameters like playfield
area (total network area), number of nodes, node velocity, bit-rate and RF
range of the nodes. We also propose a restricted form of RWP mobility model,
called the affinity based mobility model. The network scenario consists of a
source and a destination node that are located at two extreme corners of the
square playfield (to keep a maximum distance between them) and exchange data
packets with the aid of mobile 'helper' nodes. The source node and the
destination node are static. The mobile nodes only help in relaying the
message. We prove how affinity based mobility model helps in augmenting the
network reliability thereby increasing the message delivery ratio and reduce
message delivery latency.

ABSTRACT_BEGIN
  Over the past few years, mobile operators are faced with enormous challenges.
Of such challenges, evolved user demands on personalized applications.
Telecommunications industry as well as research community have paid enormous
attention to Next Generation Networks (NGN) to address this challenge. NGN is
perceived as a sophisticated platform where both application developers and
mobile operators cooperate to develop user applications with enhanced quality
of experience. The objective of this paper is twofold: first we present an
introduction to state-of-the-art NGN testbed to be developed at KAU, and second
we provide initial analysis for deploying a mobile application on top of the
testbed.

ABSTRACT_BEGIN
  Orthogonal Frequency Division Multiplexing (OFDM) is an efficient method of
data transmission for high speed communication systems. However, the main
drawback of OFDM system is the high Peak to Average Power Ratio (PAPR) of the
transmitted signals. OFDM consist of large number of independent subcarriers,
as a result of which the amplitude of such a signal can have high peak values.
Coding, phase rotation and clipping are among many PAPR reduction schemes that
have been proposed to overcome this problem. Here two different PAPR reduction
methods e.g. partial transmit sequence (PTS) and selective mapping (SLM) are
used to reduce PAPR. Significant reduction in PAPR has been achieved using
these techniques. The performances of the two methods are then compared.

ABSTRACT_BEGIN
  This paper presents an Adaptive Greedy-compass Energy-aware Multipath
protocol (AGEM), a novel routing protocol for wireless multimedia sensors
networks (WMSNs). AGEM uses sensors node positions to make packet forwarding
decisions. These decisions are made online, at each forwarding node, in such a
way that there is no need for global network topology knowledge and
maintenance. AGEM routing protocol performs load-balancing to minimize energy
consumption among nodes using twofold policy: (1) smart greedy forwarding,
based on adaptive compass and (2) walking back forwarding to avoid holes.
Performance evaluations of AGEM compared to GPSR (Greedy Perimeter Stateless
Routing) show that AGEM can: (a) maximize the network lifetime, (b) guarantee
quality of service for video stream transmission, and (c) scale better on
densely deployed wireless sensors network.

ABSTRACT_BEGIN
  In this paper, we address the problem of efficient routing in delay tolerant
network. We propose a new routing protocol dubbed as ORION. In ORION, only a
single copy of a data packet is kept in the network and transmitted, contact by
contact, towards the destination. The aim of the ORION routing protocol is
twofold: on one hand, it enhances the delivery ratio in networks where an
end-to-end path does not necessarily exist, and on the other hand, it minimizes
the routing delay and the network overhead to achieve better performance. In
ORION, nodes are aware of their neighborhood by the mean of actual and
statistical estimation of new contacts. ORION makes use of autoregressive
moving average (ARMA) stochastic processes for best contact prediction and
geographical coordinates for optimal greedy data packet forwarding. Simulation
results have demonstrated that ORION outperforms other existing DTN routing
protocols such as PRoPHET in terms of end-to-end delay, packet delivery ratio,
hop count and first packet arrival.

ABSTRACT_BEGIN
  Because sensor nodes operate on power limited batteries, sensor
functionalities have to be designed carefully. In particular, designing
energy-efficient packet forwarding is important to maximize the lifetime of the
network and to minimize the power usage at each node. This paper presents a
Geographic Energy-Aware Multipath Stream-based (GEAMS) routing protocol for
WMSNs. GEAMS routing decisions are made online, at each forwarding node in such
a way that there is no need to global topology knowledge and maintenance. GEAMS
routing protocol performs load-balancing to minimize energy consumption among
nodes using twofold policy: (1) smart greedy forwarding and (2) walking back
forwarding. Performances evaluations of GEAMS show that it can maximize the
network lifetime and guarantee quality of service for video stream transmission
in WMSNs.

ABSTRACT_BEGIN
  Routing is a challenge to Wireless Multimedia Sensor Networks (WMSNs) for
supporting multimedia applications due to nodes' energy constraints and
computational capabilities, and the ways sensor nodes obtain forwarding
information. In this paper, we propose an online multipath routing protocol
that uses nodes' positions to make forwarding decisions at each hop. Real-time
decisions are made without any need to have the entire network topology
knowledge. The protocol achieves load-balancing and minimises nodes' energy
consumption by utilizing: (a) smart greedy forwarding scheme for selecting next
hop, and (b) walking back forwarding scheme to bypass network holes.
Performance comparisons of the proposed protocol (schemes) are made with TPGF
and GPSR. The results show that our schemes: (a) maximise the overall network
lifespan by not draining energy from some specific nodes, (b) provide QoS
delivery for video streams by using best nodes along the route, and (c) scale
better in high density WMSN.

ABSTRACT_BEGIN
  Wireless Mesh Network (WMN) is a multi hop low cost, with easy maintenance
robust network providing reliable service coverage. WMNs consist of mesh
routers and mesh clients. In this architecture, while static mesh routers form
the wireless backbone, mesh clients access the network through mesh routers as
well as directly meshing with each other. Different from traditional wireless
networks, WMN is dynamically self-organized and self-configured. In other
words, the nodes in the mesh network automatically establish and maintain
network connectivity. Over the years researchers have worked, to reduce the
redundancy in broadcasting packet in the mesh network in the wireless domain
for providing reliable service coverage, the source node deserves to broadcast
or flood the control packets. The redundant control packet consumes the
bandwidth of the wireless medium and significantly reduces the average
throughput and consequently reduces the overall system performance. In this
paper I study the optimization problem in Wireless Mesh Networks. We have
proposed a novel approach to reduce the broadcast redundant packet in the
wireless mesh network. Also we have shown, a novel procedure to forward the
control packet to the destination nodes and efficiently minimize the
transmitted control packet in the wireless mesh cloud, that covers the domain.

ABSTRACT_BEGIN
  Voice over Internet Protocol (VoIP) is a technology that allows you to make
voice calls using a broadband Internet connection instead of a regular (or
analog) phone line. Voice over Internet Protocol (VoIP) has led human speech to
a new level, where conversation across continents can be much cheaper & faster.
However, as IP networks are not designed for real-time applications, the
network impairments such as packet loss, jitter and delay have a severe impact
on speech quality. The playout buffer at the receiver side is used to
compensate jitter at a trade-off of delay and loss. We found the
characteristics of delay and loss are dependent on IP network and sudden
variable delay (spike) often performs both regular and irregular
characteristics. Different playout buffer algorithms can have different impacts
on the achievement speech quality. It is important to design a playout buffer
algorithm which can help achieve an optimum speech quality. In this paper, we
investigate to the understanding how network impairments and existing adaptive
buffer algorithms affect the speech quality and further to design a modified
buffer algorithm to obtain an optimized voice quality. We conduct experiments
to existing algorithms and compared their performance under different network
conditions with high and low network delay variations. Preliminary results show
that the new algorithm can enhance the perceived speech quality in most network
conditions and it is more efficient and suitable for real buffer mechanism.

ABSTRACT_BEGIN
  Transmission Control Protocol (TCP) is often preferred to be implemented at
the transport layer of a Mobile Ad-hoc Network (MANET) because of its wide
range of applications, which enjoys the advantage of reliable data transmission
in the Internet. However, because of some unique characteristics of MANET, TCP
cannot offer reliable services while using e-mail, internet search and file
transmission in such a network. The research investigates how well the
different versions of TCP respond to various performance differentials when
subjected to different network stresses and topology changes, aside from
identifying the most efficient and robust TCP version(s) for different MANET
scenarios. Among several TCP variants, three types are considered important for
the analysis, namely TCP Reno, TCP New Reno and TCP Selective Acknowledgment
(SACK). In most cases, the TCP performance is found in our study to decrease
when the node size and mobility rate is increased in the network. There is,
however, exception to this. As our simulation results demonstrate, the
increases in the node velocity sometimes help the TCP to attain a better
performance. The study also reveals that out of the three variants, TCP SACK
can adapt relatively well to the changing network sizes while TCP Reno performs
most robustly in the presence of different mobility rates within MANET.

ABSTRACT_BEGIN
  Many studies have tried to evaluate wireless networks and especially the IEEE
802.15.4 standard. Hence, several papers have aimed to describe the
functionalities of the physical (PHY) and medium access control (MAC) layers.
They have highlighted some characteristics with experimental results and/or
have attempted to reproduce them using theoretical models. In this paper, we
use the first way to better understand IEEE 802.15.4 standard. Indeed, we
provide a comprehensive model, able more faithfully to mimic the
functionalities of this standard at the PHY and MAC layers. We propose a
combination of two relevant models for the two layers. The PHY layer behavior
is reproduced by a mathematical framework, which is based on radio and channel
models, in order to quantify link reliability. On the other hand, the MAC layer
is mimed by an enhanced Markov chain. The results show the pertinence of our
approach compared to the model based on a Markov chain for IEEE 802.15.4 MAC
layer. This contribution allows us fully and more precisely to estimate the
network performance with different network sizes, as well as different metrics
such as node reliability and delay. Our contribution enables us to catch
possible failures at both layers.

ABSTRACT_BEGIN
  In this thesis work, a QoS model for real-time interactive traffic on a real
network with constrained bandwidth and real-time traffic has been proposed. The
model supports tight guarantees of QoS to real-time interactive traffic without
over provisioning of bandwidth. A dynamic scheduling model which is adaptive to
input data rate of traffic has been proposed. In this model, A Differentiated
Service (DiffServ) based approach is proposed for QoS provisioning. The packets
are classified and distributed among finite number of queues with limited
buffer based on different priorities and total available bandwidth. The model
proposes a mechanism to derive the weighted service rates and queue length
distribution so as to meet the requirement of low packet loss and delay for
real time interactive traffic in the QoS engineered network. An adaptive
queuing strategy is proposed so that minimum bandwidth in used for real time
traffic. This ensures maximizing availability to best effort traffic. The model
assumes constrained bandwidth without having to over provision the network
resources and thus keeping the cost low. A modified version suitable for
testing on a real network is also presented. Experimental verification of these
in a test bed network in a laboratory as well as on a real network has been
carried out. The results of the QoS provisioning model for different sources of
real-time traffic such as video conferencing equipment, robotic surveillance
camera has also been shown. The thesis also introduces a real-time Variable Bit
Rate (VBR) traffic tuning parameter for controlling the service of VBR traffic
to give better and fair performance to rest of the traffic.

ABSTRACT_BEGIN
  Several works have outlined the fact that the mobility in intermittently
connected wireless networks is strongly governed by human behaviors as they are
basically human-centered. It has been shown that the users' moves can be
correlated and that the social ties shared by the users highly impact their
mobility patterns and hence the network structure. Tracking these correlations
and measuring the strength of social ties have led us to propose an efficient
distributed tensor-based link prediction technique. In fact, we are convinced
that the feedback provided by such a prediction mechanism can enhance
communication protocols such as opportunistic routing protocols. In this paper,
we aim to bring out that measuring the stabilities of the link and the
proximity at two hops can improve the efficiency of the proposed link
prediction technique. To quantify these two parameters, we propose an entropy
estimator in order to measure the two stability aspects over successive time
periods. Then, we join these entropy estimations to the tensor-based link
prediction framework by designing new prediction metrics. To assess the
contribution of these entropy estimations in the enhancement of tensor-based
link prediction efficiency, we perform prediction on two real traces. Our
simulation results show that by exploiting the information corresponding to the
link stability and/or to the proximity stability, the performance of the
tensor-based link prediction technique is improved. Moreover, the results
attest that our proposal's ability to outperform other well-known prediction
metrics.

ABSTRACT_BEGIN
  Ad-hoc networks are independent of any infrastructure. The nodes are
autonomous and make their own decisions. They also have limited energy
resources. Thus, a node tends to behave selfishly when it is asked to forward
the packets of other nodes. Indeed, it would rather choose to reject a
forwarding request in order to save its energy. To overcome this problem, the
nodes need to be motivated to cooperate. To this end, we propose a
self-learning repeated game framework to enforce cooperation between the nodes
of a network. This framework is inspired by the concept of "The Weakest Link"
TV game. Each node has a utility function whose value depends on its
cooperation in forwarding packets on a route as well as the cooperation of all
the nodes that form this same route. The more these nodes cooperate the higher
is their utility value. This would establish a cooperative spirit within the
nodes of the networks. All the nodes will then more or less equally participate
to the forwarding tasks which would then eventually guarantee a more efficient
packets forwarding from sources to respective destinations. Simulations are run
and the results show that the proposed framework efficiently enforces nodes to
cooperate and outperforms two other self-learning repeated game frameworks
which we are interested in.

ABSTRACT_BEGIN
  Through several studies, it has been highlighted that mobility patterns in
mobile networks are driven by human behaviors. This effect has been
particularly observed in intermittently connected networks like DTN (Delay
Tolerant Networks). Given that common social intentions generate similar human
behavior, it is relevant to exploit this knowledge in the network protocols
design, e.g. to identify the closeness degree between two nodes. In this paper,
we propose a temporal link prediction technique for DTN which quantifies the
behavior similarity between each pair of nodes and makes use of it to predict
future links. We attest that the tensor-based technique is effective for
temporal link prediction applied to the intermittently connected networks. The
validity of this method is proved when the prediction is made in a distributed
way (i.e. with local information) and its performance is compared to well-known
link prediction metrics proposed in the literature.

ABSTRACT_BEGIN
  The paper presents a quality of service aware routing protocol which provides
low latency for high priority packets. Packets are differentiated based on
their priority by applying queuing theory. Low priority packets are transferred
through less energy paths. The sensor nodes interact with the pivot nodes which
in turn communicate with the sink node. This protocol can be applied in
monitoring context aware physical environments for critical applications.

ABSTRACT_BEGIN
  We present a solution to evaluate the performance of transport protocols as a
function of link layer reliability schemes (i.e. ARQ, FEC and Hybrid ARQ)
applied to satellite physical layer traces. As modelling such traces is complex
and may require approximations, the use of real traces will minimise the
potential for erroneous performance evaluations resulting from imperfect
models. Our Trace Manager Tool (TMT) produces the corresponding link layer
output, which is then used within the ns-2 network simulator via the
additionally developed ns-2 interface module. We first present the analytical
models for the link layer with bursty erasure packets and for the link layer
reliability mechanisms with bursty erasures. Then, we present details of the
TMT tool and our validation methodology, demonstrating that the selected
performance metrics (recovery delay and throughput efficiency) exhibit a good
match between the theoretical results and those obtained with TMT. Finally, we
present results showing the impact of different link layer reliability
mechanisms on the performance of TCP Cubic transport layer protocol.

ABSTRACT_BEGIN
  In this paper, we analyze the feasibility of indoor broadband service
provisioning using secondary spectrum access to the 960-1215 MHz band,
primarily allocated to the distance measuring equipment (DME) system for
aeronautical navigation. We propose a practical secondary sharing scheme
customized to the characteristics of the DME. Since the primary system performs
a safety-of-life functionality, protection from harmful interference becomes
extremely critical. The proposed scheme controls aggregate interference by
imposing an individual interference threshold on the secondary users. We
examine the feasibility of large scale secondary access in terms of the
transmission probability of the secondary users that keeps the probability of
harmful interference below a given limit. Uncertainties in the estimation of
propagation loss and DME location affect the feasibility of the secondary
access. Numerical results show that large number of secondary users are able to
operate in adjacent DME channels without harming the primary system even with
limited accuracy in the estimation of the propagation loss.

ABSTRACT_BEGIN
  Bluetooth is a promising wireless technology that enables portable devices to
form short-range wireless ad hoc networks. Unlike wireless LAN, the
communication of Bluetooth devices follow a strict master slave relationship,
that is, it is not possible for a slave device to directly communicate with
another slave device even though they are within the radio coverage of each
other. For inter piconet communication, a scatternet has to be formed, in which
some Bluetooth devices have to act as bridge nodes between piconets. The
Scatternet formed have following properties in which they are connected i.e
every Bluetooth device can be reached from every other device, Piconet size is
limited to eight nodes [1]. The author of this research paper have studied
different type of routing protocol and have made efforts to improve throughput
and reduce packet loss due to failure in the routing loop and increased
mobility and improve the cohesive network structure, resolve the change
topology conflicts [2], and a successful & efficient transfer of packet from
source to destination.

ABSTRACT_BEGIN
  It is well known that biology-inspired self-maintaining algorithms in
wireless sensor nodes achieve near optimum time division multiple access (TDMA)
characteristics in a decentralized manner and with very low complexity. We
extend such distributed TDMA approaches to multiple channels (frequencies).
This is achieved by extending the concept of collaborative reactive listening
in order to balance the number of nodes in all available channels. We prove the
stability of the new protocol and estimate the delay until the balanced system
state is reached. Our approach is benchmarked against single-channel
distributed TDMA and channel hopping approaches using TinyOS imote2 wireless
sensors.

ABSTRACT_BEGIN
  Cooperative relaying has been proposed as a promising transmission technique
that effectively creates spatial diversity through the cooperation among
spatially distributed nodes. However, to achieve efficient communications while
gaining full benefits from cooperation, more interactions at higher protocol
layers, particularly the MAC (Medium Access Control) and network layers, are
vitally required. This is ignored in most existing articles that mainly focus
on physical (PHY)-layer relaying techniques. In this paper, we propose a novel
cross-layer framework involving two levels of joint design---a MAC-network
cross-layer design for forwarder selection (or termed routing) and a MAC-PHY
for relay selection---over symbol-wise varying channels. Based on location
knowledge and contention processes, the proposed cross-layer protocol, CoopGeo,
aims at providing an efficient, distributed approach to select next hops and
optimal relays along a communication path. Simulation results demonstrate that
CoopGeo not only operates properly with varying densities of nodes, but
performs significantly better than the existing protocol BOSS in terms of
packet error rate, transmission error probability, and saturated throughput.

ABSTRACT_BEGIN
  l. Introduction Modern network are multi-layer by their structure. This
requires the development of new mathematical models, which would allow to
adequately describe the existing physical and logical connections between the
elements on its different levels, and to effectively solve the problems of
design. The paper formulates the problem of synthesis of structure of MPLS
network layered with the transport SDH network or WDM and suggests the method
of its solving. The solving is based on the application of mathematical model
of multi-layer graph. II, III, IV. Main Part During the planning of MPLS
networks it is necessary to determine the topology of both the networks: that
of transport one and that of MPLS one. This means that one needs to determine:
- what nodes of the transport network should support the MPLS functionality; -
in what way the LSR nodes should be connected via the transport network; - what
should be the bandwidth between LSR links. According to the general method for
solving the problem of synthesis of multiservice telecommunication systems with
the usage of multi-layer graphs, we have to synthesize the initial redundant .
The solving of the task above can be reduced to the finding of the multi-layer
minimum weight subgraph that provides the transfer of information flows with
the consideration of the requirements to the structure of the multilayer graph
[5] and the flows on its edges [6] observed at the applying of constraints to
the bandwidth of the edges of the multi-layer graph. V. Conclusion The paper
reduces the problem of design of multiservice telecommunication system with the
transferred flows to the problem of finding a multilayer minimum weight
subgraph with the consideration of constraints to the graph edges bandwidth. It
is shown that application of given method provide to reduce MPLS network cost
to 10 - 16 %.

ABSTRACT_BEGIN
  Femtocells have been considered by the wireless industry as a cost-effective
solution not only to improve indoor service providing, but also to unload
traffic from already overburdened macro networks. Due to spectrum availability
and network infrastructure considerations, a macro network may have to share
spectrum with overlaid femtocells. In spectrum-sharing macro and femto
networks, inter-cell interference caused by different transmission powers of
macrocell base stations (MBS) and femtocell access points (FAP), in conjunction
with potentially densely deployed femtocells, may create dead spots where
reliable services cannot be guaranteed to either macro or femto users. In this
paper, based on a thorough analysis of downlink (DL) outage probabilities (OP)
of collocated spectrum-sharing orthogonal frequency division multiple access
(OFDMA) based macro and femto networks, we devise a decentralized strategy for
an FAP to self-regulate its transmission power level and usage of radio
resources depending on its distance from the closest MBS. Simulation results
show that the derived closed-form lower bounds of DL OPs are tight, and the
proposed decentralized femtocell self-regulation strategy is able to guarantee
reliable DL services in targeted macro and femto service areas while providing
superior spatial reuse, for even a large number of spectrum-sharing femtocells
deployed per cell site.

ABSTRACT_BEGIN
  This paper develops and evaluates the performance of an advanced multiple
access protocol for transmission of full complement of multimedia signals
consisting of various combinations of voice, video, data, text and images over
wireless networks. The protocol is called Advanced Multiple Access Protocol for
Multimedia Transmission (AMAPMT) and is to be used in the Data Link Layer of
the protocol stack. The principle of operation of the protocol is presented in
a number of logical flow charts. The protocol grants permission to transmit to
a source on the basis of a priority scheme that takes into account a
time-to-live (TTL) parameter of all the transactions, selectable priorities
assigned to all the sources and relevant channel state information (CSI) in
this order. Performance of the protocol is evaluated in terms of quality of
service parameters like packet loss ratio (PLR), mean packet transfer delay
(MPTD) and throughput. Using a simulation model based on an OPNET simulation
software package does the evaluation. Under various traffic loads with constant
distributions with various mean arrival rates and transaction sizes results
obtained show that the performance is improved when this priority scheme is
used than when it is not used. The results for AMAPMT are compared with that of
the best currently available multiple access protocol called Adaptive Request
Channel Multiple Access (ARCMA). AMAPMT protocol out performs ARCMA protocol.

ABSTRACT_BEGIN
  This paper considers the allocation of time slots in a frame, as well as
power and rate to multiple receivers on an energy harvesting downlink. Energy
arrival times that will occur within the frame are known at the beginning of
the frame. The goal is to optimize throughput in a proportionally fair way,
taking into account the inherent differences of channel quality among users.
Analysis of structural characteristics of the problem reveals that it can be
formulated as a biconvex optimization problem, and that it has multiple optima.
Due to the biconvex nature of the problem, a Block Coordinate Descent (BCD)
based optimization algorithm that converges to an optimal solution is
presented. Numerical and simulation results show that the power-time
allocations found by BCD achieve a good balance between total throughput and
fairness.

ABSTRACT_BEGIN
  In this paper, the proportionally fair allocation of time slots in a frame,
as well as power level to multiple receivers in an energy harvesting broadcast
system, is considered. Energy harvest times in a frame are assumed to be known
at the beginning of that frame. The goal is to solve an optimization problem
designed to maximize a throughput-based utility function that provides
proportional fairness among users. An optimal solution of the problem was
obtained by using a Block Coordinate Descent (BCD) method in earlier work
(presented in Part I of this study). However, finding the optimal allocation
entails a computational complexity that increases sharply in terms of the
number of users or slots. In this paper, certain structural characteristics of
the optimal power-time allocation policy are derived. Building on those, two
simple and computationally scalable heuristics, PTF and ProNTO are proposed.
Simulation results suggest that PTF and ProNTO can closely track the
performance of the BCD solution.

ABSTRACT_BEGIN
  Protocols developed during the last years for Wireless Sensor Networks (WSNs)
are mainly focused on energy efficiency and autonomous mechanisms (e.g.
self-organization, self-configuration, etc). Nevertheless, with new WSN
applications, appear new QoS requirements such as time constraints. Real-time
applications require the packets to be delivered before a known time bound
which depends on the application requirements. We particularly focus on
applications which consist in alarms sent to the sink node. We propose
Real-Time X-layer Protocol (RTXP), a real-time communication protocol. To the
best of our knowledge, RTXP is the first MAC and routing real-time
communication protocol that is not centralized, but instead relies only on
local information. The solution is cross-layer (X-layer) because it allows to
control the delays due to MAC and Routing layers interactions. RTXP uses a
suited hop-count-based Virtual Coordinate System which allows deterministic
medium access and forwarder selection. In this paper we describe the protocol
mechanisms. We give theoretical bound on the end-to-end delay and the capacity
of the protocol. Intensive simulation results confirm the theoretical
predictions and allow to compare with a real-time centralized solution. RTXP is
also simulated under harsh radio channel, in this case the radio link
introduces probabilistic behavior. Nevertheless, we show that RTXP it performs
better than a non-deterministic solution. It thus advocates for the usefulness
of designing real-time (deterministic) protocols even for highly unreliable
networks such as WSNs.

ABSTRACT_BEGIN
  Cyber-physical systems integrate information and communication technology
functions to the physical elements of a system for monitoring and controlling
purposes. The conversion of traditional power grid into a smart grid, a
fundamental example of a cyber-physical system, raises a number of issues that
require novel methods and applications. In this context, an important issue is
the verification of certain quantitative properties of the system. In this
technical report, we consider a specific Chinese Smart Grid implementation and
try to address the verification problem for certain quantitative properties
including performance and battery consumption. We employ stochastic model
checking approach and present our modelling and analysis study using PRISM
model checker.

ABSTRACT_BEGIN
  Emerging standardization of Geo Mobile Radio (GMR-1) for satellite system is
having strong resemblance to terrestrial GSM (Global System for Mobile
communications) at the upper protocol layers and TCP (Transmission Control
Protocol) is one of them. This space segment technology as well as terrestrial
technology, is characterized by periodic variations in communication properties
and coverage causing the termination of ongoing call as connections of Mobile
Nodes (MN) alter stochastically. Although provisions are made to provide
efficient communication infrastructure this hybrid space and terrestrial
networks must ensure the end-to-end network performance so that MN can move
seamlessly among these networks. However from connectivity point of view
current TCP performance has not been engineered for mobility events in
multi-radio MN. Traditionally, TCP has applied a set of congestion control
algorithms (slow-start, congestion avoidance, fast retransmit, fast recovery)
to probe the currently available bandwidth on the connection path. These
algorithms need several round-trip times to find the correct transmission rate
(i.e. congestion window), and adapt to sudden changes connectivity due to
handover. While there are protocols to maintain the connection continuity on
mobility events, such as Mobile IP (MIP) and Host Identity Protocol (HIP), TCP
performance engineering has had less attention. TCP is implemented as a
separate component in an operating system, and is therefore often unaware of
the mobility events or the nature of multi-radios' communication. This paper
aims to improve TCP communication performance in Mobile satellite and
terrestrial networks.

ABSTRACT_BEGIN
  The enormous success of advanced wireless devices is pushing the demand for
higher wireless data rates. Denser spectrum reuse through the deployment of
more access points per square mile has the potential to successfully meet the
increasing demand for more bandwidth. In theory, the best approach to density
increase is via distributed multiuser MIMO, where several access points are
connected to a central server and operate as a large distributed multi-antenna
access point, ensuring that all transmitted signal power serves the purpose of
data transmission, rather than creating "interference." In practice, while
enterprise networks offer a natural setup in which distributed MIMO might be
possible, there are serious implementation difficulties, the primary one being
the need to eliminate phase and timing offsets between the jointly coordinated
access points.
  In this paper we propose AirSync, a novel scheme which provides not only time
but also phase synchronization, thus enabling distributed MIMO with full
spatial multiplexing gains. AirSync locks the phase of all access points using
a common reference broadcasted over the air in conjunction with a Kalman filter
which closely tracks the phase drift. We have implemented AirSync as a digital
circuit in the FPGA of the WARP radio platform. Our experimental testbed,
comprised of two access points and two clients, shows that AirSync is able to
achieve phase synchronization within a few degrees, and allows the system to
nearly achieve the theoretical optimal multiplexing gain. We also discuss MAC
and higher layer aspects of a practical deployment. To the best of our
knowledge, AirSync offers the first ever realization of the full multiuser MIMO
gain, namely the ability to increase the number of wireless clients linearly
with the number of jointly coordinated access points, without reducing the per
client rate.

ABSTRACT_BEGIN
  Cooperative techniques have been shown to significantly improve the
performance of wireless systems. Despite being a mature technology in single
communication link scenarios, their implementation in wider, and practical,
networks poses several challenges which have not been fully identified and
understood so far. In this two-part paper, the implementation of cooperative
communications in non-centralized ad hoc networks with sensing-based channel
access is extensively discussed. Both analysis and simulation are employed to
provide a clear understanding of the mutual influence between the link layer
contention mechanism and collaborative protocols. Part I of this work focuses
on reactive cooperation, in which relaying is triggered by packet delivery
failure events, while Part II addresses proactive approaches, preemptively
initiated by the source based on channel state information. Results show that
sensing-based channel access significantly hampers the effectiveness of
cooperation by biasing the spatial distribution of available relays, and by
inducing a level of spatial and temporal correlation of the interference that
diminishes the diversity improvement on which cooperative gains are founded.
Moreover, the efficiency reduction entailed by several practical protocol
issues related to carrier sense multiple access which are typically neglected
in the literature is thoroughly investigated.

ABSTRACT_BEGIN
  This work is the second of a two-part series of papers on the effectiveness
of cooperative techniques in non-centralized carrier sense-based ad hoc
wireless networks. While Part I extensively discussed reactive cooperation,
characterized by relayed transmissions triggered by failure events at the
intended receiver, Part II investigates in depth proactive solutions, in which
the source of a packet exploits channel state information to preemptively
coordinate with relays in order to achieve the optimal overall rate to the
destination. In particular, this work shows by means of both analysis and
simulation that the performance of reactive cooperation is reduced by the
intrinsic nature of the considered medium access policy, which biases the
distribution of the available relays, locating them in unfavorable positions
for rate optimization. Moreover, the highly dynamic nature of interference that
characterizes non-infrastructured ad hoc networks is proved to hamper the
efficacy and the reliability of preemptively allocated cooperative links, as
unpredicted births and deaths of surrounding transmissions may force relays to
abort their support and/or change the maximum achievable rate at the intended
receiver. As a general conclusion, our work extensively suggests that
CSMA-based link layers are not apt to effectively support cooperative
strategies in large-scale non-centralized ad hoc networks.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks is one of the most challenging research area in the
field of Mobile Ad Hoc Networks, in this research We propose a flexible,
simple, and scalable design for VANET certificates, and new methods for
efficient certificate management, which will Reduce channel overhead by
eliminating the use of CRL, and make Better certificate Revocation Management.
Also it will increase the security of the network and helps in identifying the
adversary vehicle.

ABSTRACT_BEGIN
  Cooperative vehicle safety (CVS) systems operate based on broadcast of
vehicle position and safety information to neighboring cars. The communication
medium of CVS is a vehicular ad-hoc network. One of the main challenges in
large scale deployment of CVS systems is the issue of scalability. To address
the scalability problem, several congestion control methods have been proposed
and are currently under field study. These algorithms adapt transmission rate
and power based on network measures such as channel busy ratio. We examine two
such algorithms and study their dynamic behavior in time and space to evaluate
stability (in time) and fairness (in space) properties of these algorithms. We
present stability conditions and evaluate stability and fairness of the
algorithms through simulation experiments. Results show that there is a
trade-off between fast convergence, temporal stability and spatial fairness.
The proper ranges of parameters for achieving stability are presented for the
discussed algorithms. Stability is verified for all typical road density cases.
Fairness is shown to be naturally achieved for some algorithms, while under the
same conditions other algorithms may suffer from unfairness issues. A method
for resolving unfairness is introduced and evaluated through simulations.

ABSTRACT_BEGIN
  Wireless sensor networks sense and monitor real-time events. They supervise a
geographic area where a phenomenon is to be monitored. The data in sensor
networks have different levels of priority and hence their criticality differs.
In order to keep up the real time commitment, the applications need higher
transmission rates and reliability in information delivery. In this work we
propose a multipath routing algorithm which enables the reliable delivery of
data. By controlling the scheduling rate, it is possible to prevent congestion
and packet loss in the network. The algorithm provides an efficient way to
prevent the packet loss at each node. This results in congestion management in
the sensor networks. This protocol prevents packet clustering and provides
smoothness to the traffic. Through monitoring and controlling the scheduling
rate the flow control and congestion control are managed.

ABSTRACT_BEGIN
  Exponential backoff (EB) is a widely adopted collision resolution mechanism
in many popular random-access networks including Ethernet and wireless LAN
(WLAN). The prominence of EB is primarily attributed to its asymptotic
throughput stability, which ensures a non-zero throughput even when the number
of users in the network goes to infinity. Recent studies, however, show that EB
is fundamentally unsuitable for applications that are sensitive to large delay
and delay jitters, as it induces divergent second- and higher-order moments of
medium access delay. Essentially, the medium access delay follows a power law
distribution, a subclass of heavy-tailed distribution. To understand and
alleviate the issue, this paper systematically analyzes the tail delay
distribution of general backoff functions, with EB being a special case. In
particular, we establish a tradeoff between the tail decaying rate of medium
access delay distribution and the stability of throughput. To be more specific,
convergent delay moments are attainable only when the backoff functions $g(k)$
grows slower than exponential functions, i.e., when $g(k)\in o(r^k)$ for all
$r>1$. On the other hand, non-zero asymptotic throughput is attainable only
when backoff functions grow at least as fast as an exponential function, i.e.,
$g(k)\in\Omega(r^k)$ for some $r>1$. This implies that bounded delay moments
and stable throughput cannot be achieved at the same time. For practical
implementation, we show that polynomial backoff (PB), where $g(k)$ is a
polynomial that grows slower than exponential functions, obtains finite delay
moments and good throughput performance at the same time within a practical
range of user population. This makes PB a better alternative than EB for
multimedia applications with stringent delay requirements.

ABSTRACT_BEGIN
  In this technical report, we analyze the performance of an interference-aware
opportunistic relay selection protocol for multi-hop line networks which is
based on the following simple rule: a node always transmits if it has a packet,
except when its successive node on the line is transmitting. We derive
analytically the saturation throughput and the end-to-end delay for two and
three hop networks, and present simulation results for higher numbers of hops.
In the case of three hops, we determine the throughput-optimal relay positions.

ABSTRACT_BEGIN
  The measurement of performance of Internet Protocol IP network can be done by
Transmission Control Protocol TCP because it guarantees send data from one end
of the connection actually gets to the other end and in the same order it was
send, otherwise an error is reported. There are several methods to measure the
performance of TCP among these methods genetic algorithms, neural network, data
mining etc, all these methods have weakness and can't reach to correct measure
of TCP performance. This paper proposed a new method of measuring TCP
performance for real time IP network using Biocomputing, especially molecular
calculation because it provides wisdom results and it can exploit all
facilities of phylogentic analysis. Applying the new method at real time on
Biological Kurdish Messenger BIOKM model designed to measure the TCP
performance in two types of protocols File Transfer Protocol FTP and Internet
Relay Chat Daemon IRCD. This application gives very close result of TCP
performance comparing with TCP performance which obtains from Little's law
using same model (BIOKM), i.e. the different percentage of utilization (Busy or
traffic industry) and the idle time which are obtained from a new method base
on Bio-computing comparing with Little's law was (nearly) 0.13%.
  KEYWORDS Bio-computing, TCP performance, Phylogenetic tree, Hybridized Model
(Normalized), FTP, IRCD

ABSTRACT_BEGIN
  In this paper, we study optimal radar deployment for intrusion detection,
with focus on network coverage. In contrast to the disk-based sensing model in
a traditional sensor network, the detection range of a bistatic radar depends
on the locations of both the radar transmitter and radar receiver, and is
characterized by Cassini ovals. Furthermore, in a network with multiple radar
transmitters and receivers, since any pair of transmitter and receiver can
potentially form a bistatic radar, the detection ranges of different bistatic
radars are coupled and the corresponding network coverage is intimately related
to the locations of all transmitters and receivers, making the optimal
deployment design highly non-trivial. Clearly, the detectability of an intruder
depends on the highest SNR received by all possible bistatic radars. We focus
on the worst-case intrusion detectability, i.e., the minimum possible
detectability along all possible intrusion paths. Although it is plausible to
deploy radars on a shortest line segment across the field, it is not always
optimal in general, which we illustrate via counter-examples. We then present a
sufficient condition on the field geometry for the optimality of shortest line
deployment to hold. Further, we quantify the local structure of detectability
corresponding to a given deployment order and spacings of radar transmitters
and receivers, building on which we characterize the optimal deployment to
maximize the worst-case intrusion detectability. Our results show that the
optimal deployment locations exhibit a balanced structure. We also develop a
polynomial-time approximation algorithm for characterizing the worse-case
intrusion path for any given locations of radars under random deployment.

ABSTRACT_BEGIN
  An optimized broadcast protocol is proposed for VANETs. It is based on two
key information: the direction to the destination and the beamforming angle
{\theta}. The efficiency of this technique is demonstrated in terms of packet
delivery, bandwidth gain and probability of transmission success. An analytical
model is developed to calculate the transmission area. This model allows
capturing the propagation shape of the forwarding area. Comparisons with
simulations show that the analytical model is precise.

ABSTRACT_BEGIN
  Common Mobile IPv6 mechanisms, Bidirectional tunneling and Route
optimization, show inefficient packet overhead when both nodes are mobile.
Researchers have proposed methods to reduce packet overhead regarding to
maintain compatible with standard mechanisms. In this paper, three mechanisms
in Mobile IPv6 are discussed to show their efficiency and performance.
Following discussion, a new mechanism called Improved Tunneling-based Route
Optimization is proposed and due to performance analysis, it is shown that
proposed mechanism has less overhead comparing to common mechanisms. Analytical
results indicate that Improved Tunneling-based Route Optimization transmits
more payloads due to send packets with less overhead.

ABSTRACT_BEGIN
  One of the main pervasive problems Wireless Sensor Networks (WSN) encounter
is to maintain flawless communication sharing and cooperative processing
between sensors via radio links to ensure a reliable treatment of information.
Many applications based on these WSNs consider local clocks at each sensor node
that need to be synchronized to a common notion of time. In this context, the
majority of previous researches were focused on the study of protocols, and
algorithms that address these issues in order to resolve synchronization
problems. Previous fforts and empirical studies in wireless sensor network
(WSN) proposed several solutions (algorithms). The focus of this this paper is
to examine and evaluate the most important synchronization algorithms based on
the positions of various quantitative and qualitative synchronization protocols
for energy-efficient information processing and routing in WSNs.

ABSTRACT_BEGIN
  The aim of this work is to model the nodes battery discharge in wireless ad
hoc networks. Many work focus on the energy consumption in such networks. Even,
the research in the highest layers of the ISO model, takes into account the
energy consumption and efficiency. Indeed, the nodes that form such network are
mobiles, so no instant recharge of battery. Also with special type of ad hoc
networks are wireless sensors networks using non-rechargeable batteries. All
nodes with an exhausted battery are considered death and left the network. To
consider the energy consumption, in this work we model using a Markov chain,
the discharge of the battery considering of instant activation and deactivation
distribution function of these nodes.

ABSTRACT_BEGIN
  In recent years, the mobile devices are equipped with several wireless
interfaces in heterogeneous environments which integrate a multitude of radio
access technologies (RAT's). The evolution of these technologies will allow the
users to benefit simultaneously from these RAT's. However, the most important
issue is how to choose the most suitable access network for mobile's user which
can be used as long as possible for communication. To achieve this issue, this
paper proposes a new approach for network selection decision based on Saaty's
Fuzzy Analytical Hierarchy Process (FAHP) and the Technique for Order
Preference by Similarity to an Ideal Solution (TOPSIS). The FAHP method is used
to determine a weight for each criterion, and the TOPSIS method is applied to
rank the alternatives. Simulation results are presented to illustrate the
effectiveness of our new approach for network selection.

ABSTRACT_BEGIN
  The integrated management of multi-providers equipments is a key asset for
telecommunication operators or service providers when selecting the appropriate
network and service management platform for their network. In this paper, we
present an open and adaptable platform that support fault and configuration
management for next-generation networks. This platform Named Nagios based -
information technology management system (NB-ITMS) leverage of the well-known
Nagios platform to implement new capabilities. Offering additional features
applications and management functions enable easy and low cost management of
advanced services and networks technologies. The performance of our platform
has been compared to existing off-the-shell platforms.

ABSTRACT_BEGIN
  This work presents a performance evaluation based on elaborated analytical
expressions of error probability for broadband access network in the case of a
combined technique of dense wavelength division multiplexing (DWDM) and one
dimensional optical orthogonal codes (1D-OOC). Optical sources with relatively
large spectrum has been considered and simulated. Besides the Multiple Access
Interference (MAI) at the receiver due to the access method which is optical
code division multiple access (OCDMA), the emitted radiation of these sources
in a dense WDM communication link introduces additional interference.
Conventional correlation receiver (CCR) and parallel interference cancellation
(PIC) receiver limitations are discussed. This paper has investigated the kind
of optical sources with large spectrum bandwidth which could be accepted for a
targeted bit error rate (BER) and given number of users in broadband access
network supporting DWDM with optical orthogonal codes.

ABSTRACT_BEGIN
  We investigate the downlink scheduling problem under Markovian ON/OFF fading
channels, where the instantaneous channel state information is not directly
accessible, but is revealed via ARQ-type feedback. The scheduler can exploit
the temporal correlation/channel memory inherent in the Markovian channels to
improve network performance. However, designing low-complexity and
throughput-optimal algorithms under temporal correlation is a challenging
problem. In this paper, we find that, under an average number of transmissions
constraint, a low-complexity index policy is throughput-optimal. The policy
uses Whittle's index value, which was previously used to capture opportunistic
scheduling under temporally correlated channels. Our results build on the
interesting finding that, under the intricate queue length and channel memory
evolutions, the importance of scheduling a user is captured by a simple
multiplication of its queue length and Whittle's index value. The proposed
queue-weighted index policy has provably low complexity which is significantly
lower than existing optimal solutions.

ABSTRACT_BEGIN
  In mobile wireless communication network, caching data items at the mobile
clients is important to reduce the data access delay. However, efficient cache
invalidation strategies are used to ensure the consistency between the data in
the cache of mobile clients and at the database server. Servers use
invalidation reports (IRs) to inform the mobile clients about data item
updates. This paper proposes and implements a multicast based strategy to
maintain cache consistency in mobile environment using AVI as the cache
invalidation scheme. The proposed algorithm is outlined as follows - To resolve
a query, the mobile client searches its cache to check if its data is valid. If
yes, then query is answered, otherwise the client queries the DTA (Dynamic
Transmitting Agent) for latest updates and the query is answered. If DTA
doesn't have the latest updates, it gets it from the server. So, the main idea
here is that DTA will be multicasting updates to the clients and hence the
clients need not uplink to the server individually, thus preserving the network
bandwidth. The scenario of simulation is developed in Java. The results
demonstrate that the traffic generated in the proposed multicast model is
simplified and it also retains cache consistency when compared to the existing
methods that used broadcast strategy.

ABSTRACT_BEGIN
  Multiprotocol Label Switching (MPLS) fasten the speed of packet forwarding by
forwarding the packets based on labels and reduces the use of routing table
look up from all routers to label edge routers(LER), where as the label switch
routers (LSRs) uses Label Distribution Protocol (LDP) or RSVP (Resource
reservation Protocol) for label allocation and Label table for packet
forwarding. Dynamic protocol is implemented which carries a Updates packets for
the details of Label Switch Paths, along with this feedback mechanism is also
introduced which find the shortest path among MPLS network and also feedback is
provided which also help to overcome congestion, this feedback mechanism is on
a hop by hop basis rather than end to end thus providing a more reliable and
much faster and congestion free path for the packets.

ABSTRACT_BEGIN
  VoIP (Voice over Internet Protocol) is a growing technology during last
decade. It provides the audio, video streaming facility on successful
implementation in the network. However, it provides the text transport facility
over the network. Due to implementation of it the cost effective solution, it
can be developed for the intercommunication among the employees of a
prestigious organization. The proposed idea has been implemented on the audio
streaming area of the VoIP technology. In the audio streaming, the security
vulnerabilities are possible on the VoIP server during communication between
two parties. In the proposed model, first the VoIP system has been implemented
with IVR (Interactive Voice Response) as a case study and with the
implementation of the security parameters provided to the asterisk server which
works as a VoIP service provider. The asterisk server has been configured with
different security parameters like VPN server, Firewall iptable rules,
Intrusion Detection and Intrusion Prevention System. Every parameter will be
monitored by the system administrator of the VoIP server along with the MySQL
database. The system admin will get every update related to the attacks on the
server through Mail server attached to the asterisk server. The main beauty of
the proposed system is VoIP server alone is configured as a VoIP server, IVR
provider, Mail Server with IDS and IPS, VPN server, connection with database
server in a single asterisk server inside virtualization environment. The VoIP
system is implemented for a Local Area Network inside the university system

ABSTRACT_BEGIN
  During a disaster scenario, situational awareness information, such as
location, physical status and images of the surrounding area, is essential for
minimizing loss of life, injury, and property damage. Today's handhelds make it
easy for people to gather data from within the disaster area in many formats,
including text, images and video. Studies show that the extreme anxiety induced
by disasters causes humans to create a substantial amount of repetitive and
redundant content. Transporting this content outside the disaster zone can be
problematic when the network infrastructure is disrupted by the disaster.
  This paper presents the design of a novel architecture called CARE
(Content-Aware Redundancy Elimination) for better utilizing network resources
in disaster-affected regions. Motivated by measurement-driven insights on
redundancy patterns found in real-world disaster area photos, we demonstrate
that CARE can detect the semantic similarity between photos in the networking
layer, thus reducing redundant transfers and improving buffer utilization.
Using DTN simulations, we explore the boundaries of the usefulness of deploying
CARE on a damaged network, and show that CARE can reduce packet delivery times
and drops, and enables 20-40% more unique information to reach the rescue teams
outside the disaster area than when CARE is not deployed.

ABSTRACT_BEGIN
  The vertical handover decision is considered an NP-Hard problem. For that
reason, a large variety of vertical handoff algorithms (VHA) have been proposed
to help the user to select dynamically the best access network in terms of
quality of service (QoS). The objective of this paper is to provide a new
approach for evaluating of the vertical handoff algorithms in order to choose
the most appropriate algorithm which should be used to select the best access
network. Simulation results are presented to evaluate and to test our new
evaluation model

ABSTRACT_BEGIN
  We present comparative analysis of MANET (Mobile Ad-Hoc Network) and VANET
(Vehicular Ad-Hoc Network) routing protocols, in this paper. The analysis is
based on various design factors. The traditional routing protocols of AODV (Ad
hoc On-Demand Distance Vector), DSR (Dynamic Source Routing), and DSDV
(Destination-Sequenced Distance-Vector) of MANET are utilizing node centric
routing which leads to frequent breaking of routes, causing instability in
routing. Usage of these protocols in high mobility environment like VANET may
eventually cause many packets to drop. Route repairs and failures notification
overheads increase significantly leading to low throughput and long delays.
Such phenomenon is not suitable for Vehicular Ad hoc Networks (VANET) due to
high mobility of nodes where network can be dense or sparse. Researchers have
proposed various routing algorithms or mechanism for MANET and VANET. This
paper describes the relevant protocols, associated algorithm and the strength
and weakness of these routing protocols.

ABSTRACT_BEGIN
  Most wireless sensor networks operate with very limited energy sources-their
batteries, and hence their usefulness in real life applications is severely
constrained. The challenging issues are how to optimize the use of their energy
or to harvest their own energy in order to lengthen their lives for wider
classes of application. Tackling these important issues requires a robust
architecture that takes into account the energy consumption level of functional
constituents and their interdependency. Without such architecture, it would be
difficult to formulate and optimize the overall energy consumption of a
wireless sensor network. Unlike most current researches that focus on a single
energy constituent of WSNs independent from and regardless of other
constituents, this paper presents an Energy Driven Architecture (EDA) as a new
architecture and indicates a novel approach for minimising the total energy
consumption of a WSN

ABSTRACT_BEGIN
  Today's data centers face extreme challenges in providing low latency.
However, fair sharing, a principle commonly adopted in current congestion
control protocols, is far from optimal for satisfying latency requirements.
  We propose Preemptive Distributed Quick (PDQ) flow scheduling, a protocol
designed to complete flows quickly and meet flow deadlines. PDQ enables flow
preemption to approximate a range of scheduling disciplines. For example, PDQ
can emulate a shortest job first algorithm to give priority to the short flows
by pausing the contending flows. PDQ borrows ideas from centralized scheduling
disciplines and implements them in a fully distributed manner, making it
scalable to today's data centers. Further, we develop a multipath version of
PDQ to exploit path diversity.
  Through extensive packet-level and flow-level simulation, we demonstrate that
PDQ significantly outperforms TCP, RCP and D3 in data center environments. We
further show that PDQ is stable, resilient to packet loss, and preserves nearly
all its performance gains even given inaccurate flow information.

ABSTRACT_BEGIN
  The heterogeneous wireless networks where coexistence of different Radio
access technology (RAT) are widely deployed for various services and support
various traffic demand, channel allocation. Under heterogeneous wireless
networks, a user can send data through a single or multi RATs simultaneous. The
objective of this paper is to choose the optimal bandwidth for the services and
power allocation to that bandwidth. The proposed distributed joint allocation
algorithm using modified Newton method is adopted to maximize the total system
capacity. We validate the performance of the proposed algorithm through
numerical results.

ABSTRACT_BEGIN
  Mobile gaming presents a number of main issues which remain open. These are
concerned mainly with connectivity, computational capacities, memory and
battery constraints. In this paper, we discuss the design of a fully
distributed approach for the support of mobile Multiplayer Online Games (MOGs).
In mobile environments, several features might be exploited to enable resource
sharing among multiple devices / game consoles owned by different mobile users.
We show the advantages of trading computing / networking facilities among
mobile players. This operation mode opens a wide number of interesting sharing
scenarios, thus promoting the deployment of novel mobile online games. In
particular, once mobile nodes make their resource available for the community,
it becomes possible to distribute the software modules that compose the game
engine. This allows to distribute the workload for the game advancement
management. We claim that resource sharing is in unison with the idea of ludic
activity that is behind MOGs. Hence, such schemes can be profitably employed in
these contexts.

ABSTRACT_BEGIN
  Wireless networking is becoming an increasingly important and popular way of
providing global information access to users on the move. One of the main
challenges for seamless mobility is the availability of simple and robust
handoff algorithms, which allow a mobile node to roam among heterogeneous
wireless networks. In this paper, the authors devise a scheme, A Novel Adaptive
Channel Allocation Scheme (ACAS) where the number of guard channel(s) is
adjusted automatically based on the average handoff blocking rate measured in
the past certain period of time. The handoff blocking rate is controlled under
the designated threshold and the new call blocking rate is minimized. The
performance evaluation of the ACAS is done through simulation of nodes. The
result shows that the ACAS scheme outperforms the Static Channel Allocation
Scheme by controlling a hard constraint on the handoff rejection probability.
The proposed scheme achieves the optimal performance by maximizing the resource
utilization and adapts itself to changing traffic conditions automatically.

ABSTRACT_BEGIN
  We experimentally demonstrate a novel optical physical-layer network coding
(PNC) scheme over time-division multiplexing (TDM) passive optical network
(PON). Full-duplex error-free communications between optical network units
(ONUs) at 2.5 Gb/s are shown for all-optical virtual private network (VPN)
applications. Compared to the conventional half-duplex communications set-up,
our scheme can increase the capacity by 100% with power penalty smaller than 3
dB. Synchronization of two ONUs is not required for the proposed VPN scheme

ABSTRACT_BEGIN
  User mobility in wireless data networks is increasing because of
technological advances and the desire for voice and multimedia applications.
These applications, however, require fast handoffs between base stations to
maintain the quality of the connections. In this paper, the authors describe
the use of novel and efficient data structure which dynamically allocates guard
channel for handoffs and introduces the concept of channel borrowing strategy.
The proposed scheme allocates the guard channels for handoff requests
dynamically, based on the traffic load for certain time period. A new
originating call in the cell coverage area also uses these guard channels if
they are unused. Our basic idea is to allow Guard channels to be shared between
new calls and handoff calls. This approach maximizes the channel utilization.
The simulation results prove that the channel borrowing scheme improves the
overall throughput.

ABSTRACT_BEGIN
  The distributed system use to enhance the performance of all types of
multimedia service in the next generation network. The packet loss occurs in
the video on demand system due to delay and huge traffic load from the both
sides of client request and server response. It's bring a real challenge to the
researcher how to minimize the traffic load in the video on demand system to
provide better utilization of the available bandwidth in the very race
situation. The normal client server type architecture can't solve the huge
client requirement. In this work, I have used number of remote servers that
partly shears the total load in the video on demand system on behalf of the
total system load. This work presents the scenario of controlling the traffic
load in the localization wise sub domain that gives good impact to control the
traffic in the whole distributed system. The user increased rapidly in the
network it posed heavy load to the video servers. The requested clients,
servers are all distributed in nature and the data stream delivered according
to client request. In this work presents the performance of the video on demand
server by policy based traffic handle, at real time with respect to incoming
multi rate traffic pattern.

ABSTRACT_BEGIN
  A Mobile Ad-Hoc Network is a collection of mobile nodes that are dynamically
and arbitrarily located in such a manner that the interconnections between
nodes are capable of changing on continual basis. Due to security
vulnerabilities of the routing protocols, wireless ad-hoc networks are
unprotected to attacks of the malicious nodes. One of these attacks is the
Black Hole Attack. In this paper, we give an algorithmic approach to focus on
analyzing and improving the security of AODV, which is one of the popular
routing protocols for MANET. Our aim is on ensuring the security against Black
hole attack. The proposed solution is capable of detecting & removing Black
hole node(s) in the MANET at the beginning. Also the objective of this paper is
to provide a simulation study that illustrates the effects of Black hole attack
on network performance.

ABSTRACT_BEGIN
  In this paper, we study and improve the recovery properties of single and
multipath routing strategies when facing network failure situations. In
particular, we focus our study on two MANET routing protocols: OLSR and its
multipath extension MP-OLSR. In various wireless multi-hop network
environments, especially in multiple chain topologies, we define and seek to
evaluate the latency introduced by these protocols to find a new path after a
link failure. Theoretical estimations and simulation results show that, under
dual chain-topologies, this latency can be too long and incompatible with the
needs of loss and delay constrained applications. As the source nodes cannot
detect link failures immediately because of the delay incurred by the
well-known nature of link state protocols in general, and of OLSR Topology
Control (TC) messages in particular, these nodes keep sending packets along
broken paths. We thus study the inconsistencies between the actual network
topology and the nodes' own representation. After analyzing the consequences of
this long latency, we seek to alleviate these problems with the introduction of
adapted mechanisms. We propose three new different schemes and accordingly
extend the original OLSR and MP-OLSR protocols in order to decrease the
expected latency and improve the protocol performance. Simulation results show
a steep decrease of the latency when using these new schemes in dual
chain-topologies. We also discuss these results in terms of packet loss,
end-to-end delay and overhead.

ABSTRACT_BEGIN
  This paper gives a detail analysis of various applications based on Internet
of Thing (IoT)s. This explains about how internet of things evolved from mobile
computing and ubiquitous computing. It emphasises the fact that objects are
connected over the internet rather than people. The properties of Internet of
Things (IOT) are product information, electronic tag, standard expressed and
uploading information. It utilises the Radio Frequency Identification (RFID)
technology and wireless sensor networks (WSN). IOT applications are used in
domains such as healthcare, supply chain management, defence and agriculture.
Lastly the paper focuses on issues involved in IOT. Though it is a boon, IOT
faces certain crucial issues like privacy and security.

ABSTRACT_BEGIN
  In the mobile ad hoc networks the major role is played by the routing
protocols in order to route the data from one mobile node to another mobile
node. But in such mobile networks, routing protocols are vulnerable to various
kinds of security attacks such as blackhole node attacks. The routing protocols
of MANET are unprotected and hence resulted into the network with the malicious
mobile nodes in the network. These malicious nodes in the network are basically
acts as attacks in the network. In this paper, we modify the existing DSR
protocol with the functionality of attacks detection without affecting overall
performance of the network. Also, we are considering the various attacks on
mobile ad hoc network called blackhole attack, flooding attack and show the
comparative analysis of these attacks using network simulator ns-2.

ABSTRACT_BEGIN
  This paper introduces a protocol that distributively constructs a
collision-free schedule for multi-hop packet radio networks in the presence of
hidden terminals. As a preliminary step, each wireless station computes the
schedule length after gathering information about the number of flows in its
neighbourhood. Then, a combination of deterministic and random backoffs are
used to reach a collision-free schedule. A deterministic backoff is used after
successful transmissions and a random backoff is used otherwise. It is
explained that the short acknowledgement control packets can easily result in
channel time fragmentation and, to avoid this, the use of link layer delayed
acknowledgements is advocated and implemented. The performance results show
that a collision-free protocol easily outperforms a collision-prone protocol
such as Aloha. The time that is required for the network to converge to a
collision-free schedule is assessed by means of simulation.

ABSTRACT_BEGIN
  This paper has been withdrawn because of some paper issues. Recent studies on
MAC scheduling have shown that carrier sense multiple access (CSMA) can be
controlled to be achieve optimality in terms of throughput or utility. These
results imply that just a simple MAC algorithm without message passing is
possible to achieve high performance guarantee. However, such studies are
conducted only on the assumption that channel conditions are static. Noting
that the main drive for achieving optimality in optimal CSMA is to let it run a
good schedule for some time, formally referred to as the mixing time, it is
under-explored how such optimal CSMA performs for time-varying channel
conditions. In this paper, under the practical constraint of restricted
back-off rates (i.e., limited sensing speed), we consider two versions of
CSMAs: (i) channel-unaware CSMA (U-CSMA) and (ii) channel-aware CSMA (A-CSMA),
each of which is characterized as its ability of tracking channel conditions.
We first show that for fast channel variations, A-CSMA achieves almost zero
throughput, implying that incomplete tracking of channel conditions may
seriously degrade performance, whereas U-CSMA, accessing the media without
explicit consideration of channel conditions, has positive worst-case guarantee
in throughput, where the ratio of guarantee depends on network topology. On the
other hand, for slow channel variations, we prove that A-CSMA is
throughput-optimal for any network topology. Our results provide the precise
trade-off between sensing costs and performances of CSMA algorithms, which
guides a robust design on MAC scheduling under highly time-varying scenarios.

ABSTRACT_BEGIN
  The advance in wireless technologies and portable devices such as smart
phones has made the Wireless Local Area Networks (WLANs) popular in the recent
years. Nowadays, WLANs have become widely accepted in both private and public
sectors due to ease of installation, reasonable prices and high data rates that
can support real time applications. However, fast handoff required for such
real-time applications is not provided in the current IEEE 802.11
specifications. Consequently, providing seamless mobility in these WLANs is an
important issue. To solve this problem, a new fast handoff scheme called
Advanced Neighbor Discovery with Caching (ANDWC) is proposed. This new
mechanism is based on the user's mobility between two or more different Basic
Service Sets (Layer 2 mobility). ANDWC can eliminate scanning delay (which
contributes up to 90% of the total handoff latency) to provide seamless handoff
by using pre-neighbor-discovery and caching mechanisms.

ABSTRACT_BEGIN
  This paper collects heuristics of Go Game and employs them to achieve
coverage of dense wireless sensor networks. In this paper, we propose an
algorithm based on Go heuristics and validate it. Investigations show that it
is very promising and could be seen as a good optimization. Keywords: Go
Heuristics, Wireless Sensor Networks, Coverage, Density

ABSTRACT_BEGIN
  Considering real physical (MAC/PHY) traces inside network simulations is a
complex task that might lead to complex yet approximated models. However,
realistic cross-layer analysis with the upper layer and in particular the
transport layer cannot be driven without considering the MAC/PHY level. In this
paper, we propose to cope with this problem by introducing a software that
translates real physical events from a given trace in order to be used inside a
network simulator such as $ns$-2. The main objective is to accurately perform
analysis of the impact of link layer reliability schemes (obtained by the use
of real physical traces) on transport layer performance. We detail the internal
mechanisms and the benefits of this software with a focus on 4G satellite
communications scenarios and present the resulting metrics provided by CLIFT to
perform consistent cross-layer analysis.

ABSTRACT_BEGIN
  Quality of Service (QoS) techniques are applied in IP networks to utilize
available network resources in the most efficient manner to minimize delays and
delay variations (jitters) in network traffic having multiple type of services.
Multimedia services may include voice, video and database. Researchers have
done considerable work on queuing disciplines to analyze and improve QoS
performance in wired and wireless IP networks. This paper highlights QoS
analysis in a wired IP network with more realistic enterprise modeling and
presents simulation results of a few statistics not presented and discussed
before. Four different applications are used i.e. FTP, Database, Voice over IP
(VoIP) and Video Conferencing (VC). Two major queuing disciplines are evaluated
i.e. 'Priority Queuing' and 'Weighted Fair Queuing' for packet identification
under Differentiated Services Code Point (DSCP). The simulation results show
that WFQ has an edge over PQ in terms of queuing delays and jitters experienced
by low priority services. For high priority traffic, dependency of 'Traffic
Drop', 'Buffer Usage' and 'Packet Delay Variation' on selected buffer sizes is
simulated and discussed to evaluate QoS deeper. In the end, it is also analyzed
how network's database service with applied Quality of Service may be affected
in terms of throughput (average rate of data received) for internal network
users when the server is also accessed by external user(s) through Virtual
Private Network (VPN).

ABSTRACT_BEGIN
  Quality of Service (QoS) for MANETs becomes a necessity because of its
applications in decisive situations such as battle fields, flood and earth
quake. Users belonging to diverse hierarchical category demanding various
levels of QoS use MANETs. Sometimes, even a low category user may need to send
an urgent message in time critical applications. Hence providing prioritization
based on user category and urgency of the message the user is sending becomes
necessary. In this paper we propose Enhanced MAC parameters to support Hybrid
Dynamic priority in MANETs(H-MAC). It combines both prioritization based on
user categorization and dynamic exigency. Order Statistics is used to implement
dynamic priority. We propose dynamic TXOP, Proportional AIFS and Proportional
dynamic Backoff timers based on weights and collision, to avoid packet dropping
and starvation of lower priorities. The model is simulated in ns2. We compare
our results with IEEE 802.11e and show that, 16% more throughput is achieved by
H-MAC during extensive collision. We also observe that starvation and packet
drops are reduced with proportionate bandwidth sharing compared to the existing
model.

ABSTRACT_BEGIN
  This paper has been withdrawn by the authors. With the rigorous growth of
cellular network many mobility datasets are available publically, which
attracted researchers to study human mobility fall under spatio-temporal
phenomenon. Mobility profile building is main task in spatio-temporal trend
analysis which can be extracted from the location information available in the
dataset. The location information is usually gathered through the GPS, service
provider assisted faux GPS and Cell Global Identity (CGI). Because of high
power consumption and extra resource installation requirement in GPS related
methods, Cell Global Identity is most inexpensive method and readily available
solution for location information. CGI location information is four set head
i.e. Mobile country code (MCC), Mobile network code (MNC), Location area code
(LAC) and Cell ID, location information is retrieved in form of longitude and
latitude coordinates through any of publically available Cell Id databases e.g.
Google location API using CGI. However due to of fast growth in GSM network,
change in topology by the GSM service provider and technology shift toward 3G
exact spatial extraction is somehow a problem in it, so location extraction
must dealt with spatial outlier's problem first for mobility building. In this
paper we proposed a methodology for the detection of spatial outliers from GSM
CGI data, the proposed methodology is hierarchical clustering based and used
the basic GSM network architecture properties.

ABSTRACT_BEGIN
  This paper has been withdrawn by the authors. Mobility profile building
became extensively examined area in Location based services (LBS) through
extraction of significant locations. Mobility traces are recorded under three
reference positioning systems that are Satellite based i.e. GPS, Network based
i.e. GSM and Local positioning i.e. WLAN, RFID, IrDA. Satellite based and local
positioning due to of high power consumption, additional resource installation,
low accuracy and space limitation are less encouraging. So network based
positioning i.e. GSM is only viable solution for mobility tracing through Cell
global identity (CGI). CGI presents the Cell-ids to extract the significant
locations from mobility history. However CGI faces cell oscillation problem,
where user is assigned multiple Cell-Ids even at a stationary state for load
balancing and GSM cells overlapping. In this paper we proposed two
semi-supervised methodology for cell oscillation resolution i.e. semantic
tagging and overlapped area clustering, the proposed methodologies are equally
useful for the identification of significant places too.

ABSTRACT_BEGIN
  This paper has been withdrawn by the authors. Cellular network data has
become a hot source of study for extraction of user-mobility and
spatio-temporal trends. Location binding in mobility data can be done through
different methods like GPS, service provider assisted faux-GPS and Cell Global
Identity (CGI). Among these Cell Global Identity is most inexpensive method and
readily available solution for mobility extraction; however exact spatial
extraction is somehow a problem in it. This paper presents the spatial
extraction technique of mobile phone user raw data which carries the
information like location information, proximity location and activity of
subjects. This work mainly focuses on the data pre-processing methodology and
technique to interpret the low level mobility data into high level mobility
information using the designed clustering methodology and publically available
Cell-IDs databases. Work proposed the semi- supervised strategy to derive the
missing locations thorough the usage of semantic tag information and removal of
spatial outliers for precise mobility profile building.

ABSTRACT_BEGIN
  Mobile IPv6 control signalling messages generally act as informants to the
home agent (HA) and the correspondent node (CN) regarding a mobile node's
(MN's) new address when its network attachment point changes. Messages should
be protected to avoid different security attacks. In the existing standard,
control signalling messages between HA and MN are frequently authenticated with
IKEv2 and X.509 certificates via IPSec. These signalling messages between MN
and CN are so far protected by an effective but insecure protocol. A protocol
that uses Binding Update Route Optimisation has security vulnerabilities that
allow redirection of traffic by attackers. This traffic is intercepted and then
false binding updates is sent along with packet eavesdropping and Denial of
Service (DoS) that disrupts any communication. Due to lack of ineffective
authentication procedures to ascertain the validity of the users or hide the
location data of HoA and CoA, security issues mentioned above will occur. This
paper presents some of existing protecting control signalling message
protocols, as well as some proposed approaches for designing a secure
method-based private key IP address between an MN and a CN. Using Private Key
Based Binding Update (PKBU), care-of-address (CoA) can thus be protected
against False Binding Update (FBU), Man-in-the-Middle (MITM) and
Denial-of-Service (DoS) attacks.

ABSTRACT_BEGIN
  Now a days Many car manufacturers are planning to install wireless
connectivity equipment in their vehicles to enable communications with
"roadside base station" and also between vehicles, for the purposes of safety,
driving assistance, and entertainment. One distinct feature is that vehicles
are highly mobile, with speed up to 30 m/s, though their mobility patterns are
more predictable than those of nodes in Mobile Ad-hoc Networks (MANET) due to
the constraints imposed by road, speed limits, and commuting habits. Therefore,
these networks require specific solutions and identify a novel research area,
i.e., Vehicular Ad-hoc Networks (VANET). In this paper, we focus on a
particular VSN architecture, where the ad hoc network is operated by a
telecommunication/service provider to combine non-valuable individual sensed
data and extract from them effective feedbacks about the situation of the road
in a geographical area. In operated VSNs, providers tend to reduce the traffic
load on their network, using the free-frequency communication medium (IEEE
802.11p, for example). To do so, we propose TCDGP (Tree Clustered Data
Gathering Protocol), a cross layer protocol based on hierarchical and
geographical data collection, aggregation and dissemination mechanisms. We
analyze the performances of our solution using a simulation environment and
realistic mobility models. We demonstrate the feasibility of such solution and
show that TCDGP offers the operator precious information without overloading
his network.

ABSTRACT_BEGIN
  A fundamental concern of any secure group communication system is key
management and wireless environments create new challenges. One core
requirement in these emerging networks is self-healing. In systems where users
can be offline and miss updates, self-healing allows a user to recover lost
session keys and get back into the secure communication without putting extra
burden on the group manager. Clearly, self-healing must only be available to
authorized users. This paper fixes the problem of collusion attack in an
existing self-healing key distribution scheme and provides a highly efficient
scheme as compared to the existing works. It is computationally secure, resists
collusion attacks made between newly joined users and revoked users and
achieves forward and backward secrecy. Our security analysis is in an
appropriate security model. Unlike the existing constructions, our scheme does
not forbid revoked users from rejoining in later sessions.

ABSTRACT_BEGIN
  For mobile devices, communication via cellular networks consumes more energy,
and has a lower data rate than WiFi networks, and suffers an expensive limited
data plan. However the WiFi network coverage range and density are smaller than
those of the cellular networks. In this work, we present a behavior-aware and
preference-based approach to prefetch news webpages that a user will be
interested in and access, by exploiting the WiFi network connections to reduce
the energy and monetary cost. In our solution, we first design an efficient
preference learning algorithm based on keywords and URLs visited, which will
keep track of the user's changing interests. By predicting the appearance and
durations of the WiFi network connections, our prefetch approach then optimizes
when to prefetch what webpages to maximize the user experience while lowing the
prefetch cost. Our prefetch approach exploits the idle period of WiFi
connections to reduce the tail-energy consumption. We implement our approach in
iPhone. Our extensive evaluations show that our system achieves about 60% hit
ratio, saves about 50% cellular data usage, and reduces the energy cost by 9%.

ABSTRACT_BEGIN
  Adaptive routing algorithm has been employed in multichip interconnection
networks in order to improve network performance. Does a algorithm use local or
global network state? This is the key question in adaptive routing. In many
traffic patterns, the ignorance of global network state, leading to routing
selection based only on local congestion information, tends to violate global
load balance. To attack the load balance issue in adapting routing, some global
adaptive routing algorithms introduce a congestion propagation network to
obtain global network status information, such as Regional Congestion Awareness
(RCA) and Destination Based Adaptive Routing (DBAR).
  However, the congestion propagation network leads to additional power and
area consumption which cannot be ignored. From another view, if we just
increase the bandwidth between neighbor nodes with the wires used to build the
congestion propagation network, the network performance could be improved as
well. In this paper, we propose a global adaptive routing algorithm without
employing the additional congestion propagation network. Our algorithm obtains
the global network state in a novel way, and can offer significant improvement
than the base-line local adaptive routing algorithm (xy-adaptive algorithm
which selects routing based on local congestion information in each hop) for
both medium and high injection rates.
  In wormhole flow control, all the routing information (flit id, source node
id, destination node id, vc id and address) is contained in head flit, and data
is carried in body flits. As a result, there are always many free bits in the
head flit, especially when the bandwidth is 128-bits which is normal in
interconnection network design. Then, we can use these free bits in the head
flit to propagate global congestion information but not increase the number of
flits.

ABSTRACT_BEGIN
  The implementation of MANET for commercial purposes is not an easy task.
Unlike other wireless technologies such as cellular networks, MANET face more
difficult problems concerning management functions, routing and scalability .
As a solution to these complications, clustering schemes are proposed for MANET
in order to organize the network topology in a hierarchical manner. Many
clustering techniques have been developed. Clustering is a method which
aggregates nodes into groups. These groups are contained by the network and
they are known as clusters. By Increasing network capacity and reducing the
routing overhead through clustering brings more efficiency and effectiveness to
scalability in relation to node numbers and the necessity for high mobility.
The manager node in clustering has responsibility for many functions such as
cluster maintenance, routing table updates, and the discovery of new routes
within the network. The other node named as gateway node communicate to the
other cluster. In this paper we remove the cluster head (CH) and given a new
approach in which cluster head and gateway will be same and that node is known
as cluster head gateway (CHG), in which all the responsibilities of cluster
head and gateway will be perform by the Cluster head gateway(CHG) itself. By
applying this approach we reduce of overheads and improve the over all
performance of the network while throughput will be same in both condition with
the help of Exata simulation.

ABSTRACT_BEGIN
  A fast and parallel evolution of ways to measure and assess energy efficiency
in telecom has resulted in an entangled web of drafts and recommendations
originating from government, research, and standards organizations. This paper
focuses primarily on so-called "large network equipment" metrics and intends to
capture state-of-the-art in this area of green communications. Competing
approaches towards efficiency assessment are studied for their applicability
and completeness, with special emphasis on topics relevant to future subject
studies

ABSTRACT_BEGIN
  In this paper, we study the influence of technology, traffic properties and
price trends on optimized design of a reference IP-over-WDM network with rich
underlying fiber topology. In each network node, we investigate the optimal
degree of traffic switching in an optical (lambda) domain versus an electrical
(packet) domain, also known as measure of node transparency. This measure is
studied in connection to changes in traffic volume, demand affinity, optical
circuit speeds and equipment cost. By applying variable design constraints, we
assess the relative roles of the two distinct equipment groups, IP routers and
optical cross-connects, with respect to resulting changes in cost-sensitive
network architectures.

ABSTRACT_BEGIN
  Plankton research has always been an important area of biology. Due to
various environmental issues and other research interests, plankton hatching
and harnessing has been extremely red-marked zone for bio-aqua scientists
recently. To counter this problem, no wireless sensor assisted technique or
mechanism has yet not been devised. In this literature, we propose a novel
approach to pursue this task by the virtue of a theoretical Bio-inspired model
named Hatch-Sens, to automatically monitor different parameters of plankton
hatching in laboratory environment. This literature illustrates the concepts
and detailed mechanisms to accumulate this given problem. Hatch-Sens is a novel
idea which combines the biology with computer in its sensing network to monitor
hatching parameters of Artemia salina. This model reduces the manual tiresome
monitoring of hatching of plankton culture by wireless sensor network.

ABSTRACT_BEGIN
  We consider the problem of link scheduling for throughput maximization in
multihop wireless networks. Majority of previous methods are restricted to
graph-based interference models. In this paper we study the link scheduling
problem using a more realistic physical interference model. Through some key
observations about this model, we develop efficient link scheduling algorithms
by exploiting the intrinsic connections between the physical interference model
and the graph-based interference model. For one variant of the problem where
each node can dynamically adjust its transmission power, we design a scheduling
method with O(g(E)) approximation to the optimal throughput capacity where g(E)
denotes length diversity. For the other variant where each node has a fixed but
possible different transmission powers for different nodes, we design a method
with O(g(E))-approximation ratio when the transmission powers of all nodes are
within a constant factor of each other, and in general with an approximation
ratio of O(g(E)log\rho) where log\rho is power diversity.
  We further prove that our algorithm for fixed transmission power case retains
O(g(E)) approximation for any length-monotone, sub-linear fixed power setting.
Furthermore, all these approximation factors are independent of network size.

ABSTRACT_BEGIN
  This paper outlines how an aerial telecommunications network can optimally
meet the stringent needs of emergency relief and recovery operations. We
propose a novel architecture, made of an integrated and highly dynamic
multi-purpose aerial telecommunications infrastructure that can be contextually
extended with fast-deploying high or low altitude platforms. In particular, we
analyze the interest and challenges of adapting core concepts from substitution
networks and controlled mobility mechanisms, so that a base network can be
seamlessly augmented, both in terms of capacity and functions. We give an
estimation of the emergency traffic supported by the lower altitude platforms
in an example scenario and discuss the challenges posed by this architecture,
notably in terms of disaster resilience and ability to efficiently provide
sustained first responder communications.

ABSTRACT_BEGIN
  In recent years, network deployment based on High Altitude Platforms (HAPs)
has gained momentum through several initiatives where air vehicles and
telecommunications payloads have been adapted and refined, resulting in more
efficient and less expensive platforms. In this paper, we study HAP as an
alternative or complementary fast-evolving technology to provide mobile
services in rural areas of emerging countries, where business models need to be
carefully tailored to the reality of their related markets. In these large
areas with low user density, mobile services uptake is likely to be slowed by a
service profitability which is in turn limited by a relatively low average
revenue per user. Through three architectures enabling different business roles
and using different terrestrial, HAP and satellite backhaul solutions, we
devise how to use in an efficient and profitable fashion these multi-purpose
aerial platforms, in complement to existing access and backhauling satellite or
terrestrial technologies.

ABSTRACT_BEGIN
  Mobility support for the next generation IPv6 networks has been one of the
recent research issues due to the growing demand for wireless services over
internet. In the other hand, 3GPP has introduced IP Multimedia Subsystem as the
next generation IP based infrastructure for wireless and wired multimedia
services. In this paper we present two context transfer mechanisms based on
predictive and reactive schemes, to support seamless handover in IMS over
Mobile IPv6. Those schemes reduce handover latency by transferring appropriate
session information between the old and the new access networks. Moreover, we
present two methods for QoS parameters negotiations to preserve service quality
along the mobile user movement path. The performances of the proposed
mechanisms are evaluated by simulations.

ABSTRACT_BEGIN
  Bluetooth is a promising short-range radio network technology. We present a
low cost and easily deployed, scalable infrastructure for indoor location-based
computing of mobile devices based on Bluetooth technology. The system consists
of 2 main components, namely the Bluetooth (BT) Sensor System and the Central
Navigation System which have been developed using the JDK 6.0. The Bluetooth
Sensor System allows mobile devices whose Bluetooth mode is set to
discoverable, to be scanned and detected, and they receive customizable text
message of their positioning information, e.g. room identity. The positioning
information is also sent to the Central Navigation System which in turn
displays and updates the navigation map. The system is also used to track the
movement of different BT mobile devices within the implemented environment.

ABSTRACT_BEGIN
  In recent years, there are some major changes in the way content is being
distributed over the network. The content distribution techniques have recently
started to embrace peer-to-peer (P2P) systems as an alternative to the
traditional client-server architecture. P2P systemsthat are based on the
BitTorrent protocol uses end-users' resources to provide a cost effective
distribution of bandwidth intensive content to thousands of users. The
BitTorrent protocol system offers a scalable mechanism for distributing a large
volume of data to a set of peers over the Internet. With the growing demand for
file sharing and content distribution, BitTorrent has become one of the most
popular Internet applications and contributes to a signification fraction of
the Internet traffic. With the wide usage of the BitTorrent protocol system, it
has basically solved one of the major problems where data can be quickly
transferred to a group of interested parties. The strength of the BitTorrent
protocol lies in efficient bandwidth utilization for the downloading and
uploading processes. However, the usage of BitTorrent protocol also causes
latency for other applications in terms of network bandwidth which in turn has
caused concerns for the Internet Service Providers, who strives for quality of
service for all their customers. In this paper, we study the network traffic
patterns of theBitTorrent network traffic and investigate its behavior by
usingthe time series ARMA model. Our experimental results show that BitTorrent
network traffic can be modeled and forecasted by using ARMA models. We compared
and evaluated the forecasted network traffic with the real traffic patterns.
This modeling can be utilized by the Internet Service Providers to manage their
network bandwidth and also detect any abnormality in their network.

ABSTRACT_BEGIN
  In wireless networks, the presence of interference among wireless links in-
troduces dependencies among flows that do not share a single link or node. As a
result, when designing a resource allocation scheme, be it a medium access
scheduler or a flow rate controller, one needs to consider the interdependence
among nodes within interference range of each other. Specifically, control
plane information needs to reach nearby nodes which often lie outside the
communi- cation range, but within the interference range of a node of interest.
But how can one communicate control plane information well beyond the existing
communication range? To address this fundamental need we introduce tag
spotting. Tag spotting refers to a communication system which allows re- liable
control data transmission at SNR values as low as 0 dB. It does this by
employing a number of signal encoding techniques including adding redundancy to
multitone modulation, shaping the spectrum to reduce inter-carrier interfer-
ence, and the use of algebraic coding. Making use of a detection theory-based
model we analyze the performance achievable by our modulation as well as the
trade-off between the rate of the information transmitted and the likelihood of
error. Using real-world experiments on an OFDM system built with software
radios, we show that we can transmit data at the target SNR value of 0 dB with
a 6% overhead; that is, 6% of our packet is used for our low-SNR decodable tags
(which carry up to a couple of bytes of data in our testbed), while the remain-
ing 94% is used for traditional header and payload data. We also demonstrate
via simulations how tag spotting can be used in implementing fair and efficient
rate control and scheduling schemes.

ABSTRACT_BEGIN
  In this paper, we propose to evaluate the performance of channel estimation
techniques for Long Term Evolution (LTE) Downlink systems based on Zero Padding
technique (ZP) instead of Cyclic Prefixing (CP). LTE Downlink system is a
multiuser system based on a MIMO-OFDMA technology. Usually, in OFDM systems, a
guard interval is inserted in order to mitigate both inter-carrier interference
(ICI) and inter-symbol interference (ISI). LTE Downlink systems are based on
CP-OFDM technique which consists of a copy a last OFDM symbols inserted at the
beginning of each transmitted OFDM symbol. Although this technique shows good
performances, the CP-LTE system suffers from a power efficiency loss.With the
number of present OFDM symbols in LTE Downlink radio frame, the bandwidth loss
becomes more important. Instead of CP, we propose to evaluate the performance
of ZP-LTE systems in order to avoid the power efficiency .In this paper, we
interest to evaluate the performance of channel estimation techniques for the
two LTE Downlink systems. Simulations results show that although ZP-LTE systems
outperform CP-LTE Downlink systems in terms of power efficiency, the CP-LTE
systems show better performance than ZP-LTE systems and especially for high SNR
values. MATLAB Monte-Carlo simulations are used to evaluate the performance of
LS, LMMSE and Lr-LMMSE estimators in terms of Mean Square Error (MSE) and Bit
Error Rate (BER) for 2x2 LTE Downlink systems.

ABSTRACT_BEGIN
  Preserving security and confidentiality in wireless sensor networks (WSN) are
crucial. Wireless sensor networks in comparison with wired networks are more
substantially vulnerable to attacks and intrusions. In WSN, a third person can
eavesdrop to the information or link to the network. So, preventing these
intrusions by detecting them has become one of the most demanding challenges.
This paper, proposes an improved watchdog technique as an effective technique
for detecting malicious nodes based on a power aware hierarchical model. This
technique overcomes the common problems in the original Watchdog mechanism. The
main purpose to present this model is reducing the power consumption as a key
factor for increasing the network's lifetime. For this reason, we simulated our
model with Tiny-OS simulator and then, compared our results with non
hierarchical model to ensure the improvement. The results indicate that, our
proposed model is better in performance than the original models and it has
increased the lifetime of the wireless sensor nodes by around 2611.492 seconds
for a network with 100 sensors.

ABSTRACT_BEGIN
  Maintaining the quality of service (QOS) and controlling the network
congestion are quite complicated tasks. They cause degrading the performance of
the network, and disturbing the continuous communication process. To overcome
these issues, one step towards this dilemma has been taken in form of
Pre-congestion notification (PCN) technique. PCN uses a packet marking
technique within a PCN domain over IP networks. It is notified by egress node
that works as guard at entry point of network. Egress node gives feedback to
communicating servers whether rate on the link is exceeded than configured
admissible threshold or within the limit. Based on this feedback, admission
decisions are taken to determine whether to allow/block new coming flows or
terminate already accepted. The actual question is about selection of right
algorithm for PCN domain. In this paper, we investigate the analytical behavior
of some known PCN algorithms. We make slide modifications in originality of PCN
algorithms without disquieting working process in order to employ those within
similar types of scenarios. Our goal is to simulate them either in highly
congested or less congested realistic scenarios. On the basis of simulation
done in ns2, we are able to recommend each PCN algorithm for specific
conditions. Finally, we develop a benchmark that helps researchers and
scientific communities to pick the right algorithm. Furthermore, the benchmark
is designed to achieve specific objectives according to the users' requirements
without congesting the network.

ABSTRACT_BEGIN
  Global energy crises are increasing every moment. Every one has the attention
towards more and more energy production and also trying to save it. Electricity
can be produced through many ways which is then synchronized on a main grid for
usage. The main issue for which we have written this survey paper is losses in
electrical system. Weather these losses are technical or non-technical.
Technical losses can be calculated easily, as we discussed in section of
mathematical modeling that how to calculate technical losses. Where as
nontechnical losses can be evaluated if technical losses are known. Theft in
electricity produce non-technical losses. To reduce or control theft one can
save his economic resources. Smart meter can be the best option to minimize
electricity theft, because of its high security, best efficiency, and excellent
resistance towards many of theft ideas in electromechanical meters. So in this
paper we have mostly concentrated on theft issues.

ABSTRACT_BEGIN
  Smart grid is a modified form of electrical grid where generation,
transmission, distribution and customers are not only connected electrically
but also through strong communication network with each other as well as with
market, operation and service provider. For achieving good communication link
among them, it is very necessary to find suitable protocol. In this paper, we
discuss different hardware techniques for power monitoring, power management
and remote power controlling at home and transmission side and also discuss the
suitability of Zigbee for required communication link. Zigbee has major role in
monitoring and direct load controlling for efficient power utilization. It
covers enough area needed for communication and it works on low data rate of
20Kbps to 250Kbps with minimum power consumption. This paper describes the user
friendly control home appliances, power on/off through the internet, PDA using
Graphical User Interface (GUI) and through GSM cellular mobile phone.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) are increasing to handle complex situations
and functions. In these networks some of the nodes become Cluster Heads (CHs)
which are responsible to aggregate data of from cluster members and transmit it
to Base Stations (BS). Those clustering techniques which are designed for
homogenous network are not enough efficient for consuming energy. Stable
Election Protocol (SEP) introduces heterogeneity in WSNs, consisting of two
type of nodes. SEP is based on weighted election probabilities of each node to
become CH according to remaining energy of nodes. We propose
Heterogeneity-aware Hierarchal Stable Election Protocol (HSEP) having two level
of energies. Simulation results show that HSEP prolongs stability period and
network lifetime, as compared to conventional routing protocols and having
higher average throughput than selected clustering protocols in WSNs.

ABSTRACT_BEGIN
  In this paper, we present a comprehensive study of Medium Access Control
(MAC) protocols developed for Wireless Body Area Networks (WBANs). In WBANs,
small batteryoperated on-body or implanted biomedical sensor nodes are used to
monitor physiological signs such as temperature, blood pressure,
ElectroCardioGram (ECG), ElectroEncephaloGraphy (EEG) etc. We discuss design
requirements for WBANs with major sources of energy dissipation. Then, we
further investigate the existing designed protocols for WBANs with focus on
their strengths and weaknesses. Paper ends up with concluding remarks and open
research issues for future work.

ABSTRACT_BEGIN
  This paper presents mathematical framework and study of proactive routing
Protocols. The performance analysis of three major proactive routing protocols:
Destination-Sequenced Distance Vector (DSDV), Fish-eye State Routing (FSR) and
Optimized Link State Routing (OLSR) are under consideration in this work.
Taking these routing protocols into account, we enhance existing framework. In
the next step we further discuss and produce analytical framework by
considering variations in different network and protocol parameters. Finally,
experiments are performed regarding above mentioned routing protocols followed
with detailed comparison and analysis of different environments.

ABSTRACT_BEGIN
  This work focusses on analyzing the optimization strategies of routing
protocols with respect to energy utilization of sensor nodes in Wireless Sensor
Network (WSNs). Different routing mechanisms have been proposed to address
energy optimization problem in sensor nodes. Clustering mechanism is one of the
popular WSNs routing mechanisms. In this paper, we first address energy
limitation constraints with respect to maximizing network life time using
linear programming formulation technique. To check the efficiency of different
clustering scheme against modeled constraints, we select four cluster based
routing protocols; Low Energy Adaptive Clustering Hierarchy (LEACH), Threshold
Sensitive Energy Efficient sensor Network (TEEN), Stable Election Protocol
(SEP), and Distributed Energy Efficient Clustering (DEEC). To validate our
mathematical framework, we perform analytical simulations in MATLAB by choosing
number of alive nodes, number of dead nodes, number of packets and number of
CHs, as performance metrics.

ABSTRACT_BEGIN
  In this paper, we compare problems of cluster formation and cluster-head
selection between different protocols for data aggregation and transmission. We
focus on two aspects of the problem: (i) how to guess number of clusters
required to proficiently consume available sources for a sensor network, and
(ii) how to select number of cluster-heads to cover up sensor networks more
proficiently. A sensor in Wireless Sensor Networks (WSNs) can communicate
directly only with other sensors that are within a radio range in a cluster.
However, in order to enable communication between sensors not within
communication range, they must form new clusters in distributed sensors.
Several clustering algorithms such as LEACH, DEEC, and SEP have been proposed
with the objectives of energy minimization, route-path selection, increased
connectivity and network longevity. LEACH protocol and the similar ones assume
an energy homogeneous system where a node is not likely to fail due to failure
in connectivity and packet dropping. More recent protocols like SEP and TEEN
considered the reverse that is energy heterogeneity which is more applicable to
case of WSNs. We developed a bi-dimensional chain model to select average
number of for DEEC. Simulation results are used to compare performance of
different protocols to found optimal solutions of above mentioned problems.

ABSTRACT_BEGIN
  The advent and development in the field of Wireless Sensor Networks (WSNs) in
recent years has seen the growth of extremely small and low-cost sensors that
possess sensing, signal processing and wireless communication capabilities.
These sensors can be expended at a much lower cost and are capable of detecting
conditions such as temperature, sound, security or any other system. A good
protocol design should be able to scale well both in energy heterogeneous and
homogeneous environment, meet the demands of different application scenarios
and guarantee reliability. On this basis, we have compared six different
protocols of different scenarios which are presenting their own schemes of
energy minimizing, clustering and route selection in order to have more
effective communication. This research is motivated to have an insight that
which of the under consideration protocols suit well in which application and
can be a guide-line for the design of a more robust and efficient protocol.
MATLAB simulations are performed to analyze and compare the performance of
LEACH, multi-level hierarchal LEACH and multihop LEACH.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) contain numerous sensor nodes having limited
power resource, which report sensed data to the Base Station (BS) that requires
high energy usage. Many routing protocols have been proposed in this regard
achieving energy efficiency in heterogeneous scenarios. However, every protocol
is not suitable for heterogeneous WSNs. Efficiency of protocol degrades while
changing the heterogeneity parameters. In this paper, we first test Distributed
Energy- Efficient Clustering (DEEC), Developed DEEC (DDEEC), Enhanced DEEC
(EDEEC) and Threshold DEEC (TDEEC) under several different scenarios containing
high level heterogeneity to low level heterogeneity. We observe thoroughly
regarding the performance based on stability period, network life time and
throughput. EDEEC and TDEEC perform better in all heterogeneous scenarios
containing variable heterogeneity in terms of life time, however TDEEC is best
of all for the stability period of the network. However, the performance of
DEEC and DDEEC is highly effected by changing the heterogeneity parameters of
the network.

ABSTRACT_BEGIN
  IEEE 802.15.4 standard is designed for low power and low data rate
applications with high reliability. It operates in beacon enable and non-beacon
enable modes. In this work, we analyze delay, throughput, load, and end-to-end
delay of nonbeacon enable mode. Analysis of these parameters are performed at
varying data rates. Evaluation of non beacon enabled mode is done in a 10 node
network. We limit our analysis to non beacon or unslotted version because, it
performs better than other. Protocol performance is examined by changing
different Medium Access Control (MAC) parameters. We consider a full size MAC
packet with payload size of 114 bytes. In this paper we show that maximum
throughput and lowest delay is achieved at highest data rate.

ABSTRACT_BEGIN
  In this paper, we present analytical study of routing overhead of reactive
routing protocols for Wireless Multihop Networks (WMhNs). To accomplish the
framework of generalized routing overhead, we choose Ad-Hoc on Demand Distance
Vector (AODV), Dynamic Source Routing (DSR) and Dynamic MANET on Demand (DYMO).
Considering basic themes of these protocols, we enhance the generalized network
models by adding route monitoring overhead. Later, we take different network
parameters and produce framework discussing the impact of variations of these
parameters in network and routing performance. In the second part of our work,
we simulate above mentioned routing protocols and give a brief discussion and
comparison about the environments where these routing protocols perform better.

ABSTRACT_BEGIN
  This paper presents comparison of Access Techniques used in Medium Access
Control (MAC) protocol for Wireless Body Area Networks (WBANs). Comparison is
performed between Time Division Multiple Access (TDMA), Frequency Division
Multiple Access (FDMA), Carrier Sense Multiple Access with Collision Avoidance
(CSMA/CA), Pure ALOHA and Slotted ALOHA (S-ALOHA). Performance metrics used for
comparison are throughput (T), delay (D) and offered load (G). The main goal
for comparison is to show which technique gives highest Throughput and lowest
Delay with increase in Load. Energy efficiency is major issue in WBAN that is
why there is need to know which technique performs best for energy conservation
and also gives minimum delay.

ABSTRACT_BEGIN
  Nowadays, with increase in ageing population, Health care market keeps
growing. There is a need for monitoring of Health issues. Body Area Network
consists of wireless sensors attached on or inside human body for monitoring
vital Health related problems e.g, Electro Cardiogram (ECG),
ElectroEncephalogram (EEG), ElectronyStagmography(ENG) etc. Data is recorded by
sensors and is sent towards Health care center. Due to life threatening
situations, timely sending of data is essential. For data to reach Health care
center, there must be a proper way of sending data through reliable connection
and with minimum delay. In this paper transmission delay of different paths,
through which data is sent from sensor to Health care center over heterogeneous
multi-hop wireless channel is analyzed. Data of medical related diseases is
sent through three different paths. In all three paths, data from sensors first
reaches ZigBee, which is the common link in all three paths. After ZigBee there
are three available networks, through which data is sent. Wireless Local Area
Network (WLAN), Worldwide Interoperability for Microwave Access (WiMAX),
Universal Mobile Telecommunication System (UMTS) are connected with ZigBee.
Each network (WLAN, WiMAX, UMTS) is setup according to environmental
conditions, suitability of device and availability of structure for that
device. Data from these networks is sent to IP-Cloud, which is further
connected to Health care center. Main aim of this paper is to calculate delay
of each link in each path over multihop wireless channel.

ABSTRACT_BEGIN
  Aerial telecommunications networks based on Low Altitude Platforms (LAPs) are
expected to optimally meet the urgent needs of emergency relief and recovery
operations for tackling large scale natural disasters. The energy efficient
operation of such networks is important given the fact that the entire network
infrastructure including the battery operated ground terminals, exhibit
requirements to operate under power constrained situations. In this paper, we
propose and evaluate a real-time adaptive transmission strategy for dynamic
selection of direct and cooperative links based on the channel conditions for
improved energy efficiency. We show that the cooperation between mobile
terrestrial terminals on the ground could improve the energy efficiency in the
uplink depending on the temporal behavior of the terrestrial and the aerial
uplink channels. The simulation analysis corroborates that the adaptive
transmission technique improves the overall energy efficiency of the network.

ABSTRACT_BEGIN
  Heterogeneous wireless networks with wireless devices supporting multitude of
radio access technologies are witnessing increasing interest from network
providers and consumers alike. Energy efficiency in such networks has become an
important design consideration due to the limited battery life of mobile
terminals on one side, and the ever increasing operational expenses pertaining
to energy expenditure on the other. In this paper, we present a routing
protocol for multi-radio multi-hop wireless networks, which aims to achieve a
trade-off between energy consumption in the network and routing delay,
considering both the energy consumption at the devices and the link energy
costs. We also present optimum route-path selection strategies by defining a
utility function to minimize the energy consumption in the network while
maximizing the network lifetime. Using simulations, we verify the utility of
the route-path selection strategies and the efficiency of the energy aware
routing algorithm. It turns out that the proposed protocol is energy efficient
in terms of path selection, with a slight compromise in the end-to-end delay.

ABSTRACT_BEGIN
  An extensive body of research deals with estimating the correlation and the
Hurst parameter of Internet traffic traces. The significance of these
statistics is due to their fundamental impact on network performance. The
coverage of Internet traffic traces is, however, limited since acquiring such
traces is challenging with respect to, e.g., confidentiality, logging speed,
and storage capacity. In this work, we investigate how the correlation of
Internet traffic can be reliably estimated from random traffic samples. These
samples are observed either by passive monitoring within the network, or
otherwise by active packet probes at end systems. We analyze random sampling
processes with different inter-sample distributions and show how to obtain
asymptotically unbiased estimates from these samples. We quantify the inherent
limitations that are due to limited observations and explore the influence of
various parameters, such as sampling intensity, network utilization, or Hurst
parameter on the estimation accuracy. We design an active probing method which
enables simple and lightweight traffic sampling without support from the
network. We verify our approach in a controlled network environment and present
comprehensive Internet measurements. We find that the correlation exhibits
properties such as long range dependence as well as periodicities and that it
differs significantly across Internet paths and observation times.

ABSTRACT_BEGIN
  Dual-Path is an anonymous peer-to-peer approach which provides requester
anonymity. This approach provides anonymity between a requester and a provider
in peer-to-peer networks with trusted servers called suppernode so the provider
will not be able to identify the requester and no other peers can identify the
two communicating parties with certainty. Dual-Path establishes two paths for
transmitting data. These paths called Request path and Response path. The first
one is used for requesting data and the second one is used for sending the
requested data to the requester. As Dual-Path approach is similar to Crowds
approach, this article compares reliability and performance of Dual-Path and
Crowds. For this purpose a simulator is developed and several scenarios are
defined to compare Dual-Path and Crowds in different situations. In chapter 2
and 3 Dual-Path and Crowds approaches are briefly described. Chapter 4 is
talking about simulator. Chapter 5 explains the scenarios for comparison of
performance. Chapter 6 is about comparison of reliability and chapter 7 is
conclusion.

ABSTRACT_BEGIN
  Now a days mobile phones are most often used for data communication rather
than voice calls. Due to this change in user behavior, there is a need to
improve the QoS received by the user. One of the ways of improving the QoS is
an efficient scheduling algorithm which incorporates the needs of the users and
variation in channel condition. The parameters used to measure the efficiency
of the scheduling algorithms are the Jain Fairness Index and the overall system
throughput. In this paper we have proposed a variance based scheduling
algorithm which selects the user who has the highest variance of data
transmitted in a given time frame as a parameter for scheduling. This ensures
that eventually, the users transmit almost equal amounts of data regardless of
channel condition. The simulation results shows that the proposed algorithm
achieves high Jain Fairness Index of 0.92 with a lesser drop in the system
throughput 18% as compared to Dynamically altering Proportionally Fair
Algorithm's 20% using the Proportionally Fair Algorithm as reference.

ABSTRACT_BEGIN
  We analyze the performance of TCP and TCP with network coding (TCP/NC) in
lossy networks. We build upon the framework introduced by Padhye et al. and
characterize the throughput behavior of classical TCP and TCP/NC as a function
of erasure probability, round-trip time, maximum window size, and duration of
the connection. Our analytical results show that network coding masks random
erasures from TCP, thus preventing TCP's performance degradation in lossy
networks. It is further seen that TCP/NC has significant throughput gains over
TCP.
  In addition, we show that TCP/NC may lead to cost reduction for wireless
network providers while maintaining a certain quality of service to their
users. We measure the cost in terms of number of base stations, which is highly
correlated to the energy, capital, and operational costs of a network provider.
We show that increasing the available bandwidth may not necessarily lead to
increase in throughput, particularly in lossy networks in which TCP does not
perform well. We show that using protocols such as TCP/NC, which are more
resilient to erasures, may lead to a throughput commensurate the bandwidth
dedicated to each user.

ABSTRACT_BEGIN
  Cooperative caching is a technique used in mobile ad hoc networks to improve
the efficiency of information access by reducing the access latency and
bandwidth usage. Cache replacement policy plays a significant role in response
time reduction by selecting suitable subset of items for eviction from the
cache. In this paper we have made a review of the existing cache replacement
algorithms proposed for cooperative caching in ad hoc networks. We made an
attempt to classify existing replacement policies for ad hoc networks based on
the replacement decision taken. In addition, this paper suggests some
alternative techniques for cache replacement. Finally, the paper concludes with
a discussion on future research directions.

ABSTRACT_BEGIN
  This paper presents a description of the existing wireless technology Wi-Fi
and WiMAX, and try to compare Wi-Fi (IEEE 802.11) and WiMAX (IEEE 802.16), with
respect to which technology provides a better solution to build a wireless
access infrastructure. Each technology is evaluated based on some key
characteristics. This paper concludes with a statement of, which technology
will be the best and most cost effective solution to end user.

ABSTRACT_BEGIN
  This is the web based application, here we analysis the network traffic which
occurs when the player plays an online game. Here we are going to trace the
current position of the player to rectify the traffic while playing the game.
There are different types of measures for different applications, those can be
normalized and compared with one another but my application can resolve the
inconsistency by knowing the positions quickly and focus on quality of network
(QON) which affects a player to leave the game in middle because of poor
quality of service (QOS). The existing model leads to leave the game because of
network loss from both sides. The proposed model can resolve this problem by
applying the replacement of TCP along with the dejitter buffer (DB) it can
reduce the network loss and by applying the DEAD RECKONING (DR) vector we can
recover the network access because it can view the current position of the
player through this we can rectify why the player leaving the game and checks
the network conditions and try to reactive the game by using mobile devices
automatically or else they receive the message according to the network
conditions.

ABSTRACT_BEGIN
  Epidemic forwarding has been proposed as a forwarding technique to achieve
opportunistic communication in Delay Tolerant Networks. Even if this technique
is well known and widely referred, one has to first deal with several practical
problems before using it. In particular, in order to manage the redundancy and
to avoid useless transmissions, it has been proposed to ask nodes to exchange
information about the buffer content prior to sending information. While Bloom
filter has been proposed to transport the buffer content information, up to our
knowledge no real evaluation has been provided to study the tradeoff that
exists in practice. In this paper we describe an implementation of an epidemic
forwarding scheme using Bloom filters. Then we propose some strategies for
Bloom filter management based on windowing and describe implementation
tradeoffs. By simulating our proposed strategies in ns-3 both with random
waypoint mobility and realistic mobility traces coming from San Francisco
taxicabs, we show that our proposed strategies alleviate the challenge of using
epidemic forwarding in DTNs.

ABSTRACT_BEGIN
  Mobile Ad-hoc Networks (MANETS) consists of a collection of mobile nodes
without having a central coordination. In MANET, node mobility and dynamic
topology play an important role in the performance. MANET provide a solution
for network connection at anywhere and at any time. The major features of MANET
are quick set up, self organization and self maintenance. Routing is a major
challenge in MANET due to it's dynamic topology and high mobility. Several
routing algorithms have been developed for routing. This paper studies the AODV
protocol and how AODV is performed under multiple connections in the network.
Several issues have been identified. The bandwidth is recognized as the
prominent factor reducing the performance of the network. This paper gives an
improvement of normal AODV for simultaneous multiple connections under the
consideration of bandwidth of node.

ABSTRACT_BEGIN
  Wise arrangement of antennas is critical in wireless cellular systems for
both reduction of co-channel interference (CCI) and increase the quality of
service (QoS). In this paper, a novel architecture for antenna arrangement in
CDMA wireless cellular systems is presented. In this architecture that we
called microzone, every cell is divided into three (or more) zones and
information transmission in downlink channel is done by an antenna which is
placed at the outer region of the related zone. Also, the transmitting signal
by the mobile station (MS) in uplink channel is received by all antennas of the
related cell. Analytical calculations of the received signal to noise ratio
(SIR) and outage probability for both microzone and used architectures show
that proposed architecture has better performance in compared with the used
architecture. Also, simulation results confirm lower outage probability in
uplink channel for microzone architecture.

ABSTRACT_BEGIN
  Radio Frequency (RF) Energy Harvesting holds a promising future for
generating a small amount of electrical power to drive partial circuits in
wirelessly communicating electronics devices. Reducing power consumption has
become a major challenge in wireless sensor networks. As a vital factor
affecting system cost and lifetime, energy consumption in wireless sensor
networks is an emerging and active research area. This chapter presents a
practical approach for RF Energy harvesting and management of the harvested and
available energy for wireless sensor networks using the Improved Energy
Efficient Ant Based Routing Algorithm (IEEABR) as our proposed algorithm. The
chapter looks at measurement of the RF power density, calculation of the
received power, storage of the harvested power, and management of the power in
wireless sensor networks. The routing uses IEEABR technique for energy
management. Practical and real-time implementations of the RF Energy using
Powercast harvesters and simulations using the energy model of our Libelium
Waspmote to verify the approach were performed. The chapter concludes with
performance analysis of the harvested energy, comparison of IEEABR and other
traditional energy management techniques, while also looking at open research
areas of energy harvesting and management for wireless sensor networks.

ABSTRACT_BEGIN
  This paper presents a system consisting of the FPGA IP core, the simple
network protocol and the Linux device driver, capable of efficient and reliable
data transmission from a low resources FPGA chip to the Linux-based embedded
computer system, via a private Ethernet network (consisting of a single segment
or a few segments connected via an Ethernet switch). The embedded system may
optionally process the acquired data, and distribute them further, using
standard network protocols. Proposed design targets cost-efficient multichannel
data acquisition systems, in which multiple FPGA based front end boards (FEB)
should transmit the stream of acquired data to the computer network,
responsible for their final processing and archiving. The presented solution
allows to minimize the cost of data concentration due to use of inexpensive
Ethernet network infrastructure. The work is mainly focused on minimization of
resources consumption in the FPGA, and minimization of acknowledge latency in
the Linux based system - which allows to achieve high throughput in spite of
use of inexpensive FPGA chips with small internal memory.

ABSTRACT_BEGIN
  Electric vehicles (EVs) offer an attractive long-term solution to reduce the
dependence on fossil fuel and greenhouse gas emission. However, a fleet of EVs
with different EV battery charging rate constraints, that is distributed across
a smart power grid network requires a coordinated charging schedule to minimize
the power generation and EV charging costs. In this paper, we study a joint
optimal power flow (OPF) and EV charging problem that augments the OPF problem
with charging EVs over time. While the OPF problem is generally nonconvex and
nonsmooth, it is shown recently that the OPF problem can be solved optimally
for most practical power networks using its convex dual problem. Building on
this zero duality gap result, we study a nested optimization approach to
decompose the joint OPF and EV charging problem. We characterize the optimal
offline EV charging schedule to be a valley-filling profile, which allows us to
develop an optimal offline algorithm with computational complexity that is
significantly lower than centralized interior point solvers. Furthermore, we
propose a decentralized online algorithm that dynamically tracks the
valley-filling profile. Our algorithms are evaluated on the IEEE 14 bus system,
and the simulations show that the online algorithm performs almost near
optimality ($<1%$ relative difference from the offline optimal solution) under
different settings.

ABSTRACT_BEGIN
  We design and implement a network-coding-enabled reliability architecture for
next generation wireless networks. Our network coding (NC) architecture uses a
flexible thread-based design, with each encoder-decoder instance applying
systematic intra-session random linear network coding as a packet erasure code
at the IP layer, to ensure the fast and reliable transfer of information
between wireless nodes.
  Using Global Environment for Network Innovations (GENI) WiMAX platforms, a
series of point-to-point transmission experiments were conducted to compare the
performance of the NC architecture to that of the Automatic Repeated reQuest
(ARQ) and Hybrid ARQ (HARQ) mechanisms. At the application layer, Iperf and
UDP-based File Transfer Protocol (UFTP) are used to measure throughput, packet
loss and file transfer delay. In our selected scenarios, the proposed
architecture is able to decrease packet loss from around 11-32% to nearly 0%;
compared to HARQ and joint HARQ/ARQ mechanisms, the NC architecture offers up
to 5.9 times gain in throughput and 5.5 times reduction in end-to-end file
transfer delay. Our experiments show that network coding as a packet erasure
code in the upper layers of the protocol stack has the potential to reduce the
need for joint HARQ/ARQ schemes in the PHY/MAC layers, thus offering insights
into cross-layer designs of efficient next generation wireless networks.

ABSTRACT_BEGIN
  SYN-flooding attack uses the weakness available in TCP's three-way handshake
process to keep it from handling legitimate requests. This attack causes the
victim host to populate its backlog queue with forged TCP connections. In other
words it increases Ploss (probability of loss) and Pa (buffer occupancy
percentage of attack requests) and decreases Pr (buffer occupancy percentage of
regular requests) in the victim host and results to decreased performance of
the host. This paper proposes a self-managing approach, in which the host
defends against SYN-flooding attack by dynamically tuning of its own two
parameters, that is, m (maximum number of half-open connections) and h (hold
time for each half-open connection). In this way, it formulates the defense
problem to an optimization problem and then employs the learning automata (LA)
algorithm to solve it. The simulation results show that the proposed defense
strategy improves performance of the under attack system in terms of Ploss, Pa
and Pr.

ABSTRACT_BEGIN
  Link failures in wide area networks are common and cause significant data
losses. Mesh-based protection schemes offer high capacity efficiency but they
are slow and require complex signaling. Additionally, real-time
reconfigurations of cross-connects threaten their transmission integrity. On
the other hand, there are other schemes that are proactive. Proactivity results
in higher restoration speed, lower signaling complexity, and higher
transmission integrity. This paper introduces a coding-based proactive
protection scheme, named Coded Path Protection (CPP). In CPP, a backup stream
of the primary data is encoded with other data streams, resulting in capacity
savings. In addition to a systematic approach of building valid coding
structures, this paper presents an optimal and simple capacity placement and
coding group formation algorithm. The algorithm converts the sharing structure
of any solution of a Shared Path Protection (SPP) technique into a coding
structure with minimum extra capacity. We conducted quantitative and
qualitative comparisons of our technique with the SPP. Simulation results
confirm that CPP provides faster link failure recovery than SPP while it incurs
marginal extra capacity beyond that of SPP. In this Part 1 of the paper, we
describe the theory and an algorithm for converting a given SPP solution into a
CPP solution.

ABSTRACT_BEGIN
  In Part 1 of this paper, we introduced a coding-based proactive network
protection scheme, named Coded Path Protection (CPP). In CPP, a backup stream
of the primary data is encoded with other data streams, resulting in capacity
savings. In addition to being a systematic approach of building valid coding
structures, CPP is an optimal and simple capacity placement and coding group
formation algorithm. It converts the sharing structure of any solution of a
Shared Path Protection (SPP) technique into a coding structure with minimum
extra capacity. In this Part 2 of the paper, we describe the implementation of
our algorithm using Integer Linear Programming (ILP), its timing and
synchronization requirements, and implementation issues in networks. We present
simulation results which confirm that CPP provides faster link failure recovery
than SPP while it incurs marginal extra capacity beyond that of SPP.

ABSTRACT_BEGIN
  Transmission over multiple frequency bands combined into one logical channel
speeds up data transfer for wireless networks. On the other hand, the
allocation of multiple channels to a single user decreases the probability of
finding a free logical channel for new connections, which may result in a
network-wide throughput loss. While this relationship has been studied
experimentally, especially in the WLAN configuration, little is known on how to
analytically model such phenomena. With the advent of Opportunistic Spectrum
Access (OSA) networks, it is even more important to understand the
circumstances in which it is beneficial to bond channels occupied by primary
users with dynamic duty cycle patterns. In this paper we propose an analytical
framework which allows the investigation of the average channel throughput at
the medium access control layer for OSA networks with channel bonding enabled.
We show that channel bonding is generally beneficial, though the extent of the
benefits depend on the features of the OSA network, including OSA network size
and the total number of channels available for bonding. In addition, we show
that performance benefits can be realized by adaptively changing the number of
bonded channels depending on network conditions. Finally, we evaluate channel
bonding considering physical layer constraints, i.e. throughput reduction
compared to the theoretical throughput of a single virtual channel due to a
transmission power limit for any bonding size.

ABSTRACT_BEGIN
  In this paper, a link adaptation and untrusted relay assignment (LAURA)
framework for efficient and reliable wireless cooperative communications with
physical layer security is proposed. Using sharp channel codes in different
transmission modes, reliability for the destination and security in the
presence of untrusted relays (low probability of interception) are provided
through rate and power allocation. Within this framework, several schemes are
designed for highly spectrally efficient link adaptation and relay selection,
which involve different levels of complexity and channel state information
requirement. Analytical and simulation performance evaluation of the proposed
LAURA schemes are provided, which demonstrates the effectiveness of the
presented designs. The results indicate that power adaptation at the source
plays a critical role in spectral efficiency performance. Also, it is shown
that relay selection based on the signal to noise ratio of the source to relays
channels provides an interesting balance of performance and complexity within
the proposed LAURA framework.

ABSTRACT_BEGIN
  This paper presents an energy efficient routing algorithm for heterogeneous
Wireless Body Area Sensor Networks (WBASNs). A prototype is defined for
employing heterogeneous sensors on human body. Direct communication is used for
real-time traffic (critical data) and on-demand data while multi-hop
communication is used for normal data delivery in this proposed routing
algorithm. One of the prime challenges in WBASNs is sensing of heat generated
by implanted sensor nodes. The proposed routing algorithm is thermal-aware
which sense the link Hot-spot and routes the data away from these links.
Continuous mobility of human body causes disconnection between previous
established links. We introduce mobility support and energy-management to
overcome the problem of disconnection due to continuous mobility of human body.
MATLAB simulations of proposed routing algorithm are performed for lifetime and
reliability in comparison with multi-hop communication. The results show that
the proposed routing algorithm has less energy consumption and more reliable as
compared to multi-hop communication.

ABSTRACT_BEGIN
  Despite much theoretical work, different modifications of backoff protocols
in 802.11 networks lack empirical evidence demonstrating their real-life
performance. To fill the gap we have set out to experiment with performance of
exponential backoff by varying its backoff factor. Despite the satisfactory
results for throughput, we have witnessed poor fairness manifesting in severe
capture effect. The design of standard backoff protocol allows already
successful nodes to remain successful, giving little chance to those nodes that
failed to capture the channel in the beginning. With this at hand, we ask a
conceptual question: Can one improve the performance of wireless backoff by
introducing a mechanism of self-penalty, when overly successful nodes are
penalized with big contention windows? Our real-life measurements using
commodity hardware demonstrate that in many settings such mechanism not only
allows to achieve better throughput, but also assures nearly perfect fairness.
We further corroborate these results with simulations and an analytical model.
Finally, we present a backoff factor selection protocol which can be
implemented in access points to enable deployment of the penalty backoff
protocol to consumer devices.

ABSTRACT_BEGIN
  Coordinated multi-point (CoMP) transmission has been widely recognized as a
spectrally efficient technique in future cellular systems. To exploit the
abundant patial resources provided by the cooperating base stations, however,
considerable training overhead is required to acquire the channel information.
To avoid the extra overhead outweighing the cooperative gain, we propose a
method that allows each user to select transmission mode between coherent CoMP
and Non-CoMP. We first analyze the average throughput of each user under CoMP
and Non-CoMP transmission after taking into account the downlink training
overhead. A closed-form mode selection rule is then developed, which depends on
the user location and system settings, i.e, the number of cooperating base
stations and transmit antennas, training overhead and cell-edge signal to noise
ratio. Simulation results show that the proposed downlink transmission mode
selection method achieves higher throughput than CoMP for cell-center users and
than Non-CoMP for cell-edge users after accounting for the overhead. As a
by-product, the backhaul load is also reduced significantly.

ABSTRACT_BEGIN
  Routing in wireless mesh networks (WMNs) has been an active area of research
for the last several years. In this paper, we address the problem of packet
routing for efficient data forwarding in wireless mesh networks (WMNs) with the
help of smart ants acting as intelligent agents. The aim of this paper is to
study the use of such biologically inspired agents to effectively route the
packets in WMNs. In particular, we propose AntMesh, a distributed
interference-aware data forwarding algorithm which enables the use of smart
ants to probabilistically and concurrently perform the routing and data
forwarding in order to stochastically solve a dynamic network routing problem.
AntMesh belongs to the class of routing algorithms inspired by the behaviour of
real ants which are known to find a shortest path between their nest and a food
source. In addition, AntMesh has the capability to effectively utilize the
space/channel diversity typically common in multi radio WMNs and to discover
high throughput paths with less inter-flow and intra-flow interference while
conventional wireless network routing protocols fail to do so. We implement our
smart ant-based routing algorithm in ns-2 and carry out extensive evaluation.
We demonstrate the stability of AntMesh in terms of how quickly it adapts
itself to the changing dynamics or load on the network. We tune the parameters
of AntMesh algorithm to study the effect on its performance in terms of the
routing load and end-to-end delay and have tested its performance under various
network scenarios particularly fixed nodes mesh networks and also on mobile WMN
scenarios. The results obtained show AntMesh's advantages that make it a
valuable candidate to operate in mesh networks.

ABSTRACT_BEGIN
  We propose to use Vehicular ad hoc networks (VANET) as the infrastructure for
an urban cyber-physical system for gathering up-to-date data about a city, like
traffic conditions or environmental parameters. In this context, it is critical
to design a data collection protocol that enables retrieving the data from the
vehicles in almost real-time in an efficient way for urban scenarios.
  We propose Back off-based Per-hop Forwarding (BPF), a broadcast-based
receiver-oriented protocol that uses the destination location information to
select the forwarding order among the nodes receiving the packet. BFP does not
require nodes to exchange periodic messages with their neighbors communicating
their locations to keep a low management message overhead. It uses geographic
information about the final destination node in the header of each data packet
to route it in a hop-by-hop basis. It takes advantage of redundant forwarding
to increase packet delivery to a destination, what is more critical in an urban
scenario than in a highway, where the road topology does not represent a
challenge for forwarding.
  We evaluate the performance of the BPF protocol using ns-3 and a Manhattan
grid topology and compare it with well-known broadcast suppression techniques.
Our results show that BPF achieves significantly higher packet delivery rates
at a reduced redundancy cost.

ABSTRACT_BEGIN
  The Autonomous System (AS) topology of the Internet (up to 61k ASs) is
growing at a rate of about 10% per year. The Border Gateway Protocol (BGP)
starts to show its limits in terms of the number of routing table entries it
can dynamically process and control. Due to the increasing routing information
processing and storage, the same trend is observed for routing model simulators
such as DRMSim specialized in large-scale simulations of routing models.
Therefore, DRMSim needs enhancements to support the current size of the
Internet topology and its evolution (up to 100k ASs). To this end, this paper
proposes a feasibility study of the extension of DRMSim so as to support the
Distributed Parallel Discrete Event paradigm. We first detail the possible
distribution models and their associated communication overhead. Then, we
analyze this overhead by executing BGP on a partitioned topology according to
different scenarios. Finally, we conclude on the feasibility of such a
simulator by computing the expected additional time required by a distributed
simulation of BGP compared to its sequential simulation.

ABSTRACT_BEGIN
  Spectrum sensing is one of the key topics towards the implementation of
future wireless services like SuperWiFi. This new wireless proposal aims at
using the freed spectrum resulting from the analog-to-digital transition of TV
channels for wireless data transmission (UHF TV White Spaces). The benefits
range from better building penetration to longer distances when compared to the
set of IEEE 802.11 standards. Nevertheless, the effective use of the available
spectrum is subject to strict regulation that prohibits unlicensed users to
interfere with incumbents (like wireless microphones). Cognitive Radios (CR)
and dynamic spectrum allocation are suggested to cope with this problem. These
techniques consist on frequency sweeps of the TV-UHF band to detect White
Spaces that could be used for SuperWiFi transmissions. In this paper we develop
and implement algorithms from GNURadio in the Ettus USRP-E110 to build a
standalone White Spaces detector that can be consulted from a centralized
location via IP networks.

ABSTRACT_BEGIN
  In this work, we propose a novel power allocation mechanism which allows one
to optimize the energy-efficiency of base stations operating in the downlink.
The energy-efficiency refers to the amount of bits that can be transmitted by
the base station per unit of energy consumed. This work studies the impact of
flow-level dynamics on the energy efficiency of base stations, by considering
user arrivals and departures. Our proposed power allocation scheme optimizes
the energyefficiency, accounting for the dynamic nature of users (referred to
as the global energy-efficiency). We emphasize our numerical results that study
the influence of the radio conditions, transmit power and the user traffic on
the energy-efficiency in an LTE compliant framework. Finally, we show that the
power allocation scheme that considers traffic dynamics, is significantly
different from the power allocation scheme when the number of users is
considered as constant, and that it has a better performance.

ABSTRACT_BEGIN
  In this work, we propose a new energy efficiency metric which allows one to
optimize the performance of a wireless system through a novel power control
mechanism. The proposed metric possesses two important features. First, it
considers the whole power of the terminal and not just the radiated power.
Second, it can account for the limited buffer memory of transmitters which
store arriving packets as a queue and transmit them with a success rate that is
determined by the transmit power and channel conditions. Remarkably, this
metric is shown to have attractive properties such as quasi-concavity with
respect to the transmit power and a unique maximum, allowing to derive an
optimal power control scheme. Based on analytical and numerical results, the
influence of the packet arrival rate, the size of the queue, and the
constraints in terms of quality of service are studied. Simulations show that
the proposed cross-layer approach of power control may lead to significant
gains in terms of transmit power compared to a physical layer approach of green
communications.

ABSTRACT_BEGIN
  Mobile Adhoc Network is a wireless network without infrastructure.It is a
kind of wireless adhoc network,and is a self configuring network of mobile
routers connected by wireless links.The routers are free to move randomly and
organize themselves arbitrarily,thus the network's wireless topology may change
rapidly and unpredictably. Such a network may operate in a standalone
fashion,or may be connected to the larger Internet.There are various routing
protocols available for MANET.The most popular ones are DSR,AODV and DSDV.This
paper examines two routing protocols for mobile ad hoc networks :the
Destination Sequenced Distance Vector,the table driven protocol and the Ad hoc
On Demand Distance Vector routing,an On Demand protocol and evaluates both
protocols based on packet delivery fraction, average end to end
delay,throughput and routing overhead while varying pause time.The performance
evaluation has been done by using simulation tool NS2 which is the main
simulator, NAM(Network Animator)and excel graph which is used for preparing the
graphs from the trace files.Simulation revealed that although DSDV perfectly
scales to small networks with low node speeds,AODV is preferred due to its more
efficient use of bandwidth.

ABSTRACT_BEGIN
  The smart grid is envisioned to significantly enhance the efficiency of
energy consumption, by utilizing two-way communication channels between
consumers and operators. For example, operators can opportunistically leverage
the delay tolerance of energy demands in order to balance the energy load over
time, and hence, reduce the total operational cost. This opportunity, however,
comes with security threats, as the grid becomes more vulnerable to
cyber-attacks. In this paper, we study the impact of such malicious
cyber-attacks on the energy efficiency of the grid in a simplified setup. More
precisely, we consider a simple model where the energy demands of the smart
grid consumers are intercepted and altered by an active attacker before they
arrive at the operator, who is equipped with limited intrusion detection
capabilities. We formulate the resulting optimization problems faced by the
operator and the attacker and propose several scheduling and attack strategies
for both parties. Interestingly, our results show that, as opposed to
facilitating cost reduction in the smart grid, increasing the delay tolerance
of the energy demands potentially allows the attacker to force increased costs
on the system. This highlights the need for carefully constructed and robust
intrusion detection mechanisms at the operator.

ABSTRACT_BEGIN
  We study the problem of optimizing the cost of content delivery in a
cooperative network of caches at end-nodes. The caches could be, for example,
within the computers of users downloading videos from websites (such as
Netflix, Blockbuster etc.), DVRs (such as TiVo, or cable boxes) used as part of
video on demand services or public hot-spots (e.g. Wi-Fi access points with a
cache) deployed over a city to serve content to mobile users. Each cache serves
user requests locally over a medium that incurs no additional costs (i.e. WiFi,
home LAN); if a request is not cached, it must be fetched from another cache or
a central server. In our model, each cache has a tiered back-haul internet
connection, with a usage cap (and fixed per-byte costs thereafter). Redirecting
requests intended for the central server to other caches with unused back-haul
capacity can bring down the network costs. Our goal is to develop a mechanism
to optimally 1) place data into the caches and 2) route requests to caches to
reduce the overall cost of content delivery.
  We develop a multi-criteria approximation based on a LP rounding procedure
that with a small (constant factor) blow-up in storage and upload limits of
each cache, gives a data placement that is within constant factor of the
optimum. Further, to speed up the solution, we propose a technique to cluster
caches into groups, solve the data placement problem within a group, and
combine the results in the rounding phase to get the global solution.Based on
extensive simulations, we show that our schemes perform very well in practice,
giving costs within $5--15$% to the optimal, and reducing the network load at a
central server by as much as 55% with only a marginal blow up in the limits.
Also we demonstrate that our approach out-performs a non-cooperative caching
mechanism by about 20%.

ABSTRACT_BEGIN
  A large volume of research has been conducted in the cognitive radio (CR)
area the last decade. However, the deployment of a commercial CR network is yet
to emerge. A large portion of the existing literature does not build on real
world scenarios, hence, neglecting various important interactions of the
research with commercial telecommunication networks. For instance, a lot of
attention has been paid to spectrum sensing as the front line functionality
that needs to be completed in an efficient and accurate manner to enable an
opportunistic CR network architecture. This is necessary to detect the
existence of spectrum holes without which no other procedure can be fulfilled.
However, simply sensing (cooperatively or not) the energy received from a
primary transmitter cannot enable correct dynamic spectrum access. For example,
the low strength of a primary transmitter's signal does not assure that there
will be no interference to a nearby primary receiver. In addition, the presence
of a primary transmitter's signal does not mean that CR network users cannot
access the spectrum since there might not be any primary receiver in the
vicinity. Despite the existing elegant and clever solutions to the DSA problem
no robust, implementable scheme has emerged. In this paper, we challenge the
basic premises of the proposed schemes. We further argue that addressing the
technical challenges we face in deploying robust CR networks can only be
achieved if we radically change the way we design their basic functionalities.
In support of our argument, we present a set of real-world scenarios, inspired
by realistic settings in commercial telecommunications networks, focusing on
spectrum sensing as a basic and critical functionality in the deployment of
CRs. We use these scenarios to show why existing DSA paradigms are not amenable
to realistic deployment in complex wireless environments.

ABSTRACT_BEGIN
  In this paper, we use statistical tools to analysis dependency between
Wireless Sensor Network (WSN) parameters and overall Energy Consumption (EC).
Our approach has two main phases: profiling, and effective parameter
extraction. In former, a sensor network simulator is re-run 800 times with
different values for eight WSN parameters to profile consumed energy in nodes;
then in latter, three statistical analyses (p-value, linear and non-linear
correlation) are applied to the outcome of profiling phase to extract the most
effective parameters on WSN overall energy consumption.

ABSTRACT_BEGIN
  Battery is a major hardware component of wireless sensor networks. Most of
them have no power supply and are generally deployed for a long time.
Researches have been done on battery physical model and their adaptation for
sensors. We present an implementation on a real sensor operating system and how
architectural constraints have been assumed. Experiments have been made in
order to test the impact of some parameter, as the application throughput, on
the battery lifetime.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSN) are set of energy-limited sensors, which
recently have been point of interest due to their vast applications. One of the
efficient ways to consume energy in these networks is to utilize optimal
routing protocols. In this approach, we proposed a greedy hierarchical
chain-based routing method, named, PGC (stands for Persian Greedy Chain) which
route the network applying Spread Spectrum codes as a mask given to the grid
cells. Due to similarities between the proposed method in this article and
LEACH protocol, we compare this routing protocol with the proposed model from
diverse aspects in the simulation section such as remaining energy and being
fault tolerant and reliable. The results prove that presented method is more
robust and efficient.

ABSTRACT_BEGIN
  Identifying the occurrence of congestion in a Mobile Ad-hoc Network (MANET)
is a major task. The inbuilt congestion control techniques of existing
Transmission Control Protocol (TCP) designed for wired networks do not handle
the unique properties of shared wireless multi-hop link. There are several
approaches proposed for detecting and overcoming the congestion in the mobile
ad-hoc network. In this paper we present a Modified AD-hoc Transmission Control
Protocol (M-ADTCP) method where the receiver detects the probable current
network status and transmits this information to the sender as feedback. The
sender behavior is altered appropriately. The proposed technique is also
compatible with standard TCP.

ABSTRACT_BEGIN
  This paper discusses a new energy aware routing scheme which uses variable
transmission range. The protocol has been incorporated along with the route
discovery procedure of AODV as a case study. Both the protocols are simulated
using Network Simulator and comparisons are made to analyze their performance
based on energy consumption, network lifetime and number of alive nodes metrics
for different network scenarios. The results show that EAR makes effective node
energy utilization.

ABSTRACT_BEGIN
  This short report intends to provide an overview of the procedure and
statistics of establishing a TCP/IP link between two USRP-E110. The testings
are performed using an example GNURadio code and the networking protocol stack
provided by the Linux operating system embedded in the USRP-E110.

ABSTRACT_BEGIN
  In this paper we describe a prototype of a simulation framework and some
ideas which are to be used to study performance of a P2P TV system in a
controllable and adjustable environment. We created a simplified model
describing live video distribution in a P2P TV system. Using the model we
analyse how some of the system parameters influence its behaviour. We present
the preliminary results obtained at different granularity levels of
measurements, describing the macroscopic system performance as well as the
performance of its individual components.

ABSTRACT_BEGIN
  In data gathering applications which is a typical application paradigm in
wireless sensor networks, sensor nodes may have different traffic demands.
Assigning equal channel access to each node may lead to congestion, inefficient
use of the bandwidth and decrease of the application performance. In this
paper, we prove that the time slot assignment problem is NP-complete when p-hop
nodes are not assigned the same slot, with 1 <= p <= h for any strictly
positive integer h. We propose TRASA, a TRaffic Aware time Slot Assignment
algorithm able to allocate slots to sensors proportionally to their demand. We
evaluate the performance of TRASA for different heuristics and prove that it
provides an optimized spatial reuse and a minimized cycle length.

ABSTRACT_BEGIN
  Coloring is used in wireless networks to improve communication efficiency,
mainly in terms of bandwidth, energy and possibly end-to-end delays. In this
paper, we define the h-hop node coloring problem, with h any positive integer,
adapted to two types of applications in wireless networks. We specify both
general mode for general applications and strategic mode for data gathering
applications.We prove that the associated decision problem is NP-complete. We
then focus on grid topologies that constitute regular topologies for large or
dense wireless networks. We consider various transmission ranges and identify a
color pattern that can be reproduced to color the whole grid with the optimal
number of colors. We obtain an optimal periodic coloring of the grid for the
considered transmission range. We then present a 3-hop distributed coloring
algorithm, called SERENA. Through simulation results, we highlight the impact
of node priority assignment on the number of colors obtained for any network
and grids in particular. We then compare these optimal results on grids with
those obtained by SERENA and identify directions to improve SERENA.

ABSTRACT_BEGIN
  Implementations of tracking systems have become prevalent issues in modern
technology due to its advantage of location detection of objects. Objects are
usually tracked using trackers based on GPS, GSM, RFID and Bluetooth signal
strength implementation. These mechanisms usually require line of sight
operations, limited coverage and low level programming language for accessing
Bluetooth signal strength. This paper presents an alternative technique for
tracking the movement of indoor objects based on Bluetooth communication
technology, principles of motion and least square statistical method.
Algorithms are designed and implemented using Java

ABSTRACT_BEGIN
  The Multi-Carrier Code Division Multiple Access (MC-CDMA) is becoming a very
significant downlink multiple access technique for high-rate data transmission
in the fourth generation wireless communication systems. By means of efficient
resource allocation higher data rate i.e. throughput can be achieved. This
paper evaluates the performance of criteria used for group (subchannel)
allocation employed in downlink transmission, which results in throughput
maximization. Proposed algorithm gives the modified technique of sub channel
allocation in the downlink transmission of MC-CDMA systems. Simulation are
carried out for all the three combining schemes, results shows that for the
given power and BER proposed algorithm comparatively gives far better results

ABSTRACT_BEGIN
  Although the performance of the medium access control (MAC) of the IEEE
802.15.4 has been investigated under the assumption of ideal wireless channel,
the understanding of the cross-layer dynamics between MAC and physical layer is
an open problem when the wireless channel exhibits path loss, multi-path
fading, and shadowing. The analysis of MAC and wireless channel interaction is
essential for consistent performance prediction, correct design and
optimization of the protocols. In this paper, a novel approach to analytical
modeling of these interactions is proposed. The analysis considers
simultaneously a composite channel fading, interference generated by multiple
terminals, the effects induced by hidden terminals, and the MAC reduced carrier
sensing capabilities. Depending on the MAC parameters and physical layer
thresholds, it is shown that the MAC performance indicators over fading
channels can be far from those derived under ideal channel assumptions. As
novel results, we show to what extent the presence of fading may be beneficial
for the overall network performance by reducing the multiple access
interference, and how this can be used to drive joint selection of MAC and
physical layer parameters.

ABSTRACT_BEGIN
  One of the important issues in wireless networks is the Routing problem that
is effective on system performance, in this article the attempt is made to
propose a routing algorithm using the bee colony in order to reduce energy
consumption in wireless relay networks. In EBCD algorithm, through combined of
energy, distance and traffic parameters a routing algorithm for wireless
networks is presented with more efficiency than its predecessor. Applying the
bee colony method would allow the placement of the parameters under
conventional conditions and to get closer to a mechanism with a better
adaptability than that of the existing algorithm. According to the parameters
considered, the proposed algorithm provides a fitness function that can be
applied as a multi-hop. Unlike other algorithms of its kind this can increase
service quality based on environmental conditions through its multiple
services. This new method can store the energy accumulated in the nodes and
reduce the hop restrictions.

ABSTRACT_BEGIN
  File sharing, typically involving video or audio material in which copyright
may persist and using peer-to-peer (P2P) networks like BitTorrent, has been
reported to make up the bulk of Internet traffic. The free-riding problem
appears in this "digital gift economy" but its users exhibit rational
behaviour, subject to the characteristics of the particular network. The high
demand for the Internet as a delivery channel for entertainment underlines the
importance of understanding the dynamics of this market, especially when
considering possible business models for future pricing or licensing regimes
and for the provisioning of network capacity to support future services. The
availability of specific titles on file sharing networks is the focus of this
paper, with a special emphasis on the P2P protocol BitTorrent. The paper
compares the incentives provided in BitTorrent to those in other file-sharing
communities, including file hosting, and discusses the number of titles
available in the community at any given time, with an emphasis on popular video
items with ambiguous legal status.

ABSTRACT_BEGIN
  Cooperative multicast is an effective solution to address the bottleneck
problem of single-hop broadcast in wireless networks. By incorporating with the
random linear network coding technique, the existing schemes can reduce the
retransmission overhead significantly. However, the receivers may incur large
decoding delay and complexity due to the batch decoding scheme. In addition,
the dependency on the explicit feedback leads to scalability problem in larger
networks. In this paper, a cooperative multicast protocol named MWNCast is
proposed based on a novel moving window network coding technique. We prove
three properties of the proposed scheme. Firstly, without explicit feedback,
MWNCast can approach the cooperative capacity with the packet loss probability
dropping almost exponentially with the increase of window size. Secondly, the
average decoding delay of a receiver is on the order of
$O(\frac{1}{(1-\rho)^2})$ with respect to its traffic intensity $\rho$.
Thirdly, MWNCast can achieve the linear decoding complexity of $O(W)$ with
respect to the window size $W$. Simulation results show that MWNCast
outperforms the existing schemes by achieving better tradeoff between the
throughput and decoding delay, meanwhile keeping the packet loss probability
and decoding complexity at a very low levelwithout explicit feedback.

ABSTRACT_BEGIN
  In this paper we present a new model for the lifetime of wireless sensor
networks used for sea water communications. The new model for power
communications takes into consideration parameters such as power consumption
for the active mode, power consumption for the sleep mode, power consumption
for the transient mode, transmission period, transient mode duration, sleep
mode duration, and active mode duration. The power communications model is
incorporated in the life time model of wireless sensor networks. The life time
model takes into consideration several parameters such as the total number of
sensors, network size, percentage of sink nodes, location of sensors, the
mobility of sensors, power consumption when nodes move and the power
consumption of communications. The new model for power consumption in
communications shows more accurate results about the lifetime of the sensor
network in comparison with previously published results.

ABSTRACT_BEGIN
  The backpressure routing and scheduling, with throughput-optimal operation
guarantee, is a promising technique to improve throughput over wireless
multi-hop networks. Although the backpressure framework is conceptually viewed
as layered, the decisions of routing and scheduling are made jointly, which
imposes several challenges in practice. In this work, we present Diff-Max, an
approach that separates routing and scheduling and has three strengths: (i)
Diff-Max improves throughput significantly, (ii) the separation of routing and
scheduling makes practical implementation easier by minimizing cross-layer
operations; i.e., routing is implemented in the network layer and scheduling is
implemented in the link layer, and (iii) the separation of routing and
scheduling leads to modularity; i.e., routing and scheduling are independent
modules in Diff-Max and one can continue to operate even if the other does not.
Our approach is grounded in a network utility maximization (NUM) formulation of
the problem and its solution. Based on the structure of Diff-Max, we propose
two practical schemes: Diff-subMax and wDiff-subMax. We demonstrate the
benefits of our schemes through simulation in ns-2, and we implement a
prototype on smartphones.

ABSTRACT_BEGIN
  Corrupted frames with CRC errors potentially provide a useful channel through
which we can transmit information. Using measurements taken in an outdoor
environment, we demonstrate that for 802.11 wireless links the channel provided
by corrupted frames alone (i.e. ignoring frames with PHY errors and frames
received correctly) can be accurately modelled as a binary symmetric channel
(BSC) provided appropriate pre- and post- processing is carried out. Also, the
channel provided by corrupted frames and other frames combined can be
accurately modelled as a hybrid binary symmetric/packet erasure channel.
Importantly, we find that this hybrid channel offers capacity increases of more
than 100% compared to a conventional packet erasure channel over a wide range
of RSSIs. This is a striking observation as it indicates that the potential
exists for significant network throughput gains if the information contained in
802.11 corrupted packets is exploited.

ABSTRACT_BEGIN
  In this letter we motivate the need to revisit the MAC protocol used in Gen2
RFID system in order to leverage receiver structures with Collision Recovery
capabilities at the PHY layer. To this end we propose to consider a simple
variant of the Framed Slotted Aloha with pseudo-random (deterministic) slot
selection as opposite to the classical random selection. Pseudo-random access
allows naturally to implement Inter-frame Successive Interference Cancellation
(ISIC) without changing the PHY modulation and coding format of legacy RFID
standard. By means of simulations we show that ISIC can bring 20-25% gain in
throughput with respect to traditional intra-frame SIC. Besides that, we
elaborate on the potential of leveraging pseudo-random access protocols in
combination with advanced PHY techniques in the context of RFID applications.

ABSTRACT_BEGIN
  6LoWPAN (IPv6 over IEEE 802.15.4) standardized by IEEE 802.15.4 provides IP
communication capability for nodes in WSN. An adaptation layer is introduced
above the MAC layer to achieve header compression, fragmentation and reassembly
of IP packets. The location-based information is used to simplify the routing
policy. This paper proposes an efficient location-based routing protocol,
considering link quality and distance between nodes as the routing metric. The
proposed Enhanced Location-based routing protocol (ELBRP) was simulated in NS2
version 2.32 and performance were analysed in terms of packet delivery ratio,
throughput and average end-to-end delay. From the results obtained, it is found
that the proposed ELBRP outperforms existing LOAD protocol.

ABSTRACT_BEGIN
  In this study we examine statistical properties of traffic generated by the
popular P2P IPTV application SopCast. The analysis aims at a better
understanding of the mechanisms used by such applications and their impact on
the network. Since the most popular P2P IPTV applications use proprietary
unpublished protocols, we look directly at the generated traffic focusing on a
single session analysis, which is the major contribution of our work. We
present a basic characterisation of the traffic profile generated by SopCast
during every separate session in terms of the intensity, the burstiness, the
distribution of the packet sizes and the correlation. We show that some of
these statistical properties of the analysed traffic may be quite different
depending on the particular session.

ABSTRACT_BEGIN
  Dynamic resource management has become an active area of research in the
Cloud Computing paradigm. Cost of resources varies significantly depending on
configuration for using them. Hence efficient management of resources is of
prime interest to both Cloud Providers and Cloud Users. In this work we suggest
a probabilistic resource provisioning approach that can be exploited as the
input of a dynamic resource management scheme. Using a Video on Demand use case
to justify our claims, we propose an analytical model inspired from standard
models developed for epidemiology spreading, to represent sudden and intense
workload variations. We show that the resulting model verifies a Large
Deviation Principle that statistically characterizes extreme rare events, such
as the ones produced by "buzz/flash crowd effects" that may cause workload
overflow in the VoD context. This analysis provides valuable insight on
expectable abnormal behaviors of systems. We exploit the information obtained
using the Large Deviation Principle for the proposed Video on Demand use-case
for defining policies (Service Level Agreements). We believe these policies for
elastic resource provisioning and usage may be of some interest to all
stakeholders in the emerging context of cloud networking

ABSTRACT_BEGIN
  The rapid improvement of the mobile generations was for the purpose of
supporting as many mobile devices as possible that could benefit the users at
anytime and anywhere in terms of common practical applications such as internet
access, video-on-demand, video conferencing system and many more applications.
In this paper, a review for the mobile generations in the wireless
communications is pre-sented in order to highlight and compare the issues and
challenges that are involved in each generation starting from the earlier
generations along to the following generations and finally till the 4th
Generation (4G). The 4G wireless network is intended to complement and replace
the current generations. Accessing information anywhere, anytime, with a
seamless connection to a wide range of information and services, and receiving
a large volume of information, data, pictures, video, and so on, are the keys
features of 4G. Based on the developing trends of mobile communication, 4G will
have broader bandwidth, higher data rate, and smoother and quicker handoff to
provide seamless service across a multitude of wireless systems and networks.
One of the major issues of seamless mobility is handoff management. It is a
major challenge to design intelligent handoff management schemes for
4G-systems. In this paper we have presented the design of an adaptive
multi-attribute vertical handoff decision algorithm based on genetic algorithm
which is both cost effective and useful.

ABSTRACT_BEGIN
  Efficient distributed spectrum sharing mechanism is crucial for improving the
spectrum utilization. The spatial aspect of spectrum sharing, however, is less
understood than many other aspects. In this paper, we generalize a recently
proposed spatial congestion game framework to design efficient distributed
spectrum access mechanisms with spatial reuse. We first propose a spatial
channel selection game to model the distributed channel selection problem with
fixed user locations. We show that the game is a potential game, and develop a
distributed learning mechanism that converges to a Nash equilibrium only based
on users' local observations. We then formulate the joint channel and location
selection problem as a spatial channel selection and mobility game, and show
that it is also a potential game. We next propose a distributed strategic
mobility algorithm, jointly with the distributed learning mechanism, that can
converge to a Nash equilibrium.

ABSTRACT_BEGIN
  The real world scenario has changed from the wired connection to wireless
connection.Over the years software, development has responded to the increasing
growth of wireless connectivity in developing network enabled software.The
problem arises in the wireless domain due to random packet loss in transport
layer and as well as in data link layer for the end to end connection. The
basic problem has been considered in this work is to convert the real world
scenario of Vehicular ad hoc network into a lab oriented problem by used the
APS-system and study the result to achieve better performance in the wireless
domain. The real world physical problems map into analytical problem and
simulate that analytic problem with respect to real world scenario by Automated
Position System (APS) for antenna mounted over the mobile node in 2 Dimension
space. Here the methodology quantifies the performance and the impact of the
packet loss, delay, by the bit error rate and throughput with respect to the
real- world scenario of VANET in the MAC layer, data link layer and transport
layer. The result presents the Directional Antenna which is mounted over the
vehicle gives less bit error in comparison to Isotropic and Discone antenna.

ABSTRACT_BEGIN
  Always Best Packet Switching (ABPS) is a novel approach for wireless
communications that enables mobile nodes, equipped with multiple network
interface cards (NICs), to dynamically determine the most appropriate NIC to
use. Using ABPS, a mobile node can seamlessly switch to a different NIC in
order to get better performance, without causing communication interruptions at
the application level. To make this possible, NICs are kept always active and a
software monitor constantly probes the channels for available access points.
While this ensures maximum connection availability, considerable energy may be
wasted when no access points are available for a given NIC. In this paper we
address this issue by investigating the use of an "oracle" able to provide
information on network availability. This allows to dynamically switch on/off
NICs based on reported availability, thus reducing the power consumption. We
present a Markov model which allows us to estimate the impact of the oracle on
the ABPS mechanism: results show that significant reduction in energy
consumption can be achieved with minimal impact on connection availability. We
conclude by describing a prototype implementation of the oracle based on Web
services and geolocalization.

ABSTRACT_BEGIN
  General Packet Radio Service (GPRS) is a complex data network which upgrades
current second generation GSM networks, offering true high-speed internet (IP)
and network connectivity over existing GSM cellular networks. The increasing
population of mobile users leads to congestion problems in these systems, and
motivates the development of more efficient management schemes. This project
deals with radio resource and mobility management such as location management
and handoff management using distance method in GPRS networks. A simulator
based on MATLAB which can study the location updating is used in this GPRS
system.

ABSTRACT_BEGIN
  Security of wireless sensor network (WSN) is always considered a critical
issue and has a number of considerations that separate them from traditional
wireless sensor network. First, sensor devices are typically vulnerable to
physical compromise. Second, they have significant power and processing
constraints. Third, the most critical security issue is protecting the
aggregate output of the system, even if individual nodes may be compromised.
While a variety of security techniques are being developed and lots of
researches are going on security fields. In this paper we have proposed a new
technique to provide data authentication and privacy in faster, scalable and
cost effective way.

ABSTRACT_BEGIN
  It is by now well-known that wireless networks with file arrivals and
departures are stable if one uses alpha-fair congestion control and
back-pressure based scheduling and routing. In this paper, we examine whether
?alpha-fair congestion control is necessary for flow-level stability. We show
that stability can be ensured even with very simple congestion control
mechanisms, such as a fixed window size scheme which limits the maximum number
of packets that are allowed into the ingress queue of a flow. A key ingredient
of our result is the use of the difference between the logarithms of queue
lengths as the link weights. This result is reminiscent of results in the
context of CSMA algorithms, but for entirely different reasons.

ABSTRACT_BEGIN
  Mobile Ad hoc Network (MANET) is a infrastructure less network in which two
or more devices have wireless communication which can communicate with each
other and exchange information without need of any centralized administrator.
Each node in the ad hoc network acts as a router, forwarding data packets for
other nodes. The main issue is to compare the existing routing protocol and
finding the best one. The scope of this study is to test routing performance of
three different routing protocols (AODV, OLSR and DSDV) with respect to various
mobility models using NS2 simulator. In this paper the parameters used for
comparison are packet delivery fraction (PDF), average end to end delay (AEED),
normalized routing load (NRL) and throughput.

ABSTRACT_BEGIN
  A key challenge in \cacd\ is determining how to allocate limited server
bandwidth across a large number of files being concurrently served so as to
optimize global performance and cost objectives. In this paper, we present a
comprehensive experimental evaluation of strategies to control server bandwidth
allocation. As part of this effort, we introduce a new {\em model-based}
control approach that relies on an accurate yet concise "cheat sheet" based on
a priori offline measurement to predict swarm performance as a function of the
server bandwidth and other swarm parameters. Our evaluation using a prototype
system, \cs, instantiating static, dynamic, and model-based controllers shows
that static and dynamic controllers can both be suboptimal due to different
reasons. In comparison, a model-based approach consistently outperforms both
static and dynamic approaches provided it has access to detailed measurements
in the regime of interest. Nevertheless, the broad applicability of a
model-based approach may be limited in practice because of the overhead of
developing and maintaining a comprehensive measurement-based model of swarm
performance in each regime of interest.

ABSTRACT_BEGIN
  Several major Internet service providers (e.g., Level-3, AT&T, Verizon) today
also offer content distribution services. The emergence of such "Network-CDNs"
(NCDNs) are driven by market forces that place more value on content services
than just carrying the bits. NCDNs are also necessitated by the need to reduce
the cost of carrying ever-increasing volumes of traffic across their backbones.
An NCDN has the flexibility to determine both where content is placed and how
traffic is routed within the network. However NCDNs today continue to treat
traffic engineering independently from content placement and request
redirection decisions. In this paper, we investigate the interplay between
content distribution strategies and traffic engineering and ask how an NCDN
should engineer traffic in a content-aware manner. Our experimental analysis,
based on traces from a large content distribution network and real ISP
topologies, shows that effective content placement can significantly simplify
traffic engineering and in most cases obviate the need to engineer NCDN traffic
all together! Further, we show that simple demand-oblivious schemes for routing
and placement such as InverseCap and LRU suffice as they achieve network costs
that are close to the best possible.

ABSTRACT_BEGIN
  Data downloading on the fly is the base of commercial data services in
vehicular networks, such as office-onwheels and entertainment-on-wheels. Due to
the sparse spacial distribution of roadside Base Stations (BS) along the road,
downloading through Roadside-to-Vehicle (R2V) connections is intermittent.When
multiple vehicles with geographical proximity have common interest in certain
objects to download, they can collaborate to reduce significantly their overall
download time. In this paper, we investigate application of Network Coding (NC)
in collaborative downloading (CD). We focus on the R2V part of CD, and
analytically derive probability distribution and expected value of amount of
time needed to deliver all information to the vehicles with and without NC. Our
results show that using NC slightly improves the downloading time in addition
to removing any need for having any sort of uplink communications from vehicles
to the infrastructure.

ABSTRACT_BEGIN
  Many communication networks consist of legacy and new devices using
heterogeneous technologies, such as copper wire, optical fiber, wireless and
power line communication (PLC). Most network simulators, however, have been
designed to work well with a single underlying link layer technology.
Furthermore, there are hardly any suitable models for network simulators of
PLC. In this paper we present extensions of the Contiki OS network simulator
Cooja: A device may support multiple interfaces accessing multiple PLC segments
or wireless channels and a simple PLC medium model is introduced describing
packet loss probability as a function of distance. We test our approach to
simulate a Smart Grid scenario of Ring Main Units equipped with PLC devices.

ABSTRACT_BEGIN
  In this paper, we design a simple, low-cost, and low-power wake-up receiver
which can be used for an IEEE 802.11-compliant device to remotely wake up the
other devices by utilizing its own wireless LAN (WLAN) signals. The employed
wake-up mechanism utilizes the length of 802.11 data frame generated by a WiFi
transmitter to differentiate the information conveyed to the wake-up receiver.
The wake-up receiver is designed to reliably detect the length of transmitted
data frame only with simple envelope detection and limited signal processing.
We develop a prototype of the wake-up receiver and investigate the detection
performance of the envelope of 802.11 signals. Our numerical results show that
the proposed wake-up receiver achieves much larger detection range than the
off-the-shelf, commercial receiver having the similar functionality.

ABSTRACT_BEGIN
  Wireless Geo-Sensor Networks (GEONET) are suitable for critical applications
in hostile environments due to its flexibility in deployment. But low power
geo-sensor nodes are easily compromised by security threats like battery
exhaustion attack which may give rise to unavoidable circumstances. In this
type of attack, intruder forcefully resists legitimate sensor nodes from going
into low-power sleep state. So that compromised sensor node's battery power is
drained out and it stops working. Due to the limited capability of sensor
nodes, it is very difficult to prevent a sensor node from this type of attack
which apparently appears as innocent interaction. In this paper, a framework of
secure GEONET model (SEGNET) is proposed, based on dynamic load distribution
mechanism for heterogeneous environment. It considers hybrid detection approach
using three modules for anomaly detection, intrusion confirmation and decision
making to reduce the probability of false detection, compared to other existing
approaches.

ABSTRACT_BEGIN
  Content distribution networks (CDNs) which serve to deliver web objects
(e.g., documents, applications, music and video, etc.) have seen tremendous
growth since its emergence. To minimize the retrieving delay experienced by a
user with a request for a web object, caching strategies are often applied -
contents are replicated at edges of the network which is closer to the user
such that the network distance between the user and the object is reduced. In
this literature survey, evolution of caching is studied. A recent research
paper [15] in the field of large-scale caching for CDN was chosen to be the
anchor paper which serves as a guide to the topic. Research studies after and
relevant to the anchor paper are also analyzed to better evaluate the
statements and results of the anchor paper and more importantly, to obtain an
unbiased view of the large scale collaborate caching systems as a whole.

ABSTRACT_BEGIN
  In this paper, we present a new framework that links the two worlds of wired
and cellular users sharing systems. The approach is to propose an easy gateway
that enables the use of cellular networks based services by wireline users and
applications. The idea is to use a mobile terminal or wireless equipment for
sharing cellular services, available thanks to its cellular network, to other
users that use the wireline Internet. The software application acts as a
gateway between the cellular and the wired network; it is responsible for
supporting the services provided by the wireless network and make them
accessible and usable, in a standard and easy way, by anyone on the wireline
network. The gateway software can be integrated easily on any complex
architecture since it can interact with any cellular modem. The paper describes
an implementation prototype where some examples of services, such as the
ability of using messaging services and calls streaming, are experimented. The
proposed platform combines different standards to guarantee the use of our
gateway in heterogeneous environments.

ABSTRACT_BEGIN
  Over the last years many technological advances were introduced in Internet
television to meet user needs and expectations. However due to an overwhelming
bandwidth requirements traditional IP-based television service based on simple
client-server approach remains restricted to small group of clients. In such
situation the use of the peer-to-peer overlay paradigm to deliver live
television on the Internet is gaining increasing attention. Unfortunately the
current Internet infrastructure provides only best effort services for this
kind of applications and do not offer quality of service.
  This paper is a research proposition which presents potential solutions for
efficient IPTV streaming over P2P networks. We assume that the solutions will
not directly modify existing P2P IPTV protocols but rather will be dedicated
for a network engineer or an Internet service provider which will be able to
introduce and configure the proposed mechanisms in network routers.

ABSTRACT_BEGIN
  Recent collapses of SIP servers in the carrier networks indicates two
potential problems of SIP: (1) the current SIP design does not easily scale up
to large network sizes, and (2) the built-in SIP overload control mechanism
cannot handle overload conditions effectively. In order to help carriers
prevent widespread SIP network failure effectively, this chapter presents a
systematic investigation of current state-of-the-art overload control
algorithms. To achieve this goal, this chapter first reviews two basic
mechanisms of SIP, and summarizes numerous experiment results reported in the
literatures which demonstrate the impact of overload on SIP networks. After
surveying the approaches for modeling the dynamic behaviour of SIP networks
experiencing overload, the chapter presents a comparison and assessment of
different types of SIP overload control solutions. Finally it outlines some
research opportunities for managing SIP overload control.

ABSTRACT_BEGIN
  The location of active users is an important factor in the performance
analysis of mobile multicell networks, but it is difficult to quantify due to
the wide variety of user mobility and session patterns. In particular, the
channel holding times in each cell may be arbitrarily distributed and dependent
on those in other cells. In this work, we study the stationary distribution of
users by modeling the system as a multi-route queueing network with Poisson
inputs. We consider arbitrary routing and arbitrary joint probability
distributions for the channel holding times in each route. Using a
decomposition-composition approach, we show that the user distribution (1) is
insensitive to the user movement patterns, (2) is insensitive to general and
dependently distributed channel holding times, (3) depends only on the average
arrival rate and average channel holding time at each cell, and (4) is
completely characterized by an open network with M/M/infinity queues. This
result is validated by experiments with the Dartmouth user mobility traces.

ABSTRACT_BEGIN
  In this paper, we present an experimental and simulation based study to
evaluate the use of full-duplex as a mode in practical IEEE 802.11 networks. To
enable the study, we designed a 20 MHz multi-antenna OFDM full-duplex physical
layer and a full-duplex capable MAC protocol which is backward compatible with
current 802.11. Our extensive over-the-air experiments, simulations and
analysis demonstrate the following two results. First, the use of multiple
antennas at the physical layer leads to a higher ergodic throughput than its
hardware-equivalent multi-antenna half-duplex counterparts, for SNRs above the
median SNR encountered in practical WiFi deployments. Second, the proposed MAC
translates the physical layer rate gain into near doubling of throughput for
multi-node single-AP networks. The two combined results allow us to conclude
that there are potentially significant benefits gained from including a
full-duplex mode in future WiFi standards.

ABSTRACT_BEGIN
  Transmission control protocol (TCP) is a connection oriented protocol for
several types of distributed applications. TCP is reliable particularly for
traditional fixed networks. With emergence of faster wireless networks, TCP has
been performing poorly in its original format. The performance of TCP is
affected due to assorted factors including congestion window, maximum packet
size, retry limit, recovery mechanism, backup mechanism and mobility. To
overcome deficiency of original TCP, Several modifications have been introduced
to improve network quality. The mobility is a major hurdle in degrading the
performance of mobile wireless networks. In this paper, we introduce and
implement new TCP variant University of Bridgeport (UB) that combines the
features of TCP Westwood and Vegas. We examine the performance of TCP-UB, Vegas
and Westwood using different realistic scenarios. NS2 simulator demonstrates
the stability of TCP-UB as compared with TCP Vegas and Westwood in highly
congested networks from the mobility point of view.

ABSTRACT_BEGIN
  This paper introduces the Discrete Dithered Desynchronization (D3sync)
algorithm which is a decentralized Time Division Multiple Access (TDMA)
technique in which a set of network nodes computes iteratively a conflict-free
schedule so that each node obtains a portion of a frame that is an integer
multiple of a fixed slot size. The algorithm is inspired by the dynamics of
Pulse Coupled Oscillators (PCO), but unlike its predecessors that divide
arbitrarily the frame among the nodes in the network, the D3sync allocates
discrete resources among the network nodes.
  Our paper proves the convergence of the D3sync algorithm and gives an upper
bound on the convergence time of the algorithm.

ABSTRACT_BEGIN
  In this work, we consider the popular OPNET simulator as a tool for
performance evaluation of algorithms operating in peer-to-peer (P2P) networks.
We created simple framework and used it to analyse the flooding search
algorithm which is a popular technique for searching files in an unstructured
P2P network. We investigated the influence of the number of replicas and time
to live (TTL) of search queries on the algorithm performance. Preparing the
simulation we did not reported the problems which are commonly encountered in
P2P dedicated simulators although the size of simulated network was limited.

ABSTRACT_BEGIN
  Wireless sensor networks are harshly restricted by storage capacity, energy
and computing power. So it is essential to design effective and energy aware
protocol in order to enhance the network lifetime. In this paper, a review on
routing protocol in WSNs is carried out which are classified as data-centric,
hierarchical and location based depending on the network structure. Then some
of the multipath routing protocols which are widely used in WSNs to improve
network performance are also discussed. Advantages and disadvantages of each
routing algorithm are discussed thereafter. Furthermore, this paper compares
and summarizes the performances of routing protocols.

ABSTRACT_BEGIN
  In this paper, performance analysis of Location Aided Routing (LAR) protocol
in different city scenarios has been done. The mobility model considered is
Manhattan model. This mobility model used to emulate the movement pattern of
nodes i.e., vehicles on streets defined by maps. Our objective is to provide a
qualitative analysis of the LAR protocol in different city scenarios in
Vehicular Ad hoc Networks. We have considered three different city scenarios
for the analysis of the protocol. The simulation work has been conducted using
the Glomosim 2.03 simulator. The results show that LAR1 protocol achieves
maximum packet delivery ratio is 99.68 % and maximum average end-to-end delay
is 7.319969 ms when the network is sparsely populated. Further, for densely
populated network maximum achieved packet delivery ratio is 87.58% and average
end-to-end delay is 0.017684 ms.

ABSTRACT_BEGIN
  Unlike wired networks, the capacity of a wireless network is interference
limited due to the broadcast nature of wireless medium. Some multicast wireless
network protocols do not consider channel assignment issue, that they cause
interference at transmission nodes, hence do not use full capacity of the
network. Interference can be reduced and throughput improved with the use of
multichannel features. Therefore, this paper used orthogonal channels for
sending and receiving nodes in the network. We propose EWM (Efficient Wireless
Multicast) method that is distributed scheme for constructing multicast tree in
multi-channel multi-interface wireless mesh networks (MIMC-WMN) which selects
relay nodes and in distributed form assign orthogonal radio channels to them.
To more decrease of interference in adding a branch to the tree, the route with
minimum end-to-end delay from the source to the multicast receiver will be
chosen. Thus, the tree is suitable for multimedia applicants. We also employ
the broadcast nature of the wireless media to reduce the number of relay nodes.
The proposed algorithm is compared with MCM algorithm in NS2.

ABSTRACT_BEGIN
  Mobile Ad-hoc Network (MANET) is the self organizing collection of mobile
nodes. The communication in MANET is done via a wireless media. Ad hoc wireless
networks have massive commercial and military potential because of their
mobility support. Due to demanding real time multimedia applications, Quality
of Services (QoS) support in such infrastructure less networks have become
essential. QoS routing in mobile Ad-Hoc networks is challenging due to rapid
change in network topology. In this paper, we focused to reduce flooding
performance of the Fisheye State Routing (FSR) protocol in Grid using ns-2
network simulator under different performance metrics scenario in respect to
number of Nodes. For example, the connection establishment is costly in terms
of time and resource where the network is mostly affected by connection request
flooding. The proposed approach presents a way to reduce flooding in MANETs.
Flooding is dictated by the propagation of connection-request packets from the
source to its neighborhood nodes. The proposed architecture embarks on the
concept of sharing neighborhood information. The proposed approach focuses on
exposing its neighborhood peer to another node that is referred to as its
friend-node, which had requested/forwarded connection request. If there is a
high probability for the friend node to communicate through the exposed routes,
this could improve the efficacy of bandwidth utilization by reducing flooding,
as the routes have been acquired, without any broadcasts. Friendship between
nodes is quantized based on empirical computations and heuristic algorithms.
The nodes store the neighborhood information in their cache that is
periodically verified for consistency. Simulation results show the performance
of this proposed method.

ABSTRACT_BEGIN
  Scarcity of frequencies and the demand for more bandwidth is likely to
increase the need for devices that utilize the available frequencies more
efficiently. Radios must be able to dynamically find other users of the
frequency bands and adapt so that they are not interfered, even if they use
different radio protocols. As transmitters far away may cause as much
interference as a transmitter located nearby, this mechanism can not be based
on location alone. Central databases can be used for this purpose, but require
expensive infrastructure and planning to scale. In this paper, we propose a
decentralized protocol and architecture for discovering radio devices over the
Internet. The protocol has low resource requirements, making it suitable for
implementation on limited platforms. We evaluate the protocol through
simulation in network topologies with up to 2.3 million nodes, including
topologies generated from population patterns in Norway. The protocol has also
been implemented as proof-of-concept in real Wi-Fi routers.

ABSTRACT_BEGIN
  In this paper, the problem of assigning channel slots to a number of
contending stations is modeled as a Constraint Satisfaction Problem (CSP). A
learning MAC protocol that uses deterministic backoffs after successful
transmissions is used as a decentralized solver for the CSP. The convergence
process of the solver is modeled by an absorbing Markov chain (MC), and
analytical, closed-form expressions for its transition probabilities are
derived. Using these, the expected number of steps required to reach a solution
is found. The analysis is validated by means of simulations and the model is
extended to account for the presence of channel errors. The results are
applicable in various resource allocation scenarios in wireless networks.

ABSTRACT_BEGIN
  WiMAX is Wireless Interoperability for Microwave Access has emerged as a
promising solution for transmission of higher data rates for fixed and mobile
applications. IEEE 802.16d and e are the standards proposed. To attain higher
data rates the Multi Carrier System with Multiple Input and Multiple Output
MIMO is incorporated in the WiMAX. And all these sub carriers are considered to
be orthogonal to each other. As the number of sub carriers is increased there
is no guarantee of sustained orthogonality, i.e. at some point the carriers are
not independent to each other, and hence where the orthogonality can be loosed
which leads to interference and also owing to the synchronization between
transmitter and receiver local oscillator, it causes interference known as
Inter Carrier Interference (ICI).In this scheme at the transmitter side the
modulated data and a few predefined pilot symbols are mapped onto the non
neighboring sub carriers with weighting coefficients of +1 and -1. With the aid
of pilot symbols the frequency offset is exactly estimated by using Maximum
Likelihood Estimation MLE and hence can be minimized. At demodulation stage the
received signals are linearly combined along with their weighted oefficients
and pilot symbols, called as Pilot Aided Self Cancellation Method PASCS. The
simulations are carried out on Stanford University Interim (SUI)channels. The
simulation results shows that by incorporating this method into WiMAX systems
it performs better when the Line Of Sight (LOS) component is present in the
transmission and also it improves the Bit Error Rate (BER) and Carrier to
Interference Ratio (CIR). The CIR can be improved 20 dB. In this paper the
effectiveness of PASCS scheme is compared with the Self Cancellation Method
(SCM). It provides accurate estimation of frequency offset and when residual
CFO is less significant the ICI can be diminished successfully.

ABSTRACT_BEGIN
  Supporting quality of service (QoS) guarantees for diverse multimedia
services are the primary concerns for WiMAX (IEEE 802.16) networks. A
scheduling scheme that satisfies QoS requirements has become more important for
wireless communications. We propose a downlink scheduling scheme called
adaptive priority-based downlink scheduling (APDS) for providing QoS guarantees
in IEEE 802.16 networks. APDS comprises two major components: priority
assignment and resource allocation. Different service-type connections
primarily depend on their QoS requirements to adjust priority assignments and
dispatch bandwidth resources dynamically. We consider both starvation avoidance
and resource management. Simulation results show that our APDS methodology
outperforms the representative scheduling approaches in QoS satisfaction and
maintains fairness in starvation prevention.

ABSTRACT_BEGIN
  In this letter, we consider a joint macro-relay network with densely deployed
relay stations (RSs) and dynamically varied traffic load measured by the number
of users. An energy-efficient strategy is proposed by intelligently adjusting
the RS working modes (active or sleeping) according to the traffic variation.
Explicit expressions related to the network energy efficiency are derived based
on stochastic geometry theory. Simulation results demonstrate that the derived
analytic results are reasonable and the proposed strategy can significantly
improve the network energy efficiency.

ABSTRACT_BEGIN
  Although the technology of femtocells is highly promising, many challenging
problems should be addressed before fully harvesting its potential. In this
paper, we investigate the problem of cell association and handover management
in femtocell networks. Two extreme cases for cell association are first
discussed and analyzed. Then we propose our algorithm to maximize network
capacity while achieving fairness among users. Based on this algorithm, we
further develop a handover algorithm to reduce the number of unnecessary
handovers using Bayesian estimation. The proposed handover algorithm is
demonstrated to outperform a heuristic scheme with considerable gains in our
simulation study.

ABSTRACT_BEGIN
  Mobility causes network structures to change. In PSNs where underlying
network structure is changing rapidly, we are interested in studying how
information dissemination can be enhanced in a sparse disconnected network
where nodes lack the global knowledge about the network. We use beamforming to
study the enhancement in the information dissemination process. In order to
identify potential beamformers and nodes to which beams should be directed we
use the concept of stability. We first predict the stability of a node in the
dynamic network using truncated levy walk nature of jump lengths of human
mobility and then use this measure to identify beamforming nodes and the nodes
to which the beams are directed. We also develop our algorithm such that it
does not require any global knowledge about the network and works in a
distributed manner. We also show the effect of various parameters such as
number of sources, number of packets, mobility parameters, antenna parameters,
type of stability used and density of the network on information dissemination
in the network. We validate our findings with three validation model, no
beamforming, beamforming using different stability measure and when no
stability measure is associated but same number of node beamform and the
selection of the beamforming nodes is random. Our simulation results show that
information dissemination can be enhanced using our algorithm over other
models.

ABSTRACT_BEGIN
  Though cooperative relaying is believed to be a promising technology to
improve the energy efficiency of cellular networks, the relays' static power
consumption might worsen the energy efficiency therefore can not be neglected.
In this paper, we focus on whether and how the energy efficiency of cellular
networks can be improved via relays. Based on the spatial Poisson point
process, an analytical model is proposed to evaluate the energy efficiency of
relay-assisted cellular networks. With the aid of the technical tools of
stochastic geometry, we derive the distributions of
signal-to-interference-plus-noise ratios (SINRs) and mean achievable rates of
both non-cooperative users and cooperative users. The energy efficiency
measured by "bps/Hz/W" is expressed subsequently. These established expressions
are amenable to numerical evaluation and corroborated by simulation results.

ABSTRACT_BEGIN
  Operating in the ISM band, the wireless sensor network (WSN) risks being
interfered by other concurrent networks. Our concerns are the technologies that
do not perform listening before transmission such as Bluetooth, and the ones
that do not detect other technologies due to their channel sensing techniques
like WIFI. To overcome this issue a WSN node should be able to identify the
presence of such technologies. This will allow deducing the characteristics of
the generated traffic of these technologies, and thus the behavior of the
channel can be predicted. These predictions would help to trigger adequate
reactions as to avoid or synchronize with the concurrent net- works. Many works
exist on link adaptation, but they concern blind adaptations which are
unintelligent and solve momentarily the problem that may reappear over time.
  In this paper, we perform several experiments on a real testbed to categorize
the model of the bit errors in corrupted received packets. These experiments
are performed under different conditions of channel noise and interferences.
This allows us to identify each corruption pattern as a fingerprint for the
interfering technology. Then we propose the mechanism FIM to identify on the
fly the source of the corruption. With an implementation on "Tmote Sky" motes
using Tinyos1.x, We demonstrate the use of FIM for link adaptation in a
coexistence environment. Our mechanism led to throughput improvements of
87%-100% depending on the transmission rate and channel quality.

ABSTRACT_BEGIN
  In this paper we consider graph-coloring problems, an important subset of
general constraint satisfaction problems that arise in wireless resource
allocation. We constructively establish the existence of fully decentralized
learning-based algorithms that are able to find a proper coloring even in the
presence of strong sensing restrictions, in particular sensing asymmetry of the
type encountered when hidden terminals are present. Our main analytic
contribution is to establish sufficient conditions on the sensing behaviour to
ensure that the solvers find satisfying assignments with probability one. These
conditions take the form of connectivity requirements on the induced sensing
graph. These requirements are mild, and we demonstrate that they are commonly
satisfied in wireless allocation tasks. We argue that our results are of
considerable practical importance in view of the prevalence of both
communication and sensing restrictions in wireless resource allocation
problems. The class of algorithms analysed here requires no message-passing
whatsoever between wireless devices, and we show that they continue to perform
well even when devices are only able to carry out constrained sensing of the
surrounding radio environment.

ABSTRACT_BEGIN
  In this work we are concerned with the problem of link scheduling for
throughput maximization in wireless networks that employ a cooperative amplify
and forward (AF) protocol. To address this problem first we define the
signal-to-interference plus noise ratio (SINR) expression for the complete
cooperative AF-based transmission. Next, we formulate the problem of link
scheduling as a mixed integer linear program (MILP) that uses as a constraint
the developed SINR expression. The proposed formulation is motivated by the
observation that the aggregate interference that affects a single cooperative
transmission can be decomposed into two separate SINR constraints. Results for
the optimal solution and a polynomial time approximation algorithm are also
presented.

ABSTRACT_BEGIN
  In Cognitive Radio (CR) ad hoc networks, secondary users (SU) attempt to
utilize valuable spectral resources without causing significant interference to
licensed primary users (PU). While there is a large body of research on
spectrum opportunity detection, exploitation, and adaptive transmission in CR,
most existing approaches focus only on avoiding PU activity when making sensing
decisions. Since the myopic sensing strategy results in congestion and poor
throughput, several collision-avoidance sensing approaches were investigated in
the literature. However, they provide limited improvement. A channel-aware
myopic sensing strategy that adapts the reward to the fading channel state
information (CSI) of the SU link is proposed. This CSI varies over the CR
spectrum and from one SU pair to another due to multipath and shadow fading,
thus randomizing sensing decisions and increasing the network throughput. The
proposed joint CSI adaptation at the medium access control (MAC) and physical
layers provides large throughput gain over randomized sensing strategies and/or
conventional adaptive transmission methods. The performance of the proposed
CSI-aided sensing strategy is validated for practical network scenarios and
demonstrated to be robust to CSI mismatch, sensing errors, and spatial channel
correlation.

ABSTRACT_BEGIN
  Conventional cellular wireless networks were designed with the purpose of
providing high throughput for the user and high capacity for the service
provider, without any provisions of energy efficiency. As a result, these
networks have an enormous Carbon footprint. In this note, we describe the
sources of the inefficiencies in such networks. First we quantify how much
Carbon footprint such networks generate. We also discuss how much more mobile
traffic is expected to increase so that this Carbon footprint will even
increase tremendously more. We then discuss specific sources of inefficiency
and potential sources of improvement at the physical layer as well as higher
layers of the communication protocol hierarchy. In particular, considering that
most of the energy inefficiency in wireless cellular networks is at the base
stations, we discuss multi-tier networks and point to the potential of
exploiting mobility patterns in order to use base station energy judiciously.

ABSTRACT_BEGIN
  This paper summarizes the outcomes of the 5th International Workshop on
Femtocells held at King's College London, UK, on the 13th and 14th of February,
2012.The workshop hosted cutting-edge presentations about the latest advances
and research challenges in small cell roll-outs and heterogeneous cellular
networks. This paper provides some cutting edge information on the developments
of Self-Organizing Networks (SON) for small cell deployments, as well as
related standardization supports on issues such as carrier aggregation (CA),
Multiple-Input-Multiple-Output (MIMO) techniques, and enhanced Inter-Cell
Interference Coordination (eICIC), etc. Furthermore, some recent efforts on
issues such as energy-saving as well as Machine Learning (ML) techniques on
resource allocation and multi-cell cooperation are described. Finally, current
developments on simulation tools and small cell deployment scenarios are
presented. These topics collectively represent the current trends in small cell
deployments.

ABSTRACT_BEGIN
  Nowadays efficient usage of high-tech security tools and appliances is
considered as an important criterion for security improvement of computer
networks. Based on this assumption, Intrusion Detection and Prevention Systems
(IDPS) have key role for applying the defense in depth strategy. In this
situation, by increasing network bandwidth in addition to increasing number of
threats, Network-based IDPSes have been faced with performance challenge for
processing of huge traffic in the networks. A general solution for this
bottleneck is exploitation of efficient hardware architectures for performance
improvement of IDPS. In this paper a framework for analysis and performance
evaluation of application specific instruction set processors is presented for
usage in application of attack detection in Networkbased Intrusion Detection
Systems(NIDS). By running this framework as a security application on V850,
OR1K, MIPS32, ARM7TDMI and PowerPC32 microprocessors, their performance has
been evaluated and analyzed. For performance improvement, the compiler
optimization levels are employed and at the end; base on O2 optimization level
a new combination of optimization flags is presented. The experiments show that
the framework results 18.10% performance improvements for pattern matching on
ARM7TDMI microprocessors.

ABSTRACT_BEGIN
  Clustering of mobile ad hoc networks is a largely growing field. The
perceived benefits of clustering are comprehensively analyzed in open
literature. This paper considers the development of a new
connected-dominated-set clustering algorithm called Ring Clustering Algorithm
(RCA). RCA is a heuristic algorithm that groups mobile nodes in a network into
rings. Each ring consists of three ring-nodes. The priority of a ring is
determined according to a new parameter, the ring degree. This paper presents
the proof that the maximum number of rings that can be formed by RCA in any
disk area equals the maximum number of independent nodes that create
non-overlapping circles in a corresponding area. Moreover, RCA has achieved a
fixed approximation ratio, which is 5.146 and O(n) for both time and message
complexities. Thus, RCA algorithm outperforms the current-best CDS algorithms
that are investigated in this paper.

ABSTRACT_BEGIN
  Efficient channel selection is essential in 802.11 mesh deployments, for
minimizing contention and interference among co-channel devices and thereby
supporting a plurality of QoS-sensitive applications. A few protocols have been
proposed for frequency allocation in such networks, however they do not address
the problem end-to-end. In this paper, we present a general formulation of the
channel selection problem taking into account the performance of both
mesh-access and mesh-backhaul. Moreover, we propose ARACHNE, a routing-aware
channel selection protocol for wireless mesh networks. ARACHNE is distributed
in nature, and motivated by our measurements on a wireless testbed. The main
novelty of our protocol comes from adopting a metric that captures the
end-to-end link loads across different routes in the network. ARACHNE
prioritizes the assignment of low-interference channels to links that (a) need
to serve high-load aggregate traffic and/or (b) already suffer significant
levels of contention and interference. Our protocol takes into account the
number of potential interfaces (radios) per device, and allocates these
interfaces in a manner that efficiently utilizes the available channel
capacity. We evaluate ARACHNE through extensive, tracedriven simulations and we
show that approaches the optimal channel selection. We observe that our
protocol improves the total network throughput, as compared to three other
representative channel allocation approaches in literature.

ABSTRACT_BEGIN
  This paper will focus on the energy efficiency issue in Underwater Wireless
Sensor Networks. In underwater environment, the two main issues are namely:
reliability and energy efficiency. These two issues are twisted pair.
Reliability requires error correction, and error-correction requires energy.
More reliability tends to imply higher energy consumption, causing difficulty
in applications that require nodes to be operated underwater for long periods
of time without batteries recharging, and in aquatic environments that render
hard the task of recharging or replacing batteries. Appropriate strategy must
therefore be in-place to ensure reliable data transmission, while conserving
energy. We propose a mathematical function of the efficiency of acoustic data
communication in real underwater environment. We did the analysis of existing
error-correction techniques, and then propose a new technique that is the
hybrid error correction technique that improves the efficiency among the
existing techniques.

ABSTRACT_BEGIN
  In the last few years there has been significant growth in the area of
wireless communication. IEEE 802.16/WiMAX is the network which is designed for
providing high speed wide area broadband wireless access; WiMAX is an emerging
wireless technology for creating multi-hop Mesh network. Future generation
networks will be characterized by variable and high data rates, Quality of
Services (QoS), seamless mobility both within a network and between networks of
different technologies and service providers. A technology is developed to
accomplish these necessities is regular by IEEE, is 802.16, also called as
WiMAX (Worldwide Interoperability for Microwave Access). This architecture aims
to apply Long range connectivity, High data rates, High security, Low power
utilization and Excellent Quality of Services and squat deployment costs to a
wireless access technology on a metropolitan level. In this paper we have
observed the performance analysis of location based resource allocation for
WiMAX and WLAN-WiMAX client and in second phase we observed the rate-adaptive
algorithms. We know that base station (BS) is observed the ranging first for
all subscribers then established the link between them and in final phase they
will allocate the resource with Subcarriers allocation according to the demand
(UL) i.e. video, voice and data application. We propose linear approach,
Active-Set optimization and Genetic Algorithm for Resource Allocation in
downlink Mobile WiMAX networks. Purpose of proposed algorithms is to optimize
total throughput. Simulation results show that Genetic Algorithm and Active-Set
algorithm performs better than previous methods in terms of higher capacities
but GA have high complexity then active set.

ABSTRACT_BEGIN
  Multicasting technology uses the minimum network resources to serve multiple
clients by duplicating the data packets at the closest possible point to the
clients. This way at most only one data packets travels down a network link at
any one time irrespective of how many clients receive this packet.
Traditionally multicasting has been implemented over a specialized network
built using multicast routers. This kind of network has the drawback of
requiring the deployment of special routers that are more expensive than
ordinary routers. Recently there is new interest in delivering multicast
traffic over application layer overlay networks. Application layer overlay
networks though built on top of the physical network, behave like an
independent virtual network made up of only logical links between the nodes.
Several authors have proposed systems, mechanisms and protocols for the
implementation of multicast media streaming over overlay networks. In this
paper, the author takes a critical look at these systems and mechanism with
special reference to their strengths and weaknesses.

ABSTRACT_BEGIN
  Mobile device users can now easily capture and socially share video clips in
a timely manner by uploading them wirelessly to a server. When attending
crowded events, such as an exhibition or the Olympic Games, however, timely
sharing of videos becomes difficult due to choking bandwidth in the network
infrastructure, preventing like-minded attendees from easily sharing videos
with each other through a server. One solution to alleviate this problem is to
use direct device-to-device communication to share videos among nearby
attendees. Contact capacity between two devices, however, is limited, and thus
a recommendation algorithm, such as collaborative filtering, is needed to
select and transmit only videos of potential interest to an attendee. In this
paper, we address the question: which video clip should be transmitted to which
user. We proposed an video transmission scheduling algorithm, called CoFiGel,
that runs in a distributed manner and aims to improve both the prediction
coverage and precision of the collaborative filtering algorithm. At each
device, CoFiGel transmits the video that would increase the estimated number of
positive user-video ratings the most if this video is transferred to the
destination device. We evaluated CoFiGel using real-world traces and show that
substantial improvement can be achieved compared to baseline schemes that do
not consider rating or contact history.

ABSTRACT_BEGIN
  In this paper, we propose a Modified distributed storage algorithm for
wireless sensor networks (MDSA). Wireless Sensor Networks, as it is well known,
suffer of power limitation, small memory capacity,and limited processing
capabilities. Therefore, every node may disappear temporarily or permanently
from the network due to many different reasons such as battery failure or
physical damage. Since every node collects significant data about its region,
it is important to find a methodology to recover these data in case of failure
of the source node. Distributed storage algorithms provide reliable access to
data through the redundancy spread over individual unreliable nodes. The
proposed algorithm uses flooding to spread data over the network and unicasting
to provide controlled data redundancy through the network. We evaluate the
performance of the proposed algorithm through implementation and simulation. We
show the results and the performance evaluation of the proposed algorithm.

ABSTRACT_BEGIN
  Constructing a connected dominating set as the virtual backbone plays an
important role in wireless networks. In this paper, we propose two novel
approximate algorithms for dominating set and connected dominating set in
wireless networks, respectively. Both of the algorithms are based on edge
dominating capability which is a novel notion proposed in this paper.
Simulations show that each of proposed algorithm has good performance
especially in dense wireless networks .

ABSTRACT_BEGIN
  In this paper we present a methodology employing statistical analysis and
stochastic geometry to study geometric routing schemes in wireless ad-hoc
networks. In particular, we analyze the network layer performance of one such
scheme, the random $\frac{1}{2}$disk routing scheme, which is a localized
geometric routing scheme in which each node chooses the next relay randomly
among the nodes within its transmission range and in the general direction of
the destination. The techniques developed in this paper enable us to establish
the asymptotic connectivity and the convergence results for the mean and
variance of the routing path lengths generated by geometric routing schemes in
random wireless networks. In particular, we approximate the progress of the
routing path towards the destination by a Markov process and determine the
sufficient conditions that ensure the asymptotic connectivity for both dense
and large-scale ad-hoc networks deploying the random $\frac{1}{2}$disk routing
scheme. Furthermore, using this Markov characterization, we show that the
expected length (hop-count) of the path generated by the random
$\frac{1}{2}$disk routing scheme normalized by the length of the path generated
by the ideal direct-line routing, converges to $3\pi/4$ asymptotically.
Moreover, we show that the variance-to-mean ratio of the routing path length
converges to $9\pi^2/64-1$ asymptotically. Through simulation, we show that the
aforementioned asymptotic statistics are in fact quite accurate even for finite
granularity and size of the network.

ABSTRACT_BEGIN
  Flow-based traffic measurement is a very challenging problem: Managing
counters for each individual traffic flow in hardware resources knowingly
struggle to scale with high-speed links. In this paper we propose a novel
lattice theory-based approach that improves flow-based measurement performances
and scales by keeping the number of the maintained hardware counters to a
minimum (result mathematically established in the paper). The crucial
contribution of the lattice is to map the computational semantics of the packet
processing to user requests for traffic measurement thus allowing for a
better-informed and focused counter assignment.
  An implementation over an Openflow switch, FlowME, was developed and
evaluated upon its memory usage, performance overhead, and processing effort to
generate the minimal solution. Experimental results indicate a significant
decrease in resource consumption.

ABSTRACT_BEGIN
  Currently the area of VANET lacks in having some better designed algorithms
to handle dynamic change and frequent disruption due to the high mobility of
the vehicles. There are many techniques to disseminate messages across the
moving vehicles but they are all highly dependent on some conditions involving
flow, density and speed. The two techniques that are commonly used are AODV (Ad
Hoc on Demand Distance Vector) and DSRC (Dedicated Short Range Communication).
This work presents a detailed analysis of AODV. This study is focused on the
use of AODV in Intelligent Transportation System. The limitations in the
working of AODV routing protocol has been identified and proved. These
limitations can be removed to some extent in order to increase the performance
of vehicular networks and make the driving more safe and easy for a normal user
as well as the implementation complications will be removed and an efficient
system implementation will be possible.

ABSTRACT_BEGIN
  Efficient handover algorithms are essential for highly performing mobile
wireless communications. These algorithms depend on numerous parameters, whose
settings must be appropriately optimized to offer a seamless connectivity.
Nevertheless, such an optimization is difficult in a time varying context,
unless adaptive strategies are used. In this paper, a new approach for the
handover optimization is proposed. First, a new modeling of the handover
process by a hybrid system that takes as input the handover parameters is
established. Then, this hybrid system is used to pose some dynamical
optimization approaches where the probability of outage and the probability of
handover are considered. Since it is shown that these probabilities are
difficult to compute, simple approximations of adequate accuracy are developed.
Based on these approximations, a new approach to the solution of the handover
optimizations is proposed by the use of a trellis diagram. A distributed
optimization algorithm is then developed to maximize handover performance. From
an extensive set of results obtained by numerical computations and simulations,
it is shown that the proposed algorithm allows to improve performance of the
handover considerably when compared to more traditional approaches.

ABSTRACT_BEGIN
  Mobile inventory, mobile commerce, banking and/or commercial applications are
some distinctive examples that increasingly use distributed transactions. It is
inevitably harder to design efficient commit protocols, due to some intrinsic
mobile environment limitations. A handful of protocols for transaction
processing have been offered, but the majority considers only a limited number
of communication models. We introduce an improved Connection Fault-Tolerant
model and evaluate its performance potential by comparing results in several
deferent scenarios, as well as its contribution to the overall mobile
transaction commit rate. Our performance analysis, conducted using
general-purpose discrete-event simulation programming language, evaluates the
effect of (i) ad-hoc communication between mobile hosts and (ii) the employment
of an appropriate decision algorithm for mobile host agents. Conjointly, they
substantially improve commit rate. In respect of the stochastic nature of
random network disconnections, we determine connection timeout values that
contribute the most to the highest perceived ad-hoc communication impact.

ABSTRACT_BEGIN
  In Cognitive Radio Networks (CRNs), unlicensed users are allowed to access
the licensed spectrum when it is not currently being used by primary users
(PUs). In this paper, we study the throughput maximization problem for a
multi-channel CRN where each SU can only sense a limited number of channels. We
show that this problem is strongly NP-hard, and propose an approximation
algorithm with a factor at least $1/2\mu$ where $\mu \in [1,2]$ is a system
parameter reflecting the sensing capability of SUs across channels and their
sensing budgets. This performance guarantee is achieved by exploiting a nice
structural property of the objective function and constructing a particular
matching. Our numerical results demonstrate the advantage of our algorithm
compared with both a random and a greedy sensing assignment algorithm.

ABSTRACT_BEGIN
  We study algorithms for carrier and rate allocation in cellular systems with
distributed components such as a heterogeneous LTE system with macrocells and
femtocells. Existing work on LTE systems often involves centralized techniques
or requires significant signaling, and is therefore not always applicable in
the presence of femtocells. More distributed CSMA-based algorithms
(carrier-sense multiple access) were developed in the context of 802.11 systems
and have been proven to be utility optimal. However, the proof typically
assumes a single transmission rate on each carrier. Further, it relies on the
CSMA collision detection mechanisms to know whether a transmission is feasible.
  In this paper we present a framework for LTE scheduling that is based on CSMA
techniques. In particular we first prove that CSMA-based algorithms can be
generalized to handle multiple transmission rates in a multi-carrier setting
while maintaining utility optimality. We then show how such an algorithm can be
implemented in a heterogeneous LTE system where the existing Channel Quality
Indication (CQI) mechanism is used to decide transmission feasibility.

ABSTRACT_BEGIN
  The emergence of WIMAX has attracted significant interests from all fields of
wireless communications including students, researchers, system engineers and
operators. The WIMAX can also be considered to be the main technology in the
implementation of other networks like wireless sensor networks. Developing an
understanding of the WIMAX system can be achieved by looking at the model of
the WIMAX system. This paper discusses the model building of the WIMAX physical
layer using computer MATLAB 7.5 versions. This model is a useful tool for BER
(Bit error rate) performance evaluation for the real data communication by the
WIMAX physical layer under different communication channels AWGN and fading
channel (Rayleigh and Rician), different channel encoding rates and digital
modulation schemes which is described in this paper. This paper investigates
the effect of communication channels of IEEE 802.16 OFDM based WIMAX Physical
Layer. The performance measures we presented in this paper are: the bit error
rate (BER) versus the ratio of bit energy to noise power spectral density
(Eb/No). The system parameters used in this paper are based on IEEE 802.16
standards. The simulation model built for this research work, demonstrates that
AWGN channel has better performance than Rayleigh and Rician fading channels.
Synthetic data is used to simulate this research work.

ABSTRACT_BEGIN
  We consider a system where packets (jobs) arrive for processing using one of
the policies in a given class. We study the connection between the minimal
evacuation times and the stability region of the system under the given class
of policies. The result is used to establish the equality of information
theoretic capacity region and system stability region for the multiuser
broadcast erasure channel with feedback.

ABSTRACT_BEGIN
  Sensors placed in agricultural field should have long network life. Failure
of node or link allows rerouting and establishing a new path from the source to
the sink. In this paper, a new path is established such that it is energy aware
during path discovery and is active for longer interval of time once it is
established. The parameters used for simulation are as those used in
agricultural application.

ABSTRACT_BEGIN
  The first decade of the 21st century has seen tremendous improvements in
mobile internet and its technologies. The high traffic volume of services such
as video conference and other real-time traffic applications are imposing a
great challenge on networks. In the meantime, demand for the use of mobile
devices in computation and communication such as smart phones, personal digital
assistants, and mobile-enabled laptops has grown rapidly. These services have
driven the demand for increasing and guaranteing bandwidth requirements in the
network. A direction of this paper is in the case of resource reservation
protocol (RSVP) over mobile IPv6 networks. There are numbers of proposed
solutions for RSVP and quality of service provision over mobile IPv6 networks,
but most of them using advanced resource reservation. In this paper, we propose
a mathematical model to determine maximum end-to-end delay bound through
intermediate routers along the network. These bounds are sent back to the home
agent for further processing. Once the home agent receives maximum end-to-end
delay bounds, it calculates cumulative bound and compares this bound with the
desired application end-to-end delay bound to make final decision on resource
reservation. This approach improves network resource utilization.

ABSTRACT_BEGIN
  A fundamental problem in cognitive radio systems is that the cognitive radio
is ignorant of the primary channel state and, hence, of the amount of actual
harm it inflicts on the primary license holder. Sensing the primary transmitter
does not help in this regard. To tackle this issue, we assume in this paper
that the cognitive user can eavesdrop on the ACK/NACK Automatic Repeat reQuest
(ARQ) fed back from the primary receiver to the primary transmitter. Assuming a
primary channel state that follows a Markov chain, this feedback gives the
cognitive radio an indication of the primary link quality. Based on the
ACK/NACK received, we devise optimal transmission strategies for the cognitive
radio so as to maximize a weighted sum of primary and secondary throughput. The
actual weight used during network operation is determined by the degree of
protection afforded to the primary link. We begin by formulating the problem
for a channel with a general number of states. We then study a two-state model
where we characterize a scheme that spans the boundary of the primary-secondary
rate region. Moreover, we study a three-state model where we derive the optimal
strategy using dynamic programming. We also extend our two-state model to a
two-channel case, where the secondary user can decide to transmit on a
particular channel or not to transmit at all. We provide numerical results for
our optimal strategies and compare them with simple greedy algorithms for a
range of primary channel parameters. Finally, we investigate the case where
some of the parameters are unknown and are learned using hidden Markov models
(HMM).

ABSTRACT_BEGIN
  The evolution of the Internet has manifested itself in many ways: the traffic
characteristics, the interconnection topologies and the business relationships
among the autonomous components. It is important to understand why (and how)
this evolution came about, and how the interplay of these dynamics may affect
future evolution and services. We propose a network aware, macroscopic model
that captures the characteristics and interactions of the application and
network providers, and show how it leads to a market equilibrium of the
ecosystem. By analyzing the driving forces and the dynamics of the market
equilibrium, we obtain some fundamental understandings of the cause and effect
of the Internet evolution, which explain why some historical and recent
evolutions have happened. Furthermore, by projecting the likely future
evolutions, our model can help application and network providers to make
informed business decisions so as to succeed in this competitive ecosystem.

ABSTRACT_BEGIN
  In a Vehicular Ad-hoc Network (VANET), the amount of interference from
neighboring nodes to a communication link is governed by the vehicle density
dynamics in vicinity and transmission probabilities of terminals. It is obvious
that vehicles are distributed non-homogeneously along a road segment due to
traffic controls and speed limits at different portions of the road. The common
assumption of homogeneous node distribution in the network in most of the
previous work in mobile ad-hoc networks thus appears to be inappropriate in
VANETs. In light of the inadequacy, we present in this paper an original
methodology to study the performance of VANETs with practical vehicle
distribution in urban environment. Specifically, we introduce the stochastic
traffic model to characterize the general vehicular traffic flow as well as the
randomness of individual vehicles, from which we can acquire the mean dynamics
and the probability distribution of vehicular density. As illustrative
examples, we demonstrate how the density knowledge from the stochastic traffic
model can be utilized to derive the throughput and progress performance of
three routing strategies in different channel access protocols. We confirm the
accuracy of the analytical results through extensive simulations. Our results
demonstrate the applicability of the proposed methodology on modeling protocol
performance, and shed insight into the performance analysis of other
transmission protocols and network configurations in vehicular networks.
Furthermore, we illustrate that the optimal transmission probability for
optimized network performance can be obtained as a function of the location
space from our results. Such information can be computed by road-side nodes and
then broadcasted to road users for optimized multi-hop packet transmission in
the communication network.

ABSTRACT_BEGIN
  The traditional model of single ownership of all the physical network
elements and network layers by mobile network operators is beginning to be
challenged. This has been attributed to the rapid and complex technology
migration compounded with rigorous regulatory requirements and ever increasing
capital expenditures. These trends, combined together with the increasing
competition, rapid commoditization of telecommunication equipments and rising
separation of network and service provisioning are pushing the operators to
adopt multiple strategies, with network infrastructure sharing in the core and
radio access networks emerging as a more radical mechanism to substantially and
sustainably improve network costs. Through infrastructure sharing, developing
countries and other emerging economies can harness the technological, market
and regulatory developments that have fostered affordable access to mobile and
broadband services. Similarly, the network operators entering or consolidating
in the emerging markets can aim for substantial savings on capital and
operating expenses. The present paper aims to investigate the current
technological solutions and regulatory and the technical-economical dimensions
in connection with the sharing of mobile telecommunication networks in emerging
countries. We analyze the estimated savings on capital and operating expenses,
while assessing the technical constraints, applicability and benefits of the
network sharing solutions in an emerging market context.

ABSTRACT_BEGIN
  The Wireless Gigabit Alliance (WiGig) and IEEE 802.11ad are developing a
multigigabit wireless personal and local area network (WPAN/WLAN) specification
in the 60 GHz millimeter wave band. Chipset manufacturers, original equipment
manufacturers (OEMs), and telecom companies are also assisting in this
development. 60 GHz millimeter wave transmission will scale the speed of WLANs
and WPANs to 6.75 Gbit/s over distances less than 10 meters. This technology is
the first of its kind and will eliminate the need for cable around personal
computers, docking stations, and other consumer electronic devices.
High-definition multimedia interface (HDMI), display port, USB 3.0, and
peripheral component interconnect express (PCIe) 3.0 cables will all be
eliminated. Fast downloads and uploads, wireless sync, and
multi-gigabit-per-second WLANs will be possible over shorter distances. 60 GHz
millimeter wave supports fast session transfer (FST) protocol, which makes it
backward compatible with 5 GHz or 2.4 GHz WLAN so that end users experience the
same range as in today's WLANs. IEEE 802.11ad specifies the physical (PHY)
sublayer and medium access control (MAC) sublayer of the protocol stack. The
MAC protocol is based on time-division multiple access (TDMA), and the PHY
layer uses single carrier (SC) and orthogonal frequency division multiplexing
(OFDM) to simultaneously enable low-power, high-performance applications.

ABSTRACT_BEGIN
  This paper studies the problem of content distribution in wireless
peer-to-peer networks where all nodes are selfish and non-cooperative. We
propose a model that considers both the broadcast nature of wireless channels
and the incentives of nodes, where each node aims to increase its own download
rate and reduces its upload rate through the course of content distribution. We
then propose a protocol for these selfish nodes to exchange contents. Our
protocol is distributed and does not require the exchange of money, reputation,
etc., and hence can be easily implemented without additional infrastructure.
Moreover, we show that our protocol can be easily modified to employ network
coding.
  The performance of our protocol is studied. We derive a closed-form
expression of Nash Equilibriums when there are only two files in the system.
The prices of anarchy, both from each node's perspective and the whole system's
perspective, are also characterized. Moreover, we propose a distributed
mechanism where each node adjusts its strategies only based on local
information and show that the mechanism converges to a Nash Equilibrium. We
also introduce an approach for calculating Nash Equilibriums for systems that
incorporate network coding when there are more than two files.

ABSTRACT_BEGIN
  Mobile ad-hoc networks (MANETs) are a set of self organized wireless mobile
nodes that works without any predefined infrastructure. For routing data in
MANETs, the routing protocols relay on mobile wireless nodes. In general, any
routing protocol performance suffers i) with resource constraints and ii) due
to the mobility of the nodes. Due to existing routing challenges in MANETs
clustering based protocols suffers frequently with cluster head failure
problem, which degrades the cluster stability. This paper proposes, Enhanced
CBRP, a schema to improve the cluster stability and in-turn improves the
performance of traditional cluster based routing protocol (CBRP), by electing
better cluster head using weighted clustering algorithm and considering some
crucial routing challenges. Moreover, proposed protocol suggests a secondary
cluster head for each cluster, to increase the stability of the cluster and
implicitly the network infrastructure in case of sudden failure of cluster
head.

ABSTRACT_BEGIN
  Now a days, Internet plays a major role in our day to day activities e.g.,
for online transactions, online shopping, and other network related
applications. Internet suffers from slow convergence of routing protocols after
a network failure which becomes a growing problem. Multiple Routing
Configurations [MRC] recovers network from single node/link failures, but does
not support network from multiple node/link failures. In this paper, we propose
Enhanced MRC [EMRC], to support multiple node/link failures during data
transmission in IP networks without frequent global re-convergence. By
recovering these failures, data transmission in network will become fast.

ABSTRACT_BEGIN
  Vehicle-to-Vehicle (V2V) communication is a core technology for enabling
safety and non-safety applications in next generation Intelligent
Transportation Systems. Due to relatively low heights of the antennas, V2V
communication is often influenced by topographic features, man-made structures,
and other vehicles located between the communicating vehicles. On highways, it
was shown experimentally that vehicles can obstruct the line of sight (LOS)
communication up to 50 percent of the time; furthermore, a single obstructing
vehicle can reduce the power at the receiver by more than 20 dB. Based on both
experimental measurements and simulations performed using a validated channel
model, we show that the elevated position of antennas on tall vehicles improves
communication performance. Tall vehicles can significantly increase the
effective communication range, with an improvement of up to 50 percent in
certain scenarios. Using these findings, we propose a new V2V relaying scheme
called Tall Vehicle Relaying (TVR) that takes advantage of better channel
characteristics provided by tall vehicles. TVR distinguishes between tall and
short vehicles and, where appropriate, chooses tall vehicles as next hop
relays. We investigate TVR's system-level performance through a combination of
link-level experiments and system-level simulations and show that it
outperforms existing techniques.

ABSTRACT_BEGIN
  In this paper we analyze power and frequency allocation in wireless networks
through potential games. Potential games are used frequently in the literature
for this purpose due to their desirable properties, such as convergence and
stability. However, potential games usually assume massive message passing to
obtain the necessary neighbor information at each user to achieve these
properties. In this paper we show an example of a game where we are able to
characterize the necessary neighbor information in order to show that the game
has a potential function and the properties of potential games. We consider a
network consisting of local access points where the goal of each AP is to
allocate power and frequency to achieve some SINR requirement. We use the
physical SINR model to validate a successful allocation, and show that given a
suitable payoff function the game emits a generalized ordinal potential
function under the assumption of sufficient neighbor information. Through
simulations we evaluate the performance of the proposed game on a large scale
in relation to the amount of information at each AP.

ABSTRACT_BEGIN
  In this paper we introduced Modified Sized-based Queue Management as a
dropping scheme that aims to fairly prioritize and allocate more service to
VoIP traffic over bulk data like FTP as the former one usually has small packet
size with less impact to the network congestion. In the same time, we want to
guarantee that this prioritization is fair enough for both traffic types. On
the other hand we study the total link delay over the congestive link with the
attempt to alleviate this congestion as much as possible at the by function of
early congestion notification. Our M-SQM scheme has been evaluated with NS2
experiments to measure the packets received from both and total link-delay for
different traffic. The performance evaluation results of M-SQM have been
validated and graphically compared with the performance of other three legacy
AQMs (RED, RIO, and PI). It is depicted that our M-SQM outperformed these AQMs
in providing QoS level of service differentiation.

ABSTRACT_BEGIN
  In this paper, we focus on the experimental evaluation of TCP over the High
Speed Downlink Packet Access (HSDPA), an upgrade of UMTS that is getting
worldwide deployment. Today, this is particularly important in view of the
"liberalization" brought in by the Linux OS which offers several variants of
TCP congestion control. In particular, we consider four TCP variants: 1) TCP
NewReno, which is the only congestion control standardized by the IETF; 2) TCP
BIC, that was, and 3) TCP Cubic that is the default algorithm in the Linux OS;
4) Westwood+ TCP that has been shown to be particularly effective over wireless
links. Main results are that all the TCP variants provide comparable goodputs
but with significant larger round trip times and number of retransmissions and
timeouts in the case of TCP BIC/Cubic, which is a consequence of their more
aggressive probing phases. On the other hand, TCP Westwood+ provides the
shortest round trip delays, which is an effect of its unique way of setting
control windows after congestion episode based on bandwidth measurements.

ABSTRACT_BEGIN
  We introduce CTCP, a novel multi-path transport protocol using network
coding. CTCP is designed to incorporate TCP's good features, such as congestion
control and reliability, while improving on TCP's performance in lossy and/or
dynamic networks. CTCP builds upon the ideas of TCP/NC introduced by
Sundararajan et al. and uses network coding to provide robustness against
losses. We introduce the use of multiple paths to provide robustness against
mobility and network failures. We provide an implementation of CTCP (in
userspace) to demonstrate its performance.

ABSTRACT_BEGIN
  We introduce CTCP, a reliable transport protocol using network coding. CTCP
is designed to incorporate TCP features such as congestion control,
reliability, and fairness while significantly improving on TCP's performance in
lossy, interference-limited and/or dynamic networks. A key advantage of
adopting a transport layer over a link layer approach is that it provides
backward compatibility with wireless equipment installed throughout existing
networks. We present a portable userspace implementation of CTCP and
extensively evaluate its performance in both testbed and production wireless
networks.

ABSTRACT_BEGIN
  We consider the problem of inferring the topology of a network with $M$
sources and $N$ receivers (hereafter referred to as an $M$-by-$N$ network), by
sending probes between the sources and receivers. Prior work has shown that
this problem can be decomposed into two parts: first, infer smaller subnetwork
components (i.e., $1$-by-$N$'s or $2$-by-$2$'s) and then merge these components
to identify the $M$-by-$N$ topology. In this paper, we focus on the second
part, which had previously received less attention in the literature. In
particular, we assume that a $1$-by-$N$ topology is given and that all
$2$-by-$2$ components can be queried and learned using end-to-end probes. The
problem is which $2$-by-$2$'s to query and how to merge them with the given
$1$-by-$N$, so as to exactly identify the $2$-by-$N$ topology, and optimize a
number of performance metrics, including the number of queries (which directly
translates into measurement bandwidth), time complexity, and memory usage. We
provide a lower bound, $\lceil \frac{N}{2} \rceil$, on the number of
$2$-by-$2$'s required by any active learning algorithm and propose two greedy
algorithms. The first algorithm follows the framework of multiple hypothesis
testing, in particular Generalized Binary Search (GBS), since our problem is
one of active learning, from $2$-by-$2$ queries. The second algorithm is called
the Receiver Elimination Algorithm (REA) and follows a bottom-up approach: at
every step, it selects two receivers, queries the corresponding $2$-by-$2$, and
merges it with the given $1$-by-$N$; it requires exactly $N-1$ steps, which is
much less than all $\binom{N}{2}$ possible $2$-by-$2$'s. Simulation results
over synthetic and realistic topologies demonstrate that both algorithms
correctly identify the $2$-by-$N$ topology and are near-optimal, but REA is
more efficient in practice.

ABSTRACT_BEGIN
  The performances of multimedia applications built on wireless systems depend
on bandwidth availability that might heavily affect the quality of service. The
IEEE 802.11 standards do not provide performed mechanism for bandwidth
management through data load distribution among different APs of the network.
Then, an AP can be heavily overloaded causing throughput degradation. Load
Balancing Algorithms (LBAs) was been considered as one of the attractive
solution to share the traffic through the available access points bandwidths.
However, applying the load balancing algorithm and shifting a mobile connection
from an access point to another without considering the received signal
strength indicator of the concerned APs might causes worst communication
performances. This paper is a contribution to check the performance's limits of
the LBA algorithm through experimental evaluation of communication metrics for
MPEG-4 video transmission over IEEE 802.11 network. Then, the paper focuses on
the proposition of a new LBA algorithm structure with the consideration of the
RSS level.

ABSTRACT_BEGIN
  Mobile Ad hoc Networks are dynamic networks populated by mobile devices, or
mobile nodes.The Mobile Nodes are free to move anywhere and at any time. The
population of the nodes may have some influence on the mobility rate of the
mobile nodes. This paper presents simulation results using Matrix Laboratory
software. The study investigates the influence of mobile nodes parameters such
as number of nodes on the nodes speeds and nodes distribution in a given area.
The results have indicated that the number of mobile nodes have impact on the
speeds of the nodes in a location.

ABSTRACT_BEGIN
  Wireless Mesh Networks (WMNs) consist of radio nodes organized in a mesh
topology for serving wireless mesh clients to communicate with one another or
to connect to the Internet. Nodes in a mesh network can communicate with each
other either directly or through one or more intermediate nodes, similar to
social networks. WMNs share many common properties with social networks. We
first identify the differences and similarities between social networks and
WMNs and then use metrics that are typically used for social network analysis
(SNA) to assess real WMNs. Analyzing real WMN data collected from the UCSB
MeshNet and MIT Roofnet testbeds reveals that using SNA metrics are helpful in
designing WMNs with better performance. We demonstrate the validity of our
conclusions and this new approach by focusing on two sample applications of
social networks: network reliability assessment and channel access scheduling.

ABSTRACT_BEGIN
  In this work we focus on modeling a little studied type of traffic, namely
the network traffic generated from endhosts. We introduce a parsimonious
parametric model of the marginal distribution for connection arrivals. We
employ mixture models based on a convex combination of component distributions
with both heavy and light-tails. These models can be fitted with high accuracy
using maximum likelihood techniques. Our methodology assumes that the
underlying user data can be fitted to one of many modeling options, and we
apply Bayesian model selection criteria as a rigorous way to choose the
preferred combination of components. Our experiments show that a simple
Pareto-exponential mixture model is preferred for a wide range of users, over
both simpler and more complex alternatives. This model has the desirable
property of modeling the entire distribution, effectively segmenting the
traffic into the heavy-tailed as well as the non-heavy-tailed components. We
illustrate that this technique has the flexibility to capture the wide
diversity of user behaviors.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks (VANETs) allow vehicles to form a self-organized
network without any fixed infrastructure. VANETs have received wide attention
and numerous research issues have been identified in the recent time. The
design and implementation of efficient and scalable routing protocols for
VANETs is a challenging task due to high dynamics and mobility constraints. In
this paper, we have proposed D-LAR (Directional-Location Aided Routing), is an
extension of Location Aided Routing (LAR) with Directional Routing (DIR)
capability. D-LAR is a greedy approach based-position based routing protocol to
forward packet to the node present in request zone within the transmission
range of the source node as most suitable next-hop node. We have justified the
feasibility of our proposed protocol for VANET.

ABSTRACT_BEGIN
  The current functionality supported by OpenFlowbased software defined
networking (SDN) includes switching, routing, tunneling, and some basic fire
walling while operating on traffic flows. However, the semantics of SDN do not
allow for other operations on the traffic, nor does it allow operations at a
higher granularity. In this work, we describe a method to expand the SDN
framework to add other network primitives. In particular, we present a method
to integrate different network elements (like cache, proxy etc). Here, we focus
on storage and caching, but our method could be expanded to other functionality
seamlessly. We also present a method to identify content so as to perform
per-content policy, as opposed to per flow policy. We have implemented the
proposed mechanisms to demonstrate its feasibility.

ABSTRACT_BEGIN
  This paper studies price-based spectrum access control in cognitive radio
networks, which characterizes network operators' service provisions to
delay-sensitive secondary users (SUs) via pricing strategies. Based on the two
paradigms of shared-use and exclusive-use dynamic spectrum access (DSA), we
examine three network scenarios corresponding to three types of secondary
markets. In the first monopoly market with one operator using opportunistic
shared-use DSA, we study the operator's pricing effect on the equilibrium
behaviors of self-optimizing SUs in a queueing system. %This queue represents
the congestion of the multiple SUs sharing the operator's single \ON-\OFF
channel that models the primary users (PUs) traffic. We provide a queueing
delay analysis with the general distributions of the SU service time and PU
traffic using the renewal theory. In terms of SUs, we show that there exists a
unique Nash equilibrium in a non-cooperative game where SUs are players
employing individual optimal strategies. We also provide a sufficient condition
and iterative algorithms for equilibrium convergence. In terms of operators,
two pricing mechanisms are proposed with different goals: revenue maximization
and social welfare maximization. In the second monopoly market, an operator
exploiting exclusive-use DSA has many channels that will be allocated
separately to each entering SU. We also analyze the pricing effect on the
equilibrium behaviors of the SUs and the revenue-optimal and socially-optimal
pricing strategies of the operator in this market. In the third duopoly market,
we study a price competition between two operators employing shared-use and
exclusive-use DSA, respectively, as a two-stage Stackelberg game. Using a
backward induction method, we show that there exists a unique equilibrium for
this game and investigate the equilibrium convergence.

ABSTRACT_BEGIN
  We present the design of a cross-layer protocol to maintain connectivity in
an earthquake monitoring and early warning sensor network in the absence of
communications infrastructure. Such systems, by design, warn of events that
severely damage or destroy communications infrastructure. However, the data
they provide is of critical importance to emergency and rescue decision making
in the immediate aftermath of such events, as is continued early warning of
aftershocks, tsunamis, or other subsequent dangers. Utilizing a beyond
line-of-sight (BLOS) HF physical layer, we propose an adaptable cross-layer
network design that meets these specialized requirements. We are able to
provide ultra high connectivity (UHC) early warning on strict time deadlines
under worst-case channel conditions along with providing sufficient capacity
for continued seismic data collection from a 1000 sensor network.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) are composed of nodes that gather metrics
like temperature, pollution or pressure from events generated by external
entities. Localization in WSNs is paramount, given that the collected metrics
must be related to the place of occurrence. This document presents an
alternative way towards localization in randomly deployed WSNs based on the
composability of localization protocols. Results show a totally distributed
localization procedure that achieves a higher number of located nodes than the
conventional, individual execution of localization protocols while maintaining
the same low levels of battery consumption.

ABSTRACT_BEGIN
  We study the profit maximization problem of a cognitive virtual network
operator in a dynamic network environment. We consider a downlink OFDM
communication system with various network dynamics, including dynamic user
demands, uncertain sensing spectrum resources, dynamic spectrum prices, and
time-varying channel conditions. In addition, heterogenous users and imperfect
sensing technology are incorporated to make the network model more realistic.
By exploring the special structural of the problem, we develop a low-complexity
on-line control policies that determine pricing and resource scheduling without
knowing the statistics of dynamic network parameters. We show that the proposed
algorithms can achieve arbitrarily close to the optimal profit with a proper
trade-off with the queuing delay.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs) are expected to find wide applicability and
increasing deployment in near future. In this paper, we propose a new protocol,
Threshold Sensitive Stable Election Protocol (TSEP), which is reactive protocol
using three levels of heterogeneity. Reactive networks, as opposed to proactive
networks, respond immediately to changes in relevant parameters of interest. We
evaluate performance of our protocol for a simple temperature sensing
application and compare results of protocol with some other protocols LEACH,
DEEC, SEP, ESEP and TEEN. And from simulation results it is observed that
protocol outperforms concerning life time of sensing nodes used.

ABSTRACT_BEGIN
  Wireless Sensor Networks (WSNs), with growing applications in the environment
which are not within human reach have been addressed tremendously in the recent
past. For optimized working of network many routing algorithms have been
proposed, mainly focusing energy efficiency, network lifetime, clustering
processes. Considering homogeneity of network, we proposed Energy Efficient
Sleep Awake Aware (EESAA) intelligent routing protocol for WSNs. In our
proposed technique we evaluate and enhance certain issues like network
stability, network lifetime and cluster head selection process. Utilizing the
concept of characteristical pairing among sensor nodes energy utilization is
optimized. Simulation results show that our proposed protocolnificantly
improved the

ABSTRACT_BEGIN
  This paper presents the link availability probability. We evaluate and
compare the link availability probability for routing protocols; Ad hoc
On-demand Distance vector (AODV), Dynamic Source Routing (DSR) and Fisheye
State Routing (FSR) for different number of connections and node density. A
novel contribution of this work is enhancement in existing parameters of
routing protocols; AODV, DSR and FSR as MOD-AODV, MOD-DSR and MOD-FSR. From the
results, we observe that MOD-DSR and DSR outperform MOD-AODV, AODV, MODOLSR and
OLSR in terms of Packet Delivery Ratio (PDR), Average End-to End Delay (AE2ED),
link availability probability at the cost of high value of Normalized Routing
Overhead (NRO).

ABSTRACT_BEGIN
  Mobile Ad-hoc NETworks (MANETs) comprise on wireless mobile nodes that are
communicating with each other without any infrastructure. Vehicular Ad-hoc
NETwork (VANET) is a special type of MANETs in which vehicles with high
mobility need to communicate with each other. In this paper, we present a novel
framework for link availability of paths for static as well as dynamic
networks. Moreover, we evaluate our frame work for routing protocols
performance with different number of nodes in MANETs and in VANETs. We select
three routing protocols namely Ad-hoc On-demand Distance Vector (AODV),
Fish-eye State Routing (FSR) and Optimized Link State Routing (OLSR).
Furthermore, we have also modified default parameters of selected protocols to
check their efficiencies. Performance of these protocols is analyzed using
three performance metrics; Packet Delivery Ratio (PDR), Normalized Routing
Overhead (NRO) and End-to-End Delay (E2ED) against varying scalabilities of
nodes. We perform these simulations with NS-2 using TwoRayGround propagation
model. The SUMO simulator is used to generate a random mobility pattern for
VANETs. From the extensive simulations, we observe that AODV outperforms among
all three protocols.

ABSTRACT_BEGIN
  In a Symbiotic Cognitive Relaying (SCR) scenario, the Secondary users(SU)
nodes can act as multihop relays to assist the communication between Primary
User(PU) nodes in the case of a weak direct link. In return, the SU nodes are
incentivised with the right to carry out SU-SU communication using licensed PU
band for a fixed amount of time, referred to as the 'Time Incentive'. Existing
work on SCR is constrained to a fixed ad-hoc SU network. In this paper, we
introduce mobility in SCR by considering mobile SU nodes while keeping the PU
nodes fixed. This paper uses a specific mobility pattern and routing strategy
for the SU nodes to propose theoretical bounds on the throughput and delay of
PU-PU transmission. We derive analytically the least throughput and maximum
delay possible in our model.

ABSTRACT_BEGIN
  In this paper, we experimentally investigate the combined effect of human,
antenna orientation in elevation direction and the ground effect on the
Received Signal Strength Indicator (RSSI) parameter in the Wireless Sensor
Network (WSN). In experiment, we use MICAz motes and consider different
scenarios where antenna of the transmitter node is tilted in elevation
direction. The motes were placed on the ground to take into account the ground
effect on the RSSI. The effect of one, two and four persons on the RSSI is
recorded. For one and two persons, different walking paces e.g. slow, medium
and fast pace, are analysed. However, in case of four persons, random movement
is carried out between the pair of motes. The experimental results show that
some antenna orientation angles have drastic effect on the RSSI, even without
any human activity. The fluctuation count and range of RSSI in different
scenarios with same walking pace are completely different. Therefore, an
efficient human activity algorithm is need that effectively takes into count
the antenna elevation and other parameters to accurately detect the human
activity in the WSN deployment region.

ABSTRACT_BEGIN
  In this paper we analyzed the Radio Frequency (RF) propagation
characteristics of monopole antenna in MICAz mote. During the experimental
analysis, two scenarios are considered. In Scenario-I, a pair of MICAz nodes
(one transmitting and one receiving node) are placed on the ground and the RSSI
is measured in presence of the ground effect. In Scenario-II, only the
transmitting node is placed above the ground; however, the receiving node is
placed on the ground. The RSSI is measured by changing the antenna orientation
at different angles and distances between them. The results show that the
ground effect, antenna orientation and distance between the sensor nodes
drastically affect the RSSI.

ABSTRACT_BEGIN
  The issue of optimizing the limited and often non-renewable energy of sensor
nodes due to its direct impact on network lifetime dominates every aspect of
wireless sensor networks. Existing techniques for optimizing energy consumption
are based on exploiting node redundancy, adaptive radio transmission power and
topology control. Topology control protocols have a significant impact on
network lifetime, available energy and connectivity. In this paper we classify
sensor nodes as strong and weak nodes based on their residual energy as well as
operational lifetime and propose a Clustering based topology control protocol
(CTCP) which extends network lifetime while guarantying the minimum
connectivity. Extensive simulations in Java-Simulator (J-Sim) show that our
proposed protocol outperforms the existing protocols in terms of various
performance metrics life network lifetime, average delay and minimizes energy
utilization.

ABSTRACT_BEGIN
  Peer-to-Peer (P2P) technology has been regarded as a promising way to help
Content Providers (CPs) cost-effectively distribute content. However, under the
traditional Internet pricing mechanism, the fact that most P2P traffic flows
among peers can dramatically decrease the profit of ISPs, who may take actions
against P2P and impede the progress of P2P technology. In this paper, we
develop a mathematical framework to analyze such economic issues. Inspired by
the idea from cooperative game theory, we propose a cooperative
profit-distribution model based on Nash Bargaining Solution (NBS), in which
eyeball ISPs and Peer-assisted CPs (PCPs) form two coalitions respectively and
then compute a fair Pareto point to determine profit distribution. Moreover, we
design a fair and feasible mechanism for profit distribution within each
coalition. We show that such a cooperative method not only guarantees the fair
profit distribution among network participators, but also helps to improve the
economic efficiency of the overall network system. To our knowledge, this is
the first work that systematically studies solutions for P2P caused unbalanced
profit distribution and gives a feasible cooperative method to increase and
fairly share profit.

ABSTRACT_BEGIN
  In recent years, WSNs are garnering lot of interest from research community
because of their unique characteristics and potential for enormous range of
applications. Envision for new class of applications are being emerged such as
human augmentation, enhancing social interaction etc. Misunderstanding or
misinterpretation of behaviors from individuals leads to social conflicts.
There are various theories that classify people into different personality
types. Most of the existing theories rely on questionnaires, which is highly
unreliable. Anyone can lead such theories in practice to incorrect
classification intentionally or unintentionally. The objective of this research
is to investigate existing solutions and propose a basic infrastructure for an
automated context-aware psychological classification based on different
parameters. The idea is to use wearable sensors to sense and measure various
human body parameters (i.e. body temperature, blood pressure, perspiration,
brain impulses etc) that coerce human psychological condition. The data
collected from these parameters is transformed in to information, to determine
personality type, mood and psychological condition of interacting parties. This
information is shared among counterparts to better understand each other in
order to avoid potential conflicting situations. We believe that it will help
peoples understand each other, improve their quality of life and minimize
possible conflicting situations.

ABSTRACT_BEGIN
  The objective of this paper is to increase life time of homogeneous wireless
sensor networks (WSNs) through minimizing long range communication and energy
balancing. Sensor nodes are resource constrained particularly with limited
energy that is difficult or impossible to replenish. LEACH (Low Energy Adaptive
Clustering Hierarchy) is most well-known cluster based architecture for WSN
that aims to evenly dissipate energy among all sensor nodes. In cluster based
architecture, the role of cluster head is very crucial for the successful
operation of WSN because once the cluster head becomes non functional, the
whole cluster becomes dysfunctional. We have proposed a modified cluster based
WSN architecture by introducing a coordinator node (CN) that is rich in terms
of resources. This CN take up the responsibility of transmitting data to the
base station over longer distances from cluster heads. We have proposed a
cluster head selection algorithm based on K-theorem and other parameters i.e.
residual energy, distance to coordinator node, reliability and degree of
mobility. The K-theorem is used to select candidate cluster heads based on
bunch of sensor nodes in a cluster. We believe that the proposed architecture
and algorithm achieves higher energy efficiency through minimizing
communication and energy balancing. The proposed architecture is more scalable
and proposed algorithm is robust against even/uneven node deployment and node
mobility.

ABSTRACT_BEGIN
  This paper evaluates and classifies existing and emerging energy-control
technologies for computer networks based on their relative value functions.
Using formal decision analysis methods, we demonstrate the impact of
risk-benefit dimensions on technology certain equivalent and deployment
perspective. We demonstrate how energy control solutions can be cost-effective
or unsustainable depending on network type and operator risk tolerance.

ABSTRACT_BEGIN
  Mobile phones play increasingly bigger role in our everyday lives. Today,
most smart phones comprise a wide variety of sensors which can sense the
physical environment. The Internet of Things vision encompasses participatory
sensing which is enabled using mobile phones based sensing and reasoning. In
this research, we propose and demonstrate our DAM4GSN architecture to capture
sensor data using sensors built into the mobile phones. Specifically, we
combine an open source sensor data stream processing engine called 'Global
Sensor Network (GSN)' with the Android platform to capture sensor data. To
achieve this goal, we proposed and developed a prototype application that can
be installed on Android devices as well as a AndroidWrapper as a GSN middleware
component. The process and the difficulty of manually connecting sensor devices
to sensor data processing middleware systems are examined. We evaluated the
performance of the system based on power consumption of the mobile client.

ABSTRACT_BEGIN
  We consider a data aggregating wireless network where all nodes have data to
send to a single destination node, the sink. We consider a linear placement of
nodes with the sink at one end. The nodes communicate directly to the sink
(single hop transmission) and we assume that the nodes are scheduled one at a
time by a central scheduler (possibly the sink). The wireless nodes are power
limited and our network objective (notion of fairness) is to maximize the
minimum throughput of the nodes subject to the node power constraints. In this
work, we consider network designs that permit adapting node transmission time,
node transmission power and node placements, and study cross- layer strategies
that seek to maximize the network throughput. Using simulations, we
characterize the performance of the dif- ferent strategies and comment on their
applicability for various network scenarios.

ABSTRACT_BEGIN
  WSNs can be considered a distributed control system designed to react to
sensor information with an effective and timely action. For this reason, in
WSNs it is important to provide real-time coordination and communication to
guarantee timely execution of the right actions. In this paper a new
communication protocol RRRT to support robust real-time and reliable event data
delivery with minimum energy consumption and with congestion avoidance in WSNs
is proposed. The proposed protocol uses the fault tolerant optimal path for
data delivery. The proposed solution dynamically adjust their protocol
configurations to adapt to the heterogeneous characteristics of WSNs.
Specifically, the interactions between contention resolution and congestion
control mechanisms as well as the physical layer effects in WSNs are
investigated.

ABSTRACT_BEGIN
  The Load-Balanced Router architecture has received a lot of attention because
it does not require centralized scheduling at the internal switch fabrics. In
this paper we reexamine the architecture, motivated by its potential to turn
off multiple components and thereby conserve energy in the presence of low
traffic.
  We perform a detailed analysis of the queue and delay performance of a
Load-Balanced Router under a simple random routing algorithm. We calculate
probabilistic bounds for queue size and delay, and show that the probabilities
drop exponentially with increasing queue size or delay. We also demonstrate a
tradeoff in energy consumption against the queue and delay performance.

ABSTRACT_BEGIN
  This research aims at using vehicular ad-hoc networks as infra-structure for
an urban cyber-physical system in order to gather data about a city. In this
scenario, all nodes are data sources and there is a gateway as ultimate
destination for all packets. Because of the volatility of the network
connections and uncertainty of actual node placement, we argue that a
broadcast-based protocol is the most adequate solution, despite the high
overhead. The Urban Data Collector (UDC) protocol has been proposed which uses
a distributed election of the forwarding node among the nodes receiving the
packet: nodes that are nearer to the gateway have shorter timers and a higher
forwarding probabilities. The performance of the UDC protocol has been
evaluated with different suppression levels in terms of the amount of collected
data from each road segment using NS-3, and our results show that UDC can
achieve significantly higher sensing accuracy than to other broadcast-based
protocols.

ABSTRACT_BEGIN
  In order to gather information more efficiently, wireless sensor networks
(WSNs) are partitioned into clusters. Most proposed clustering algorithms do
not consider the location of the base station. This situation causes hot spot
problems in multi-hop WSNs. In this paper, we analyze a fuzzy clustering
algorithm (FCA) which aims to prolong the lifetime of WSNs. This algorithm
adjusts the cluster-head radius considering the residual energy and distance to
the base station parameters of the sensor nodes. This helps to decrease the
intra-cluster work of the sensor nodes which are closer to the base station or
have lower battery level. Fuzzy logic is utilized for handling the
uncertainties in cluster-head radius estimation. We compare this algorithm with
the low energy adaptive clustering hierarchy (LEACH) algorithm according to the
parameters of first node dies half of the nodes alive and energy-efficiency
metrics. Our simulation results show that the fuzzy clustering approach
performs better than LEACH. Therefore, the FCA is a stable and energy-efficient
clustering algorithm.

ABSTRACT_BEGIN
  A mobile ad hoc network (MANET) is a non-centralised, multihop, wireless
network that lacks a common infrastructure and hence it needs
self-organisation. The biggest challenge in MANETs is to find a path between
communicating nodes, which is the MANET routing problem. Biology-inspired
techniques such as ant colony optimisation (ACO) which have proven to be very
adaptable in other problem domains, have been applied to the MANET routing
problem as it forms a good fit to the problem. The general characteristics of
these biological systems, which include their capability for self-organisation,
self-healing and local decision making, make them suitable for routing in
MANETs. In this paper, we discuss a few ACO based protocols, namely AntNet,
hybrid ACO (AntHocNet), ACO based routing algorithm (ARA), imProved ant colony
optimisation routing algorithm for mobile ad hoc NETworks (PACONET), ACO based
on demand distance vector (Ant-AODV) and ACO based dynamic source routing
(Ant-DSR), and determine their performance in terms of quality of service (QoS)
parameters, such as end-to-end delay and packet delivery ratio, using Network
Simulator 2 (NS2). We also compare them with well known protocols, ad hoc on
demand distance vector (AODV) and dynamic source routing (DSR), based on the
random waypoint mobility model. The simulation results show how this
biology-inspired approach helps in improving QoS parameters.

ABSTRACT_BEGIN
  Developing routing protocols for Vehicular Ad Hoc Networks (VANETs) is a
significant challenge in these large, self- organized and distributed networks.
We address this challenge by studying VANETs from a network science perspective
to develop solutions that act locally but influence the network performance
globally. More specifically, we look at snapshots from highway and urban VANETs
of different sizes and vehicle densities, and study parameters such as the node
degree distribution, the clustering coefficient and the average shortest path
length, in order to better understand the networks' structure and compare it to
structures commonly found in large real world networks such as small-world and
scale-free networks. We then show how to use this information to improve
existing VANET protocols. As an illustrative example, it is shown that, by
adding new mechanisms that make use of this information, the overhead of the
urban vehicular broadcasting (UV-CAST) protocol can be reduced substantially
with no significant performance degradation.

ABSTRACT_BEGIN
  Cognitive radio (CR) is found to be an emerging key for efficient spectrum
utilization. In this paper, spectrum sharing among service providers with the
help of cognitive radio has been investigated. The technique of spectrum
sharing among service providers to share the licensed spectrum of licensed
service providers in a dynamic manner is considered. The performance of the
wireless network with opportunistic spectrum sharing techniques is analyzed.
Thus, the spectral utilization and efficiency of sensing is increased, the
interference is minimized, and the call blockage is reduced.

ABSTRACT_BEGIN
  Our paper presents solutions that can significantly improve the delay
performance of putting and retrieving data in and out of cloud storage. We
first focus on measuring the delay performance of a very popular cloud storage
service Amazon S3. We establish that there is significant randomness in service
times for reading and writing small and medium size objects when assigned
distinct keys. We further demonstrate that using erasure coding, parallel
connections to storage cloud and limited chunking (i.e., dividing the object
into a few smaller objects) together pushes the envelope on service time
distributions significantly (e.g., 76%, 80%, and 85% reductions in mean, 90th,
and 99th percentiles for 2 Mbyte files) at the expense of additional storage
(e.g., 1.75x). However, chunking and erasure coding increase the load and hence
the queuing delays while reducing the supportable rate region in number of
requests per second per node. Thus, in the second part of our paper we focus on
analyzing the delay performance when chunking, FEC, and parallel connections
are used together. Based on this analysis, we develop load adaptive algorithms
that can pick the best code rate on a per request basis by using off-line
computed queue backlog thresholds. The solutions work with homogeneous services
with fixed object sizes, chunk sizes, operation type (e.g., read or write) as
well as heterogeneous services with mixture of object sizes, chunk sizes, and
operation types. We also present a simple greedy solution that
opportunistically uses idle connections and picks the erasure coding rate
accordingly on the fly. Both backlog and greedy solutions support the full rate
region and provide best mean delay performance when compared to the best fixed
coding rate policy. Our evaluations show that backlog based solutions achieve
better delay performance at higher percentile values than the greedy solution.

ABSTRACT_BEGIN
  In this paper we investigate the performance of mobile user connectivity in
femtocell/macrocell networks. The femto user equipment (FUE) can connect to
femto access point (FAP) with low communication range rather than higher
communication range to macro base station (MBS). Furthermore, in such emerging
networks, the spatial reuse of resources is permissible and the transmission
range can be decreased, then the probability of connectivity is high. Thereby
in this study, we propose a tractable analytical model for the connectivity
probability based on communication range and the mobility of mobile users in
femtocell/macrocell networks. Further, we study the interplays between outage
probability and spectral efficiency in such networks. Numerical results
demonstrate the effectiveness of computing the connectivity probability in
femtocell/macrocell networks.

ABSTRACT_BEGIN
  A variety of mathematical tools have been developed for predicting the
spreading patterns in a number of varied environments including infectious
diseases, computer viruses, and urgent messages broadcast to mobile agent
(e.g., humans, vehicles, and mobile devices). These tools have mainly focused
on estimating the average time for the spread to reach a fraction (e.g.,
$\alpha$) of the agents, i.e., the so-called average completion time
$E(T_{\alpha})$. We claim that providing probabilistic guarantee on the time
for the spread $T_{\alpha}$ rather than only its average gives a much better
understanding of the spread, and hence could be used to design improved methods
to prevent epidemics or devise accelerated methods for distributing data. To
demonstrate the benefits, we introduce a new metric $G_{\alpha, \beta}$ that
denotes the time required to guarantee $\alpha$ completion with probability
$\beta$, and develop a new framework to characterize the distribution of
$T_\alpha$ for various spread parameters such as number of seeds, level of
contact rates, and heterogeneity in contact rates. We apply our technique to an
experimental mobility trace of taxies in Shanghai and show that our framework
enables us to allocate resources (i.e., to control spread parameters) for
acceleration of spread in a far more efficient way than the state-of-the-art.

ABSTRACT_BEGIN
  MillimeterWave communications in the 60 GHz band are considered one of the
key technologies for enabling multi-gigabit wireless access. However, the high
propagation loss in such a band poses major obstacles to the optimal
utilization of the wireless resources, where the problem of efficient client
association to access points (APs) is of vital importance. In this paper, the
client association in 60 GHz wireless access networks is investigated. The AP
utilization and the quality of the rapidly vanishing communication links are
the control parameters. Because of the tricky non-convex and combinatorial
nature of the client association optimization problem, a novel solution method
is developed to guarantee balanced and fair resource allocation. A new
distributed, lightweight and easy to implement association algorithm, based on
Lagrangian duality theory and subgradient methods, was proposed. It is shown
that the algorithm is asymptotically optimal, that is, the relative duality gap
diminishes to zero as the number of clients increases. Both theoretical and
numerical results evince numerous useful properties of the algorithm, such as
fast convergence, scalability, time efficiency, and fair execution in
comparison to existing approaches. It is concluded that the proposed solution
can be applied in the forthcoming 60 GHz wireless access networks.

ABSTRACT_BEGIN
  According to FCC's ruling for white-space spectrum access, white-space
devices are required to query a database to determine the spectrum
availability. In this paper, we study the database-assisted distributed
white-space access point (AP) network design. We first model the cooperative
and non-cooperative channel selection problems among the APs as the system-wide
throughput optimization and non-cooperative AP channel selection games,
respectively, and design distributed AP channel selection algorithms that
achieve system optimal point and Nash equilibrium, respectively. We then
propose a state-based game formulation for the distributed AP association
problem of the secondary users by taking the cost of mobility into account. We
show that the state-based distributed AP association game has the finite
improvement property, and design a distributed AP association algorithm that
can converge to a state-based Nash equilibrium. Numerical results show that the
algorithm is robust to the perturbation by secondary users' dynamical leaving
and entering the system.

ABSTRACT_BEGIN
  We consider the problem of distributed admission control without knowledge of
the capacity region in single-hop wireless networks, for flows that require a
pre-specified bandwidth from the network. We present an optimization framework
that allows us to design a scheduler and resource allocator, and by properly
choosing a suitable utility function in the resource allocator, we prove that
existing flows can be served with a pre-specified bandwidth, while the link
requesting admission can determine the largest rate that it can get such that
it does not interfere with the allocation to the existing flows.

ABSTRACT_BEGIN
  We study a problem of scheduling real-time traffic with hard delay
constraints in an unreliable wireless channel. Packets arrive at a constant
rate to the network and have to be delivered within a fixed number of slots in
a fading wireless channel. For an infrastructure mode of traffic with a
centralized scheduler, we are interested in the long time average throughput
achievable for the real time traffic. In [1], the authors have stud- ied the
feasible throughput vectors by identifying the necessary and sufficient
conditions using work load characterization. In our work, we provide a
characterization of the feasible throughput vectors using the notion of the
rate region. We then discuss an extension to the network model studied in [1]
by allowing multiple access during contention and propose an enhancement to the
rate region of the wireless network. We characterize the feasible throughput
vectors with the multiple access technique and study throughput optimal and
utility maximizing strategies for the network scenario. Using simulations, we
evaluate the performance of the proposed strategy and discuss its advantages.

ABSTRACT_BEGIN
  We study the problem of optimal sequential ("as-you-go") deployment of
wireless relay nodes as a person walks along a line of random length (with a
known distribution). The objective is to create an impromptu multihop wireless
network for connecting a packet source to be placed at the end of the line with
a sink node located at the starting point, to operate in the light traffic
regime. As the deployment person walks along the line from the sink towards the
source, at every step, he measures the channel quality to one (or more)
previously placed relays, and places the relay nodes based on these
measurements, so as to minimize either the sum power or the maximum power from
the source to the sink node in the resultant network, subject to a constraint
on the expected number of relays placed. For each of these two objectives, two
different relay selection strategies are considered: (i) each relay
communicates with the sink via its immediate previous relay, (ii) the
communication path can skip some of the deployed relays. With appropriate
modeling assumptions, we formulate each of these problems as a Markov decision
process (MDP). We provide the optimal policy structures for all these cases,
and provide illustrations, via numerical results, for some typical parameters.

ABSTRACT_BEGIN
  In this paper a packet level simulator is used to explore the performance of
the proposed DLMT and CLMT algorithms under various traffic conditions.
Performance of the proposed algorithms is compared with already existing E-Span
tree structure. These proposed algorithms tend to extend the node lifetime in
order to increase the amount of information gathered by the tree root.
Decentralized lifetime maximizing tree (DLMT) features in nodes with higher
energy to be chosen as data aggregating parents while Centralized Lifetime
Maximizing Tree (CLMT) features with the identification of the bottleneck node
to collect data in a central manner among given set of nodes. By choosing
Forwarded Diffusion as our underlying routing platform the simulations are
carried on J-Sim. Our simulation results have shown that the functional
lifetime of event sources can be enhanced by a maximum of 147% when data is
aggregated via DLMT and by 139% when data is aggregated via CLMT. Our proposed
DLMT algorithm has shown maximum of 13% additional lifetime saving without
increasing the delay. Packet delivery ratio has also shown a remarkable
increase when the tree depth is considered in these proposed tree structures.
Furthermore, the delay is also reduced by using DLMT & CLMT in comparison with
E-Span.

ABSTRACT_BEGIN
  Networking in Wireless Sensor networks is a challenging task due to the lack
of resources in the network as well as the frequent changes in network
topology. Although lots of research has been done on supporting QoS in the
Internet and other networks, but they are not suitable for wireless sensor
networks and still QoS support for such networks remains an open problem. In
this paper, a new scheme has been proposed for achieving QoS in terms of packet
delivery, multiple connections, better power management and stable routes in
case of failure. It offers quick adaptation to distributed processing, dynamic
linking, low processing overhead and loop freedom at all times. The proposed
scheme has been incorporated using QDPRA protocol and by extensive simulation
the performance has been studied, and it is clearly shown that the proposed
scheme performs very well for different network scenarios.

ABSTRACT_BEGIN
  We study the reliability maximization problem in WDM networks with random
link failures. Reliability in these networks is defined as the probability that
the logical network is connected, and it is determined by the underlying
lightpath routing, network topologies and the link failure probability. By
introducing the notion of lexicographical ordering for lightpath routings, we
characterize precise optimization criteria for maximum reliability in the low
failure probability regime. Based on the optimization criteria, we develop
lightpath routing algorithms that maximize the reliability, and logical
topology augmentation algorithms for further improving reliability. We also
study the reliability maximization problem in the high failure probability
regime.

ABSTRACT_BEGIN
  In Machine to Machine (M2M) networks, a robust Medium Access Control (MAC)
protocol is crucial to enable numerous machine-type devices to concurrently
access the channel. Most literatures focus on developing simplex (reservation
or contention based)MAC protocols which cannot provide a scalable solution for
M2M networks with large number of devices. In this paper, a frame-based Hybrid
MAC scheme, which consists of a contention period and a transmission period, is
proposed for M2M networks. In the proposed scheme, the devices firstly contend
the transmission opportunities during the contention period, only the
successful devices will be assigned a time slot for transmission during the
transmission period. To balance the tradeoff between the contention and
transmission period in each frame, an optimization problem is formulated to
maximize the system throughput by finding the optimal contending probability
during contention period and optimal number of devices that can transmit during
transmission period. A practical hybrid MAC protocol is designed to implement
the proposed scheme. The analytical and simulation results demonstrate the
effectiveness of the proposed Hybrid MAC protocol.

ABSTRACT_BEGIN
  To meet the demands of wireless sensor networks (WSNs) where data are usually
aggregated at a single source prior to transmitting to any distant user, there
is a need to establish a tree structure inside any given event region. In this
paper, we propose a novel technique to create one such tree, which preserves
the energy and minimizes the lifetime of event sources while they are
constantly transmitting for data aggregation in future WSNs. We use the term
Decentralized Lifetime-Minimizing Tree (DLMT) to denote this tree. DLMT
features in nodes with higher energy tend to be chosen as data aggregating
parents so that the time to detect the first broken tree link can be extended
and less energy is involved in tree maintenance. In addition, by constructing
the tree in such a way, the protocol is also able to reduce the frequency of
tree reconstruction, minimizes the amount of data loss, minimizes the delay
during data collection and preserves the energy. Forwarded directed Diffusion
protocol is chosen as the routing platform.

ABSTRACT_BEGIN
  In this paper we propose a new scheduling algorithm called Real Time
Scheduling (RTS) which uses virtual nodes for self stabilization. This
algorithm deals with all the contributing components of the end-to-end
travelling delay of data packets in sensor network and with virtual nodes
algorithm achieves QoS in terms of packet delivery, multiple connections,
better power management and stable routes in case of failure. RTS delays
packets at intermediate hops (not just prioritizes them) for a duration that is
a function of their deadline. Delaying packets allows the network to avoid hot
spotting while maintaining deadline-faithfulness. We compare RTS with another
prioritizing and scheduling algorithm for real-time data dissemination in
sensor networks, velocity monotonic scheduling. This paper simulates RTS based
on two typical routing protocols, shortest path routing and greedy forwarding
with J-Sim.

ABSTRACT_BEGIN
  We present an NS-2 module, Physical Channel Access (PCA), to simulate
different access methods on a link shared with Multi-Frequency Time Division
Multiple Access (MF-TDMA). This tech- nique is widely used in various network
technologies, such as satellite communication. In this context, different
access methods at the gateway induce different queuing delays and available
capacities, which strongly impact transport layer performance. Depending on QoS
requirements, design of new congestion and flow control mechanisms and/or
access methods requires evaluation through simulations.
  PCA module emulates the delays that packets will experience using the shared
link, based on descriptive parameters of lower layers characteris- tics. Though
PCA has been developed with DVB-RCS2 considerations in mind (for which we
present a use case), other MF-TDMA-based appli- cations can easily be simulated
by adapting input parameters. Moreover, the presented implementation details
highlight the main methods that might need modifications to implement more
specific functionality or emulate other similar access methods (e.g., OFDMA).

ABSTRACT_BEGIN
  Sensor networks aim at monitoring their surroundings for event detection and
object tracking. But, due to failure, or death of sensors, false signal can be
transmitted. In this paper, we consider the problems of distributed fault
detection in wireless sensor network (WSN). In particular, we consider how to
take decision regarding fault detection in a noisy environment as a result of
false detection or false response of event by some sensors, where the sensors
are placed at the center of regular hexagons and the event can occur at only
one hexagon. We propose fault detection schemes that explicitly introduce the
error probabilities into the optimal event detection process. We introduce two
types of detection probabilities, one for the center node, where the event
occurs and the other one for the adjacent nodes. This second type of detection
probability is new in sensor network literature. We develop schemes under the
model selection procedure, multiple model selection procedure and use the
concept of Bayesian model averaging to identify a set of likely fault sensors
and obtain an average predictive error.

ABSTRACT_BEGIN
  This work presents, to the best of our knowledge of the literature, the first
analytic model to address the performance of an LRU (Least Recently Used)
implementing cache under non-stationary traffic conditions, i.e., when the
popularity of content evolves with time. We validate the accuracy of the model
using Monte Carlo simulations. We show that the model is capable of accurately
estimating the cache hit probability, when the popularity of content is
non-stationary.
  We find that there exists a dependency between the performance of an LRU
implementing cache and i) the lifetime of content in a system, ii) the volume
of requests associated with it, iii) the distribution of content request
volumes and iv) the shape of the popularity profile over time.

ABSTRACT_BEGIN
  To enable data aggregation among the event sources in wireless sensor
networks and to reduce the communication cost there is a need to establish a
coveraged tree structure inside any given event region to allow data reports to
be aggregated at a single processing point prior to transmission to the
network. In this paper we propose a novel technique to create one such tree
which maximizes the lifetime of the event sources while they are constantly
transmitting for data aggregation. We use the term Centralized Lifetime
Maximizing Tree (CLMT) to denote this tree. CLMT features with identification
of bottleneck node among the given set of nodes. This node collects the data
from every other node via routes with the highest branch energy subject to
condition loop is not created. By constructing tree in such a way, protocol is
able to reduce the frequency of tree reconstruction, minimize the delay and
maximize the functional lifetime of source nodes by minimizing the additional
energy involved in tree reconstruction.

ABSTRACT_BEGIN
  Broadcasting systems such as P2P streaming systems represent important
network applications that support up to millions of online users. An efficient
broadcasting mechanism is at the core of the system design. Despite substantial
efforts on developing efficient broadcasting algorithms, the following
important question remains open: How to achieve the maximum broadcast rate in a
distributed manner with each user maintaining information queues only for its
direct neighbors? In this work, we first derive an innovative formulation of
the problem over acyclic overlay networks with arbitrary underlay capacity
constraints. Then, based on the formulation, we develop a distributed algorithm
to achieve the maximum broadcast rate and every user only maintains one queue
per-neighbor. Due to its lightweight nature, our algorithm scales very well
with the network size and remains robust against high system dynamics. Finally,
by conducting simulations we validate the optimality of our algorithm under
different network capacity models. Simulation results further indicate that the
convergence time of our algorithm grows linearly with the network size, which
suggests an interesting direction for future investigation.

ABSTRACT_BEGIN
  The efficiency of wireless technology depends upon the seamless connectivity
to the user at anywhere any time.Heterogeneous wireless networks are an
integration of different networks with diversified technologies. The most
essential requirement for Seamless vertical handover is that the received
signal strength should always be healthy. Mobile device enabled with multiple
wireless technologies makes it possible to maintain seamless connectivity in
highly dynamic environment.Since the available bandwidth is limited and the
number of users is growing rapidly, it is a real challenge to maintain the
received signal strength in a healthy stage.In this work, the proposed, cost
effective parametric estimation for vertical handover shows that the received
signal strength maintains a healthy level by considering the novel concept.

ABSTRACT_BEGIN
  This paper presents a new Compressive Sensing (CS) scheme for detecting
network congested links. We focus on decreasing the required number of
measurements to detect all congested links in the context of network
tomography. We have expanded the LASSO objective function by adding a new term
corresponding to the prior knowledge based on the relationship between the
congested links and the corresponding link Betweenness Centrality (BC). The
accuracy of the proposed model is verified by simulations on two real datasets.
The results demonstrate that our model outperformed the state-of-the-art CS
based method with significant improvements in terms of F-Score.

ABSTRACT_BEGIN
  In this research paper, the problems dealing with sensor network
architecture, sensor fusion are addressed. Time/Computationally optimal network
architectures are investigated. Some novel ideas on sensor fusion are proposed.

ABSTRACT_BEGIN
  Today's increasing demand for wirelessly uploading a large volume of User
Generated Content (UGC) is still significantly limited by the throttled
backhaul of residential broadband (typically between 1 and 3Mbps). We propose
BaPu, a carefully designed system with implementation for bunching WiFi access
points' backhaul to achieve a high aggregated throughput. BaPu is inspired by a
decade of networking design principles and techniques to enable efficient TCP
over wireless links and multipath. BaPu aims to achieve two major goals:1)
requires no client modification for easy incremental adoption; 2) supports not
only UDP, but also TCP traffic to greatly extend its applicability to a broad
class of popular applications such as HD streaming or large file transfer. We
prototyped BaPu with commodity hardware. Our extensive experiments shows that
despite TCP's sensitivity to typical channel factors such as high wireless
packet loss, out-of-order packets arrivals due to multipath, heterogeneous
backhaul capacity, and dynamic delays, BaPu achieves a backhaul aggregation up
to 95% of the theoretical maximum throughput for UDP and 88% for TCP. We also
empirically estimate the potential idle bandwidth that can be harnessed from
residential broadband.

ABSTRACT_BEGIN
  This paper analyses the performance of TCP over random and dedicated access
methods in the context of DVB-RCS2. Random access methods introduce a lower
connection delay compared to dedicated methods. We investigate the poten- tial
to improve the performance of short flows in regards to transmission delay,
over random access methods for DVB- RCS2 that is currently under development.
Our simulation experiments show that the transmission of the first ten IP
datagrams of each TCP flow can be 500 ms faster with ran- dom access than with
dedicated access making the former of interest to carry Internet traffic. Such
methods, however, are less efficient in regards to bandwidth usage than
dedicated access mecanisms and less reliable in overloaded network conditions.
Two aspects of channel usage optimization can be distinguished: reducing the
duration of ressource utiliza- tion with random access methods, or increasing
the spec- trum efficiency with dedicated access methods. This article argues
that service providers may let low-cost users exploit the DVB-RCS2 to browse
the web by introducing different services, which choice is based on the channel
access method.

ABSTRACT_BEGIN
  Information Centric Networking (ICN) has been proposed as a new networking
paradigm in which the network provides users with content instead communication
channels between hosts. The Software Defined Networking (SDN) approach promises
to be a solution to enable the continuous evolution of networking
architectures. In this paper we propose and discuss solutions to support ICN
using SDN concepts. We focus on an ICN framework called CONET, which grounds
its roots in the CCN/NDN architecture. We face the problem in two complementary
ways. First we discuss a general and long term solution based on SDN concepts
without taking into account specific limitations of SDN standards and
equipment. Then we focus on an experiment to support ICN functionality over a
large scale SDN testbed based on OpenFlow, developed in the context of the
OFELIA European research project. The current OFELIA testbed is based on
OpenFlow 1.0 equipment from a variety of vendors, therefore we had to design
the experiment taking into account the features that are currently available on
off-the-shelf OpenFlow equipment.

ABSTRACT_BEGIN
  In this Letter, a discrete-time Markov Chain model is developed to study
performance of IEEE 802.15.4 under low duty cycle. The performance is measured
in terms of aggregate throughput and average power consumption per packet. The
proposed analytical model is verified through ns2 simulations.

ABSTRACT_BEGIN
  Advances in the price, performance, and power consumption of Wi-Fi (IEEE
802.11) technology have led to the adoption of wireless functionality in
diverse consumer electronics. These trends have enabled an exciting vision of
rich wireless applications that combine the unique features of different
devices for a better user experience. To meet the needs of these applications,
a wireless network must be configured well to provide good performance at the
physical layer. But because of wireless technology and usage trends, finding
these configurations is an increasingly challenging problem.
  Wireless configuration objectives range from simply choosing the fastest way
to encode data on a single wireless link to the global optimization of many
interacting parameters over multiple sets of communicating devices. As more
links are involved, as technology advances (e.g., the adoption of OFDM and MIMO
techniques in Wi-Fi), and as devices are used in changing wireless channels,
the size of the configuration space grows. Thus algorithms must find good
operating points among a growing number of options.
  ... continued inside thesis.

ABSTRACT_BEGIN
  In an attempt to utilize spectrum resources more efficiently, protocols
sharing licensed spectrum with unlicensed users are receiving increased
attention. From the perspective of cellular networks, spectrum underutilization
makes spatial reuse a feasible complement to existing standards. Interference
management is a major component in designing these schemes as it is critical
that licensed users maintain their expected quality of service. We develop a
distributed dynamic spectrum protocol in which ad-hoc device-to-device users
opportunistically access the spectrum actively in use by cellular users.
  First, channel gain estimates are used to set feasible transmit powers for
device-to-device users that keeps the interference they cause within the
allowed interference temperature. Then network information is distributed by
route discovery packets in a random access manner to help establish either a
single-hop or multi-hop route between two device-to-device users. We show that
network information in the discovery packet can decrease the failure rate of
the route discovery and reduce the number of necessary transmissions to find a
route. Using the found route, we show that two device-to-device users can
communicate with a low probability of outage while only minimally affecting the
cellular network, and can achieve significant power savings when communicating
directly with each other instead of utilizing the cellular base station.

ABSTRACT_BEGIN
  Broadcast is a fundamental operation in networks, especially in wireless
Mobile Ad Hoc NETworks (MANET). For example, some form of broadcasting is used
by all on-demand MANET routing protocols, when there is uncertainty as to the
location of the destination node, or for service discovery. Being such a basic
operation of the networking protocols, the importance of efficient broadcasting
has long been recognized by the networking community. Numerous papers proposed
increasingly more efficient implementation of broadcasting, while other studies
presented bounds on broadcast performance. In this work, we present a new
approach to efficient broadcast in networks with dynamic topologies, such as
MANET, and we introduce a new broadcasting algorithm for such networking
environments. We evaluate our algorithm, showing that its performance comes
remarkably close to the corresponding theoretical performance bounds, even in
the presence of packet loss due to, for example, MAC-layer collisions.
Furthermore, we compare the performance of the proposed algorithm with other
recently proposed schemes, including in various mobility settings.

ABSTRACT_BEGIN
  Vehicular Ad hoc Networks (VANET) is one of the most challenging research
areas in the field of Mobile Ad Hoc Networks. In this research, we propose a
new mechanism for increasing network visibility, by taking the information
gained from periodic safety messages (beacons), and inserting it into a
'neighbor' table. The table will be propagated to all neighbors giving a wider
vision for each vehicle belonging to the network. It will also decrease the
risk of collision at road junctions as each vehicle will have prior knowledge
oncoming vehicles before reaching the junction.

